table 2 shows comparison of different position features. f1 of plain text pf f1 is 83.21. f1 of tpf1 f1 is 83.99. f1 of tpf2 f1 is 83.90.
table 3 shows pearson correlation values between human and model preferences for each construction and the verb-bias score; training on raw frequencies and 2 constructions. correlation of do - is 0.06*. correlation of do level 1 is 0.23. correlation of do level 2 is 0.25. correlation of pd - is 0.33. correlation of pd level 1 is 0.38. correlation of pd level 2 is 0.32. correlation of do-pd - is 0.39. correlation of do-pd level 1 is 0.53. correlation of do-pd level 2 is 0.59.
table 4 shows comparison between rationale models (middle and bottom rows) and the baselines using full title or body (top row). map (dev) of full title map (dev) is 56.5. map (test) of full title map (test) is 60.0. % words of full title % words is 10.1. map (dev) of full body map (dev) is 54.2. map (test) of full body map (test) is 53.0. % words of full body % words is 89.9. map (dev) of independent map (dev) is 55.7. map (test) of independent map (test) is 53.6. % words of independent % words is 9.7. map (dev) of independent map (dev) is 56.3. map (test) of independent map (test) is 52.6. % words of independent % words is 19.7. map (dev) of dependent map (dev) is 56.1. map (test) of dependent map (test) is 54.6. % words of dependent % words is 11.6. map (dev) of dependent map (dev) is 56.5. map (test) of dependent map (test) is 55.6. % words of dependent % words is 32.8.
table 2 shows spearman’s rank correlation results on the scws dataset correlation of avgsim ρ × 100 is 62.8. correlation of avgsimc ρ × 100 is 65.7. correlation of avgsim ρ × 100 is 66.2. correlation of avgsimc ρ × 100 is 68.9. correlation of avgsim ρ × 100 is 67.2. correlation of avgsimc ρ × 100 is 69.2. correlation of - ρ × 100 is 69.7. correlation of model_m ρ × 100 is 63.6. correlation of model_w ρ × 100 is 65.4. correlation of avgsimc ρ × 100 is 61.2. correlation of hardsim ρ × 100 is 64.3. correlation of softsim ρ × 100 is 65.6. correlation of hardsim ρ × 100 is 64.9. correlation of softsim ρ × 100 is 66.1.
table 4 shows examples of attention weights in different hops for aspect level sentiment classification. weights of great service is 0.20. weights of great food is 0.22. weights of great service is 0.15. weights of great food is 0.12. weights of great service is 0.14. weights of great food is 0.14. weights of great service is 0.13. weights of great food is 0.12. weights of great service is 0.23. weights of great food is 0.20. weights of food service is 0.11. weights of food food is 0.21. weights of food service is 0.07. weights of food food is 0.11. weights of food service is 0.08. weights of food food is 0.10. weights of food service is 0.12. weights of food food is 0.11. weights of food service is 0.06. weights of food food is 0.12. weights of but service is 0.20. weights of but food is 0.03. weights of but service is 0.10. weights of but food is 0.11. weights of but service is 0.10. weights of but food is 0.08. weights of but service is 0.12. weights of but food is 0.11. weights of but service is 0.13. weights of but food is 0.06. weights of the service is 0.03. weights of the food is 0.11. weights of the service is 0.07. weights of the food is 0.11. weights of the service is 0.08. weights of the food is 0.08. weights of the service is 0.12. weights of the food is 0.11. weights of the service is 0.06. weights of the food is 0.06. weights of was service is 0.08. weights of was food is 0.04. weights of was service is 0.07. weights of was food is 0.11. weights of was service is 0.08. weights of was food is 0.08. weights of was service is 0.12. weights of was food is 0.11. weights of was service is 0.06. weights of was food is 0.06. weights of dreadful service is 0.20. weights of dreadful food is 0.22. weights of dreadful service is 0.45. weights of dreadful food is 0.32. weights of dreadful service is 0.45. weights of dreadful food is 0.45. weights of dreadful service is 0.28. weights of dreadful food is 0.32. weights of dreadful service is 0.40. weights of dreadful food is 0.43. weights of ! service is 0.19. weights of ! food is 0.16. weights of ! service is 0.08. weights of ! food is 0.11. weights of ! service is 0.08. weights of ! food is 0.08. weights of ! service is 0.12. weights of ! food is 0.11. weights of ! service is 0.07. weights of ! food is 0.07.
table 2 shows overall results on the he set: bleu, computed against the original reference translation, and ter, computed with respect to the targeted post-edit (hter) and multiple postedits (mter). bleu of pbsy bleu is 25.3. hter of pbsy hter is 28.0. mter of pbsy mter is 21.8. bleu of hpb bleu is 24.6. hter of hpb hter is 29.9. mter of hpb mter is 23.4. bleu of spb bleu is 25.8. hter of spb hter is 29.0. mter of spb mter is 22.7. bleu of nmt bleu is 31.1*. hter of nmt hter is 21.1*. mter of nmt mter is 16.2*.
table 4 shows word reordering evaluation in terms of shift operations in hter calculation and of krs. #words of pbsy #words is 11517. #shifts of pbsy #shifts is 354. %shifts of pbsy %shifts is 3.1. krs of pbsy krs is 84.6. #words of hpb #words is 11417. #shifts of hpb #shifts is 415. %shifts of hpb %shifts is 3.6. krs of hpb krs is 84.3. #words of spb #words is 11420. #shifts of spb #shifts is 398. %shifts of spb %shifts is 3.5. krs of spb krs is 84.5. #words of nmt #words is 11284. #shifts of nmt #shifts is 173. %shifts of nmt %shifts is 1.5*. krs of nmt krs is 88.3*.
table 2 shows human evaluation results on the generated and true recipes. syntax of attention syntax is 4.47. ingridient use of attention ingredient use is 3.02. follows goal of attention follows goal is 3.47. syntax of encdec syntax is 4.58. ingridient use of encdec ingredient use is 3.29. follows goal of encdec follows goal is 3.61. syntax of nn syntax is 4.22. ingridient use of nn ingredient use is 3.02. follows goal of nn follows goal is 3.36. syntax of nn-swap syntax is 4.11. ingridient use of nn-swap ingredient use is 3.51. follows goal of nn-swap follows goal is 3.78. syntax of checklist syntax is 4.58. ingridient use of checklist ingredient use is 3.80. follows goal of checklist follows goal is 3.94. syntax of checklist+ syntax is 4.39. ingridient use of checklist+ ingredient use is 3.95. follows goal of checklist+ follows goal is 4.10. syntax of truth syntax is 4.39. ingridient use of truth ingredient use is 4.03. follows goal of truth follows goal is 4.34.
table 4 shows performance comparison with other state-of-the-art systems on rst-dt. s of joty et al. (2013) s is 82.7. n of joty et al. (2013) n is 68.4. r of joty et al. (2013) r is 55.7. s of ji and eisenstein. (2014) s is 82.1. n of ji and eisenstein. (2014) n is 71.1. r of ji and eisenstein. (2014) r is 61.6. s of feng and hirst. (2014) s is 85.7. n of feng and hirst. (2014) n is 71.0. r of feng and hirst. (2014) r is 58.2. s of li et al. (2014a) s is 84.0. n of li et al. (2014a) n is 70.8. r of li et al. (2014a) r is 58.6. s of li et al. (2014b) s is 83.4. n of li et al. (2014b) n is 73.8. r of li et al. (2014b) r is 57.8. s of heilman and sagae. (2015) s is 83.5. n of heilman and sagae. (2015) n is 68.1. r of heilman and sagae. (2015) r is 55.1. s of ours s is 85.8. n of ours n is 71.1. r of ours r is 58.9. s of human s is 88.7. n of human n is 77.7. r of human r is 65.8.
table 7 shows domain transfer results. f1 of train nw test nw msep is 58.5. f1 of train nw test nw supervised is 63.7. f1 of train df test nw msep is 55.1. f1 of train df test nw supervised is 54.8. f1 of train df test df msep is 57.9. f1 of train df test df supervised is 62.6. f1 of train nw test df msep is 52.8. f1 of train nw test df supervised is 52.3. f1 of train nw test nw msep is 73.2. f1 of train nw test nw supervised is 73.6. f1 of train df test nw msep is 71. f1 of train df test nw supervised is 70.1. f1 of train df test df msep is 68.6. f1 of train df test df supervised is 68.9. f1 of train nw test df msep is 67.9. f1 of train nw test df supervised is 67.
table 2 shows performance results for the bless and entailment datasets. accuracy of bless accuracy is 90.4%. accuracy of bless accuracy is 83.8%. accuracy of bless accuracy is 84.0%. accuracy of bless accuracy is 91.1%. accuracy of bless accuracy is 93.6%. accuracy of entail accuracy is 87.5%. accuracy of entail accuracy is 82.8%. accuracy of entail accuracy is 83.3%. accuracy of entail accuracy is 88.2%. accuracy of entail accuracy is 91.7%.
table 3 shows performance results for the general domain datasets when using one domain for training and another domain for testing. accuracy of entail accuracy is 83.7%. accuracy of entail accuracy is 76.5%. accuracy of entail accuracy is 77.1%. accuracy of entail accuracy is 85.8%. accuracy of entail accuracy is 89.4%. accuracy of bless accuracy is 87.1%. accuracy of bless accuracy is 78.0%. accuracy of bless accuracy is 78.9%. accuracy of bless accuracy is 87.1%. accuracy of bless accuracy is 90.6%.
table 5 shows performance on common coverage subsets of the datasets (men* and simlex*). similarity of text sl is 0.248. similarity of text men is 0.654. similarity of text sl is 0.248. similarity of text men is 0.654. similarity of text sl is 0.248. similarity of text men is 0.654. similarity of text sl is 0.248. similarity of text men is 0.654. similarity of text sl is 0.248. similarity of text men is 0.654. similarity of text sl is 0.248. similarity of text men is 0.654. similarity of visual sl is 0.406. similarity of visual men is 0.549. similarity of visual sl is 0.402. similarity of visual men is 0.552. similarity of visual sl is 0.420. similarity of visual men is 0.570. similarity of visual sl is 0.434. similarity of visual men is 0.579. similarity of visual sl is 0.430. similarity of visual men is 0.576. similarity of visual sl is 0.406. similarity of visual men is 0.560. similarity of mm sl is 0.366. similarity of mm men is 0.691. similarity of mm sl is 0.344. similarity of mm men is 0.693. similarity of mm sl is 0.366. similarity of mm men is 0.701. similarity of mm sl is 0.342. similarity of mm men is 0.699. similarity of mm sl is 0.378. similarity of mm men is 0.701. similarity of mm sl is 0.341. similarity of mm men is 0.693. similarity of visual sl is 0.431. similarity of visual men is 0.613. similarity of visual sl is 0.425. similarity of visual men is 0.601. similarity of visual sl is 0.410. similarity of visual men is 0.612. similarity of visual sl is 0.414. similarity of visual men is 0.603. similarity of visual sl is 0.400. similarity of visual men is 0.611. similarity of visual sl is 0.398. similarity of visual men is 0.569. similarity of mm sl is 0.384. similarity of mm men is 0.715. similarity of mm sl is 0.355. similarity of mm men is 0.708. similarity of mm sl is 0.374. similarity of mm men is 0.725. similarity of mm sl is 0.343. similarity of mm men is 0.712. similarity of mm sl is 0.363. similarity of mm men is 0.720. similarity of mm sl is 0.340. similarity of mm men is 0.705. similarity of visual sl is 0.382. similarity of visual men is 0.577. similarity of visual sl is 0.371. similarity of visual men is 0.544. similarity of visual sl is 0.378. similarity of visual men is 0.547. similarity of visual sl is 0.354. similarity of visual men is 0.518. similarity of visual sl is 0.378. similarity of visual men is 0.567. similarity of visual sl is 0.340. similarity of visual men is 0.511. similarity of mm sl is 0.372. similarity of mm men is 0.725. similarity of mm sl is 0.344. similarity of mm men is 0.712. similarity of mm sl is 0.367. similarity of mm men is 0.728. similarity of mm sl is 0.336. similarity of mm men is 0.716. similarity of mm sl is 0.370. similarity of mm men is 0.726. similarity of mm sl is 0.330. similarity of mm men is 0.711. similarity of visual sl is 0.316. similarity of visual men is 0.560. similarity of visual sl is 0.316. similarity of visual men is 0.560. similarity of visual sl is 0.347. similarity of visual men is 0.538. similarity of visual sl is 0.423. similarity of visual men is 0.600. similarity of visual sl is 0.412. similarity of visual men is 0.581. similarity of visual sl is 0.413. similarity of visual men is 0.574. similarity of mm sl is 0.348. similarity of mm men is 0.711. similarity of mm sl is 0.348. similarity of mm men is 0.711. similarity of mm sl is 0.364. similarity of mm men is 0.717. similarity of mm sl is 0.394. similarity of mm men is 0.729. similarity of mm sl is 0.418. similarity of mm men is 0.724. similarity of mm sl is 0.405. similarity of mm men is 0.721. similarity of visual sl is 0.037. similarity of visual men is 0.431. similarity of visual sl is 0.039. similarity of visual men is 0.347. similarity of visual sl is 0.104. similarity of visual men is 0.501. similarity of visual sl is 0.125. similarity of visual men is 0.438. similarity of visual sl is 0.188. similarity of visual men is 0.514. similarity of visual sl is 0.125. similarity of visual men is 0.460. similarity of mm sl is 0.179. similarity of mm men is 0.666. similarity of mm sl is 0.147. similarity of mm men is 0.651. similarity of mm sl is 0.224. similarity of mm men is 0.692. similarity of mm sl is 0.226. similarity of mm men is 0.683. similarity of mm sl is 0.268. similarity of mm men is 0.697. similarity of mm sl is 0.222. similarity of mm men is 0.688.
table 1 shows comparison of multimodal pooling methods. accuracy of element-wise sum accuracy is 56.50. accuracy of concatenation accuracy is 57.49. accuracy of concatenation + fc accuracy is 58.40. accuracy of concatenation + fc + fc accuracy is 57.10. accuracy of element-wise product accuracy is 58.57. accuracy of element-wise product + fc accuracy is 56.44. accuracy of element-wise product + fc + fc accuracy is 57.88. accuracy of mcb (2048 × 2048 → 16k) accuracy is 59.83. accuracy of full bilinear (128 × 128 → 16k) accuracy is 58.46. accuracy of mcb (128 × 128 → 4k) accuracy is 58.69. accuracy of element-wise product with vgg-19 accuracy is 55.97. accuracy of mcb (d = 16k) with vgg-19 accuracy is 57.05. accuracy of concatenation + fc with attention accuracy is 58.36. accuracy of mcb (d = 16k) with attention accuracy is 62.50.
table 1 shows overall synthetic data results. acc. (std) of b-wm acc. (std) is 75.47(3.05). # wins of b-wm # wins is 9/10. gener. of b-wm gener. is 10/10. acc. (std) of b-wm acc. (std) is 63.18 (1.32). # wins of b-wm # wins is 9/10. gener. of b-wm gener. is 10/10. acc. (std) of b-wm acc. (std) is 28.48 (1.9). # wins of b-wm # wins is 5/10. gener. of b-wm gener. is 10/10. acc. (std) of b-wmr acc. (std) is 75.96 (2.42). # wins of b-wmr # wins is 8/10. gener. of b-wmr gener. is 10/10. acc. (std) of b-wmr acc. (std) is 63.02 (2.49). # wins of b-wmr # wins is 9/10. gener. of b-wmr gener. is 10/10. acc. (std) of b-wmr acc. (std) is 24.31 (5.2). # wins of b-wmr # wins is 4/10. gener. of b-wmr gener. is 10/10. acc. (std) of a-wm acc. (std) is 74.18 (2.16). # wins of a-wm # wins is 7/10. gener. of a-wm gener. is 10/10. acc. (std) of a-wm acc. (std) is 61.65 (2.30). # wins of a-wm # wins is 9/10. gener. of a-wm gener. is 10/10. acc. (std) of a-wm acc. (std) is 30.45 (1.0). # wins of a-wm # wins is 6/10. gener. of a-wm gener. is 10/10. acc. (std) of a-wmr acc. (std) is 75.17 (3.07). # wins of a-wmr # wins is 7/10. gener. of a-wmr gener. is 10/10. acc. (std) of a-wmr acc. (std) is 61.02 (1.93). # wins of a-wmr # wins is 8/10. gener. of a-wmr gener. is 10/10. acc. (std) of a-wmr acc. (std) is 25.8 (3.18). # wins of a-wmr # wins is 2/10. gener. of a-wmr gener. is 10/10. acc. (std) of csp acc. (std) is 72.24 (3.45). # wins of csp # wins is na. gener. of csp gener. is na. acc. (std) of csp acc. (std) is 57.89 (2.85). # wins of csp # wins is na. gener. of csp gener. is na. acc. (std) of csp acc. (std) is 25.27(8.55). # wins of csp # wins is na. gener. of csp gener. is na.
table 2 shows the performance of cross-lingually similarized chinese dependency grammars with different configurations. similarity (%) of baseline similarity (%) is 34.2. dep. p (%) of baseline dep. p (%) is 84.5. ada. p (%) of baseline ada. p (%) is 84.5. bleu-4 (%) of baseline bleu-4 (%) is 24.6. similarity (%) of proj : fixed similarity (%) is 46.3. dep. p (%) of proj : fixed dep. p (%) is 54.1. ada. p (%) of proj : fixed ada. p (%) is 82.3. bleu-4 (%) of proj : fixed bleu-4 (%) is 25.8 (+1.2). similarity (%) of proj : proj similarity (%) is 63.2. dep. p (%) of proj : proj dep. p (%) is 72.2. ada. p (%) of proj : proj ada. p (%) is 84.6. bleu-4 (%) of proj : proj bleu-4 (%) is 26.1 (+1.5). similarity (%) of proj : nonproj similarity (%) is 64.3. dep. p (%) of proj : nonproj dep. p (%) is 74.6. ada. p (%) of proj : nonproj ada. p (%) is 84.7. bleu-4 (%) of proj : nonproj bleu-4 (%) is 26.2 (+1.6). similarity (%) of nonproj : fixed similarity (%) is 48.4. dep. p (%) of nonproj : fixed dep. p (%) is 56.1. ada. p (%) of nonproj : fixed ada. p (%) is 82.6. bleu-4 (%) of nonproj : fixed bleu-4 (%) is 20.1 (−4.5). similarity (%) of nonproj : proj similarity (%) is 63.6. dep. p (%) of nonproj : proj dep. p (%) is 71.4. ada. p (%) of nonproj : proj ada. p (%) is 84.4. bleu-4 (%) of nonproj : proj bleu-4 (%) is 22.9 (−1.7). similarity (%) of nonproj : nonproj similarity (%) is 64.1. dep. p (%) of nonproj : nonproj dep. p (%) is 73.9. ada. p (%) of nonproj : nonproj ada. p (%) is 84.9. bleu-4 (%) of nonproj : nonproj bleu-4 (%) is 20.7 (−3.9).
table 3 shows the performance of the cross-lingually similarized grammar on dependency tree-based translation, compared with related work. bleu of (liu et al. 2006) nist 04 is 34.55. bleu of (liu et al. 2006) nist 05 is 31.94. bleu of (chiang 2007) nist 04 is 35.29. bleu of (chiang 2007) nist 05 is 33.22. bleu of (xie et al. 2011) nist 04 is 35.82. bleu of (xie et al. 2011) nist 05 is 33.62. bleu of original grammar nist 04 is 35.44. bleu of original grammar nist 05 is 33.08. bleu of similarized grammar nist 04 is 36.78. bleu of similarized grammar nist 05 is 35.12.
table 1 shows bleu scores on the nist chinese-english translation task. bleu of moses mt05 is 33.68. bleu of moses mt02 is 34.19. bleu of moses mt03 is 34.39. bleu of moses mt04 is 35.34. bleu of moses mt06 is 29.20. bleu of moses mt08 is 22.94. bleu of moses avg is 31.21. bleu of groundhog mt05 is 31.38. bleu of groundhog mt02 is 33.32. bleu of groundhog mt03 is 32.59. bleu of groundhog mt04 is 35.05. bleu of groundhog mt06 is 29.80. bleu of groundhog mt08 is 22.82. bleu of groundhog avg is 30.72. bleu of vnmt w/o kl mt05 is 31.40. bleu of vnmt w/o kl mt02 is 33.50. bleu of vnmt w/o kl mt03 is 32.92. bleu of vnmt w/o kl mt04 is 34.95. bleu of vnmt w/o kl mt06 is 28.74. bleu of vnmt w/o kl mt08 is 22.07. bleu of vnmt w/o kl avg is 30.44. bleu of vnmt mt05 is 32.25. bleu of vnmt mt02 is 34.50++. bleu of vnmt mt03 is 33.78++. bleu of vnmt mt04 is 36.72⇑++. bleu of vnmt mt06 is 30.92⇑++. bleu of vnmt mt08 is 24.41↑++. bleu of vnmt avg is 32.07.
table 1 shows alignment quality results for ibm2-hmm (2h) and its convex relaxation (2hc) using either hmm-style dynamic programming or “joint” decoding. aer of 1 aer is 0.0956. f-measure of 1 f-measure is 0.7829. aer of 1 aer is 0.1076. f-measure of 1 f-measure is 0.7797. aer of 1 aer is 0.1538. f-measure of 1 f-measure is 0.7199. aer of 1 aer is 0.1814. f-measure of 1 f-measure is 0.6914. aer of 1 aer is 0.5406. f-measure of 1 f-measure is 0.2951. aer of 1 aer is 0.1761. f-measure of 1 f-measure is 0.7219. aer of 2 aer is 0.0884. f-measure of 2 f-measure is 0.7854. aer of 2 aer is 0.0943. f-measure of 2 f-measure is 0.7805. aer of 2 aer is 0.1093. f-measure of 2 f-measure is 0.7594. aer of 2 aer is 0.1343. f-measure of 2 f-measure is 0.733. aer of 2 aer is 0.1625. f-measure of 2 f-measure is 0.7111. aer of 2 aer is 0.0873. f-measure of 2 f-measure is 0.8039. aer of 3 aer is 0.0844. f-measure of 3 f-measure is 0.7899. aer of 3 aer is 0.0916. f-measure of 3 f-measure is 0.7806. aer of 3 aer is 0.1023. f-measure of 3 f-measure is 0.7651. aer of 3 aer is 0.1234. f-measure of 3 f-measure is 0.7427. aer of 3 aer is 0.1254. f-measure of 3 f-measure is 0.7484. aer of 3 aer is 0.0786. f-measure of 3 f-measure is 0.8112. aer of 4 aer is 0.0828. f-measure of 4 f-measure is 0.7908. aer of 4 aer is 0.0904. f-measure of 4 f-measure is 0.7813. aer of 4 aer is 0.0996. f-measure of 4 f-measure is 0.7668. aer of 4 aer is 0.1204. f-measure of 4 f-measure is 0.7457. aer of 4 aer is 0.1169. f-measure of 4 f-measure is 0.7589. aer of 4 aer is 0.0753. f-measure of 4 f-measure is 0.8094. aer of 5 aer is 0.0808. f-measure of 5 f-measure is 0.7928. aer of 5 aer is 0.0907. f-measure of 5 f-measure is 0.7806. aer of 5 aer is 0.0992. f-measure of 5 f-measure is 0.7673. aer of 5 aer is 0.1197. f-measure of 5 f-measure is 0.7461. aer of 5 aer is 0.1131. f-measure of 5 f-measure is 0.7624. aer of 5 aer is 0.0737. f-measure of 5 f-measure is 0.8058. aer of 6 aer is 0.0804. f-measure of 6 f-measure is 0.7928. aer of 6 aer is 0.0906. f-measure of 6 f-measure is 0.7807. aer of 6 aer is 0.0989. f-measure of 6 f-measure is 0.7678. aer of 6 aer is 0.1199. f-measure of 6 f-measure is 0.7457. aer of 6 aer is 0.1128. f-measure of 6 f-measure is 0.763. aer of 6 aer is 0.0719. f-measure of 6 f-measure is 0.8056. aer of 7 aer is 0.0795. f-measure of 7 f-measure is 0.7939. aer of 7 aer is 0.091. f-measure of 7 f-measure is 0.7817. aer of 7 aer is 0.0986. f-measure of 7 f-measure is 0.7679. aer of 7 aer is 0.1197. f-measure of 7 f-measure is 0.7457. aer of 7 aer is 0.1116. f-measure of 7 f-measure is 0.7633. aer of 7 aer is 0.0717. f-measure of 7 f-measure is 0.8046. aer of 8 aer is 0.0789. f-measure of 8 f-measure is 0.7942. aer of 8 aer is 0.09. f-measure of 8 f-measure is 0.7814. aer of 8 aer is 0.0988. f-measure of 8 f-measure is 0.7679. aer of 8 aer is 0.1195. f-measure of 8 f-measure is 0.7458. aer of 8 aer is 0.1086. f-measure of 8 f-measure is 0.7658. aer of 8 aer is 0.0725. f-measure of 8 f-measure is 0.8024. aer of 9 aer is 0.0793. f-measure of 9 f-measure is 0.7937. aer of 9 aer is 0.0904. f-measure of 9 f-measure is 0.7813. aer of 9 aer is 0.0986. f-measure of 9 f-measure is 0.768. aer of 9 aer is 0.1195. f-measure of 9 f-measure is 0.7457. aer of 9 aer is 0.1076. f-measure of 9 f-measure is 0.7672. aer of 9 aer is 0.0738. f-measure of 9 f-measure is 0.8007. aer of 10 aer is 0.0793. f-measure of 10 f-measure is 0.7927. aer of 10 aer is 0.0902. f-measure of 10 f-measure is 0.7816. aer of 10 aer is 0.0986. f-measure of 10 f-measure is 0.768. aer of 10 aer is 0.1195. f-measure of 10 f-measure is 0.7457. aer of 10 aer is 0.1072. f-measure of 10 f-measure is 0.7679. aer of 10 aer is 0.0734. f-measure of 10 f-measure is 0.801.
table 3 shows comparison of fleiss’ κ scores with scores from snli quality control sentence pairs. fleiss’κ of contradiction 4gs is 0.37. fleiss’κ of contradiction 5gs is 0.59. fleiss’κ of contradiction bowman et al. 2015 is 0.77. fleiss’κ of entailment 4gs is 0.48. fleiss’κ of entailment 5gs is 0.63. fleiss’κ of entailment bowman et al. 2015 is 0.72. fleiss’κ of neutral 4gs is 0.41. fleiss’κ of neutral 5gs is 0.54. fleiss’κ of neutral bowman et al. 2015 is 0.6. fleiss’κ of overall 4gs is 0.43. fleiss’κ of overall 5gs is 0.6. fleiss’κ of overall bowman et al. 2015 is 0.7.
table 5 shows theta scores and area under curve percentiles for lstm trained on snli and tested on gsirt . theta score of entailment theta score is -0.133. percentile of entailment percentile is 44.83%. test acc. of entailment test acc. is 96.5%. theta score of contradiction theta score is 1.539. percentile of contradiction percentile is 93.82%. test acc. of contradiction test acc. is 87.9%. theta score of neutral theta score is 0.423. percentile of neutral percentile is 66.28%. test acc. of neutral test acc. is 88%. theta score of contradiction theta score is 1.777. percentile of contradiction percentile is 96.25%. test acc. of contradiction test acc. is 78.9%. theta score of neutral theta score is 0.441. percentile of neutral percentile is 67%. test acc. of neutral test acc. is 83%.
table 2 shows performance of different rho functions on text8 dataset with 17m tokens. robi of similarity - is 41.2. ρ0 of similarity off is 69.0. ρ0 of similarity on is 71.0. ρ1 of similarity off is 66.7. ρ1 of similarity on is 70.4. ρ2 of similarity off is 66.8. ρ2 of similarity on is 70.8. ρ3 of similarity off is 68.1. ρ3 of similarity on is 68.0. robi of analogy - is 22.7. ρ0 of analogy off is 24.9. ρ0 of analogy on is 31.9. ρ1 of analogy off is 34.3. ρ1 of analogy on is 44.5. ρ2 of analogy off is 32.3. ρ2 of analogy on is 40.4. ρ3 of analogy off is 33.6. ρ3 of analogy on is 42.9.
table 3 shows comparison between our joint approaches and the pipelined counterparts. p of jamr (fixed) p is 0.67. r of jamr (fixed) r is 0.58. f1 of jamr (fixed) f1 is 0.62. p of system 1 p is 0.72. r of system 1 r is 0.65. f1 of system 1 f1 is 0.68. p of system 2 p is 0.73. r of system 2 r is 0.69. f1 of system 2 f1 is 0.71. p of jamr (fixed) p is 0.68. r of jamr (fixed) r is 0.59. f1 of jamr (fixed) f1 is 0.63. p of system 1 p is 0.74. r of system 1 r is 0.63. f1 of system 1 f1 is 0.68. p of system 2 p is 0.73. r of system 2 r is 0.68. f1 of system 2 f1 is 0.71.
table 4 shows final results of various methods. p of camr* p is .69. r of camr* r is .67. f1 of camr* f1 is .68. p of camr p is .71. r of camr r is .69. f1 of camr f1 is .70. p of our approach p is .73. r of our approach r is .69. f1 of our approach f1 is .71. p of camr* p is .70. r of camr* r is .66. f1 of camr* f1 is .68. p of camr p is .72. r of camr r is .67. f1 of camr f1 is .70. p of ccg-based p is .67. r of ccg-based r is .66. f1 of ccg-based f1 is .66. p of our approach p is .73. r of our approach r is .68. f1 of our approach f1 is .71.
table 5 shows final results on the full ldc2014t12 dataset. p of jamr (fixed) p is .64. r of jamr (fixed) r is .53. f1 of jamr (fixed) f1 is .58. p of camr* p is .68. r of camr* r is .60. f1 of camr* f1 is .64. p of camr p is .70. r of camr r is .62. f1 of camr f1 is .66. f1 of smbt-based f1 is .67. p of our approach p is .70. r of our approach r is .62. f1 of our approach f1 is .66.
table 2 shows per language uas for the fully supervised setup. uas of swedish turboparser is 87.12. uas of swedish bgi-pp is 86.35. uas of swedish bgi-pp+i+b is 86.93. uas of swedish bgi-pp+i+b+e is 87.12. uas of swedish turboparser is 88.65. uas of swedish bgi-pp is 86.14. uas of swedish bgi-pp+i+b is 87.85. uas of swedish bgi-pp+i+b+e is 89.29. uas of bulgarian turboparser is 90.66. uas of bulgarian bgi-pp is 90.22. uas of bulgarian bgi-pp+i+b is 90.42. uas of bulgarian bgi-pp+i+b+e is 90.66. uas of bulgarian turboparser is 92.43. uas of bulgarian bgi-pp is 89.73. uas of bulgarian bgi-pp+i+b is 91.50. uas of bulgarian bgi-pp+i+b+e is 92.58. uas of chinese turboparser is 84.88. uas of chinese bgi-pp is 83.89. uas of chinese bgi-pp+i+b is 84.17. uas of chinese bgi-pp+i+b+e is 84.17. uas of chinese turboparser is 86.53. uas of chinese bgi-pp is 81.33. uas of chinese bgi-pp+i+b is 85.18. uas of chinese bgi-pp+i+b+e is 86.59. uas of czech turboparser is 83.53. uas of czech bgi-pp is 83.46. uas of czech bgi-pp+i+b is 83.44. uas of czech bgi-pp+i+b+e is 83.44. uas of czech turboparser is 86.35. uas of czech bgi-pp is 84.91. uas of czech bgi-pp+i+b is 86.26. uas of czech bgi-pp+i+b+e is 87.50. uas of dutch turboparser is 88.48. uas of dutch bgi-pp is 88.56. uas of dutch bgi-pp+i+b is 88.43. uas of dutch bgi-pp+i+b+e is 88.43. uas of dutch turboparser is 91.30. uas of dutch bgi-pp is 89.64. uas of dutch bgi-pp+i+b is 90.49. uas of dutch bgi-pp+i+b+e is 91.34. uas of japanese turboparser is 93.03. uas of japanese bgi-pp is 93.18. uas of japanese bgi-pp+i+b is 93.27. uas of japanese bgi-pp+i+b+e is 93.27. uas of japanese turboparser is 93.83. uas of japanese bgi-pp is 93.78. uas of japanese bgi-pp+i+b is 94.01. uas of japanese bgi-pp+i+b+e is 94.01. uas of catalan turboparser is 88.94. uas of catalan bgi-pp is 88.50. uas of catalan bgi-pp+i+b is 88.67. uas of catalan bgi-pp+i+b+e is 88.93. uas of catalan turboparser is 92.25. uas of catalan bgi-pp is 89.3. uas of catalan bgi-pp+i+b is 90.46. uas of catalan bgi-pp+i+b+e is 92.24. uas of english turboparser is 87.18. uas of english bgi-pp is 86.94. uas of english bgi-pp+i+b is 86.84. uas of english bgi-pp+i+b+e is 87.18. uas of english turboparser is 90.70. uas of english bgi-pp is 86.52. uas of english bgi-pp+i+b is 88.24. uas of english bgi-pp+i+b+e is 90.66.
table 3 shows word relation results. mrr of a real is 0.03. mrr of a opt is 0.04. mrr of a sum is 0.05. mrr of a real is 0.02. mrr of a opt is 0.05. mrr of a sum is 0.05. mrr of a lamb is 0.06. mrr of a real is 0.03‡. mrr of a opt is 0.05†. mrr of a sum is 0.07. mrr of a real is 0.04†. mrr of a opt is 0.08. mrr of a sum is 0.08. mrr of a - is 0.09. mrr of n real is 0.15‡. mrr of n opt is 0.21‡. mrr of n sum is 0.24‡. mrr of n real is 0.18‡. mrr of n opt is 0.27‡. mrr of n sum is 0.26‡. mrr of n lamb is 0.30. mrr of n real is 0.17‡. mrr of n opt is 0.23‡. mrr of n sum is 0.26‡. mrr of n real is 0.20‡. mrr of n opt is 0.29‡. mrr of n sum is 0.28‡. mrr of n - is 0.32. mrr of v real is 0.07‡. mrr of v opt is 0.13‡. mrr of v sum is 0.16†. mrr of v real is 0.08‡. mrr of v opt is 0.14‡. mrr of v sum is 0.16‡. mrr of v lamb is 0.18. mrr of v real is 0.09‡. mrr of v opt is 0.15‡. mrr of v sum is 0.17‡. mrr of v real is 0.09‡. mrr of v opt is 0.17†. mrr of v sum is 0.18. mrr of v - is 0.20. mrr of all real is 0.12‡. mrr of all opt is 0.18‡. mrr of all sum is 0.20‡. mrr of all real is 0.14‡. mrr of all opt is 0.22‡. mrr of all sum is 0.21‡. mrr of all lamb is 0.25. mrr of a real is 0.14‡. mrr of a opt is 0.22‡. mrr of a sum is 0.25†. mrr of a real is 0.17‡. mrr of a opt is 0.26. mrr of a sum is 0.21‡. mrr of a lamb is 0.27. mrr of a real is 0.17‡. mrr of a opt is 0.25‡. mrr of a sum is 0.27‡. mrr of a real is 0.23‡. mrr of a opt is 0.33. mrr of a sum is 0.33. mrr of a - is 0.33. mrr of n real is 0.23‡. mrr of n opt is 0.35‡. mrr of n sum is 0.30‡. mrr of n real is 0.28‡. mrr of n opt is 0.35†. mrr of n sum is 0.33‡. mrr of n lamb is 0.36. mrr of n real is 0.24‡. mrr of n opt is 0.36‡. mrr of n sum is 0.31‡. mrr of n real is 0.28‡. mrr of n opt is 0.36. mrr of n sum is 0.35‡. mrr of n - is 0.37. mrr of v real is 0.11‡. mrr of v opt is 0.19‡. mrr of v sum is 0.18‡. mrr of v real is 0.11‡. mrr of v opt is 0.22. mrr of v sum is 0.18‡. mrr of v lamb is 0.23. mrr of v real is 0.13‡. mrr of v opt is 0.20‡. mrr of v sum is 0.21‡. mrr of v real is 0.13‡. mrr of v opt is 0.24‡. mrr of v sum is 0.23‡. mrr of v - is 0.26. mrr of all real is 0.21‡. mrr of all opt is 0.32‡. mrr of all sum is 0.28‡. mrr of all real is 0.24‡. mrr of all opt is 0.33†. mrr of all sum is 0.30‡. mrr of all lamb is 0.34. mrr of a real is 0.22‡. mrr of a opt is 0.25‡. mrr of a sum is 0.24‡. mrr of a real is 0.16‡. mrr of a opt is 0.26‡. mrr of a sum is 0.25‡. mrr of a lamb is 0.28. mrr of a real is 0.25‡. mrr of a opt is 0.28‡. mrr of a sum is 0.28‡. mrr of a real is 0.18‡. mrr of a opt is 0.29‡. mrr of a sum is 0.32. mrr of a - is 0.31. mrr of n real is 0.24‡. mrr of n opt is 0.27‡. mrr of n sum is 0.28‡. mrr of n real is 0.22‡. mrr of n opt is 0.30. mrr of n sum is 0.28‡. mrr of n lamb is 0.30. mrr of n real is 0.25‡. mrr of n opt is 0.28‡. mrr of n sum is 0.29‡. mrr of n real is 0.23‡. mrr of n opt is 0.31†. mrr of n sum is 0.31‡. mrr of n - is 0.32. mrr of v real is 0.29‡. mrr of v opt is 0.35‡. mrr of v sum is 0.37. mrr of v real is 0.17‡. mrr of v opt is 0.35. mrr of v sum is 0.24‡. mrr of v lamb is 0.37. mrr of v real is 0.33‡. mrr of v opt is 0.39‡. mrr of v sum is 0.42‡. mrr of v real is 0.21‡. mrr of v opt is 0.42†. mrr of v sum is 0.39‡. mrr of v - is 0.44. mrr of all real is 0.23‡. mrr of all opt is 0.26‡. mrr of all sum is 0.27‡. mrr of all real is 0.20‡. mrr of all opt is 0.28‡. mrr of all sum is 0.25‡. mrr of all lamb is 0.29. mrr of a real is 0.20‡. mrr of a opt is 0.23‡. mrr of a sum is 0.23‡. mrr of a real is 0.08‡. mrr of a opt is 0.21‡. mrr of a sum is 0.18‡. mrr of a lamb is 0.27. mrr of a real is 0.21‡. mrr of a opt is 0.25‡. mrr of a sum is 0.26‡. mrr of a real is 0.10‡. mrr of a opt is 0.26‡. mrr of a sum is 0.26‡. mrr of a - is 0.30. mrr of n real is 0.21‡. mrr of n opt is 0.25‡. mrr of n sum is 0.25‡. mrr of n real is 0.16‡. mrr of n opt is 0.25‡. mrr of n sum is 0.23‡. mrr of n lamb is 0.29. mrr of n real is 0.22‡. mrr of n opt is 0.26‡. mrr of n sum is 0.27‡. mrr of n real is 0.17‡. mrr of n opt is 0.27‡. mrr of n sum is 0.26‡. mrr of n - is 0.30. mrr of v real is 0.19‡. mrr of v opt is 0.35†. mrr of v sum is 0.36. mrr of v real is 0.11‡. mrr of v opt is 0.29‡. mrr of v sum is 0.19‡. mrr of v lamb is 0.38. mrr of v real is 0.22‡. mrr of v opt is 0.36‡. mrr of v sum is 0.36‡. mrr of v real is 0.16‡. mrr of v opt is 0.36‡. mrr of v sum is 0.33‡. mrr of v - is 0.42. mrr of all real is 0.20‡. mrr of all opt is 0.26‡. mrr of all sum is 0.26‡. mrr of all real is 0.14‡. mrr of all opt is 0.24‡. mrr of all sum is 0.21‡. mrr of all lamb is 0.30. mrr of a real is 0.02‡. mrr of a opt is 0.06‡. mrr of a sum is 0.06‡. mrr of a real is 0.05‡. mrr of a opt is 0.08. mrr of a sum is 0.08. mrr of a lamb is 0.09. mrr of a real is 0.04‡. mrr of a opt is 0.08‡. mrr of a sum is 0.08‡. mrr of a real is 0.06‡. mrr of a opt is 0.12. mrr of a sum is 0.11. mrr of a - is 0.12. mrr of n real is 0.01‡. mrr of n opt is 0.04‡. mrr of n sum is 0.05‡. mrr of n real is 0.03‡. mrr of n opt is 0.07. mrr of n sum is 0.06‡. mrr of n lamb is 0.07. mrr of n real is 0.01‡. mrr of n opt is 0.04‡. mrr of n sum is 0.05‡. mrr of n real is 0.04‡. mrr of n opt is 0.07†. mrr of n sum is 0.06‡. mrr of n - is 0.07. mrr of v real is 0.04‡. mrr of v opt is 0.11‡. mrr of v sum is 0.13‡. mrr of v real is 0.07‡. mrr of v opt is 0.14‡. mrr of v sum is 0.15. mrr of v lamb is 0.17. mrr of v real is 0.05‡. mrr of v opt is 0.13‡. mrr of v sum is 0.14‡. mrr of v real is 0.07‡. mrr of v opt is 0.15‡. mrr of v sum is 0.16†. mrr of v - is 0.19. mrr of all real is 0.02‡. mrr of all opt is 0.05‡. mrr of all sum is 0.06‡. mrr of all real is 0.04‡. mrr of all opt is 0.08‡. mrr of all sum is 0.07‡. mrr of all lamb is 0.09.
table 5 shows polarity classification results. f1 of brychcin et al. (2013) f1 is 81.53. acc of form acc is 80.86. f1 of form f1 is 80.75. acc of stem acc is 81.51. f1 of stem f1 is 81.39. acc of lamb acc is 81.21. f1 of lamb f1 is 81.09. f1 of hagen et al. (2015) f1 is 64.84. acc of form acc is 66.78. f1 of form f1 is 62.21. acc of stem acc is 66.95. f1 of stem f1 is 62.06. acc of lamb acc is 67.49. f1 of lamb f1 is 63.01.
table 2 shows pos tagging performance of online and offline pruning with different r and λ on ctb5 and pd. accuracy (%) of 0.98 ctb5-dev is 94.25. accuracy (%) of 0.98 pd-dev is 95.03. #tags (pruned) of 0.98 ctb-side is 2.0. #tags (pruned) of 0.98 pd-side is 2.0. accuracy (%) of 0.98 ctb5-dev is 95.06. accuracy (%) of 0.98 pd-dev is 95.66. #tags (pruned) of 0.98 ctb-side is 3.9. #tags (pruned) of 0.98 pd-side is 4.0. accuracy (%) of 0.98 ctb5-dev is 95.14. accuracy (%) of 0.98 pd-dev is 95.83. #tags (pruned) of 0.98 ctb-side is 6.3. #tags (pruned) of 0.98 pd-side is 7.4. accuracy (%) of 0.98 ctb5-dev is 95.12. accuracy (%) of 0.98 pd-dev is 95.81. #tags (pruned) of 0.98 ctb-side is 7.8. #tags (pruned) of 0.98 pd-side is 14.1. accuracy (%) of 0.90 ctb5-dev is 95.15. accuracy (%) of 0.90 pd-dev is 95.79. #tags (pruned) of 0.90 ctb-side is 3.7. #tags (pruned) of 0.90 pd-side is 6.3. accuracy (%) of 0.95 ctb5-dev is 95.13. accuracy (%) of 0.95 pd-dev is 95.82. #tags (pruned) of 0.95 ctb-side is 5.1. #tags (pruned) of 0.95 pd-side is 7.1. accuracy (%) of 0.99 ctb5-dev is 95.15. accuracy (%) of 0.99 pd-dev is 95.74. #tags (pruned) of 0.99 ctb-side is 7.4. #tags (pruned) of 0.99 pd-side is 7.9. accuracy (%) of 1.00 ctb5-dev is 95.15. accuracy (%) of 1.00 pd-dev is 95.76. #tags (pruned) of 1.00 ctb-side is 8.0. #tags (pruned) of 1.00 pd-side is 8.0. accuracy (%) of 0.9999 ctb5-dev is 94.95. accuracy (%) of 0.9999 pd-dev is 96.05. #tags (pruned) of 0.9999 ctb-side is 4.1. #tags (pruned) of 0.9999 pd-side is 5.1. accuracy (%) of 0.9999 ctb5-dev is 95.15. accuracy (%) of 0.9999 pd-dev is 96.09. #tags (pruned) of 0.9999 ctb-side is 5.2. #tags (pruned) of 0.9999 pd-side is 7.6. accuracy (%) of 0.9999 ctb5-dev is 95.13. accuracy (%) of 0.9999 pd-dev is 96.09. #tags (pruned) of 0.9999 ctb-side is 5.5. #tags (pruned) of 0.9999 pd-side is 9.3. accuracy (%) of 0.99 ctb5-dev is 94.42. accuracy (%) of 0.99 pd-dev is 95.77. #tags (pruned) of 0.99 ctb-side is 1.6. #tags (pruned) of 0.99 pd-side is 2.2. accuracy (%) of 0.999 ctb5-dev is 95.02. accuracy (%) of 0.999 pd-dev is 96.10. #tags (pruned) of 0.999 ctb-side is 2.6. #tags (pruned) of 0.999 pd-side is 4.0. accuracy (%) of 0.99999 ctb5-dev is 95.10. accuracy (%) of 0.99999 pd-dev is 96.09. #tags (pruned) of 0.99999 ctb-side is 6.8. #tags (pruned) of 0.99999 pd-side is 8.9.
table 3 shows pos tagging performance of difference approaches on ctb5 and pd. accuracy (%) of coupled (offline) ctb5-test is 94.83. accuracy (%) of coupled (offline) pd-test is 95.90. speed of coupled (offline) toks/sec is 246. accuracy (%) of coupled (online) ctb5-test is 94.74. accuracy (%) of coupled (online) pd-test is 95.95. speed of coupled (online) toks/sec is 365. accuracy (%) of coupled (no prune) ctb5-test is 94.58. accuracy (%) of coupled (no prune) pd-test is 95.79. speed of coupled (no prune) toks/sec is 3. accuracy (%) of coupled (relaxed) ctb5-test is 94.63. accuracy (%) of coupled (relaxed) pd-test is 95.87. speed of coupled (relaxed) toks/sec is 127. accuracy (%) of guide-feature ctb5-test is 94.35. accuracy (%) of guide-feature pd-test is 95.63. speed of guide-feature toks/sec is 584. accuracy (%) of baseline ctb5-test is 94.07. accuracy (%) of baseline pd-test is 95.82. speed of baseline toks/sec is 1573. accuracy (%) of li et al. (2012b) ctb5-test is 94.60. accuracy (%) of li et al. (2012b) pd-test is —. speed of li et al. (2012b) toks/sec is —.
table 4 shows ws&pos tagging performance of online and offline pruning with different r and λ on ctb5 and pd. accuracy (%) of 1.00 ctb5-dev is 90.41. accuracy (%) of 1.00 pd-dev is 89.91. #tags (pruned) of 1.00 ctb-side is 8.0. #tags (pruned) of 1.00 pd-side is 8.0. accuracy (%) of 0.95 ctb5-dev is 90.65. accuracy (%) of 0.95 pd-dev is 90.22. #tags (pruned) of 0.95 ctb-side is 15.9. #tags (pruned) of 0.95 pd-side is 16.0. accuracy (%) of 0.99 ctb5-dev is 90.77. accuracy (%) of 0.99 pd-dev is 90.49. #tags (pruned) of 0.99 ctb-side is 16.0. #tags (pruned) of 0.99 pd-side is 16.0. accuracy (%) of 1.00 ctb5-dev is 90.79. accuracy (%) of 1.00 pd-dev is 90.49. #tags (pruned) of 1.00 ctb-side is 16.0. #tags (pruned) of 1.00 pd-side is 16.0. accuracy (%) of 0.99 ctb5-dev is 91.64. accuracy (%) of 0.99 pd-dev is 91.92. #tags (pruned) of 0.99 ctb-side is 2.5. #tags (pruned) of 0.99 pd-side is 3.5.
table 5 shows ws&pos tagging performance of difference approaches on ctb5 and pd. f (%) on ctb5-test of coupled (offline) only ws is 95.55. f (%) on ctb5-test of coupled (offline) joint ws&pos is 90.58. f (%) on pd-test of coupled (offline) only ws is 96.12. f (%) on pd-test of coupled (offline) joint ws&pos is 92.44. speed (char/sec) of coupled (offline) - is 115. f (%) on ctb5-test of coupled (online) only ws is 94.94. f (%) on ctb5-test of coupled (online) joint ws&pos is 89.58. f (%) on pd-test of coupled (online) only ws is 95.60. f (%) on pd-test of coupled (online) joint ws&pos is 91.56. speed (char/sec) of coupled (online) - is 26. f (%) on ctb5-test of guide-feature only ws is 95.07. f (%) on ctb5-test of guide-feature joint ws&pos is 89.79. f (%) on pd-test of guide-feature only ws is 95.66. f (%) on pd-test of guide-feature joint ws&pos is 91.61. speed (char/sec) of guide-feature - is 27. f (%) on ctb5-test of baseline only ws is 94.88. f (%) on ctb5-test of baseline joint ws&pos is 89.49. f (%) on pd-test of baseline only ws is 96.28. f (%) on pd-test of baseline joint ws&pos is 92.47. speed (char/sec) of baseline - is 119.
table 6 shows ws&pos tagging performance of difference approaches on ctb5x and pd. f (%) on ctb5x-test of coupled (offline) only ws is 98.01. f (%) on ctb5x-test of coupled (offline) joint ws&pos is 94.39. f (%) on ctb5x-test of guide-feature only ws is 97.96. f (%) on ctb5x-test of guide-feature joint ws&pos is 94.06. f (%) on ctb5x-test of baseline only ws is 97.37. f (%) on ctb5x-test of baseline joint ws&pos is 93.23. f (%) on ctb5x-test of sun and wan (2012) only ws is â€”. f (%) on ctb5x-test of sun and wan (2012) joint ws&pos is 94.36. f (%) on ctb5x-test of jiang et al. (2009) only ws is 98.23. f (%) on ctb5x-test of jiang et al. (2009) joint ws&pos is 94.03.
table 3 shows performance of various approaches on stream summarization on five topics. p@50 of random p@50 is 0.02. p@100 of random p@100 is 0.08. p@50 of random p@50 is 0. p@100 of random p@100 is 0. p@50 of random p@50 is 0.02. p@100 of random p@100 is 0.04. p@50 of random p@50 is 0. p@100 of random p@100 is 0. p@50 of random p@50 is 0.02. p@100 of random p@100 is 0.03. p@50 of nb p@50 is 0.08. p@100 of nb p@100 is 0.12. p@50 of nb p@50 is 0.18. p@100 of nb p@100 is 0.19. p@50 of nb p@50 is 0.42. p@100 of nb p@100 is 0.36. p@50 of nb p@50 is 0.18. p@100 of nb p@100 is 0.17. p@50 of nb p@50 is 0.38. p@100 of nb p@100 is 0.31. p@50 of b-hac p@50 is 0.10. p@100 of b-hac p@100 is 0.13. p@50 of b-hac p@50 is 0.30. p@100 of b-hac p@100 is 0.26. p@50 of b-hac p@50 is 0.50. p@100 of b-hac p@100 is 0.47. p@50 of b-hac p@50 is 0.30. p@100 of b-hac p@100 is 0.22. p@50 of b-hac p@50 is 0.36. p@100 of b-hac p@100 is 0.32. p@50 of tahbm p@50 is 0.18. p@100 of tahbm p@100 is 0.15. p@50 of tahbm p@50 is 0.30. p@100 of tahbm p@100 is 0.29. p@50 of tahbm p@50 is 0.50. p@100 of tahbm p@100 is 0.43. p@50 of tahbm p@50 is 0.46. p@100 of tahbm p@100 is 0.36. p@50 of tahbm p@50 is 0.38. p@100 of tahbm p@100 is 0.33. p@50 of ge et al. (2015b) p@50 is 0.20. p@100 of ge et al. (2015b) p@100 is 0.15. p@50 of ge et al. (2015b) p@50 is 0.38. p@100 of ge et al. (2015b) p@100 is 0.36. p@50 of ge et al. (2015b) p@50 is 0.64. p@100 of ge et al. (2015b) p@100 is 0.53. p@50 of ge et al. (2015b) p@50 is 0.54. p@100 of ge et al. (2015b) p@100 is 0.41. p@50 of ge et al. (2015b) p@50 is 0.40. p@100 of ge et al. (2015b) p@100 is 0.33. p@50 of binet-noderank p@50 is 0.24. p@100 of binet-noderank p@100 is 0.20. p@50 of binet-noderank p@50 is 0.38. p@100 of binet-noderank p@100 is 0.30. p@50 of binet-noderank p@50 is 0.54. p@100 of binet-noderank p@100 is 0.51. p@50 of binet-noderank p@50 is 0.48. p@100 of binet-noderank p@100 is 0.43. p@50 of binet-noderank p@50 is 0.36. p@100 of binet-noderank p@100 is 0.33. p@50 of binet-arearank p@50 is 0.40. p@100 of binet-arearank p@100 is 0.33. p@50 of binet-arearank p@50 is 0.40. p@100 of binet-arearank p@100 is 0.34. p@50 of binet-arearank p@50 is 0.80. p@100 of binet-arearank p@100 is 0.62. p@50 of binet-arearank p@50 is 0.50. p@100 of binet-arearank p@100 is 0.49. p@50 of binet-arearank p@50 is 0.32. p@100 of binet-arearank p@100 is 0.30.
table 2 shows the performances on the abstracts sub-corpus. p (%) of baseline p (%) is 94.71. r (%) of baseline r (%) is 90.54. f1 of baseline f1 is 92.56. pclb (%) of baseline pclb (%) is 84.81. pcrb (%) of baseline pcrb (%) is 85.11. pcs (%) of baseline pcs (%) is 72.47. p (%) of cnn_c p (%) is 95.95. r (%) of cnn_c r (%) is 95.19. f1 of cnn_c f1 is 95.56. pclb (%) of cnn_c pclb (%) is 93.16. pcrb (%) of cnn_c pcrb (%) is 91.50. pcs (%) of cnn_c pcs (%) is 85.75. p (%) of cnn_d p (%) is 92.25. r (%) of cnn_d r (%) is 94.98. f1 of cnn_d f1 is 93.55. pclb (%) of cnn_d pclb (%) is 86.39. pcrb (%) of cnn_d pcrb (%) is 84.50. pcs (%) of cnn_d pcs (%) is 74.43. p (%) of baseline p (%) is 85.46. r (%) of baseline r (%) is 72.95. f1 of baseline f1 is 78.63. pclb (%) of baseline pclb (%) is 84.00. pcrb (%) of baseline pcrb (%) is 58.29. pcs (%) of baseline pcs (%) is 46.42. p (%) of cnn_c p (%) is 85.10. r (%) of cnn_c r (%) is 92.74. f1 of cnn_c f1 is 89.64. pclb (%) of cnn_c pclb (%) is 81.04. pcrb (%) of cnn_c pcrb (%) is 87.73. pcs (%) of cnn_c pcs (%) is 70.86. p (%) of cnn_d p (%) is 89.49. r (%) of cnn_d r (%) is 90.54. f1 of cnn_d f1 is 89.91. pclb (%) of cnn_d pclb (%) is 91.91. pcrb (%) of cnn_d pcrb (%) is 83.54. pcs (%) of cnn_d pcs (%) is 77.14.
table 4 shows comparison of our cnn-based model with the state- pcs of morante (2009a) abstracts is 77.13. pcs of morante (2009a) cli is 60.59. pcs of morante (2009a) papers is 47.94. pcs of özgür (2009) abstracts is 79.89. pcs of özgür (2009) cli is n/a. pcs of özgür (2009) papers is 61.13. pcs of velldal (2012) abstracts is 79.56. pcs of velldal (2012) cli is 78.69. pcs of velldal (2012) papers is 75.15. pcs of zou (2013) abstracts is 84.21. pcs of zou (2013) cli is 72.92. pcs of zou (2013) papers is 67.24. pcs of ours abstracts is 85.75. pcs of ours cli is 73.92. pcs of ours papers is 59.82. pcs of morante (2008) abstracts is 57.33. pcs of morante (2008) cli is n/a. pcs of morante (2008) papers is n/a. pcs of morante (2009b) abstracts is 73.36. pcs of morante (2009b) cli is 87.27. pcs of morante (2009b) papers is 50.26. pcs of li (2010) abstracts is 81.84. pcs of li (2010) cli is 89.79. pcs of li (2010) papers is 64.02. pcs of velldal (2012) abstracts is 74.35. pcs of velldal (2012) cli is 90.74. pcs of velldal (2012) papers is 70.21. pcs of zou (2013) abstracts is 76.90. pcs of zou (2013) cli is 85.31. pcs of zou (2013) papers is 61.19. pcs of ours abstracts is 77.14. pcs of ours cli is 89.66. pcs of ours papers is 55.32.
table 4 shows effects of embedding on performance. p of weu p is 80.74%. r of weu r is 81.19%. f1 of weu f1 is 80.97%. p of wenu p is 74.10%. r of wenu r is 69.30%. f1 of wenu f1 is 71.62%. p of reu p is 79.01%. r of reu r is 79.75%. f1 of reu f1 is 79.38%. p of renu p is 78.16%. r of renu r is 64.55%. f1 of renu f1 is 70.70%.
table 3 shows classification results across the behavioral features (bf), the reviewer embeddings (re) , product embeddings (pe) and bigram of the review texts. p of 50.50.00 hotel is 75.7. p of 50.50.00 restaurant is 80.5. r of 50.50.00 hotel is 83. r of 50.50.00 restaurant is 83.2. f1 of 50.50.00 hotel is 79.1. f1 of 50.50.00 restaurant is 81.8. a of 50.50.00 hotel is 81. a of 50.50.00 restaurant is 82.5. p of n.d. hotel is 26.5. p of n.d. restaurant is 50.1. r of n.d. hotel is 56. r of n.d. restaurant is 70.5. f1 of n.d. hotel is 36. f1 of n.d. restaurant is 58.6. a of n.d. hotel is 80.4. a of n.d. restaurant is 82. p of 50.50.00 hotel is 82.4. p of 50.50.00 restaurant is 82.8. r of 50.50.00 hotel is 85.2. r of 50.50.00 restaurant is 88.5. f1 of 50.50.00 hotel is 83.7. f1 of 50.50.00 restaurant is 85.6. a of 50.50.00 hotel is 83.8. a of 50.50.00 restaurant is 83.3. p of n.d. hotel is 41.4. p of n.d. restaurant is 48.2. r of n.d. hotel is 84.6. r of n.d. restaurant is 87.9. f1 of n.d. hotel is 55.6. f1 of n.d. restaurant is 62.3. a of n.d. hotel is 82.4. a of n.d. restaurant is 78.6. p of 50.50.00 hotel is 82.8. p of 50.50.00 restaurant is 84.5. r of 50.50.00 hotel is 86.9. r of 50.50.00 restaurant is 87.8. f1 of 50.50.00 hotel is 84.8. f1 of 50.50.00 restaurant is 86.1. a of 50.50.00 hotel is 85.1. a of 50.50.00 restaurant is 86.5. p of n.d. hotel is 46.5. p of n.d. restaurant is 48.9. r of n.d. hotel is 82.5. r of n.d. restaurant is 87.3. f1 of n.d. hotel is 59.4. f1 of n.d. restaurant is 62.7. a of n.d. hotel is 84.9. a of n.d. restaurant is 82.3. p of 50.50.00 hotel is 83.3. p of 50.50.00 restaurant is 85.4. r of 50.50.00 hotel is 88.1. r of 50.50.00 restaurant is 90.2. f1 of 50.50.00 hotel is 85.6. f1 of 50.50.00 restaurant is 87.7. a of 50.50.00 hotel is 85.5. a of 50.50.00 restaurant is 87.4. p of n.d. hotel is 47.1. p of n.d. restaurant is 56.9. r of n.d. hotel is 83.5. r of n.d. restaurant is 90.1. f1 of n.d. hotel is 60.2. f1 of n.d. restaurant is 69.8. a of n.d. hotel is 85. a of n.d. restaurant is 85.8. p of 50.50.00 hotel is 83.6. p of 50.50.00 restaurant is 86. r of 50.50.00 hotel is 89. r of 50.50.00 restaurant is 90.7. f1 of 50.50.00 hotel is 86.2. f1 of 50.50.00 restaurant is 88.3. a of 50.50.00 hotel is 85.7. a of 50.50.00 restaurant is 88. p of n.d. hotel is 47.5. p of n.d. restaurant is 57.4. r of n.d. hotel is 84.1. r of n.d. restaurant is 89.9. f1 of n.d. hotel is 60.7. f1 of n.d. restaurant is 70.1. a of n.d. hotel is 85.3. a of n.d. restaurant is 86.1. p of 50.50.00 hotel is 84.2. p of 50.50.00 restaurant is 86.8. r of 50.50.00 hotel is 89.9. r of 50.50.00 restaurant is 91.8. f1 of 50.50.00 hotel is 87. f1 of 50.50.00 restaurant is 89.2. a of 50.50.00 hotel is 86.5. a of 50.50.00 restaurant is 89.9. p of n.d. hotel is 48.2. p of n.d. restaurant is 58.2. r of n.d. hotel is 85. r of n.d. restaurant is 90.3. f1 of n.d. hotel is 61.5. f1 of n.d. restaurant is 70.8. a of n.d. hotel is 85.9. a of n.d. restaurant is 87.8.
table 4 shows svm 5-fold cv classification results by dropping relations from our method utilizing re+pe+bigram. f1 of 1 f1 is -2.1. a of 1 a is -2.0. f1 of 1 f1 is -2.0. a of 1 a is -3.1. f1 of 2 f1 is -2.3. a of 2 a is -2.1. f1 of 2 f1 is -1.9. a of 2 a is -2.9. f1 of 3 f1 is -3.9. a of 3 a is -4.0. f1 of 3 f1 is -4.0. a of 3 a is -6.3. f1 of 4 f1 is -3.7. a of 4 a is -3.5. f1 of 4 f1 is -3.6. a of 4 a is -5.5. f1 of 5 f1 is -3.5. a of 5 a is -3.6. f1 of 5 f1 is -2.8. a of 5 a is -4.5. f1 of 6 f1 is -2.5. a of 6 a is -2.5. f1 of 6 f1 is -3.4. a of 6 a is -5.2. f1 of 7 f1 is -3.2. a of 7 a is -3.2. f1 of 7 f1 is -3.3. a of 7 a is -5.0. f1 of 8 f1 is -2.8. a of 8 a is -2.6. f1 of 8 f1 is -3.0. a of 8 a is -4.6. f1 of 9 f1 is -4.0. a of 9 a is -3.7. f1 of 9 f1 is -3.7. a of 9 a is -5.4. f1 of 10 f1 is -2.2. a of 10 a is -2.4. f1 of 10 f1 is -1.8. a of 10 a is -2.8. f1 of 11 f1 is -2.6. a of 11 a is -2.4. f1 of 11 f1 is -2.7. a of 11 a is -4.4.
table 4 shows results for the unseen target stance detection development setup using bicond, with single vs separate embeddings matrices for tweet and target and different initialisations p of favor p is 0.1982. r of favor r is 0.3846. f1 of favor f1 is 0.2616. p of against p is 0.6263. r of against r is 0.5929. f1 of against f1 is 0.6092. f1 of macro f1 is 0.4354. p of favor p is 0.2278. r of favor r is 0.5043. f1 of favor f1 is 0.3138. p of against p is 0.6706. r of against r is 0.4300. f1 of against f1 is 0.5240. f1 of macro f1 is 0.4189. p of favor p is 0.6000. r of favor r is 0.0513. f1 of favor f1 is 0.0945. p of against p is 0.5761. r of against r is 0.9440. f1 of against f1 is 0.7155. f1 of macro f1 is 0.4050. p of favor p is 0.1429. r of favor r is 0.0342. f1 of favor f1 is 0.0552. p of against p is 0.5707. r of against r is 0.9033. f1 of against f1 is 0.6995. f1 of macro f1 is 0.3773. p of favor p is 0.2588. r of favor r is 0.3761. f1 of favor f1 is 0.3066. p of against p is 0.7081. r of against r is 0.5802. f1 of against f1 is 0.6378. f1 of macro f1 is 0.4722. p of favor p is 0.2243. r of favor r is 0.4103. f1 of favor f1 is 0.2900. p of against p is 0.6185. r of against r is 0.5445. f1 of against f1 is 0.5792. f1 of macro f1 is 0.4346.
table 7 shows stance detection test results, compared against the state of the art. f1 of favor f1 is 0.1842. f1 of against f1 is 0.3845. f1 of macro f1 is 0.2843. f1 of favor f1 is 0.0. f1 of against f1 is 0.5944. f1 of macro f1 is 0.2972. f1 of favor f1 is 0.3902. f1 of against f1 is 0.5899. f1 of macro f1 is 0.4901.
table 4 shows event recognition performance before/after incorporating subevents recall of (huang and riloff 2013) recall is 71. precision of (huang and riloff 2013) precision is 88. f1-score of (huang and riloff 2013) f1-score is 79. recall of +subevents recall is 81. precision of +subevents precision is 83. f1-score of +subevents f1-score is 82.
table 1 shows zero-shot recognition results on awa (% accuracy). accuracy of awa linreg is 44.0. accuracy of awa nlinreg is 48.4. accuracy of awa cme is 43.1. accuracy of awa es-zsl is 58.2. accuracy of awa gaussian is 65.4.
table 1 shows single system results in terms of (ter-bleu)/2 (the lower the better) on 5 million chinese to english training set. bp of tree-to-string bp is 0.95. bleu of tree-to-string bleu is 34.93. t-b of tree-to-string t-b is 9.45. bp of tree-to-string bp is 0.94. bleu of tree-to-string bleu is 31.12. t-b of tree-to-string t-b is 12.90. bp of tree-to-string bp is 0.90. bleu of tree-to-string bleu is 23.45. t-b of tree-to-string t-b is 17.72. t-b of tree-to-string t-b is 13.36. bp of lvnmt bp is 0.96. bleu of lvnmt bleu is 34.53. t-b of lvnmt t-b is 12.25. bp of lvnmt bp is 0.93. bleu of lvnmt bleu is 28.86. t-b of lvnmt t-b is 17.40. bp of lvnmt bp is 0.97. bleu of lvnmt bleu is 26.78. t-b of lvnmt t-b is 17.57. t-b of lvnmt t-b is 15.74. bp of ugru bp is 0.92. bleu of ugru bleu is 35.59. t-b of ugru t-b is 10.71. bp of ugru bp is 0.89. bleu of ugru bleu is 30.18. t-b of ugru t-b is 15.33. bp of ugru bp is 0.97. bleu of ugru bleu is 27.48. t-b of ugru t-b is 16.67. t-b of ugru t-b is 14.24. bp of usub bp is 0.91. bleu of usub bleu is 35.90. t-b of usub t-b is 10.29. bp of usub bp is 0.88. bleu of usub bleu is 30.49. t-b of usub t-b is 15.23. bp of usub bp is 0.96. bleu of usub bleu is 27.63. t-b of usub t-b is 16.12. t-b of usub t-b is 13.88. bp of ugru+usub bp is 0.92. bleu of ugru+usub bleu is 36.60. t-b of ugru+usub t-b is 9.36. bp of ugru+usub bp is 0.89. bleu of ugru+usub bleu is 31.86. t-b of ugru+usub t-b is 13.69. bp of ugru+usub bp is 0.95. bleu of ugru+usub bleu is 27.12. t-b of ugru+usub t-b is 16.37. t-b of ugru+usub t-b is 13.14. bp of +obj. bp is 0.93. bleu of +obj. bleu is 36.80. t-b of +obj. t-b is 9.78. bp of +obj. bp is 0.90. bleu of +obj. bleu is 31.83. t-b of +obj. t-b is 14.20. bp of +obj. bp is 0.95. bleu of +obj. bleu is 28.28. t-b of +obj. t-b is 15.73. t-b of +obj. t-b is 13.24.
table 2 shows single system results in terms of (ter-bleu)/2 on 11 million set. bp of tree-to-string bp is 0.90. t-b of tree-to-string t-b is 8.70. bp of tree-to-string bp is 0.84. t-b of tree-to-string t-b is 12.65. bp of tree-to-string bp is 0.84. t-b of tree-to-string t-b is 17.00. t-b of tree-to-string t-b is 12.78. bp of lvnmt bp is 0.96. t-b of lvnmt t-b is 9.78. bp of lvnmt bp is 0.94. t-b of lvnmt t-b is 14.15. bp of lvnmt bp is 0.97. t-b of lvnmt t-b is 15.89. t-b of lvnmt t-b is 13.27. bp of ugru bp is 0.97. t-b of ugru t-b is 8.62. bp of ugru bp is 0.95. t-b of ugru t-b is 12.79. bp of ugru bp is 0.97. t-b of ugru t-b is 15.34. t-b of ugru t-b is 12.31.
table 2 shows average results for dsms over four different frequency ranges for the items in the toefl, esl, sl, men, and rw tests. correlation of co high is 32.61 (↑62.5,↓04.6). correlation of co medium is 35.77 (↑66.6,↓21.2). correlation of co low is 12.57 (↑35.7,↓00.0). correlation of co mixed is 27.14 (↑56.6,↓07.9). correlation of ppmi high is 55.51 (↑75.3,↓28.0). correlation of ppmi medium is 57.83 (↑88.8,↓18.7). correlation of ppmi low is 25.84 (↑50.0,↓00.0). correlation of ppmi mixed is 47.73 (↑83.3,↓27.1). correlation of tsvd high is 50.52 (↑70.9,↓23.2). correlation of tsvd medium is 54.75 (↑77.9,↓24.1). correlation of tsvd low is 17.85 (↑50.0,↓00.0). correlation of tsvd mixed is 41.08 (↑56.6,↓19.6). correlation of isvd high is 63.31 (↑87.5,↓36.5). correlation of isvd medium is 69.25 (↑88.8,↓46.3). correlation of isvd low is 10.94 (↑16.0,↓00.0). correlation of isvd mixed is 57.24 (↑83.3,↓33.0). correlation of ri high is 53.11 (↑62.5,↓30.1). correlation of ri medium is 48.02 (↑72.2,↓20.4). correlation of ri low is 23.29 (↑39.0,↓00.0). correlation of ri mixed is 46.39 (↑66.6,↓21.0). correlation of sgns high is 68.81 (↑87.5,↓36.4). correlation of sgns medium is 62.00 (↑83.3,↓27.4). correlation of sgns low is 18.76 (↑42.8,↓00.0). correlation of sgns mixed is 56.93 (↑83.3,↓30.2). correlation of cbow high is 62.73 (↑81.2,↓31.9). correlation of cbow medium is 59.50 (↑83.3,↓32.4). correlation of cbow low is 27.13 (↑78.5,↓00.0). correlation of cbow mixed is 52.21 (↑76.6,↓25.9).
table 2 shows estimated precision and recall for tamil, bengali and malayalam before and after non-expert curation. p of partial p is 1.0. p of partial p is 0.84. r of partial r is 0.68. f1 of partial f1 is 0.75. %agree of partial %agree is 0.67. p of exact p is 1.0. p of exact p is 0.83. r of exact r is 0.68. f1 of exact f1 is 0.75. %agree of exact %agree is 0.67. p of partial p is 1.0. p of partial p is 0.88. r of partial r is 0.69. f1 of partial f1 is 0.78. %agree of partial %agree is 0.67. p of exact p is 1.0. p of exact p is 0.87. r of exact r is 0.69. f1 of exact f1 is 0.77. %agree of exact %agree is 0.67. p of partial p is 0.99. p of partial p is 0.87. r of partial r is 0.65. f1 of partial f1 is 0.75. %agree of partial %agree is 0.65. p of exact p is 0.99. p of exact p is 0.79. r of exact r is 0.63. f1 of exact f1 is 0.7. %agree of exact %agree is 0.65. p of partial p is 0.99. p of partial p is 0.92. r of partial r is 0.69. f1 of partial f1 is 0.78. %agree of partial %agree is 0.65. p of exact p is 0.99. p of exact p is 0.84. r of exact r is 0.67. f1 of exact f1 is 0.74. %agree of exact %agree is 0.65. p of partial p is 0.77. p of partial p is 0.49. r of partial r is 0.59. f1 of partial f1 is 0.53. %agree of partial %agree is 0.75. p of exact p is 0.77. p of exact p is 0.45. r of exact r is 0.58. f1 of exact f1 is 0.5. %agree of exact %agree is 0.75. p of partial p is 0.77. p of partial p is 0.62. r of partial r is 0.67. f1 of partial f1 is 0.64. %agree of partial %agree is 0.75. p of exact p is 0.77. p of exact p is 0.58. r of exact r is 0.65. f1 of exact f1 is 0.61. %agree of exact %agree is 0.75. p of partial p is 0.97. p of partial p is 0.93. r of partial r is 0.83. f1 of partial f1 is 0.88. %agree of partial %agree is 0.92. p of exact p is 0.97. p of exact p is 0.83. r of exact r is 0.81. f1 of exact f1 is 0.82. %agree of exact %agree is 0.92. p of partial p is 0.96. p of partial p is 0.95. r of partial r is 0.73. f1 of partial f1 is 0.83. %agree of partial %agree is 0.92. p of exact p is 0.96. p of exact p is 0.91. r of exact r is 0.73. f1 of exact f1 is 0.81. %agree of exact %agree is 0.92. p of partial p is 0.91. p of partial p is 0.93. r of partial r is 0.66. f1 of partial f1 is 0.77. %agree of partial %agree is 0.81. p of exact p is 0.91. p of exact p is 0.58. r of exact r is 0.54. f1 of exact f1 is 0.56. %agree of exact %agree is 0.81.
table 2 shows performance of unigrams versus our similarity-based features using embeddings from word2vec p of unigrams p is 67.2. r of unigrams r is 78.8. f of unigrams f is 72.53. p of s p is 64.6. r of s r is 75.2. f of s f is 69.49. p of ws p is 67.6. r of ws r is 51.2. f of ws f is 58.26. p of both p is 67. r of both r is 52.8. f of both f is 59.05.
table 3 shows performance obtained on augmenting word embedding features to features from four prior works, for four word embeddings; l: liebrecht et al. p of l p is 73. r of l r is 79. f of l f is 75.8. p of l p is 73. r of l r is 79. f of l f is 75.8. p of l p is 73. r of l r is 79. f of l f is 75.8. p of l p is 73. r of l r is 79. f of l f is 75.8. p of +s p is 81.8. r of +s r is 78.2. f of +s f is 79.95. p of +s p is 81.8. r of +s r is 79.2. f of +s f is 80.47. p of +s p is 81.8. r of +s r is 78.8. f of +s f is 80.27. p of +s p is 80.4. r of +s r is 80. f of +s f is 80.2. p of +ws p is 76.2. r of +ws r is 79.8. f of +ws f is 77.9. p of +ws p is 76.2. r of +ws r is 79.6. f of +ws f is 77.86. p of +ws p is 81.4. r of +ws r is 80.8. f of +ws f is 81.09. p of +ws p is 80.8. r of +ws r is 78.6. f of +ws f is 79.68. p of +s+ws p is 77.6. r of +s+ws r is 79.8. f of +s+ws f is 78.68. p of +s+ws p is 74. r of +s+ws r is 79.4. f of +s+ws f is 76.60. p of +s+ws p is 82. r of +s+ws r is 80.4. f of +s+ws f is 81.19. p of +s+ws p is 81.6. r of +s+ws r is 78.2. f of +s+ws f is 79.86. p of g p is 84.8. r of g r is 73.8. f of g f is 78.91. p of g p is 84.8. r of g r is 73.8. f of g f is 78.91. p of g p is 84.8. r of g r is 73.8. f of g f is 78.91. p of g p is 84.8. r of g r is 73.8. f of g f is 78.91. p of +s p is 84.2. r of +s r is 74.4. f of +s f is 79. p of +s p is 84. r of +s r is 72.6. f of +s f is 77.8. p of +s p is 84.4. r of +s r is 72. f of +s f is 77.7. p of +s p is 84. r of +s r is 72.8. f of +s f is 78. p of +ws p is 84.4. r of +ws r is 73.6. f of +ws f is 78.63. p of +ws p is 84. r of +ws r is 75.2. f of +ws f is 79.35. p of +ws p is 84.4. r of +ws r is 72.6. f of +ws f is 78.05. p of +ws p is 83.8. r of +ws r is 70.2. f of +ws f is 76.4. p of +s+ws p is 84.2. r of +s+ws r is 73.6. f of +s+ws f is 78.54. p of +s+ws p is 84. r of +s+ws r is 74. f of +s+ws f is 78.68. p of +s+ws p is 84.2. r of +s+ws r is 72.2. f of +s+ws f is 77.73. p of +s+ws p is 84. r of +s+ws r is 72.8. f of +s+ws f is 78. p of b p is 81.6. r of b r is 72.2. f of b f is 76.61. p of b p is 81.6. r of b r is 72.2. f of b f is 76.61. p of b p is 81.6. r of b r is 72.2. f of b f is 76.61. p of b p is 81.6. r of b r is 72.2. f of b f is 76.61. p of +s p is 78.2. r of +s r is 75.6. f of +s f is 76.87. p of +s p is 80.4. r of +s r is 76.2. f of +s f is 78.24. p of +s p is 81.2. r of +s r is 74.6. f of +s f is 77.76. p of +s p is 81.4. r of +s r is 72.6. f of +s f is 76.74. p of +ws p is 75.8. r of +ws r is 77.2. f of +ws f is 76.49. p of +ws p is 76.6. r of +ws r is 77. f of +ws f is 76.79. p of +ws p is 76.2. r of +ws r is 76.4. f of +ws f is 76.29. p of +ws p is 81.6. r of +ws r is 73.4. f of +ws f is 77.28. p of +s+ws p is 74.8. r of +s+ws r is 77.4. f of +s+ws f is 76.07. p of +s+ws p is 76.2. r of +s+ws r is 78.2. f of +s+ws f is 77.18. p of +s+ws p is 75.6. r of +s+ws r is 78.8. f of +s+ws f is 77.16. p of +s+ws p is 81. r of +s+ws r is 75.4. f of +s+ws f is 78.09. p of j p is 85.2. r of j r is 74.4. f of j f is 79.43. p of j p is 85.2. r of j r is 74.4. f of j f is 79.43. p of j p is 85.2. r of j r is 74.4. f of j f is 79.43. p of j p is 85.2. r of j r is 74.4. f of j f is 79.43. p of +s p is 84.8. r of +s r is 73.8. f of +s f is 78.91. p of +s p is 85.6. r of +s r is 74.8. f of +s f is 79.83. p of +s p is 85.4. r of +s r is 74.4. f of +s f is 79.52. p of +s p is 85.4. r of +s r is 74.6. f of +s f is 79.63. p of +ws p is 85.6. r of +ws r is 75.2. f of +ws f is 80.06. p of +ws p is 85.4. r of +ws r is 72.6. f of +ws f is 78.48. p of +ws p is 85.4. r of +ws r is 73.4. f of +ws f is 78.94. p of +ws p is 85.6. r of +ws r is 73.4. f of +ws f is 79.03. p of +s+ws p is 84.8. r of +s+ws r is 73.6. f of +s+ws f is 78.8. p of +s+ws p is 85.8. r of +s+ws r is 75.4. f of +s+ws f is 80.26. p of +s+ws p is 85.6. r of +s+ws r is 74.4. f of +s+ws f is 79.6. p of +s+ws p is 85.2. r of +s+ws r is 73.2. f of +s+ws f is 78.74.
table 4 shows spearman rank correlation of thread ˜si,j with karma scores. correlation of askmen hyb-500.30 is 0.392*. correlation of askmen word only is 0.222*. correlation of askmen topic-100 is 0.055. correlation of askscience hyb-500.30 is 0.321*. correlation of askscience word only is -0.110. correlation of askscience topic-100 is -0.166*. correlation of askwomen hyb-500.30 is 0.501*. correlation of askwomen word only is 0.388*. correlation of askwomen topic-100 is 0.005. correlation of atheism hyb-500.30 is 0.137*. correlation of atheism word only is -0.229*. correlation of atheism topic-100 is -0.251. correlation of chgmyvw hyb-500.30 is 0.167*. correlation of chgmyvw word only is -0.121*. correlation of chgmyvw topic-100 is -0.306*. correlation of fitness hyb-500.30 is 0.130*. correlation of fitness word only is 0.017. correlation of fitness topic-100 is -0.313*. correlation of politics hyb-500.30 is 0.533*. correlation of politics word only is 0.341*. correlation of politics topic-100 is 0.011. correlation of worldnews hyb-500.30 is 0.374*. correlation of worldnews word only is 0.148*. correlation of worldnews topic-100 is -0.277*.
table 2 shows event detection performance (ndcg; higher is better) using thirty-nine well-known events that took place between 1973 and 1978. ndcg of capsule (this paper) ndcg is 0.693. ndcg of term-count deviation + tf-idf (equation (7)) ndcg is 0.652. ndcg of term-count deviation (equation (6)) ndcg is 0.642. ndcg of random ndcg is 0.557. ndcg of “event-only” capsule (this paper) ndcg is 0.426.
table 2 shows results of multi-label classification from experiment 1. h-loss of ban plastic water bottles? h-loss is 0.092. one-e of ban plastic water bottles? one-e is 0.283. h-loss of ban plastic water bottles? h-loss is 0.090. one-e of ban plastic water bottles? one-e is 0.305. h-loss of christianity or atheism h-loss is 0.105. one-e of christianity or atheism one-e is 0.212. h-loss of christianity or atheism h-loss is 0.105. one-e of christianity or atheism one-e is 0.218. h-loss of evolution vs. creation h-loss is 0.093. one-e of evolution vs. creation one-e is 0.196. h-loss of evolution vs. creation h-loss is 0.094. one-e of evolution vs. creation one-e is 0.234. h-loss of firefox vs. internet explorer h-loss is 0.080. one-e of firefox vs. internet explorer one-e is 0.312. h-loss of firefox vs. internet explorer h-loss is 0.078. one-e of firefox vs. internet explorer one-e is 0.345. h-loss of gay marriage: right or wrong? h-loss is 0.095. one-e of gay marriage: right or wrong? one-e is 0.243. h-loss of gay marriage: right or wrong? h-loss is 0.094. one-e of gay marriage: right or wrong? one-e is 0.270. h-loss of should parents use spanking? h-loss is 0.082. one-e of should parents use spanking? one-e is 0.312. h-loss of should parents use spanking? h-loss is 0.083. one-e of should parents use spanking? one-e is 0.344. h-loss of if your spouse committed murder... h-loss is 0.094. one-e of if your spouse committed murder... one-e is 0.297. h-loss of if your spouse committed murder... h-loss is 0.094. one-e of if your spouse committed murder... one-e is 0.272. h-loss of india has the potential to lead the world h-loss is 0.088. one-e of india has the potential to lead the world one-e is 0.294. h-loss of india has the potential to lead the world h-loss is 0.086. one-e of india has the potential to lead the world one-e is 0.322. h-loss of is it better to have a lousy father or to be fatherless? h-loss is 0.086. one-e of is it better to have a lousy father or to be fatherless? one-e is 0.367. h-loss of is it better to have a lousy father or to be fatherless? h-loss is 0.085. one-e of is it better to have a lousy father or to be fatherless? one-e is 0.381. h-loss of is porn wrong? h-loss is 0.098. one-e of is porn wrong? one-e is 0.278. h-loss of is porn wrong? h-loss is 0.100. one-e of is porn wrong? one-e is 0.270. h-loss of is the school uniform a good or bad idea? h-loss is 0.081. one-e of is the school uniform a good or bad idea? one-e is 0.279. h-loss of is the school uniform a good or bad idea? h-loss is 0.077. one-e of is the school uniform a good or bad idea? one-e is 0.406. h-loss of pro-choice vs. pro-life h-loss is 0.095. one-e of pro-choice vs. pro-life one-e is 0.218. h-loss of pro-choice vs. pro-life h-loss is 0.098. one-e of pro-choice vs. pro-life one-e is 0.218. h-loss of should physical education be mandatory? h-loss is 0.095. one-e of should physical education be mandatory? one-e is 0.273. h-loss of should physical education be mandatory? h-loss is 0.095. one-e of should physical education be mandatory? one-e is 0.277. h-loss of tv is better than books h-loss is 0.091. one-e of tv is better than books one-e is 0.265. h-loss of tv is better than books h-loss is 0.087. one-e of tv is better than books one-e is 0.300. h-loss of personal pursuit or common good? h-loss is 0.095. one-e of personal pursuit or common good? one-e is 0.328. h-loss of personal pursuit or common good? h-loss is 0.094. one-e of personal pursuit or common good? one-e is 0.343. h-loss of w. farquhar ought to be honored... h-loss is 0.054. one-e of w. farquhar ought to be honored... one-e is 0.528. h-loss of w. farquhar ought to be honored... h-loss is 0.052. one-e of w. farquhar ought to be honored... one-e is 0.570. h-loss of average h-loss is 0.089. one-e of average one-e is 0.293. h-loss of average h-loss is 0.088. one-e of average one-e is 0.317.
table 2 shows results of intra-sentential subject zero anaphora resolution #cols. of - #cols. is —. recall of - recall is 0.539. precision of - precision is 0.612. f-score of - f-score is 0.573. avg.p of - avg.p is 0.670. #cols. of - #cols. is —. recall of - recall is 0.484. precision of - precision is 0.357. f-score of - f-score is 0.411. avg.p of - avg.p is —. #cols. of - #cols. is 1. recall of - recall is 0.365. precision of - precision is 0.524. f-score of - f-score is 0.430. avg.p of - avg.p is 0.540. #cols. of base #cols. is 1. recall of base recall is 0.446. precision of base precision is 0.394. f-score of base f-score is 0.419. avg.p of base avg.p is 0.448. #cols. of base+surfseq #cols. is 4. recall of base+surfseq recall is 0.458. precision of base+surfseq precision is 0.597. f-score of base+surfseq f-score is 0.518. avg.p of base+surfseq avg.p is 0.679. #cols. of base+deptree #cols. is 5. recall of base+deptree recall is 0.339. precision of base+deptree precision is 0.688. f-score of base+deptree f-score is 0.454. avg.p of base+deptree avg.p is 0.690. #cols. of base+surfseq+deptree #cols. is 8. recall of base+surfseq+deptree recall is 0.417. precision of base+surfseq+deptree precision is 0.695. f-score of base+surfseq+deptree f-score is 0.521. avg.p of base+surfseq+deptree avg.p is 0.730. #cols. of base+surfseq+predcontext #cols. is 7. recall of base+surfseq+predcontext recall is 0.459. precision of base+surfseq+predcontext precision is 0.631. f-score of base+surfseq+predcontext f-score is 0.531. avg.p of base+surfseq+predcontext avg.p is 0.702. #cols. of base+deptree+predcontext #cols. is 8. recall of base+deptree+predcontext recall is 0.298. precision of base+deptree+predcontext precision is 0.728. f-score of base+deptree+predcontext f-score is 0.422. avg.p of base+deptree+predcontext avg.p is 0.702. #cols. of base+surfseq+deptree+predcontext (proposed) #cols. is 11. recall of base+surfseq+deptree+predcontext (proposed) recall is 0.418. precision of base+surfseq+deptree+predcontext (proposed) precision is 0.704. f-score of base+surfseq+deptree+predcontext (proposed) f-score is 0.525. avg.p of base+surfseq+deptree+predcontext (proposed) avg.p is 0.732.
table 4 shows spearman’s rank correlation for monolingual similarity measurement on 3 datasets ws-de (353 pairs), ws-en (353 pairs) and rw-en (2034 pairs). correlation of klementiev et al. (2012) ws-de is 23.8. correlation of klementiev et al. (2012) ws-en is 13.2. correlation of klementiev et al. (2012) rw-en is 7.3. correlation of chandar a p et al. (2014) ws-de is 34.6. correlation of chandar a p et al. (2014) ws-en is 39.8. correlation of chandar a p et al. (2014) rw-en is 20.5. correlation of hermann and blunsom (2014) ws-de is 28.3. correlation of hermann and blunsom (2014) ws-en is 19.8. correlation of hermann and blunsom (2014) rw-en is 13.6. correlation of luong et al. (2015) ws-de is 47.4. correlation of luong et al. (2015) ws-en is 49.3. correlation of luong et al. (2015) rw-en is 25.3. correlation of gouws and sogaard (2015) ws-de is 67.4. correlation of gouws and sogaard (2015) ws-en is 71.8. correlation of gouws and sogaard (2015) rw-en is 31.0. correlation of cbow ws-de is 62.2. correlation of cbow ws-en is 70.3. correlation of cbow rw-en is 42.7. correlation of +combine ws-de is 65.8. correlation of +combine ws-en is 74.1. correlation of +combine rw-en is 43.1. correlation of yih and qazvinian (2012) ws-en is 81.0. correlation of shazeer et al. (2016) ws-en is 74.8. correlation of shazeer et al. (2016) rw-en is 48.3. correlation of our joint-model ws-de is 59.3. correlation of our joint-model ws-en is 68.6. correlation of our joint-model rw-en is 38.1. correlation of +combine ws-de is 71.1. correlation of +combine ws-en is 76.2. correlation of +combine rw-en is 44.0.
table 6 shows cldc performance for both en → de and de → en direction for many clwe. accuracy of mt baseline en → de is 68.1. accuracy of mt baseline de → en is 67.4. accuracy of klementiev et al. (2012) en → de is 77.6. accuracy of klementiev et al. (2012) de → en is 71.1. accuracy of gouws et al. (2015) en → de is 86.5. accuracy of gouws et al. (2015) de → en is 75.0. accuracy of kocisk ˇ y et al. (2014) en → de is 83.1. accuracy of kocisk ˇ y et al. (2014) de → en is 75.4 ´. accuracy of chandar a p et al. (2014) 91.8 en → de is 91.8. accuracy of chandar a p et al. (2014) 91.8 de → en is 74.2. accuracy of hermann and blunsom (2014) en → de is 86.4. accuracy of hermann and blunsom (2014) de → en is 74.7. accuracy of luong et al. (2015) en → de is 88.4. accuracy of luong et al. (2015) de → en is 80.3. accuracy of our model en → de is 86.3. accuracy of our model de → en is 76.8.
table 3 shows average accuracy over all the morphological inﬂection datasets. avg. accuracy of seq2seq avg. accuracy is 79.08. avg. accuracy of seq2seq w/ attention avg. accuracy is 95.64. avg. accuracy of adapted-seq2seq (ftnd16) avg. accuracy is 96.20. avg. accuracy of unissnt+ avg. accuracy is 87.85. avg. accuracy of bissnt+ avg. accuracy is 95.32.
table 4 shows study of typing performance on the three datasets. acc of clpl (cour et al. 2011) acc is 0.162. ma-f1 of clpl (cour et al. 2011) ma-f1 is 0.431. mi-f1 of clpl (cour et al. 2011) mi-f1 is 0.411. acc of clpl (cour et al. 2011) acc is 0.201. ma-f1 of clpl (cour et al. 2011) ma-f1 is 0.347. mi-f1 of clpl (cour et al. 2011) mi-f1 is 0.358. acc of clpl (cour et al. 2011) acc is 0.438. ma-f1 of clpl (cour et al. 2011) ma-f1 is 0.603. mi-f1 of clpl (cour et al. 2011) mi-f1 is 0.536. acc of pl-svm (nguyen and caruana 2008) acc is 0.428. ma-f1 of pl-svm (nguyen and caruana 2008) ma-f1 is 0.613. mi-f1 of pl-svm (nguyen and caruana 2008) mi-f1 is 0.571. acc of pl-svm (nguyen and caruana 2008) acc is 0.225. ma-f1 of pl-svm (nguyen and caruana 2008) ma-f1 is 0.455. mi-f1 of pl-svm (nguyen and caruana 2008) mi-f1 is 0.437. acc of pl-svm (nguyen and caruana 2008) acc is 0.465. ma-f1 of pl-svm (nguyen and caruana 2008) ma-f1 is 0.648. mi-f1 of pl-svm (nguyen and caruana 2008) mi-f1 is 0.582. acc of figer (ling and weld 2012) acc is 0.474. ma-f1 of figer (ling and weld 2012) ma-f1 is 0.692. mi-f1 of figer (ling and weld 2012) mi-f1 is 0.655. acc of figer (ling and weld 2012) acc is 0.369. ma-f1 of figer (ling and weld 2012) ma-f1 is 0.578. mi-f1 of figer (ling and weld 2012) mi-f1 is 0.516. acc of figer (ling and weld 2012) acc is 0.467. ma-f1 of figer (ling and weld 2012) ma-f1 is 0.672. mi-f1 of figer (ling and weld 2012) mi-f1 is 0.612. acc of figer-min (gillick et al. 2014) acc is 0.453. ma-f1 of figer-min (gillick et al. 2014) ma-f1 is 0.691. mi-f1 of figer-min (gillick et al. 2014) mi-f1 is 0.631. acc of figer-min (gillick et al. 2014) acc is 0.373. ma-f1 of figer-min (gillick et al. 2014) ma-f1 is 0.570. mi-f1 of figer-min (gillick et al. 2014) mi-f1 is 0.509. acc of figer-min (gillick et al. 2014) acc is 0.444. ma-f1 of figer-min (gillick et al. 2014) ma-f1 is 0.671. mi-f1 of figer-min (gillick et al. 2014) mi-f1 is 0.613. acc of hyena (yosef et al. 2012) acc is 0.288. ma-f1 of hyena (yosef et al. 2012) ma-f1 is 0.528. mi-f1 of hyena (yosef et al. 2012) mi-f1 is 0.506. acc of hyena (yosef et al. 2012) acc is 0.249. ma-f1 of hyena (yosef et al. 2012) ma-f1 is 0.497. mi-f1 of hyena (yosef et al. 2012) mi-f1 is 0.446. acc of hyena (yosef et al. 2012) acc is 0.523. ma-f1 of hyena (yosef et al. 2012) ma-f1 is 0.576. mi-f1 of hyena (yosef et al. 2012) mi-f1 is 0.587. acc of hyena-min acc is 0.325. ma-f1 of hyena-min ma-f1 is 0.566. mi-f1 of hyena-min mi-f1 is 0.536. acc of hyena-min acc is 0.295. ma-f1 of hyena-min ma-f1 is 0.523. mi-f1 of hyena-min mi-f1 is 0.470. acc of hyena-min acc is 0.524. ma-f1 of hyena-min ma-f1 is 0.582. mi-f1 of hyena-min mi-f1 is 0.595. acc of clustype (ren et al. 2015) acc is 0.274. ma-f1 of clustype (ren et al. 2015) ma-f1 is 0.429. mi-f1 of clustype (ren et al. 2015) mi-f1 is 0.448. acc of clustype (ren et al. 2015) acc is 0.305. ma-f1 of clustype (ren et al. 2015) ma-f1 is 0.468. mi-f1 of clustype (ren et al. 2015) mi-f1 is 0.404. acc of clustype (ren et al. 2015) acc is 0.441. ma-f1 of clustype (ren et al. 2015) ma-f1 is 0.498. mi-f1 of clustype (ren et al. 2015) mi-f1 is 0.573. acc of hnm (dong et al. 2015) acc is 0.237. ma-f1 of hnm (dong et al. 2015) ma-f1 is 0.409. mi-f1 of hnm (dong et al. 2015) mi-f1 is 0.417. acc of hnm (dong et al. 2015) acc is 0.122. ma-f1 of hnm (dong et al. 2015) ma-f1 is 0.288. mi-f1 of hnm (dong et al. 2015) mi-f1 is 0.272. acc of hnm (dong et al. 2015) acc is 0.551. ma-f1 of hnm (dong et al. 2015) ma-f1 is 0.591. mi-f1 of hnm (dong et al. 2015) mi-f1 is 0.606. acc of deepwalk (perozzi et al. 2014) acc is 0.414. ma-f1 of deepwalk (perozzi et al. 2014) ma-f1 is 0.563. mi-f1 of deepwalk (perozzi et al. 2014) mi-f1 is 0.511. acc of deepwalk (perozzi et al. 2014) acc is 0.479. ma-f1 of deepwalk (perozzi et al. 2014) ma-f1 is 0.669. mi-f1 of deepwalk (perozzi et al. 2014) mi-f1 is 0.611. acc of deepwalk (perozzi et al. 2014) acc is 0.586. ma-f1 of deepwalk (perozzi et al. 2014) ma-f1 is 0.638. mi-f1 of deepwalk (perozzi et al. 2014) mi-f1 is 0.628. acc of line (tang et al. 2015b) acc is 0.181. ma-f1 of line (tang et al. 2015b) ma-f1 is 0.480. mi-f1 of line (tang et al. 2015b) mi-f1 is 0.499. acc of line (tang et al. 2015b) acc is 0.436. ma-f1 of line (tang et al. 2015b) ma-f1 is 0.634. mi-f1 of line (tang et al. 2015b) mi-f1 is 0.578. acc of line (tang et al. 2015b) acc is 0.576. ma-f1 of line (tang et al. 2015b) ma-f1 is 0.687. mi-f1 of line (tang et al. 2015b) mi-f1 is 0.690. acc of pte (tang et al. 2015a) acc is 0.405. ma-f1 of pte (tang et al. 2015a) ma-f1 is 0.575. mi-f1 of pte (tang et al. 2015a) mi-f1 is 0.526. acc of pte (tang et al. 2015a) acc is 0.436. ma-f1 of pte (tang et al. 2015a) ma-f1 is 0.630. mi-f1 of pte (tang et al. 2015a) mi-f1 is 0.572. acc of pte (tang et al. 2015a) acc is 0.604. ma-f1 of pte (tang et al. 2015a) ma-f1 is 0.684. mi-f1 of pte (tang et al. 2015a) mi-f1 is 0.695. acc of wsabie (yogatama et al. 2015) acc is 0.480. ma-f1 of wsabie (yogatama et al. 2015) ma-f1 is 0.679. mi-f1 of wsabie (yogatama et al. 2015) mi-f1 is 0.657. acc of wsabie (yogatama et al. 2015) acc is 0.404. ma-f1 of wsabie (yogatama et al. 2015) ma-f1 is 0.580. mi-f1 of wsabie (yogatama et al. 2015) mi-f1 is 0.527. acc of wsabie (yogatama et al. 2015) acc is 0.619. ma-f1 of wsabie (yogatama et al. 2015) ma-f1 is 0.670. mi-f1 of wsabie (yogatama et al. 2015) mi-f1 is 0.680. acc of afet-noco acc is 0.526. ma-f1 of afet-noco ma-f1 is 0.693. mi-f1 of afet-noco mi-f1 is 0.654. acc of afet-noco acc is 0.486. ma-f1 of afet-noco ma-f1 is 0.652. mi-f1 of afet-noco mi-f1 is 0.594. acc of afet-noco acc is 0.655. ma-f1 of afet-noco ma-f1 is 0.711. mi-f1 of afet-noco mi-f1 is 0.716. acc of afet-nopa acc is 0.513. ma-f1 of afet-nopa ma-f1 is 0.675. mi-f1 of afet-nopa mi-f1 is 0.642. acc of afet-nopa acc is 0.463. ma-f1 of afet-nopa ma-f1 is 0.637. mi-f1 of afet-nopa mi-f1 is 0.591. acc of afet-nopa acc is 0.669. ma-f1 of afet-nopa ma-f1 is 0.715. mi-f1 of afet-nopa mi-f1 is 0.724. acc of afet-coh acc is 0.433. ma-f1 of afet-coh ma-f1 is 0.583. mi-f1 of afet-coh mi-f1 is 0.551. acc of afet-coh acc is 0.521. ma-f1 of afet-coh ma-f1 is 0.680. mi-f1 of afet-coh mi-f1 is 0.609. acc of afet-coh acc is 0.657. ma-f1 of afet-coh ma-f1 is 0.703. mi-f1 of afet-coh mi-f1 is 0.712. acc of afet acc is 0.533. ma-f1 of afet ma-f1 is 0.693. mi-f1 of afet mi-f1 is 0.664. acc of afet acc is 0.551. ma-f1 of afet ma-f1 is 0.711. mi-f1 of afet mi-f1 is 0.647. acc of afet acc is 0.670. ma-f1 of afet ma-f1 is 0.727. mi-f1 of afet mi-f1 is 0.735.
table 4 shows breakdown of test results (% hits@1) on wikimovies for key-value memory networks using different knowledge representations. hits@1 of writer to movie kb is 97. hits@1 of writer to movie ie is 72. hits@1 of writer to movie doc is 91. hits@1 of tag to movie kb is 85. hits@1 of tag to movie ie is 35. hits@1 of tag to movie doc is 49. hits@1 of movie to year kb is 95. hits@1 of movie to year ie is 75. hits@1 of movie to year doc is 89. hits@1 of movie to writer kb is 95. hits@1 of movie to writer ie is 61. hits@1 of movie to writer doc is 64. hits@1 of movie to tags kb is 94. hits@1 of movie to tags ie is 47. hits@1 of movie to tags doc is 48. hits@1 of movie to language kb is 96. hits@1 of movie to language ie is 62. hits@1 of movie to language doc is 84. hits@1 of movie to imdb votes kb is 92. hits@1 of movie to imdb votes ie is 92. hits@1 of movie to imdb votes doc is 92. hits@1 of movie to imdb rating kb is 94. hits@1 of movie to imdb rating ie is 75. hits@1 of movie to imdb rating doc is 92. hits@1 of movie to genre kb is 97. hits@1 of movie to genre ie is 84. hits@1 of movie to genre doc is 86. hits@1 of movie to director kb is 93. hits@1 of movie to director ie is 76. hits@1 of movie to director doc is 79. hits@1 of movie to actors kb is 91. hits@1 of movie to actors ie is 64. hits@1 of movie to actors doc is 64. hits@1 of director to movie kb is 90. hits@1 of director to movie ie is 78. hits@1 of director to movie doc is 91. hits@1 of actor to movie kb is 93. hits@1 of actor to movie ie is 66. hits@1 of actor to movie doc is 83.
table 3 shows convergence t-values of paired t-tests comparing team-level partner differences (t dif fp) of first 3, 5, 7 minutes vs. t-value of pitch-min game1 is 2.474*. t-value of pitch-min game2 is -0.709. t-value of pitch-min game1 is 1.487. t-value of pitch-min game2 is -1.299. t-value of pitch-min game1 is 1.359. t-value of pitch-min game2 is -1.622. t-value of pitch-min game1 is 0.329. t-value of pitch-min game2 is -0.884. t-value of pitch-max game1 is 4.947*. t-value of pitch-max game2 is 1.260. t-value of pitch-max game1 is 1.892. t-value of pitch-max game2 is -0.468. t-value of pitch-max game1 is 1.348. t-value of pitch-max game2 is -0.424. t-value of pitch-max game1 is 0.457. t-value of pitch-max game2 is 0.627. t-value of pitch-mean game1 is -2.687*. t-value of pitch-mean game2 is 0.109. t-value of pitch-mean game1 is -2.900*. t-value of pitch-mean game2 is 0.417. t-value of pitch-mean game1 is -2.965*. t-value of pitch-mean game2 is -0.361. t-value of pitch-mean game1 is -1.905. t-value of pitch-mean game2 is -0.266. t-value of pitch-sd game1 is 1.364. t-value of pitch-sd game2 is 0.409. t-value of pitch-sd game1 is 1.919. t-value of pitch-sd game2 is 0.591. t-value of pitch-sd game1 is 1.807. t-value of pitch-sd game2 is 0.576. t-value of pitch-sd game1 is 1.271. t-value of pitch-sd game2 is 0.089. t-value of intensity-mean game1 is -0.275. t-value of intensity-mean game2 is -2.946*. t-value of intensity-mean game1 is -0.454. t-value of intensity-mean game2 is -2.245*. t-value of intensity-mean game1 is -0.229. t-value of intensity-mean game2 is -1.825. t-value of intensity-mean game1 is -0.360. t-value of intensity-mean game2 is -1.540. t-value of intensity-min game1 is 0.595. t-value of intensity-min game2 is -3.188*. t-value of intensity-min game1 is -0.136. t-value of intensity-min game2 is -4.335*. t-value of intensity-min game1 is 0.009. t-value of intensity-min game2 is -3.317*. t-value of intensity-min game1 is -0.972. t-value of intensity-min game2 is -3.324*. t-value of intensity-max game1 is 0.328. t-value of intensity-max game2 is 0.327. t-value of intensity-max game1 is -0.731. t-value of intensity-max game2 is 1.081. t-value of intensity-max game1 is -0.140. t-value of intensity-max game2 is 0.511. t-value of intensity-max game1 is -0.222. t-value of intensity-max game2 is 0.469. t-value of shimmer-local game1 is 2.896*. t-value of shimmer-local game2 is -0.476. t-value of shimmer-local game1 is 3.396*. t-value of shimmer-local game2 is -1.941. t-value of shimmer-local game1 is 3.006*. t-value of shimmer-local game2 is -1.704. t-value of shimmer-local game1 is 2.794*. t-value of shimmer-local game2 is -0.914. t-value of jitter-local game1 is 3.205*. t-value of jitter-local game2 is 0.725. t-value of jitter-local game1 is 2.796*. t-value of jitter-local game2 is 0.242. t-value of jitter-local game1 is 2.867*. t-value of jitter-local game2 is 0.469. t-value of jitter-local game1 is 2.973*. t-value of jitter-local game2 is 0.260.
table 1 shows results of the regression analysis regression of agreeableness safety is 0.39. regression of agreeableness fuel is -0.52. regression of agreeableness quality is -0.53. regression of agreeableness style is 0.54. regression of agreeableness price is 0.81. regression of agreeableness luxury is 0.004. regression of agreeableness perf is -0.62. regression of agreeableness durab is -0.27. regression of conscientiousness safety is -1.75. regression of conscientiousness fuel is -0.31. regression of conscientiousness quality is 0.80. regression of conscientiousness style is 0.29. regression of conscientiousness price is -0.01. regression of conscientiousness luxury is 0.27. regression of conscientiousness perf is 0.83. regression of conscientiousness durab is -0.12. regression of extroversion safety is 0.69. regression of extroversion fuel is -0.71. regression of extroversion quality is 0.008. regression of extroversion style is -0.25. regression of extroversion price is -0.37. regression of extroversion luxury is 0.48. regression of extroversion perf is -0.07. regression of extroversion durab is 0.224. regression of neurotisim safety is 1.08. regression of neurotisim fuel is -0.01. regression of neurotisim quality is -0.46. regression of neurotisim style is -0.11. regression of neurotisim price is -0.32. regression of neurotisim luxury is -0.07. regression of neurotisim perf is 0.18. regression of neurotisim durab is -0.28. regression of openness safety is 1.59. regression of openness fuel is -0.05. regression of openness quality is 0.01. regression of openness style is -0.99. regression of openness price is 0.36. regression of openness luxury is -0.53. regression of openness perf is -0.46. regression of openness durab is 0.07. regression of conservation safety is 1.99. regression of conservation fuel is -0.99. regression of conservation quality is -0.66. regression of conservation style is 0.84. regression of conservation price is -1.72. regression of conservation luxury is 0.21. regression of conservation perf is 0.38. regression of conservation durab is -0.03. regression of hedonism safety is 1.47. regression of hedonism fuel is -0.15. regression of hedonism quality is -0.69. regression of hedonism style is 0.16. regression of hedonism price is 0.51. regression of hedonism luxury is -0.06. regression of hedonism perf is -0.82. regression of hedonism durab is -0.43. regression of openness to change safety is -2.15. regression of openness to change fuel is 0.08. regression of openness to change quality is 0.58. regression of openness to change style is 0.48. regression of openness to change price is -1.99. regression of openness to change luxury is -0.38. regression of openness to change perf is 2.29*. regression of openness to change durab is 1.07. regression of self-enhancement safety is -1.39. regression of self-enhancement fuel is -1.12. regression of self-enhancement quality is 0.58. regression of self-enhancement style is 0.47. regression of self-enhancement price is -0.31. regression of self-enhancement luxury is 2.41. regression of self-enhancement perf is 0.77. regression of self-enhancement durab is -1.41. regression of self-transcendence safety is 1.33. regression of self-transcendence fuel is 2.37. regression of self-transcendence quality is 1.36. regression of self-transcendence style is -2.47. regression of self-transcendence price is -0.91. regression of self-transcendence luxury is -1.01. regression of self-transcendence perf is -0.33. regression of self-transcendence durab is -0.32.
table 9 shows performance of different feature groups for alignment. p of - p is 39.55. r of - r is 14.59. f1 of - f1 is 21.32. p of +lexical p is 50.75. r of +lexical r is 26.02. f1 of +lexical f1 is 34.40. p of +syntactic p is 62.31. r of +syntactic r is 31.47. f1 of +syntactic f1 is 41.82. p of +sentence p is 62.33. r of +sentence r is 31.41. f1 of +sentence f1 is 41.53.
table 4 shows evaluation results on the neel-test and tacl datasets for different systems. p of ntel-nonstruct p is 80.0. r of ntel-nonstruct r is 68.0. f1 of ntel-nonstruct f1 is 73.5. p of ntel-nonstruct p is 64.7. r of ntel-nonstruct r is 62.3. f1 of ntel-nonstruct f1 is 63.5. avg. f1 of ntel-nonstruct avg. f1 is 68.5. p of ntel p is 82.8. r of ntel r is 69.3. f1 of ntel f1 is 75.4. p of ntel p is 68.0. r of ntel r is 66.0. f1 of ntel f1 is 67.0. avg. f1 of ntel avg. f1 is 71.2. p of ntel user-entity p is 82.3. r of ntel user-entity r is 71.8. f1 of ntel user-entity f1 is 76.7. p of ntel user-entity p is 66.9. r of ntel user-entity r is 68.7. f1 of ntel user-entity f1 is 67.8. avg. f1 of ntel user-entity avg. f1 is 72.2. p of ntel mention-entity p is 80.2. r of ntel mention-entity r is 75.8. f1 of ntel mention-entity f1 is 77.9. p of ntel mention-entity p is 66.9. r of ntel mention-entity r is 69.3. f1 of ntel mention-entity f1 is 68.1. avg. f1 of ntel mention-entity avg. f1 is 73.0. p of ntel user-entity mention-entity p is 81.9. r of ntel user-entity mention-entity r is 75.6. f1 of ntel user-entity mention-entity f1 is 78.6. p of ntel user-entity mention-entity p is 69.0. r of ntel user-entity mention-entity r is 69.0. f1 of ntel user-entity mention-entity f1 is 69.0. avg. f1 of ntel user-entity mention-entity avg. f1 is 73.8. p of s-mart p is 80.2. r of s-mart r is 75.4. f1 of s-mart f1 is 77.7. p of s-mart p is 60.1. r of s-mart r is 67.7. f1 of s-mart f1 is 63.6. avg. f1 of s-mart avg. f1 is 70.7.
table 6 shows nist evaluations for uyghur. f1 of lample et al. (2016) f1 is 37.1. f1 of our best transfer model f1 is 51.2.
table 3 shows lms performance on the ltcb test set. perplexity of kn model is 239. perplexity of kn model is 156. perplexity of kn model is 132. perplexity of kn+cache model is 188. perplexity of kn+cache model is 127. perplexity of kn+cache model is 109. perplexity of ffnn [m*200]-600-600-80k model is 235. perplexity of ffnn [m*200]-600-600-80k model is 150. perplexity of ffnn [m*200]-600-600-80k model is 114. perplexity of fofe [m*200]-600-600-80k model is 112. perplexity of fofe [m*200]-600-600-80k model is 107. perplexity of fofe [m*200]-600-600-80k model is 100. perplexity of rnn [600]-r600-80k model is 85. perplexity of rnn [600]-r600-80k model is 85. perplexity of rnn [600]-r600-80k model is 85. perplexity of lstm [200]-r600-80k model is 66. perplexity of lstm [200]-r600-80k model is 66. perplexity of lstm [200]-r600-80k model is 66. perplexity of lstm [200]-r600-r600-80k model is 61. perplexity of lstm [200]-r600-r600-80k model is 61. perplexity of lstm [200]-r600-r600-80k model is 61. perplexity of lsrc [200]-r600-80k model is 63. perplexity of lsrc [200]-r600-80k model is 63. perplexity of lsrc [200]-r600-80k model is 63. perplexity of lsrc [200]-r600-600-80k model is 59. perplexity of lsrc [200]-r600-600-80k model is 59. perplexity of lsrc [200]-r600-600-80k model is 59.
table 2 shows results on our subset of the pascal-50s and pascal-context-50s datasets. instance-level jaccard index of deeplab-crf instance-level jaccard index is 66.83. instance-level.1 jaccard index of deeplab-crf instance-level.1 jaccard index is 43.94. ppar acc. of stanford parser ppar acc. is 62.42. ppar acc. of stanford parser ppar acc. is 50.75. average of average average is 64.63. average of average average is 47.345. ppar acc. of domain adaptation ppar acc. is 72.08. ppar acc. of domain adaptation ppar acc. is 58.32. instance-level jaccard index of ours cascade instance-level jaccard index is 67.56. ppar acc. of ours cascade ppar acc. is 75.00. average of ours cascade average is 71.28. instance-level.1 jaccard index of ours cascade instance-level.1 jaccard index is 43.94. ppar acc. of ours cascade ppar acc. is 63.58. average of ours cascade average is 53.76. instance-level jaccard index of ours mediator instance-level jaccard index is 67.58. ppar acc. of ours mediator ppar acc. is 80.33. average of ours mediator average is 73.96. instance-level.1 jaccard index of ours mediator instance-level.1 jaccard index is 43.94. ppar acc. of ours mediator ppar acc. is 63.58. average of ours mediator average is 53.76. instance-level jaccard index of oracle instance-level jaccard index is 69.96. ppar acc. of oracle ppar acc. is 96.50. average of oracle average is 83.23. instance-level.1 jaccard index of oracle instance-level.1 jaccard index is 49.21. ppar acc. of oracle ppar acc. is 75.75. average of oracle average is 62.48.
table 4 shows results on part-of-speech tagging. accuracy (%) of charcnn accuracy (%) is 97.02. accuracy (%) of charlstm accuracy (%) is 96.90. accuracy (%) of charagram accuracy (%) is 96.99. accuracy (%) of charagram (2-layer) accuracy (%) is 97.10.
table 1 shows translation results (bleu score) for different translation methods. bleu of moses mt03 is 30.30. bleu of moses mt04 is 31.04. bleu of moses mt05 is 28.19. bleu of moses mt06 is 30.04. bleu of rnnsearch mt03 is 28.38. bleu of rnnsearch mt04 is 30.85. bleu of rnnsearch mt05 is 26.78. bleu of rnnsearch mt06 is 29.27. bleu of rnnsearch-mono-sl (25%) mt03 is 29.65. bleu of rnnsearch-mono-sl (25%) mt04 is 31.92. bleu of rnnsearch-mono-sl (25%) mt05 is 28.65. bleu of rnnsearch-mono-sl (25%) mt06 is 29.86. bleu of rnnsearch-mono-sl (50%) mt03 is 32.43. bleu of rnnsearch-mono-sl (50%) mt04 is 33.16. bleu of rnnsearch-mono-sl (50%) mt05 is 30.43. bleu of rnnsearch-mono-sl (50%) mt06 is 32.35. bleu of rnnsearch-mono-sl (75%) mt03 is 30.24. bleu of rnnsearch-mono-sl (75%) mt04 is 31.18. bleu of rnnsearch-mono-sl (75%) mt05 is 29.33. bleu of rnnsearch-mono-sl (75%) mt06 is 28.82. bleu of rnnsearch-mono-sl (100%) mt03 is 29.97. bleu of rnnsearch-mono-sl (100%) mt04 is 30.78. bleu of rnnsearch-mono-sl (100%) mt05 is 26.45. bleu of rnnsearch-mono-sl (100%) mt06 is 28.06. bleu of rnnsearch-mono-mtl (25%) mt03 is 31.68. bleu of rnnsearch-mono-mtl (25%) mt04 is 32.51. bleu of rnnsearch-mono-mtl (25%) mt05 is 29.8. bleu of rnnsearch-mono-mtl (25%) mt06 is 31.29. bleu of rnnsearch-mono-mtl (50%) mt03 is 33.38. bleu of rnnsearch-mono-mtl (50%) mt04 is 34.3. bleu of rnnsearch-mono-mtl (50%) mt05 is 31.57. bleu of rnnsearch-mono-mtl (50%) mt06 is 33.4. bleu of rnnsearch-mono-mtl (75%) mt03 is 31.69. bleu of rnnsearch-mono-mtl (75%) mt04 is 32.83. bleu of rnnsearch-mono-mtl (75%) mt05 is 28.17. bleu of rnnsearch-mono-mtl (75%) mt06 is 30.26. bleu of rnnsearch-mono-mtl (100%) mt03 is 30.31. bleu of rnnsearch-mono-mtl (100%) mt04 is 30.62. bleu of rnnsearch-mono-mtl (100%) mt05 is 27.23. bleu of rnnsearch-mono-mtl (100%) mt06 is 28.85. bleu of rnnsearch-mono-autoencoder (50%) mt03 is 31.55. bleu of rnnsearch-mono-autoencoder (50%) mt04 is 32.07. bleu of rnnsearch-mono-autoencoder (50%) mt05 is 28.19. bleu of rnnsearch-mono-autoencoder (50%) mt06 is 30.85. bleu of rnnsearch-mono-autoencoder (100%) mt03 is 27.81. bleu of rnnsearch-mono-autoencoder (100%) mt04 is 30.32. bleu of rnnsearch-mono-autoencoder (100%) mt05 is 25.84. bleu of rnnsearch-mono-autoencoder (100%) mt06 is 27.73.
table 2 shows translation results (bleu score) for different translation methods in large-scale training data. bleu of rnnsearch mt03 is 35.18. bleu of rnnsearch mt04 is 36.20. bleu of rnnsearch mt05 is 33.21. bleu of rnnsearch mt06 is 32.86. bleu of rnnsearch-mono-mtl (50%) mt03 is 36.32. bleu of rnnsearch-mono-mtl (50%) mt04 is 37.51. bleu of rnnsearch-mono-mtl (50%) mt05 is 35.08. bleu of rnnsearch-mono-mtl (50%) mt06 is 34.26. bleu of rnnsearch-mono-mtl (100%) mt03 is 35.75. bleu of rnnsearch-mono-mtl (100%) mt04 is 36.74. bleu of rnnsearch-mono-mtl (100%) mt05 is 34.23. bleu of rnnsearch-mono-mtl (100%) mt06 is 33.52.
table 4 shows best results in restricted setting with added unrestricted language model for original (2014) and extended (2014-10) conll test set (trained with public data only). prec. of - prec. is 48.97. recall of - recall is 26.03. m2 of - m2 is 41.63. prec. of - prec..1 is 69.29. recall of - recall is 31.35. m2 of - m2 is 55.78. prec. of +cclm prec. is 58.91. recall of +cclm recall is 25.05. m2 of +cclm m2 is 46.37. prec. of +cclm prec..1 is 77.17. recall of +cclm recall is 29.38. m2 of +cclm m2 is 58.23. prec. of - prec. is 50.94. recall of - recall is 26.21. m2 of - m2 is 42.85. prec. of - prec..1 is 71.21. recall of - recall is 31.70. m2 of - m2 is 57.00. prec. of +cclm prec. is 59.98. recall of +cclm recall is 28.17. m2 of +cclm m2 is 48.93. prec. of +cclm prec..1 is 79.98. recall of +cclm recall is 32.76. m2 of +cclm m2 is 62.08. prec. of - prec. is 57.99. recall of - recall is 25.11. m2 of - m2 is 45.95. prec. of - prec..1 is 76.61. recall of - recall is 29.74. m2 of - m2 is 58.25. prec. of +cclm prec. is 61.27. recall of +cclm recall is 27.98. m2 of +cclm m2 is 49.49. prec. of +cclm prec..1 is 80.93. recall of +cclm recall is 32.47. m2 of +cclm m2 is 62.33.
table 3 shows our transfer method applied to re-scoring output nbest lists from the sbmt system. bleu of none hausa is 23.7. bleu of none turkish is 20.4. bleu of none uzbek is 17.9. bleu of none urdu is 17.9. bleu of nmt hausa is 24.5. bleu of nmt turkish is 21.4. bleu of nmt uzbek is 19.5. bleu of nmt urdu is 18.2. bleu of xfer hausa is 24.8. bleu of xfer turkish is 21.8. bleu of xfer uzbek is 19.5. bleu of xfer urdu is 19.1. bleu of lm hausa is 23.6. bleu of lm turkish is 21.1. bleu of lm uzbek is 17.9. bleu of lm urdu is 18.2.
table 2 shows results of the ablation study. map of - map is 54.51. avgrec of - avgrec is 60.93. mrr of - mrr is 62.94. map of - lexical similarity map is 45.89. avgrec of - lexical similarity avgrec is 51.54. mrr of - lexical similarity mrr is 53.29. ∆map of - lexical similarity ∆map is -8.62. map of - domain-specific map is 48.48. avgrec of - domain-specific avgrec is 50.46. mrr of - domain-specific mrr is 53.78. ∆map of - domain-specific ∆map is -6.03. map of - distributed rep. map is 51.17. avgrec of - distributed rep. avgrec is 56.63. mrr of - distributed rep. mrr is 56.91. ∆map of - distributed rep. ∆map is -3.34. map of - map is 52.19. avgrec of - avgrec is 58.23. mrr of - mrr is 59.95. ∆map of - ∆map is -2.32.
table 2 shows human evaluation results on pairwise-comparisons between full and -syn, and full and human, on startest and cartoon datasets. pairwise-comparisons of - startest is 65.0. pairwise-comparisons of - cartoon is 57.9. pairwise-comparisons of -syn startest is 35.0. pairwise-comparisons of -syn cartoon is 42.1. pairwise-comparisons of - startest is 68.8. pairwise-comparisons of - cartoon is 69.4. pairwise-comparisons of -sem startest is 31.2. pairwise-comparisons of -sem cartoon is 30.6. pairwise-comparisons of - startest is 17.9. pairwise-comparisons of - cartoon is 10.0. pairwise-comparisons of - startest is 82.1. pairwise-comparisons of - cartoon is 90.0.
table 3 shows human evaluation results for full, -syn, -semand human on thematicity, coherence and solvability on startest. thematicity of - thematicity is 3.7. coherence of - coherence is 3.175. solvability of - solvability is 4.025. thematicity of - thematicity is 3.7. coherence of - coherence is 3.025. solvability of - solvability is 3.9. thematicity of -syn thematicity is 3.375. coherence of -syn coherence is 3.075. solvability of -syn solvability is 3.825. thematicity of -sem thematicity is 3.325. coherence of -sem coherence is 2.65. solvability of -sem solvability is 3.7.
table 1 shows classification performance on sst2. accuracy (%) of cnn (kim 2014) accuracy (%) is 86.6. accuracy (%) of cnn+rel q accuracy (%) is 87.8. accuracy (%) of cnn+rel p accuracy (%) is 87.1. accuracy (%) of cnn+rel+lex q accuracy (%) is 88.0. accuracy (%) of cnn+rel+lex p accuracy (%) is 87.2. accuracy (%) of mc-cnn (kim 2014) accuracy (%) is 86.8. accuracy (%) of tensor-cnn (lei et al. 2015) accuracy (%) is 87.0. accuracy (%) of cnn+but-q (hu et al. 2016) accuracy (%) is 87.1. accuracy (%) of cnn (kim 2014) accuracy (%) is 87.2. accuracy (%) of tree-lstm (tai et al. 2015) accuracy (%) is 88.0. accuracy (%) of mc-cnn (kim 2014) accuracy (%) is 88.1. accuracy (%) of cnn+but-q (hu et al. 2016) accuracy (%) is 89.2. accuracy (%) of mvcnn (yin and schutze  2015) accuracy (%) is 89.4.
table 2 shows classification performance on the cr dataset. accuracy (%) of cnn (kim, 2014) accuracy (%) is 84.1±0.2. accuracy (%) of cnn+rel accuracy (%) is q: 85.0±0.2, p: 84.7±0.2. accuracy (%) of cnn+rel+lex accuracy (%) is q: 85.3±0.3, p: 85.0±0.2. accuracy (%) of mc-cnn (kim, 2014) accuracy (%) is 85.0. accuracy (%) of bi-rnn (lai et al. 2015) accuracy (%) is 82.6. accuracy (%) of crf-pr (yang and cardie, 2014) accuracy (%) is 82.7. accuracy (%) of adasent (zhao et al. 2015) accuracy (%) is 86.3.
table 5 shows evaluation results on the word to sense similarity test dataset of the semeval-14 task on cross-level semantic similarity, according to pearson (r × 100) and spearman (ρ × 100) correlations. r of deconf* r is 36.4. rho of deconf* rho is 37.6. r of deconf* r is 36.8. rho of deconf* rho is 38.8. r of deconf* r is 34.9. rho of deconf* rho is 35.6. r of deconf* r is 37.5. rho of deconf* rho is 39.3. r of rothe and schutze (2015)* r is 34.0. rho of rothe and schutze (2015)* rho is 33.8. r of rothe and schutze (2015)* r is 34.1. rho of rothe and schutze (2015)* rho is 33.6. r of rothe and schutze (2015)* r is 33.4. rho of rothe and schutze (2015)* rho is 32.0. r of rothe and schutze (2015)* r is 35.4. rho of rothe and schutze (2015)* rho is 34.9. r of iacobacci et al. (2015)* r is 19.1. rho of iacobacci et al. (2015)* rho is 21.5. r of iacobacci et al. (2015)* r is 21.3. rho of iacobacci et al. (2015)* rho is 24.2. r of iacobacci et al. (2015)* r is 22.7. rho of iacobacci et al. (2015)* rho is 21.7. r of iacobacci et al. (2015)* r is 19.5. rho of iacobacci et al. (2015)* rho is 21.1. r of chen et al. (2014)* r is 17.7. rho of chen et al. (2014)* rho is 18.0. r of chen et al. (2014)* r is 17.2. rho of chen et al. (2014)* rho is 16.8. r of chen et al. (2014)* r is 27.7. rho of chen et al. (2014)* rho is 26.7. r of chen et al. (2014)* r is 17.9. rho of chen et al. (2014)* rho is 18.8. r of deconf r is 35.5. rho of deconf rho is 36.4. r of deconf r is 36.2. rho of deconf rho is 38.0. r of deconf r is 34.9. rho of deconf rho is 35.6. r of deconf r is 36.8. rho of deconf rho is 38.4. r of pilehvar and navigli (2015) r is 19.4. rho of pilehvar and navigli (2015) rho is 23.8. r of pilehvar and navigli (2015) r is 21.2. rho of pilehvar and navigli (2015) rho is 26.0. r of iacobacci et al. (2015) r is 19.0. rho of iacobacci et al. (2015) rho is 21.5. r of iacobacci et al. (2015) r is 20.9. rho of iacobacci et al. (2015) rho is 23.2. r of iacobacci et al. (2015) r is 22.3. rho of iacobacci et al. (2015) rho is 20.6. r of iacobacci et al. (2015) r is 19.2. rho of iacobacci et al. (2015) rho is 20.4.
table 1 shows example feature spaces for the lexemes white and clothes extracted from the dependency tree of figure 1. co-occurrence count of :shoes co-occurrence count is 1. co-occurrence count of - co-occurrence count is 1. co-occurrence count of dobj:bought co-occurrence count is 1. co-occurrence count of - co-occurrence count is 1. co-occurrence count of dobj:folded co-occurrence count is 1. co-occurrence count of - co-occurrence count is 1. co-occurrence count of dobj:nsubj:we co-occurrence count is 1. co-occurrence count of - co-occurrence count is 1.
table 3 shows effect of the magnitude of the shift parameter k in sppmi on the word similarity tasks. similarity of k = 1 without di is 0.54. similarity of k = 1 with di is 0.52. similarity of k = 1 without di is 0.31. similarity of k = 1 with di is 0.30. similarity of k = 1 without di is 0.34. similarity of k = 1 with di is 0.27. similarity of k = 1 without di is 0.62. similarity of k = 1 with di is 0.60. similarity of k = 5 without di is 0.64. similarity of k = 5 with di is 0.65. similarity of k = 5 without di is 0.35. similarity of k = 5 with di is 0.36. similarity of k = 5 without di is 0.56. similarity of k = 5 with di is 0.51. similarity of k = 5 without di is 0.74. similarity of k = 5 with di is 0.73. similarity of k = 10 without di is 0.63. similarity of k = 10 with di is 0.66. similarity of k = 10 without di is 0.35. similarity of k = 10 with di is 0.36. similarity of k = 10 without di is 0.56. similarity of k = 10 with di is 0.55. similarity of k = 10 without di is 0.75. similarity of k = 10 with di is 0.74. similarity of k = 40 without di is 0.63. similarity of k = 40 with di is 0.68. similarity of k = 40 without di is 0.30. similarity of k = 40 with di is 0.32. similarity of k = 40 without di is 0.55. similarity of k = 40 with di is 0.61. similarity of k = 40 without di is 0.75. similarity of k = 40 with di is 0.76. similarity of k = 100 without di is 0.61. similarity of k = 100 with di is 0.67. similarity of k = 100 without di is 0.26. similarity of k = 100 with di is 0.29. similarity of k = 100 without di is 0.47. similarity of k = 100 with di is 0.60. similarity of k = 100 without di is 0.71. similarity of k = 100 with di is 0.72.
table 4 shows neighbour retrieval function comparison. similarity of men no distributional inference is 0.63. similarity of men density window is 0.67. similarity of men static top n is 0.68. similarity of men wordnet is 0.63. similarity of simlex-999 no distributional inference is 0.3. similarity of simlex-999 density window is 0.32. similarity of simlex-999 static top n is 0.32. similarity of simlex-999 wordnet is 0.38. similarity of wordsim-353 (rel) no distributional inference is 0.55. similarity of wordsim-353 (rel) density window is 0.62. similarity of wordsim-353 (rel) static top n is 0.61. similarity of wordsim-353 (rel) wordnet is 0.56. similarity of wordsim-353 (sub) no distributional inference is 0.75. similarity of wordsim-353 (sub) density window is 0.78. similarity of wordsim-353 (sub) static top n is 0.76. similarity of wordsim-353 (sub) wordnet is 0.77. similarity of men* no distributional inference is 0.71. similarity of men* density window is 0.71. similarity of men* static top n is 0.71. similarity of men* wordnet is 0.71. similarity of simlex-999 no distributional inference is 0.3. similarity of simlex-999 density window is 0.29. similarity of simlex-999 static top n is 0.3. similarity of simlex-999 wordnet is 0.36. similarity of wordsim-353 (rel) no distributional inference is 0.6. similarity of wordsim-353 (rel) density window is 0.64. similarity of wordsim-353 (rel) static top n is 0.64. similarity of wordsim-353 (rel) wordnet is 0.52. similarity of wordsim-353 (sub) no distributional inference is 0.7. similarity of wordsim-353 (sub) density window is 0.73. similarity of wordsim-353 (sub) static top n is 0.72. similarity of wordsim-353 (sub) wordnet is 0.67.
table 6 shows neighbour retrieval function. similarity of adjective-noun intersection is 0.10. similarity of adjective-noun union is 0.41. similarity of adjective-noun intersection is 0.31. similarity of adjective-noun union is 0.39. similarity of adjective-noun intersection is 0.25. similarity of adjective-noun union is 0.40. similarity of adjective-noun intersection is 0.12. similarity of adjective-noun union is 0.41. similarity of noun-noun intersection is 0.18. similarity of noun-noun union is 0.42. similarity of noun-noun intersection is 0.34. similarity of noun-noun union is 0.38. similarity of noun-noun intersection is 0.37. similarity of noun-noun union is 0.45. similarity of noun-noun intersection is 0.24. similarity of noun-noun union is 0.36. similarity of verb-object intersection is 0.17. similarity of verb-object union is 0.36. similarity of verb-object intersection is 0.36. similarity of verb-object union is 0.36. similarity of verb-object intersection is 0.34. similarity of verb-object union is 0.35. similarity of verb-object intersection is 0.25. similarity of verb-object union is 0.36. similarity of average intersection is 0.15. similarity of average union is 0.40. similarity of average intersection is 0.34. similarity of average union is 0.38. similarity of average intersection is 0.32. similarity of average union is 0.40. similarity of average intersection is 0.20. similarity of average union is 0.38.
table 7 shows results for the mitchell and lapata (2010) dataset. similarity of apt – union adjective-noun is 0.45 (0.45). similarity of apt – union noun-noun is 0.45 (0.43). similarity of apt – union verb-object is 0.38 (0.37). similarity of apt – union average is 0.43 (0.42). similarity of apt – intersect adjective-noun is 0.50 (0.38). similarity of apt – intersect noun-noun is 0.49 (0.44). similarity of apt – intersect verb-object is 0.43 (0.36). similarity of apt – intersect average is 0.47 (0.39). similarity of untyped vsm – addition adjective-noun is 0.46 (0.46). similarity of untyped vsm – addition noun-noun is 0.40 (0.41). similarity of untyped vsm – addition verb-object is 0.38 (0.33). similarity of untyped vsm – addition average is 0.41 (0.40). similarity of untyped vsm – multiplication adjective-noun is 0.46 (0.42). similarity of untyped vsm – multiplication noun-noun is 0.48 (0.45). similarity of untyped vsm – multiplication verb-object is 0.40 (0.39). similarity of untyped vsm – multiplication average is 0.45 (0.42). similarity of mitchell and lapata (2010) (untyped vsm & multiplication) adjective-noun is 0.46. similarity of mitchell and lapata (2010) (untyped vsm & multiplication) noun-noun is 0.49. similarity of mitchell and lapata (2010) (untyped vsm & multiplication) verb-object is 0.37. similarity of mitchell and lapata (2010) (untyped vsm & multiplication) average is 0.44. similarity of blacoe and lapata (2012) (untyped vsm & multiplication) adjective-noun is 0.48. similarity of blacoe and lapata (2012) (untyped vsm & multiplication) noun-noun is 0.50. similarity of blacoe and lapata (2012) (untyped vsm & multiplication) verb-object is 0.35. similarity of blacoe and lapata (2012) (untyped vsm & multiplication) average is 0.44. similarity of hashimoto et al. (2014) (pas-clblm & addnl) adjective-noun is 0.52. similarity of hashimoto et al. (2014) (pas-clblm & addnl) noun-noun is 0.46. similarity of hashimoto et al. (2014) (pas-clblm & addnl) verb-object is 0.45. similarity of hashimoto et al. (2014) (pas-clblm & addnl) average is 0.48. similarity of wieting et al. (2015) (paragram word embeddings & rnn) adjective-noun is 0.51. similarity of wieting et al. (2015) (paragram word embeddings & rnn) noun-noun is 0.40. similarity of wieting et al. (2015) (paragram word embeddings & rnn) verb-object is 0.50. similarity of wieting et al. (2015) (paragram word embeddings & rnn) average is 0.47. similarity of weir et al. (2016) (apt & union) adjective-noun is 0.45. similarity of weir et al. (2016) (apt & union) noun-noun is 0.42. similarity of weir et al. (2016) (apt & union) verb-object is 0.42. similarity of weir et al. (2016) (apt & union) average is 0.43.
table 2 shows vpe detection results (baseline f1, machine learning f1, ml f1 improvement) obtained with 5-fold cross validation. f1 of do baseline is 0.83. f1 of do ml is 0.89. f1 of do change is 0.06. f1 of be baseline is 0.34. f1 of be ml is 0.63. f1 of be change is 0.29. f1 of have baseline is 0.43. f1 of have ml is 0.75. f1 of have change is 0.32. f1 of modal baseline is 0.8. f1 of modal ml is 0.86. f1 of modal change is 0.06. f1 of to baseline is 0.76. f1 of to ml is 0.79. f1 of to change is 0.03. f1 of so baseline is 0.67. f1 of so ml is 0.86. f1 of so change is 0.19. f1 of all baseline is 0.71. f1 of all ml is 0.82. f1 of all change is 0.11.
table 3 shows results (precision, recall, f1) for vpe detection using the train-test split proposed by bos and spenader (2011). p of liu et al. (2016) p is 0.8022. r of liu et al. (2016) r is 0.6134. f1 of liu et al. (2016) f1 is 0.6952. p of this work p is 0.7574. r of this work r is 0.8655. f1 of this work f1 is 0.8078.
table 6 shows feature ablation results (feature set excluded, precision, recall, f1) on vpe detection; obtained with 5-fold cross validation. p of auxiliary p is 0.7982. r of auxiliary r is 0.7611. f1 of auxiliary f1 is 0.7781. p of lexical p is 0.6937. r of lexical r is 0.8408. f1 of lexical f1 is 0.7582. p of syntactic p is 0.7404. r of syntactic r is 0.733. f1 of syntactic f1 is 0.7343. p of none p is 0.8242. r of none r is 0.812. f1 of none f1 is 0.817.
table 7 shows feature ablation results (feature set excluded, precision, recall, f1) on antecedent identification; obtained with 5fold cross validation. accuracy of alignment accuracy is 0.6511. accuracy of np relation accuracy is 0.6428. accuracy of syntactic accuracy is 0.5495. accuracy of matching accuracy is 0.6504. accuracy of none accuracy is 0.6518.
table 1 shows 1-best supertagging results on both the dev and test sets. accuracy of c&c dev is 91.50. accuracy of c&c test is 92.02. accuracy of xu et al. (2015) dev is 93.07. accuracy of xu et al. (2015) test is 93.00. accuracy of xu et al. (2016) dev is 93.49. accuracy of xu et al. (2016) test is 93.52. accuracy of lewis et al. (2016) dev is 94.1. accuracy of lewis et al. (2016) test is 94.3. accuracy of vaswani et al. (2016) dev is 94.08. accuracy of vaswani et al. (2016) +lm +beam dev is 94.24. accuracy of vaswani et al. (2016) +lm +beam test is 94.50. accuracy of blstm dev is 94.11. accuracy of blstm test is 94.29. accuracy of blstm-local dev is 94.31. accuracy of blstm-local test is 94.46. accuracy of blstm-global dev is 94.22. accuracy of blstm-global test is 94.42.
table 4 shows parsing results on the dev (section 00) and test (section 23) sets with 100% coverage, with all lstm models using the blstm-local supertagging model. lp of c&c (normal-form) lp is 85.18. lr of c&c (normal-form) lr is 82.53. lf of c&c (normal-form) lf is 83.83. cat of c&c (normal-form) cat is 92.39. lp of c&c (normal-form) lp is 85.45. lr of c&c (normal-form) lr is 83.97. lf of c&c (normal-form) lf is 84.70. cat of c&c (normal-form) cat is 92.83. lp of c&c (dependency hybrid) lp is 86.07. lr of c&c (dependency hybrid) lr is 82.77. lf of c&c (dependency hybrid) lf is 84.39. cat of c&c (dependency hybrid) cat is 92.57. lp of c&c (dependency hybrid) lp is 86.24. lr of c&c (dependency hybrid) lr is 84.17. lf of c&c (dependency hybrid) lf is 85.19. cat of c&c (dependency hybrid) cat is 93.0. beam of zhang and clark (2011) - is 16. lp of zhang and clark (2011) lp is 87.15. lr of zhang and clark (2011) lr is 82.95. lf of zhang and clark (2011) lf is 85.0. cat of zhang and clark (2011) cat is 92.77. lp of zhang and clark (2011) lp is 87.43. lr of zhang and clark (2011) lr is 83.61. lf of zhang and clark (2011) lf is 85.48. cat of zhang and clark (2011) cat is 93.12. beam of xu et al. (2014) - is 128. lp of xu et al. (2014) lp is 86.29. lr of xu et al. (2014) lr is 84.09. lf of xu et al. (2014) lf is 85.18. cat of xu et al. (2014) cat is 92.75. lp of xu et al. (2014) lp is 87.03. lr of xu et al. (2014) lr is 85.08. lf of xu et al. (2014) lf is 86.04. cat of xu et al. (2014) cat is 93.1. beam of ambati et al. (2016) - is 16. lf of ambati et al. (2016) lf is 85.69. cat of ambati et al. (2016) cat is 93.02. lf of ambati et al. (2016) lf is 85.57. cat of ambati et al. (2016) cat is 92.86. beam of xu et al. (2016)-greedy - is 1. lp of xu et al. (2016)-greedy lp is 88.12. lr of xu et al. (2016)-greedy lr is 81.38. lf of xu et al. (2016)-greedy lf is 84.61. cat of xu et al. (2016)-greedy cat is 93.42. lp of xu et al. (2016)-greedy lp is 88.53. lr of xu et al. (2016)-greedy lr is 81.65. lf of xu et al. (2016)-greedy lf is 84.95. cat of xu et al. (2016)-greedy cat is 93.57. beam of xu et al. (2016)-xf1 - is 8. lp of xu et al. (2016)-xf1 lp is 88.20. lr of xu et al. (2016)-xf1 lr is 83.40. lf of xu et al. (2016)-xf1 lf is 85.73. cat of xu et al. (2016)-xf1 cat is 93.56. lp of xu et al. (2016)-xf1 lp is 88.74. lr of xu et al. (2016)-xf1 lr is 84.22. lf of xu et al. (2016)-xf1 lf is 86.42. cat of xu et al. (2016)-xf1 cat is 93.87. beam of lstm-greedy - is 1. lp of lstm-greedy lp is 89.43. lr of lstm-greedy lr is 83.86. lf of lstm-greedy lf is 86.56. cat of lstm-greedy cat is 94.47. lp of lstm-greedy lp is 89.75. lr of lstm-greedy lr is 84.10. lf of lstm-greedy lf is 86.83. cat of lstm-greedy cat is 94.63. beam of lstm-xf1 - is 1. lp of lstm-xf1 lp is 89.68. lr of lstm-xf1 lr is 85.29. lf of lstm-xf1 lf is 87.43. cat of lstm-xf1 cat is 94.41. lp of lstm-xf1 lp is 89.85. lr of lstm-xf1 lr is 85.51. lf of lstm-xf1 lf is 87.62. cat of lstm-xf1 cat is 94.53. beam of lstm-xf1 - is 8. lp of lstm-xf1 lp is 89.54. lr of lstm-xf1 lr is 85.46. lf of lstm-xf1 lf is 87.45. cat of lstm-xf1 cat is 94.39. lp of lstm-xf1 lp is 89.81. lr of lstm-xf1 lr is 85.81. lf of lstm-xf1 lf is 87.76. cat of lstm-xf1 cat is 94.57.
table 1 shows parsers’ performance in terms of accuracy and robustness. uas of malt ptb §23 is 89.58. robustness f1 of malt esl is 93.05. robustness f1 of malt mt is 76.26. uaf1 of malt tweebanktest is 77.48. robustness f1 of malt esl is 94.36. robustness f1 of malt mt is 80.66. uas of mate ptb §23 is 93.16. robustness f1 of mate esl is 93.24. robustness f1 of mate mt is 77.07. uaf1 of mate tweebanktest is 76.26. robustness f1 of mate esl is 91.83. robustness f1 of mate mt is 75.74. uas of mst ptb §23 is 91.17. robustness f1 of mst esl is 92.80. robustness f1 of mst mt is 76.51. uaf1 of mst tweebanktest is 73.99. robustness f1 of mst esl is 92.37. robustness f1 of mst mt is 77.71. uas of snn ptb §23 is 90.70. robustness f1 of snn esl is 93.15. robustness f1 of snn mt is 74.18. uaf1 of snn tweebanktest is 53.4. robustness f1 of snn esl is 88.90. robustness f1 of snn mt is 71.54. uas of syntaxnet ptb §23 is 93.04. robustness f1 of syntaxnet esl is 93.24. robustness f1 of syntaxnet mt is 76.39. uaf1 of syntaxnet tweebanktest is 75.75. robustness f1 of syntaxnet esl is 88.78. robustness f1 of syntaxnet mt is 81.87. uas of turbo ptb §23 is 92.84. robustness f1 of turbo esl is 93.72. robustness f1 of turbo mt is 77.79. uaf1 of turbo tweebanktest is 79.42. robustness f1 of turbo esl is 93.28. robustness f1 of turbo mt is 78.26. uaf1 of tweebo tweebanktest is 80.91. robustness f1 of tweebo esl is 93.39. robustness f1 of tweebo mt is 79.47. uas of yara ptb §23 is 93.09. robustness f1 of yara esl is 93.52. robustness f1 of yara mt is 73.15. uaf1 of yara tweebanktest is 78.06. robustness f1 of yara esl is 93.04. robustness f1 of yara mt is 75.83.
table 2 shows test smatch results.12 p of jamr p is 67.8. r of jamr r is 59.2. f of jamr f is 63.2. p of cky (artzi et al. 2015) p is 66.8. r of cky (artzi et al. 2015) r is 65.7. f of cky (artzi et al. 2015) f is 66.3. p of shift reduce p is 68.1. r of shift reduce r is 64.2. f of shift reduce f is 66.1. p of wang et al. (2015a) p is 72.0. r of wang et al. (2015a) r is 67.0. f of wang et al. (2015a) f is 70.0.
table 6 shows parsing performance on web queries uas of stanford uas is 0.694. las of stanford las is 0.602. uas of stanford uas is 0.670. las of stanford las is 0.568. uas of stanford uas is 0.834. las of stanford las is 0.799. uas of mstparser uas is 0.699. las of mstparser las is 0.616. uas of mstparser uas is 0.683. las of mstparser las is 0.691. uas of mstparser uas is 0.799. las of mstparser las is 0.766. uas of lstmparser uas is 0.700. las of lstmparser las is 0.608. uas of lstmparser uas is 0.679. las of lstmparser las is 0.578. uas of lstmparser uas is 0.827. las of lstmparser las is 0.790. uas of queryparser + label refinement uas is 0.829. las of queryparser + label refinement las is 0.769. uas of queryparser + label refinement uas is 0.824. las of queryparser + label refinement las is 0.761. uas of queryparser + label refinement uas is 0.858. las of queryparser + label refinement las is 0.818. uas of queryparser + word2vec uas is 0.843. las of queryparser + word2vec las is 0.788. uas of queryparser + word2vec uas is 0.843. las of queryparser + word2vec las is 0.784. uas of queryparser + word2vec uas is 0.838. las of queryparser + word2vec las is 0.812. uas of queryparser + label refinement + word2vec uas is 0.862. las of queryparser + label refinement + word2vec las is 0.804. uas of queryparser + label refinement + word2vec uas is 0.858. las of queryparser + label refinement + word2vec las is 0.795. uas of queryparser + label refinement + word2vec uas is 0.883. las of queryparser + label refinement + word2vec las is 0.854.
table 2 shows experimental results on different methods using descriptions. rouge-1 of ilp-ext (banerjee et al. 2015) rouge-1 is 0.308. rouge-2 of ilp-ext (banerjee et al. 2015) rouge-2 is 0.112. rouge-su4 of ilp-ext (banerjee et al. 2015) rouge-su4 is 0.091. rouge-1 of ilp-abs (banerjee et al. 2015) rouge-1 is 0.361. rouge-2 of ilp-abs (banerjee et al. 2015) rouge-2 is 0.158. rouge-su4 of ilp-abs (banerjee et al. 2015) rouge-su4 is 0.12. rouge-1 of our approach trem rouge-1 is 0.405. rouge-2 of our approach trem rouge-2 is 0.207. rouge-su4 of our approach trem rouge-su4 is 0.148. rouge-1 of w/o sr rouge-1 is 0.393. rouge-2 of w/o sr rouge-2 is 0.189. rouge-su4 of w/o sr rouge-su4 is 0.144. rouge-1 of w/o cc rouge-1 is 0.383. rouge-2 of w/o cc rouge-2 is 0.171. rouge-su4 of w/o cc rouge-su4 is 0.132. rouge-1 of w/o sr&cc (summarization only) rouge-1 is 0.374. rouge-2 of w/o sr&cc (summarization only) rouge-2 is 0.168. rouge-su4 of w/o sr&cc (summarization only) rouge-su4 is 0.129.
table 3 shows spam and nonspam review detection results in the doctor, hotel, and restaurant review domains. accuracy of smtl-llr doctor is 85.4%. accuracy of smtl-llr hotel is 88.7%. accuracy of smtl-llr restaurant is 87.5%. accuracy of smtl-llr average is 87.2%. accuracy of mtl-lr doctor is 83.1%. accuracy of mtl-lr hotel is 86.7%. accuracy of mtl-lr restaurant is 85.7%. accuracy of mtl-lr average is 85.2%. accuracy of mtrl doctor is 82.0%. accuracy of mtrl hotel is 85.4%. accuracy of mtrl restaurant is 84.7%. accuracy of mtrl average is 84.0%. accuracy of tsvm doctor is 80.6%. accuracy of tsvm hotel is 84.2%. accuracy of tsvm restaurant is 83.8%. accuracy of tsvm average is 82.9%. accuracy of lr doctor is 79.8%. accuracy of lr hotel is 83.5%. accuracy of lr restaurant is 83.1%. accuracy of lr average is 82.1%. accuracy of svm doctor is 79.0%. accuracy of svm hotel is 83.5%. accuracy of svm restaurant is 82.9%. accuracy of svm average is 81.8%. accuracy of pu doctor is 68.5%. accuracy of pu hotel is 75.4%. accuracy of pu restaurant is 74.0%. accuracy of pu average is 72.6%.
table 5 shows evaluation of annotators performance kappa of true-positive expert 1 is 130. kappa of true-positive expert 2 is 99. kappa of true-positive expert 3 is 125. kappa of true-negative expert 1 is 161. kappa of true-negative expert 2 is 164. kappa of true-negative expert 3 is 166. kappa of false-positive expert 1 is 20. kappa of false-positive expert 2 is 51. kappa of false-positive expert 3 is 25. kappa of false-negative expert 1 is 14. kappa of false-negative expert 2 is 11. kappa of false-negative expert 3 is 9. kappa of precision expert 1 is 86.67. kappa of precision expert 2 is 66.00. kappa of precision expert 3 is 83.33. kappa of recall expert 1 is 90.27. kappa of recall expert 2 is 90.00. kappa of recall expert 3 is 93.28. kappa of accuracy expert 1 is 89.54. kappa of accuracy expert 2 is 80.92. kappa of accuracy expert 3 is 89.54. kappa of f-measure expert 1 is 88.43. kappa of f-measure expert 2 is 76.15. kappa of f-measure expert 3 is 88.03.
table 6 shows evaluation of nlds r (%) of ub r (%) is 92.38. p (%) of ub p (%) is 86.67. a (%) of ub a (%) is 89.54. f1 (%) of ub f1 (%) is 88.43. r (%) of sts r (%) is 100.0. p (%) of sts p (%) is 46.15. a (%) of sts a (%) is 46.15. f1 (%) of sts f1 (%) is 63.16. r (%) of rae r (%) is 100.0. p (%) of rae p (%) is 46.4. a (%) of rae a (%) is 46.4. f1 (%) of rae f1 (%) is 63.39. r (%) of uni-gram r (%) is 11.11. p (%) of uni-gram p (%) is 35.29. a (%) of uni-gram a (%) is 52.8. f1 (%) of uni-gram f1 (%) is 16.9. r (%) of bi-gram r (%) is 44.44. p (%) of bi-gram p (%) is 61.54. a (%) of bi-gram a (%) is 64.0. f1 (%) of bi-gram f1 (%) is 51.61. r (%) of tri-gram r (%) is 50.0. p (%) of tri-gram p (%) is 62.79. a (%) of tri-gram a (%) is 65.6. f1 (%) of tri-gram f1 (%) is 55.67. r (%) of pos r (%) is 77.78. p (%) of pos p (%) is 72.77. a (%) of pos a (%) is 78.4. f1 (%) of pos f1 (%) is 76.52. r (%) of lexical r (%) is 85.18. p (%) of lexical p (%) is 59.74. a (%) of lexical a (%) is 68.8. f1 (%) of lexical f1 (%) is 70.23. r (%) of flickr r (%) is 48.96. p (%) of flickr p (%) is 94.0. a (%) of flickr a (%) is 74.0. f1 (%) of flickr f1 (%) is 64.38. r (%) of nlds r (%) is 80.95. p (%) of nlds p (%) is 96.22. a (%) of nlds a (%) is 88.8. f1 (%) of nlds f1 (%) is 87.93.
table 3 shows results ilci corpus (% bleu). bleu of ben-hin w is 31.23. bleu of ben-hin wx is 32.79. bleu of ben-hin m is 32.17. bleu of ben-hin mx is 32.32. bleu of ben-hin c is 27.95. bleu of ben-hin o is 33.46. bleu of pan-hin w is 68.96. bleu of pan-hin wx is 71.71. bleu of pan-hin m is 71.29. bleu of pan-hin mx is 71.42. bleu of pan-hin c is 71.26. bleu of pan-hin o is 72.51. bleu of kok-mar w is 21.39. bleu of kok-mar wx is 21.90. bleu of kok-mar m is 22.81. bleu of kok-mar mx is 22.82. bleu of kok-mar c is 19.83. bleu of kok-mar o is 23.53. bleu of mal-tam w is 6.52. bleu of mal-tam wx is 7.01. bleu of mal-tam m is 7.61. bleu of mal-tam mx is 7.65. bleu of mal-tam c is 4.50. bleu of mal-tam o is 7.86. bleu of tel-mal w is 6.62. bleu of tel-mal wx is 6.94. bleu of tel-mal m is 7.86. bleu of tel-mal mx is 7.89. bleu of tel-mal c is 6.00. bleu of tel-mal o is 8.51. bleu of hin-mal w is 8.49. bleu of hin-mal wx is 8.77. bleu of hin-mal m is 9.23. bleu of hin-mal mx is 9.26. bleu of hin-mal c is 6.28. bleu of hin-mal o is 10.45. bleu of mal-hin w is 15.23. bleu of mal-hin wx is 16.26. bleu of mal-hin m is 17.08. bleu of mal-hin mx is 17.30. bleu of mal-hin c is 12.33. bleu of mal-hin o is 18.50.
table 3 shows results for our system and other participants in the semeval 2015 task 4: timeline. f1 of gplsiua 1 f1 is 22.35. f1 of gplsiua 1 f1 is 19.28. f1 of gplsiua 1 f1 is 33.59. p of gplsiua 1 p is 21.73. r of gplsiua 1 r is 30.46. f1 of gplsiua 1 f1 is 25.36. f1 of gplsiua 2 f1 is 20.47. f1 of gplsiua 2 f1 is 16.17. f1 of gplsiua 2 f1 is 29.90. p of gplsiua 2 p is 20.08. r of gplsiua 2 r is 26.00. f1 of gplsiua 2 f1 is 22.66. f1 of heideltoul 1 f1 is 19.62. f1 of heideltoul 1 f1 is 7.25. f1 of heideltoul 1 f1 is 20.37. p of heideltoul 1 p is 20.11. r of heideltoul 1 r is 14.76. f1 of heideltoul 1 f1 is 17.03. f1 of heideltoul 2 f1 is 16.50. f1 of heideltoul 2 f1 is 10.82. f1 of heideltoul 2 f1 is 25.89. p of heideltoul 2 p is 13.58. r of heideltoul 2 r is 28.23. f1 of heideltoul 2 f1 is 18.34. f1 of our system binary f1 is 17.99. f1 of our system binary f1 is 20.97. f1 of our system binary f1 is 34.95. p of our system binary p is 25.97. r of our system binary r is 24.79. f1 of our system binary f1 is 25.37. f1 of our system alignment f1 is 25.65. f1 of our system alignment f1 is 26.64. f1 of our system alignment f1 is 32.35. p of our system alignment p is 29.05. r of our system alignment r is 28.12. f1 of our system alignment f1 is 28.58.
table 1 shows youtube dataset: meteor and bleu@4 in %, and human ratings (1-5) on relevance and grammar. meteor of - meteor is 29.2. b-4 of - b-4 is 37.0. relevance of - relevance is 2.06. grammar of - grammar is 3.76. meteor of - meteor is 29.6. b-4 of - b-4 is 37.6. meteor of - meteor is 29.4. b-4 of - b-4 is 37.2. meteor of - meteor is 29.6. b-4 of - b-4 is 39.3. meteor of - meteor is 30.0. b-4 of - b-4 is 37.0. meteor of - web corpus meteor is 30.3. b-4 of - web corpus b-4 is 38.1. relevance of - web corpus relevance is 2.12. grammar of - web corpus grammar is 4.05*. meteor of - in-domain meteor is 30.3. b-4 of - in-domain b-4 is 38.8. relevance of - in-domain relevance is 2.21*. grammar of - in-domain grammar is 4.17*. meteor of - meteor is 31.4. b-4 of - b-4 is 42.1. relevance of - relevance is 2.24*. grammar of - grammar is 4.20*.
table 2 shows accuracy under cross-domain evaluation; the best result for each dataset is indicated in bold. accuracy of dropout (beta) = 0.3 67.5 is 71.6. accuracy of dropout (beta) = 0.3 61.0 is 62.2. accuracy of dropout (beta) = 0.5 67.5 is 71.0. accuracy of dropout (beta) = 0.5 61.0 is 62.1. accuracy of dropout (beta) = 0.7 67.5 is 70.9. accuracy of dropout (beta) = 0.7 61.0 is 62.0. accuracy of robust regularization (lambda) = 10^-3 67.5 is 70.8. accuracy of robust regularization (lambda) = 10^-3 61.0 is 61.6. accuracy of robust regularization (lambda) = 10^-2 67.5 is 71.1. accuracy of robust regularization (lambda) = 10^-2 61.0 is 62.5. accuracy of robust regularization (lambda) = 10^-1 67.5 is 72.0. accuracy of robust regularization (lambda) = 10^-1 61.0 is 62.2. accuracy of robust regularization (lambda) = 1 67.5 is 71.8. accuracy of robust regularization (lambda) = 1 61.0 is 62.3. accuracy of dropout + robust beta = 0.5 lambda = 10^-2 67.5 is 72.0. accuracy of dropout + robust beta = 0.5 lambda = 10^-2 61.0 is 62.4.
table 2 shows word alignment performance. f-measure of hmm+none f-measure is 0.7900. aer of hmm+none aer is 0.0646. f-measure of hmm+none f-measure is 0.4623. aer of hmm+none aer is 0.5377. f-measure of hmm+none f-measure is 0.4425. aer of hmm+none aer is 0.5575. f-measure of hmm+sym f-measure is 0.7923. aer of hmm+sym aer is 0.0597. f-measure of hmm+sym f-measure is 0.4678. aer of hmm+sym aer is 0.5322. f-measure of hmm+sym f-measure is 0.4534. aer of hmm+sym aer is 0.5466. f-measure of hmm+itg f-measure is 0.7869. aer of hmm+itg aer is 0.0629. f-measure of hmm+itg f-measure is 0.4690. aer of hmm+itg aer is 0.5310. f-measure of hmm+itg f-measure is 0.4499. aer of hmm+itg aer is 0.5501. f-measure of ibm model 4+none f-measure is 0.7780. aer of ibm model 4+none aer is 0.0775. f-measure of ibm model 4+none f-measure is 0.5379. aer of ibm model 4+none aer is 0.4621. f-measure of ibm model 4+none f-measure is 0.4454. aer of ibm model 4+none aer is 0.5546. f-measure of ibm model 4+sym f-measure is 0.7800. aer of ibm model 4+sym aer is 0.0693. f-measure of ibm model 4+sym f-measure is 0.5545. aer of ibm model 4+sym aer is 0.4455. f-measure of ibm model 4+sym f-measure is 0.4761. aer of ibm model 4+sym aer is 0.5239. f-measure of ibm model 4+itg f-measure is 0.7791. aer of ibm model 4+itg aer is 0.0710. f-measure of ibm model 4+itg f-measure is 0.5613. aer of ibm model 4+itg aer is 0.4387. f-measure of ibm model 4+itg f-measure is 0.4809. aer of ibm model 4+itg aer is 0.5191.
table 2 shows performance on the proverb test data. p of b# p is 0.75. r of b# r is 0.70. f of b# f is 0.73. p of n* p is 0.86. r of n* r is 0.83. f of n* f is 0.85. p of n \ s* p is 0.82. r of n \ s* r is 0.87. f of n \ s* f is 0.85. p of b ∪ n* p is 0.87. r of b ∪ n* r is 0.85. f of b ∪ n* f is 0.86.
table 3 shows benchmark results: accuracy on addressee-response selection (adr-res), addressee selection (adr), and response selection (res). accuracy of - adr-res is 0.62. accuracy of - adr is 1.24. accuracy of - res is 50.00. accuracy of - adr-res is 0.12. accuracy of - adr is 1.24. accuracy of - res is 10.00. accuracy of 5 adr-res is 36.97. accuracy of 5 adr is 55.73. accuracy of 5 res is 65.68. accuracy of 5 adr-res is 16.34. accuracy of 5 adr is 55.73. accuracy of 5 res is 28.19. accuracy of 10 adr-res is 37.42. accuracy of 10 adr is 55.63. accuracy of 10 res is 67.79. accuracy of 10 adr-res is 16.11. accuracy of 10 adr is 55.63. accuracy of 10 res is 29.48. accuracy of 15 adr-res is 37.13. accuracy of 15 adr is 55.62. accuracy of 15 res is 67.89. accuracy of 15 adr-res is 15.44. accuracy of 15 adr is 55.62. accuracy of 15 res is 29.19. accuracy of 5 adr-res is 46.99. accuracy of 5 adr is 60.39. accuracy of 5 res is 75.07. accuracy of 5 adr-res is 21.98. accuracy of 5 adr is 60.26. accuracy of 5 res is 33.27. accuracy of 10 adr-res is 48.67. accuracy of 10 adr is 60.97. accuracy of 10 res is 77.75. accuracy of 10 adr-res is 23.31. accuracy of 10 adr is 60.66. accuracy of 10 res is 35.91. accuracy of 15 adr-res is 49.27. accuracy of 15 adr is 61.95. accuracy of 15 res is 78.14. accuracy of 15 adr-res is 23.49. accuracy of 15 adr is 60.98. accuracy of 15 res is 36.58. accuracy of 5 adr-res is 49.80. accuracy of 5 adr is 63.19. accuracy of 5 res is 76.07. accuracy of 5 adr-res is 23.72. accuracy of 5 adr is 63.28. accuracy of 5 res is 33.62. accuracy of 10 adr-res is 53.85. accuracy of 10 adr is 66.94. accuracy of 10 res is 78.16. accuracy of 10 adr-res is 25.95. accuracy of 10 adr is 66.70. accuracy of 10 res is 36.14. accuracy of 15 adr-res is 54.88. accuracy of 15 adr is 68.54. accuracy of 15 res is 78.64. accuracy of 15 adr-res is 27.19. accuracy of 15 adr is 68.41. accuracy of 15 res is 36.93.
table 4 shows performance comparison for different numbers of agents appearing in the context. accuracy of baseline 3731 is 52.13. accuracy of baseline 5962 is 43.51. accuracy of baseline 5475 is 39.98. accuracy of baseline 4495 is 42.96. accuracy of baseline 5619 is 39.70. accuracy of baseline 7956 is 36.55. accuracy of baseline 18659 is 29.22. accuracy of static 3731 is 64.17. accuracy of static 5962 is 55.92. accuracy of static 5475 is 50.72. accuracy of static 4495 is 53.04. accuracy of static 5619 is 48.69. accuracy of static 7956 is 49.61. accuracy of static 18659 is 42.86. accuracy of dynamic 3731 is 66.9. accuracy of dynamic 5962 is 57.73. accuracy of dynamic 5475 is 54.32. accuracy of dynamic 4495 is 55.64. accuracy of dynamic 5619 is 51.61. accuracy of dynamic 7956 is 55.88. accuracy of dynamic 18659 is 52.14. accuracy of baseline 3731 is 84.94. accuracy of baseline 5962 is 70.82. accuracy of baseline 5475 is 62.14. accuracy of baseline 4495 is 65.52. accuracy of baseline 5619 is 58.89. accuracy of baseline 7956 is 51.28. accuracy of baseline 18659 is 41.47. accuracy of static 3731 is 86.33. accuracy of static 5962 is 74.37. accuracy of static 5475 is 66.12. accuracy of static 4495 is 68.54. accuracy of static 5619 is 63.43. accuracy of static 7956 is 59.24. accuracy of static 18659 is 50.99. accuracy of dynamic 3731 is 87.64. accuracy of dynamic 5962 is 76.48. accuracy of dynamic 5475 is 69.99. accuracy of dynamic 4495 is 72.21. accuracy of dynamic 5619 is 66.90. accuracy of dynamic 7956 is 66.78. accuracy of dynamic 18659 is 62.11. accuracy of baseline 3731 is 60.71. accuracy of baseline 5962 is 61.24. accuracy of baseline 5475 is 64.51. accuracy of baseline 4495 is 65.58. accuracy of baseline 5619 is 67.93. accuracy of baseline 7956 is 71.66. accuracy of baseline 18659 is 71.38. accuracy of static 3731 is 73.6. accuracy of static 5962 is 73.45. accuracy of static 5475 is 74.54. accuracy of static 4495 is 75.95. accuracy of static 5619 is 75.17. accuracy of static 7956 is 81.5. accuracy of static 18659 is 81.6. accuracy of dynamic 3731 is 75.64. accuracy of dynamic 5962 is 74.12. accuracy of dynamic 5475 is 75.53. accuracy of dynamic 4495 is 75.17. accuracy of dynamic 5619 is 76.05. accuracy of dynamic 7956 is 81.96. accuracy of dynamic 18659 is 81.81.
table 2 shows f-score for headlines and images datasets. f-score of baseline ali is 0.8462. f-score of baseline score is 0.7610. f-score of baseline ali is 0.5462. f-score of baseline score is 0.5461. f-score of rank 1 ali is 0.8194. f-score of rank 1 score is 0.7865. f-score of rank 1 ali is 0.7031. f-score of rank 1 score is 0.696. f-score of da ali is 0.9257. f-score of da score is 0.8377. f-score of da ali is 0.735. f-score of da score is 0.6776. f-score of da +ds ali is 0.9235. f-score of da +ds score is 0.8591. f-score of da +ds ali is 0.7281. f-score of da +ds score is 0.6948. f-score of baseline ali is 0.8556. f-score of baseline score is 0.7456. f-score of baseline ali is 0.4799. f-score of baseline score is 0.4799.1. f-score of rank 1 ali is 0.8922. f-score of rank 1 score is 0.8408. f-score of rank 1 ali is 0.6867. f-score of rank 1 score is 0.6708. f-score of da ali is 0.8689. f-score of da score is 0.7905. f-score of da ali is 0.6933. f-score of da score is 0.6411. f-score of da +ds ali is 0.8738. f-score of da +ds score is 0.8193. f-score of da +ds ali is 0.7011. f-score of da +ds score is 0.6769.
table 1 shows parsing accuracy on ptb test set. uas of c&m (2014) uas is 91.8. las of c&m (2014) las is 89.6. uas of dyer et al. (2015) uas is 93.2. las of dyer et al. (2015) las is 90.9. uas of b&n (2012) uas is 93.33. las of b&n (2012) las is 91.22. uas of alberti et al. (2015) uas is 94.23. las of alberti et al. (2015) las is 92.41. uas of weiss et al. (2015) uas is 94.26. las of weiss et al. (2015) las is 92.41. uas of andor et al. (2016) uas is 94.41. las of andor et al. (2016) las is 92.55. uas of bohnet (2010) uas is 92.88. las of bohnet (2010) las is 90.71. uas of martins et al. (2013) uas is 92.89. las of martins et al. (2013) las is 90.55. uas of z&m (2014) uas is 93.22. las of z&m (2014) las is 91.02. uas of biatt-dp uas is 94.10. las of biatt-dp las is 91.49.
table 3 shows uas on 12 languages in the conll 2006 shared task (buchholz and marsi, 2006). uas of arabic biatt-dp is 80.34 [68.58]. uas of arabic rbgparser is 79.95. uas of arabic best published is 81.12 (ma11). uas of arabic crossed is 17.24. uas of arabic uncrossed is 80.71. uas of arabic %crossed is 0.58. uas of bulgarian biatt-dp is 93.96 [89.55]. uas of bulgarian rbgparser is 93.5. uas of bulgarian best published is 94.02 (zh14). uas of bulgarian crossed is 79.59. uas of bulgarian uncrossed is 94.1. uas of bulgarian %crossed is 0.98. uas of czech biatt-dp is 91.16 [85.14]. uas of czech rbgparser is 90.5. uas of czech best published is 90.32 (ma13). uas of czech crossed is 81.62. uas of czech uncrossed is 91.63. uas of czech %crossed is 4.68. uas of danish biatt-dp is 91.56 [85.53]. uas of danish rbgparser is 91.39. uas of danish best published is 92.00 (zh13). uas of danish crossed is 73.33. uas of danish uncrossed is 91.89. uas of danish %crossed is 1.8. uas of dutch biatt-dp is 87.15 [82.41]. uas of dutch rbgparser is 86.41. uas of dutch best published is 86.19 (ma13). uas of dutch crossed is 82.82. uas of dutch uncrossed is 87.66. uas of dutch %crossed is 10.48. uas of german biatt-dp is 92.71 [89.80]. uas of german rbgparser is 91.97. uas of german best published is 92.41 (ma13). uas of german crossed is 85.93. uas of german uncrossed is 92.9. uas of german %crossed is 2.7. uas of japanese biatt-dp is 93.44 [90.67]. uas of japanese rbgparser is 93.71. uas of japanese best published is 93.72 (ma11). uas of japanese crossed is 48.67. uas of japanese uncrossed is 94.48. uas of japanese %crossed is 2.26. uas of portuguese biatt-dp is 92.77 [88.44]. uas of portuguese rbgparser is 91.92. uas of portuguese best published is 93.03 (ko10). uas of portuguese crossed is 73.02. uas of portuguese uncrossed is 93.28. uas of portuguese %crossed is 2.52. uas of slovene biatt-dp is 86.01 [75.90]. uas of slovene rbgparser is 86.24. uas of slovene best published is 86.95 (ma11). uas of slovene crossed is 60.11. uas of slovene uncrossed is 86.99. uas of slovene %crossed is 3.66. uas of spanish biatt-dp is 88.74 [84.03]. uas of spanish rbgparser is 88.0. uas of spanish best published is 87.98 (zh14). uas of spanish crossed is 50.0. uas of spanish uncrossed is 88.77. uas of spanish %crossed is 0.08. uas of swedish biatt-dp is 90.50 [84.05]. uas of swedish rbgparser is 91.0. uas of swedish best published is 91.85 (zh14). uas of swedish crossed is 45.16. uas of swedish uncrossed is 90.78. uas of swedish %crossed is 0.62. uas of turkish biatt-dp is 78.43 [66.16]. uas of turkish rbgparser is 76.84. uas of turkish best published is 77.55 (ko10). uas of turkish crossed is 38.85. uas of turkish uncrossed is 79.71. uas of turkish %crossed is 3.13.
table 4 shows the results on a subset of jsem that is a translation of fracas. accuracy of quantifier.1 gold is 92.5. accuracy of quantifier.1 system is 78.2. accuracy of quantifier.1 m15 is 78.4. accuracy of plural gold is 65.8. accuracy of plural system is 52.6. accuracy of plural m15 is 66.7. accuracy of adjective gold is 57.1. accuracy of adjective system is 47.6. accuracy of adjective m15 is 68.2. accuracy of verb gold is 66.7. accuracy of verb system is 66.7. accuracy of verb m15 is 62.5. accuracy of attitude gold is 78.6. accuracy of attitude system is 78.6. accuracy of attitude m15 is 76.9. accuracy of total gold is 87.3. accuracy of total system is 74.1. accuracy of total m15 is 73.3.
table 2 shows experimental results. perp. of raw perp. is 28.31. aic of raw aic is 0.108×10^5. acc. of raw acc. is 28.75%. perp. of buckets perp. is 16.01. aic of buckets aic is 0.131×10^5. acc. of buckets acc. is 38.59%. perp. of fourier perp. is 15.05. aic of fourier aic is 8.86×10^5. acc. of fourier acc. is 38.97%. perp. of raw perp. is 13.27. aic of raw aic is 8.40×10^5. acc. of raw acc. is 40.11%. perp. of buckets perp. is 13.03. aic of buckets aic is 0.126×10^5. acc. of buckets acc. is 39.94%. perp. of fourier perp. is 12.35. aic of fourier aic is 8.33×10^5. acc. of fourier acc. is 40.40%. perp. of buckets perp. is 14.41. aic of buckets aic is 0.482×10^5. acc. of buckets acc. is 39.40%. perp. of raw perp. is 13.61. aic of raw aic is 0.413×10^5. acc. of raw acc. is 39.55%. perp. of fourier perp. is 12.58. aic of fourier aic is 0.403×10^5. acc. of fourier acc. is 40.22%.
table 4 shows detailed ja → en insertion position selection experimental result. bleu of pbsmt bleu is 18.45. ribes of pbsmt ribes is 64.51. bleu of pbsmt bleu is 27.48. ribes of pbsmt ribes is 68.37. bleu of pbsmt bleu is 27.96. ribes of pbsmt ribes is 78.90. bleu of pbsmt bleu is 34.65. ribes of pbsmt ribes is 77.25. bleu of hiero bleu is 18.72. ribes of hiero ribes is 65.11. bleu of hiero bleu is 30.19. ribes of hiero ribes is 73.47. bleu of hiero bleu is 27.71. ribes of hiero ribes is 80.91. bleu of hiero bleu is 35.43. ribes of hiero ribes is 81.04. bleu of no flexible bleu is 20.28. ribes of no flexible ribes is 65.08. time of no flexible time is 1.00. bleu of no flexible bleu is 28.77. ribes of no flexible ribes is 75.21. time of no flexible time is 1.00. bleu of no flexible bleu is 24.85. ribes of no flexible ribes is 66.60. time of no flexible time is 1.00. bleu of no flexible bleu is 30.51. ribes of no flexible ribes is 73.08. time of no flexible time is 1.00. bleu of baseline bleu is 21.61. ribes of baseline ribes is 69.82. time of baseline time is 6.28. bleu of baseline bleu is 30.57. ribes of baseline ribes is 76.13. time of baseline time is 3.30. bleu of baseline bleu is 28.79. ribes of baseline ribes is 78.11. time of baseline time is 5.16. bleu of baseline bleu is 34.32. ribes of baseline ribes is 77.82. time of baseline time is 5.28. bleu of proposed bleu is 22.07†. ribes of proposed ribes is 70.49†. time of proposed time is 2.25. bleu of proposed bleu is 30.50. ribes of proposed ribes is 76.69†. time of proposed time is 1.27. bleu of proposed bleu is 29.83†. ribes of proposed ribes is 79.73†. time of proposed time is 2.21. bleu of proposed bleu is 34.71†. ribes of proposed ribes is 79.25†. time of proposed time is 1.89.
table 1 shows single system results in terms of (ter-bleu)/2 (t-b, the lower the better) on 5 million chinese to english training set. bp of - bp is 0.95. bleu of - bleu is 34.93. t-b of - t-b is 9.45. bp of - bp is 0.94. bleu of - bleu is 31.12. t-b of - t-b is 12.90. bp of - bp is 0.90. bleu of - bleu is 23.45. t-b of - t-b is 17.72. t-b of - t-b is 13.36. bp of - bp is 0.92. bleu of - bleu is 35.59. t-b of - t-b is 10.71. bp of - bp is 0.89. bleu of - bleu is 30.18. t-b of - t-b is 15.33. bp of - bp is 0.97. bleu of - bleu is 27.48. t-b of - t-b is 16.67. t-b of - t-b is 14.24. bp of a → j bp is 0.95. bleu of a → j bleu is 35.71. t-b of a → j t-b is 10.38. bp of a → j bp is 0.93. bleu of a → j bleu is 30.73. t-b of a → j t-b is 14.98. bp of a → j bp is 0.96. bleu of a → j bleu is 27.38. t-b of a → j t-b is 16.24. t-b of a → j t-b is 13.87. bp of a → t bp is 0.95. bleu of a → t bleu is 28.59. t-b of a → t t-b is 16.99. bp of a → t bp is 0.92. bleu of a → t bleu is 24.09. t-b of a → t t-b is 20.89. bp of a → t bp is 0.97. bleu of a → t bleu is 20.48. t-b of a → t t-b is 23.31. t-b of a → t t-b is 20.40. bp of a → t → j bp is 0.95. bleu of a → t → j bleu is 35.95. t-b of a → t → j t-b is 10.24. bp of a → t → j bp is 0.92. bleu of a → t → j bleu is 30.95. t-b of a → t → j t-b is 14.62. bp of a → t → j bp is 0.97. bleu of a → t → j bleu is 26.76. t-b of a → t → j t-b is 17.04. t-b of a → t → j t-b is 13.97. bp of j bp is 0.96. bleu of j bleu is 36.76. t-b of j t-b is 9.67. bp of j bp is 0.94. bleu of j bleu is 31.24. t-b of j t-b is 14.80. bp of j bp is 0.96. bleu of j bleu is 28.35. t-b of j t-b is 15.61. t-b of j t-b is 13.36. bp of j bp is 0.96. bleu of j bleu is 36.44. t-b of j t-b is 10.16. bp of j bp is 0.94. bleu of j bleu is 30.66. t-b of j t-b is 15.01. bp of j bp is 0.96. bleu of j bleu is 26.67. t-b of j t-b is 16.72. t-b of j t-b is 13.96. bp of j bp is 0.95. bleu of j bleu is 36.80. t-b of j t-b is 9.49. bp of j bp is 0.93. bleu of j bleu is 31.74. t-b of j t-b is 14.02. bp of j bp is 0.96. bleu of j bleu is 27.53. t-b of j t-b is 16.21. t-b of j t-b is 13.24. bp of j + gau. bp is 0.96. bleu of j + gau. bleu is 36.95. t-b of j + gau. t-b is 9.71. bp of j + gau. bp is 0.94. bleu of j + gau. bleu is 32.43. t-b of j + gau. t-b is 13.61. bp of j + gau. bp is 0.97. bleu of j + gau. bleu is 28.63. t-b of j + gau. t-b is 15.80. t-b of j + gau. t-b is 13.04.
table 1 shows our results in bilingual and monolingual tasks. accuracy of - en an. is 76.66%. accuracy of - en-it is 34.93%. accuracy of - en an. is 73.80%. accuracy of + length normalization en-it is 33.80%. accuracy of + length normalization en an. is 73.61%. accuracy of + mean centering en-it is 38.47%. accuracy of + mean centering en an. is 73.71%. accuracy of - en-it is 36.73%. accuracy of - en an. is 76.66%. accuracy of + length normalization en-it is 36.87%. accuracy of + length normalization en an. is 76.66%. accuracy of + mean centering en-it is 39.27%. accuracy of + mean centering en an. is 76.59%.
table 2 shows comparison of our method to other work. accuracy of original embeddings en an. is 76.66%. accuracy of mikolov et al. (2013b) en-it is 34.93%. accuracy of mikolov et al. (2013b) en an. is 73.80%. accuracy of xing et al. (2015) en-it is 36.87%. accuracy of xing et al. (2015) en an. is 76.66%. accuracy of faruqui and dyer (2014) en-it is 37.80%. accuracy of faruqui and dyer (2014) en an. is 69.64%. accuracy of our method en-it is 39.27%. accuracy of our method en an. is 76.59%.
table 2 shows results of 4-way classification on the pdtb. f1 of p stn is 33.33. f1 of p mt nbi is 34.48. f1 of r stn is 14.55. f1 of r mt nbi is 18.18. f1 of f1 stn is 20.25. f1 of f1 mt nbi is 23.81. f1 of p stn is 38.54. f1 of p mt nbi is 42.11. f1 of r stn is 25.52. f1 of r mt nbi is 33.10. f1 of f1 stn is 30.71. f1 of f1 mt nbi is 37.07. f1 of p stn is 38.36. f1 of p mt nbi is 44.22. f1 of r stn is 41.03. f1 of r mt nbi is 40.66. f1 of f1 stn is 39.65. f1 of f1 mt nbi is 42.37. f1 of p stn is 59.60. f1 of p mt nbi is 62.56. f1 of r stn is 66.36. f1 of r mt nbi is 71.75. f1 of f1 stn is 62.80. f1 of f1 mt nbi is 66.84. f1 of - stn is 38.35. f1 of - mt nbi is 42.52.
table 3 shows unlabeled attachment scores (uas) on the ptb validation set after parsing and aligning the output. uas of zgen-64(z ? ) words is 39.7. uas of zgen-64(z ? ) words+bnps is 64.9. uas of zgen-64 words is 40.8. uas of zgen-64 words+bnps is 65.2. uas of ngram-64 words is 46.1. uas of ngram-64 words+bnps is 67.0. uas of ngram-512 words is 47.2. uas of ngram-512 words+bnps is 67.8. uas of lstm-64 words is 51.3. uas of lstm-64 words+bnps is 71.9. uas of lstm-512 words is 52.8. uas of lstm-512 words+bnps is 73.1.
table 3 shows evaluation results on relation prediction. mean rank of transe raw is 1.53. mean rank of transe filter is 1.48. hits@1 (%) of transe raw is 69.4. hits@1 (%) of transe filter is 73.0. mean rank of ttranse raw is 1.42. mean rank of ttranse filter is 1.35. hits@1 (%) of ttranse raw is 71.1. hits@1 (%) of ttranse filter is 75.7. mean rank of transh raw is 1.51. mean rank of transh filter is 1.37. hits@1 (%) of transh raw is 70.5. hits@1 (%) of transh filter is 72.2. mean rank of ttransh raw is 1.38. mean rank of ttransh filter is 1.30. hits@1 (%) of ttransh raw is 74.6. hits@1 (%) of ttransh filter is 76.9. mean rank of transr raw is 1.40. mean rank of transr filter is 1.28. hits@1 (%) of transr raw is 71.1. hits@1 (%) of transr filter is 74.3. mean rank of ttransr raw is 1.27. mean rank of ttransr filter is 1.12. hits@1 (%) of ttransr raw is 74.5. hits@1 (%) of ttransr filter is 78.9.
table 5 shows evaluation results on triple classification (%). accuracy of transe yg15k is 63.9. accuracy of transe yg36k is 71.9. accuracy of ttranse yg15k is 75.0. accuracy of ttranse yg36k is 82.7. accuracy of transh yg15k is 63.4. accuracy of transh yg36k is 72.1. accuracy of ttransh yg15k is 75.1. accuracy of ttransh yg36k is 82.3. accuracy of transr yg15k is 64.5. accuracy of transr yg36k is 74.9. accuracy of ttransr yg15k is 78.5. accuracy of ttransr yg36k is 83.9.
table 4 shows parsing results trained with different update methods. dev f1 of greedy dev f1 is 87.9. optimal of greedy optimal is 99.2%. explored of greedy explored is 2313.8. dev f1 of max-violation dev f1 is 88.1. optimal of max-violation optimal is 99.9%. explored of max-violation explored is 217.3. dev f1 of all-violations dev f1 is 88.4. optimal of all-violations optimal is 99.8%. explored of all-violations explored is 309.6.
table 5 shows performance of various methods and humans. accuracy of random guess dev is 1.1%. accuracy of random guess test is 1.3%. f1 of random guess dev is 4.1%. f1 of random guess test is 4.3%. accuracy of sliding window dev is 13.2%. accuracy of sliding window test is 12.5%. f1 of sliding window dev is 20.2%. f1 of sliding window test is 19.7%. accuracy of sliding win. + dist. dev is 13.3%. accuracy of sliding win. + dist. test is 13.0%. f1 of sliding win. + dist. dev is 20.2%. f1 of sliding win. + dist. test is 20.0%. accuracy of logistic regression dev is 40.0%. accuracy of logistic regression test is 40.4%. f1 of logistic regression dev is 51.0%. f1 of logistic regression test is 51.0%. accuracy of human dev is 80.3%. accuracy of human test is 77.0%. f1 of human dev is 90.5%. f1 of human test is 86.8%.
table 3 shows evaluation results on the test set, where ∗ represents p-value < 0.05 against our method. recall of human recall is 90.65. prec. of human prec. is 88.21. uas of human uas is –. % of human % is –. recall of proposed recall is 83.64. prec. of proposed prec. is 78.91. uas of proposed uas is 93.49. % of proposed % is 98. recall of monotonic recall is 82.86*. prec. of monotonic prec. is 77.97*. uas of monotonic uas is 93.49. % of monotonic % is 98. recall of w/o em recall is 81.33*. prec. of w/o em prec. is 75.09*. uas of w/o em uas is 92.91*. % of w/o em % is 86. recall of 1-best tree recall is 80.11*. prec. of 1-best tree prec. is 73.26*. uas of 1-best tree uas is 93.56. % of 1-best tree % is 100.
table 4 shows model performance on the test set of tacred, micro-averaged over instances. p of patterns p is 85.3. r of patterns r is 23.4. f1 of patterns f1 is 36.8. p of lr p is 72.0. r of lr r is 47.8. f1 of lr f1 is 57.5. p of lr + patterns p is 71.4. r of lr + patterns r is 50.1. f1 of lr + patterns f1 is 58.9. p of cnn p is 72.1. r of cnn r is 50.3. f1 of cnn f1 is 59.2. p of cnn-pe p is 68.2. r of cnn-pe r is 55.4. f1 of cnn-pe f1 is 61.1. p of sdp-lstm p is 62.0. r of sdp-lstm r is 54.8. f1 of sdp-lstm f1 is 58.2. p of lstm p is 61.4. r of lstm r is 61.7. f1 of lstm f1 is 61.5. p of our model p is 67.7. r of our model r is 63.2. f1 of our model f1 is 65.4. p of ensemble p is 69.4. r of ensemble r is 64.8. f1 of ensemble f1 is 67.0.
table 5 shows model performance on tac kbp 2015 slot filling evaluation, micro-averaged over queries. p of patterns p is 63.8. r of patterns r is 17.7. f1 of patterns f1 is 27.7. p of patterns p is 49.3. r of patterns r is 8.6. f1 of patterns f1 is 14.7. p of patterns p is 58.9. r of patterns r is 13.3. f1 of patterns f1 is 21.8. p of lr p is 36.6. r of lr r is 21.9. f1 of lr f1 is 27.4. p of lr p is 15.1. r of lr r is 10.1. f1 of lr f1 is 12.2. p of lr p is 25.6. r of lr r is 16.3. f1 of lr f1 is 19.9. p of lr + patterns (2015 winning system) p is 37.5. r of lr + patterns (2015 winning system) r is 24.5. f1 of lr + patterns (2015 winning system) f1 is 29.7. p of lr + patterns (2015 winning system) p is 16.5. r of lr + patterns (2015 winning system) r is 12.8. f1 of lr + patterns (2015 winning system) f1 is 14.4. p of lr + patterns (2015 winning system) p is 26.6. r of lr + patterns (2015 winning system) r is 19.0. f1 of lr + patterns (2015 winning system) f1 is 22.2. p of lr trained on tacred p is 32.7. r of lr trained on tacred r is 20.6. f1 of lr trained on tacred f1 is 25.3. p of lr trained on tacred p is 7.9. r of lr trained on tacred r is 9.5. f1 of lr trained on tacred f1 is 8.6. p of lr trained on tacred p is 16.8. r of lr trained on tacred r is 15.3. f1 of lr trained on tacred f1 is 16.0. p of lr trained on tacred + patterns p is 36.5. r of lr trained on tacred + patterns r is 26.5. f1 of lr trained on tacred + patterns f1 is 30.7. p of lr trained on tacred + patterns p is 11.0. r of lr trained on tacred + patterns r is 15.3. f1 of lr trained on tacred + patterns f1 is 12.8. p of lr trained on tacred + patterns p is 20.1. r of lr trained on tacred + patterns r is 21.2. f1 of lr trained on tacred + patterns f1 is 20.6. p of our model p is 39.0. r of our model r is 28.9. f1 of our model f1 is 33.2. p of our model p is 17.7. r of our model r is 13.9. f1 of our model f1 is 15.6. p of our model p is 28.2. r of our model r is 21.5. f1 of our model f1 is 24.4. p of our model + patterns p is 40.2. r of our model + patterns r is 31.5. f1 of our model + patterns f1 is 35.3. p of our model + patterns p is 19.4. r of our model + patterns r is 16.5. f1 of our model + patterns f1 is 17.8. p of our model + patterns p is 29.7. r of our model + patterns r is 24.2. f1 of our model + patterns f1 is 26.7.
table 3 shows final results. accuracy of pmi g&c16 is 30.52. accuracy of pmi c&j08 is 30.92. accuracy of bigram g&c16 is 29.67. accuracy of bigram c&j08 is 25.43. accuracy of event-comp g&c16 is 49.57. accuracy of event-comp c&j08 is 43.28. accuracy of rnn g&c16 is 45.74. accuracy of rnn c&j08 is 43.17. accuracy of memnet g&c16 is 55.12. accuracy of memnet c&j08 is 46.67.
table 1 shows results for swear compared to top published results on the wikireading test set. mean f1 of placeholder seq2seq (he16) mean f1 is 71.8. mean f1 of softattend (ch17) mean f1 is 71.6. mean f1 of reinforce (ch17) mean f1 is 74.5. mean f1 of placeholder seq2seq (ch17) mean f1 is 75.6. mean f1 of swear (w/ zeros) mean f1 is 76.4. mean f1 of swear mean f1 is 76.8.
table 2 shows mean f1 for swear on each type of property compared with the best results for each type reported in hewlett et al. f1 of categorical he16 best is 88.6. f1 of categorical swear is 88.6. f1 of relational he16 best is 56.5. f1 of relational swear is 63.4. f1 of date he16 best is 73.8. f1 of date swear is 82.5.
table 4 shows mean f1 results for swear (fully supervised) and swear-ss (semi-supervised) trained on 1%, 0.5%, and 0.1% subsets, respectively. f1 of swear 1% is 63.5. f1 of swear 0.5% is 57.6. f1 of swear 0.1% is 39.5. f1 of swear-ss (rae) 1% is 64.7. f1 of swear-ss (rae) 0.5% is 62.8. f1 of swear-ss (rae) 0.1% is 55.3. f1 of swear-ss (vrae) 1% is 65.7. f1 of swear-ss (vrae) 0.5% is 64.0. f1 of swear-ss (vrae) 0.1% is 60.7.
table 6 shows results for semi-supervised reviewer models trained on the 1% subset of wikireading. mean f1 of - mean f1 is 66.5. mean f1 of dropout on input only mean f1 is 65.4. mean f1 of no dropout mean f1 is 64.6. mean f1 of shared reviewer cells mean f1 is 63.8. mean f1 of - mean f1 is 63.0. mean f1 of w/o skip connections mean f1 is 60.0.
table 5 shows transferability of adversarial examples across models. f1 of ml single single is 27.3. f1 of ml single ens. is 33.4. f1 of ml single single is 40.3. f1 of ml single ens. is 39.1. f1 of ml ens. single is 31.6. f1 of ml ens. ens. is 29.4. f1 of ml ens. single is 40.2. f1 of ml ens. ens. is 38.7. f1 of bidaf single single is 32.7. f1 of bidaf single ens. is 34.8. f1 of bidaf single single is 34.3. f1 of bidaf single ens. is 37.4. f1 of bidaf ens. single is 32.7. f1 of bidaf ens. ens. is 34.2. f1 of bidaf ens. single is 38.3. f1 of bidaf ens. ens. is 34.2. f1 of ml single single is 7.6. f1 of ml single ens. is 54.1. f1 of ml single single is 57.1. f1 of ml single ens. is 60.9. f1 of ml ens. single is 44.9. f1 of ml ens. ens. is 11.7. f1 of ml ens. single is 50.4. f1 of ml ens. ens. is 54.8. f1 of bidaf single single is 58.4. f1 of bidaf single ens. is 60.5. f1 of bidaf single single is 4.8. f1 of bidaf single ens. is 46.4. f1 of bidaf ens. single is 48.8. f1 of bidaf ens. ens. is 51.1. f1 of bidaf ens. single is 25. f1 of bidaf ens. ens. is 2.7.
table 4 shows comparison of accuracy for our model and three baselines on rocstories spring 2016 test set. accuracy of narrative event chain accuracy is 57.62%. accuracy of dssm accuracy is 58.52%. accuracy of rnn model accuracy is 58.93%. accuracy of  - accuracy is 67.02%.
table 5 shows comparison of the performance using single type of knowledge. accuracy of event narrative knowledge accuracy is 60.98%. accuracy of entity semantic knowledge accuracy is 57.14%. accuracy of sentiment coherent knowledge accuracy is 61.30%. accuracy of - accuracy is 67.02%.
table 7 shows comparison of the performance using different inference rule selection mechanism. accuracy of minimum cost mechanism accuracy is 54.84%. accuracy of average cost mechanism accuracy is 63.01%. accuracy of  - accuracy is 67.02%.
table 8 shows comparison of the performance by removing negation rules. accuracy of our model accuracy is 67.02%. accuracy of -w/o negation rules accuracy is 63.12%.
table 3 shows cross-domain experiments, best values per column are highlighted, in-domain results (for comparison) in italics; results only for selected systems. macro-f1 of mt macro-f1 is 78.6. f1 of mt f1 is 67.3. macro-f1 of mt macro-f1 is 51. f1 of mt f1 is 7.4. macro-f1 of mt macro-f1 is 56.9. f1 of mt f1 is 22.1. macro-f1 of mt macro-f1 is 57.2. f1 of mt f1 is 15.7. macro-f1 of mt macro-f1 is 52.4. f1 of mt f1 is 9.4. macro-f1 of mt macro-f1 is 49.4. f1 of mt f1 is 10.9. macro-f1 of mt macro-f1 is 53.4. f1 of mt f1 is 13.1. macro-f1 of oc macro-f1 is 57.1. f1 of oc f1 is 39.7. macro-f1 of oc macro-f1 is 60.5. f1 of oc f1 is 25.6. macro-f1 of oc macro-f1 is 56.4. f1 of oc f1 is 42.8. macro-f1 of oc macro-f1 is 58.9. f1 of oc f1 is 37.3. macro-f1 of oc macro-f1 is 54.6. f1 of oc f1 is 13.2. macro-f1 of oc macro-f1 is 58.4. f1 of oc f1 is 28.9. macro-f1 of oc macro-f1 is 57.1. f1 of oc f1 is 32.4. macro-f1 of pe macro-f1 is 59.8. f1 of pe f1 is 18. macro-f1 of pe macro-f1 is 54.2. f1 of pe f1 is 9.5. macro-f1 of pe macro-f1 is 73.6. f1 of pe f1 is 61.1. macro-f1 of pe macro-f1 is 57.5. f1 of pe f1 is 18.7. macro-f1 of pe macro-f1 is 55.5. f1 of pe f1 is 15.9. macro-f1 of pe macro-f1 is 54.7. f1 of pe f1 is 16. macro-f1 of pe macro-f1 is 56.3. f1 of pe f1 is 15.6. macro-f1 of vg macro-f1 is 68.7. f1 of vg f1 is 51.5. macro-f1 of vg macro-f1 is 55.8. f1 of vg f1 is 19.2. macro-f1 of vg macro-f1 is 57. f1 of vg f1 is 32. macro-f1 of vg macro-f1 is 65.9. f1 of vg f1 is 45. macro-f1 of vg macro-f1 is 51.7. f1 of vg f1 is 10.5. macro-f1 of vg macro-f1 is 54.7. f1 of vg f1 is 22. macro-f1 of vg macro-f1 is 57.6. f1 of vg f1 is 27. macro-f1 of wd macro-f1 is 64.4. f1 of wd f1 is 3.5. macro-f1 of wd macro-f1 is 51.3. f1 of wd f1 is 1.3. macro-f1 of wd macro-f1 is 41.3. f1 of wd f1 is 0. macro-f1 of wd macro-f1 is 44.5. f1 of wd f1 is 0. macro-f1 of wd macro-f1 is 61.1. f1 of wd f1 is 25.8. macro-f1 of wd macro-f1 is 46.7. f1 of wd f1 is 0. macro-f1 of wd macro-f1 is 49.6. f1 of wd f1 is 1. macro-f1 of wtp macro-f1 is 58.5. f1 of wtp f1 is 26.6. macro-f1 of wtp macro-f1 is 56.8. f1 of wtp f1 is 15.4. macro-f1 of wtp macro-f1 is 56. f1 of wtp f1 is 18.5. macro-f1 of wtp macro-f1 is 55.3. f1 of wtp f1 is 19.4. macro-f1 of wtp macro-f1 is 52.9. f1 of wtp f1 is 11.6. macro-f1 of wtp macro-f1 is 58.6. f1 of wtp f1 is 28.9. macro-f1 of wtp macro-f1 is 55.9. f1 of wtp f1 is 18.3. macro-f1 of  average macro-f1 is 61.7. f1 of  average f1 is 27.9. macro-f1 of  average macro-f1 is 53.8. f1 of  average f1 is 10.6. macro-f1 of  average macro-f1 is 53.5. f1 of  average f1 is 23.1. macro-f1 of  average macro-f1 is 54.7. f1 of  average f1 is 18.2. macro-f1 of  average macro-f1 is 53.4. f1 of  average f1 is 12.1. macro-f1 of  average macro-f1 is 52.8. f1 of  average f1 is 15.6. macro-f1 of  average macro-f1 is 55. f1 of  average f1 is 17.9. macro-f1 of mt macro-f1 is 74.4. f1 of mt f1 is 62.7. macro-f1 of mt macro-f1 is 53.9. f1 of mt f1 is 17. macro-f1 of mt macro-f1 is 51.9. f1 of mt f1 is 29.5. macro-f1 of mt macro-f1 is 56.1. f1 of mt f1 is 34.2. macro-f1 of mt macro-f1 is 55.1. f1 of mt f1 is 14.5. macro-f1 of mt macro-f1 is 52.5. f1 of mt f1 is 21.2. macro-f1 of mt macro-f1 is 53.9. f1 of mt f1 is 23.3. macro-f1 of oc macro-f1 is 60. f1 of oc f1 is 45.1. macro-f1 of oc macro-f1 is 59.9. f1 of oc f1 is 22.9. macro-f1 of oc macro-f1 is 56.7. f1 of oc f1 is 47. macro-f1 of oc macro-f1 is 58.6. f1 of oc f1 is 38. macro-f1 of oc macro-f1 is 54.1. f1 of oc f1 is 12.2. macro-f1 of oc macro-f1 is 57.7. f1 of oc f1 is 27.5. macro-f1 of oc macro-f1 is 57.4. f1 of oc f1 is 34. macro-f1 of pe macro-f1 is 58.1. f1 of pe f1 is 36.3. macro-f1 of pe macro-f1 is 54.6. f1 of pe f1 is 17.3. macro-f1 of pe macro-f1 is 70.6. f1 of pe f1 is 60.6. macro-f1 of pe macro-f1 is 54.1. f1 of pe f1 is 21.4. macro-f1 of pe macro-f1 is 54. f1 of pe f1 is 13.5. macro-f1 of pe macro-f1 is 54.4. f1 of pe f1 is 20.4. macro-f1 of pe macro-f1 is 55. f1 of pe f1 is 21.8. macro-f1 of vg macro-f1 is 65.8. f1 of vg f1 is 51.4. macro-f1 of vg macro-f1 is 57.3. f1 of vg f1 is 21.7. macro-f1 of vg macro-f1 is 57. f1 of vg f1 is 45.1. macro-f1 of vg macro-f1 is 62.5. f1 of vg f1 is 42.6. macro-f1 of vg macro-f1 is 54.5. f1 of vg f1 is 13.1. macro-f1 of vg macro-f1 is 55.1. f1 of vg f1 is 24.8. macro-f1 of vg macro-f1 is 57.9. f1 of vg f1 is 31.2. macro-f1 of wd macro-f1 is 62.6. f1 of wd f1 is 38.5. macro-f1 of wd macro-f1 is 55.4. f1 of wd f1 is 19. macro-f1 of wd macro-f1 is 56. f1 of wd f1 is 30.1. macro-f1 of wd macro-f1 is 55.1. f1 of wd f1 is 23.3. macro-f1 of wd macro-f1 is 63.8. f1 of wd f1 is 23.3. macro-f1 of wd macro-f1 is 53.6. f1 of wd f1 is 20.9. macro-f1 of wd macro-f1 is 56.5. f1 of wd f1 is 26.3. macro-f1 of wtp macro-f1 is 58. f1 of wtp f1 is 41.7. macro-f1 of wtp macro-f1 is 56.1. f1 of wtp f1 is 20.3. macro-f1 of wtp macro-f1 is 56.8. f1 of wtp f1 is 42.6. macro-f1 of wtp macro-f1 is 59.1. f1 of wtp f1 is 38. macro-f1 of wtp macro-f1 is 52.2. f1 of wtp f1 is 11.2. macro-f1 of wtp macro-f1 is 59.7. f1 of wtp f1 is 30.2. macro-f1 of wtp macro-f1 is 56.5. f1 of wtp f1 is 30.8. macro-f1 of average macro-f1 is 60.9. f1 of average f1 is 42.6. macro-f1 of average macro-f1 is 55.5. f1 of average f1 is 19.1. macro-f1 of average macro-f1 is 55.7. f1 of average f1 is 38.9. macro-f1 of average macro-f1 is 56.6. f1 of average f1 is 31. macro-f1 of average macro-f1 is 54. f1 of average f1 is 12.9. macro-f1 of average macro-f1 is 54.7. f1 of average f1 is 23. macro-f1 of average macro-f1 is 56.2. f1 of average f1 is 27.9. macro-f1 of +discourse macro-f1 is 40.2. f1 of +discourse f1 is 15. macro-f1 of +discourse macro-f1 is 31.7. f1 of +discourse f1 is 5.8. macro-f1 of +discourse macro-f1 is 30.3. f1 of +discourse f1 is 27.4. macro-f1 of +discourse macro-f1 is 27.7. f1 of +discourse f1 is 19.9. macro-f1 of +discourse macro-f1 is 40.9. f1 of +discourse f1 is 4.5. macro-f1 of +discourse macro-f1 is 25.3. f1 of +discourse f1 is 13.3. macro-f1 of +discourse macro-f1 is 32.7. f1 of +discourse f1 is 14.3. macro-f1 of +embeddings macro-f1 is 56.6. f1 of +embeddings f1 is 35.2. macro-f1 of +embeddings macro-f1 is 51.4. f1 of +embeddings f1 is 12.8. macro-f1 of +embeddings macro-f1 is 53.6. f1 of +embeddings f1 is 30.7. macro-f1 of +embeddings macro-f1 is 53.3. f1 of +embeddings f1 is 24.3. macro-f1 of +embeddings macro-f1 is 54.2. f1 of +embeddings f1 is 13.2. macro-f1 of +embeddings macro-f1 is 52.9. f1 of +embeddings f1 is 19. macro-f1 of +embeddings macro-f1 is 53.7. f1 of +embeddings f1 is 22.5. macro-f1 of +lexical macro-f1 is 61. f1 of +lexical f1 is 42.2. macro-f1 of +lexical macro-f1 is 55.2. f1 of +lexical f1 is 18.3. macro-f1 of +lexical macro-f1 is 56.2. f1 of +lexical f1 is 38.6. macro-f1 of +lexical macro-f1 is 54.7. f1 of +lexical f1 is 29.1. macro-f1 of +lexical macro-f1 is 53.1. f1 of +lexical f1 is 11.9. macro-f1 of +lexical macro-f1 is 54.9. f1 of +lexical f1 is 23.4. macro-f1 of +lexical macro-f1 is 55.9. f1 of +lexical f1 is 27.2. macro-f1 of +structure macro-f1 is 44.2. f1 of +structure f1 is 22.9. macro-f1 of +structure macro-f1 is 53.6. f1 of +structure f1 is 18.5. macro-f1 of +structure macro-f1 is 52.5. f1 of +structure f1 is 38.4. macro-f1 of +structure macro-f1 is 53.6. f1 of +structure f1 is 32.1. macro-f1 of +structure macro-f1 is 49.1. f1 of +structure f1 is 9. macro-f1 of +structure macro-f1 is 53.4. f1 of +structure f1 is 23.3. macro-f1 of +structure macro-f1 is 51.1. f1 of +structure f1 is 24. macro-f1 of +syntax macro-f1 is 54.8. f1 of +syntax f1 is 37. macro-f1 of +syntax macro-f1 is 54.2. f1 of +syntax f1 is 17.5. macro-f1 of +syntax macro-f1 is 54.3. f1 of +syntax f1 is 40.6. macro-f1 of +syntax macro-f1 is 55.7. f1 of +syntax f1 is 32. macro-f1 of +syntax macro-f1 is 53. f1 of +syntax f1 is 11.8. macro-f1 of +syntax macro-f1 is 53.8. f1 of +syntax f1 is 22.5. macro-f1 of +syntax macro-f1 is 54.3. f1 of +syntax f1 is 26.9. macro-f1 of majority bsl macro-f1 is 42.9. f1 of majority bsl f1 is 0. macro-f1 of majority bsl macro-f1 is 48. f1 of majority bsl f1 is 0. macro-f1 of majority bsl macro-f1 is 41.3. f1 of majority bsl f1 is 0. macro-f1 of majority bsl macro-f1 is 44.5. f1 of majority bsl f1 is 0. macro-f1 of majority bsl macro-f1 is 48.6. f1 of majority bsl f1 is 0. macro-f1 of majority bsl macro-f1 is 46.7. f1 of majority bsl f1 is 0. macro-f1 of majority bsl macro-f1 is 45.3. f1 of majority bsl f1 is 0. macro-f1 of random bsl macro-f1 is 47.5. f1 of random bsl f1 is 30.6. macro-f1 of random bsl macro-f1 is 50.5. f1 of random bsl f1 is 14. macro-f1 of random bsl macro-f1 is 51. f1 of random bsl f1 is 38.4. macro-f1 of random bsl macro-f1 is 51. f1 of random bsl f1 is 29.3. macro-f1 of random bsl macro-f1 is 49.3. f1 of random bsl f1 is 9.3. macro-f1 of random bsl macro-f1 is 50.3. f1 of random bsl f1 is 20.2. macro-f1 of random bsl macro-f1 is 49.9. f1 of random bsl f1 is 23.6.
table 2 shows results for the full qg systems using bleu 1–4, meteor. bleu 1 of lreg(c&l)+ nqg bleu 1 is 38.3. bleu 2 of lreg(c&l)+ nqg bleu 2 is 23.15. bleu 3 of lreg(c&l)+ nqg bleu 3 is 15.64. bleu 4 of lreg(c&l)+ nqg bleu 4 is 10.97. meteor of lreg(c&l)+ nqg meteor is 15.09. bleu 1 of ours + nqg bleu 1 is 40.08. bleu 2 of ours + nqg bleu 2 is 24.26. bleu 3 of ours + nqg bleu 3 is 16.39. bleu 4 of ours + nqg bleu 4 is 11.5. meteor of ours + nqg meteor is 15.67. bleu 1 of lreg(c&l)+ nqg bleu 1 is 51.55. bleu 2 of lreg(c&l)+ nqg bleu 2 is 40.17. bleu 3 of lreg(c&l)+ nqg bleu 3 is 34.35. bleu 4 of lreg(c&l)+ nqg bleu 4 is 30.59. meteor of lreg(c&l)+ nqg meteor is 24.17. bleu 1 of ours + nqg bleu 1 is 52.89. bleu 2 of ours + nqg bleu 2 is 41.16. bleu 3 of ours + nqg bleu 3 is 35.15. bleu 4 of ours + nqg bleu 4 is 31.25. meteor of ours + nqg meteor is 24.76.
table 1 shows comparison results on overall evaluation r-1 of lead r-1 is 0.48029. r-2 of lead r-2 is 0.16183. r-su4 of lead r-su4 is 0.21156. r-1 of coverage r-1 is 0.48085. r-2 of coverage r-2 is 0.15849. r-su4 of coverage r-su4 is 0.20615. r-1 of textrank r-1 is 0.49453. r-2 of textrank r-2 is 0.1637. r-su4 of textrank r-su4 is 0.21457. r-1 of centroid r-1 is 0.48582. r-2 of centroid r-2 is 0.16099. r-su4 of centroid r-su4 is 0.20919. r-1 of ilp r-1 is 0.49302. r-2 of ilp r-2 is 0.16651. r-su4 of ilp r-su4 is 0.21493. r-1 of clustercmrw r-1 is 0.49363. r-2 of clustercmrw r-2 is 0.17205. r-su4 of clustercmrw r-su4 is 0.22033. r-1 of submodular r-1 is 0.50273. r-2 of submodular r-2 is 0.16963. r-su4 of submodular r-su4 is 0.21775. r-1 of sendivrank r-1 is 0.48701. r-2 of sendivrank r-2 is 0.17491. r-su4 of sendivrank r-su4 is 0.22382. r-1 of our approach r-1 is 0.50215. r-2 of our approach r-2 is 0.18631. r-su4 of our approach r-su4 is 0.23426.
table 2 shows comparison results on two-part evaluation i r-1 of lead r-1 is 0.38757. r-2 of lead r-2 is 0.10631. r-su4 of lead r-su4 is 0.15138. r-1 of coverage r-1 is 0.38932. r-2 of coverage r-2 is 0.10399. r-su4 of coverage r-su4 is 0.14714. r-1 of textrank r-1 is 0.40246. r-2 of textrank r-2 is 0.10651. r-su4 of textrank r-su4 is 0.15327. r-1 of centroid r-1 is 0.3891. r-2 of centroid r-2 is 0.10297. r-su4 of centroid r-su4 is 0.14774. r-1 of ilp r-1 is 0.40004. r-2 of ilp r-2 is 0.11256. r-su4 of ilp r-su4 is 0.15641. r-1 of clustercmrw r-1 is 0.40565. r-2 of clustercmrw r-2 is 0.11855. r-su4 of clustercmrw r-su4 is 0.16195. r-1 of submodular r-1 is 0.3999. r-2 of submodular r-2 is 0.11044. r-su4 of submodular r-su4 is 0.15442. r-1 of sendivrank r-1 is 0.39462. r-2 of sendivrank r-2 is 0.11575. r-su4 of sendivrank r-su4 is 0.16028. r-1 of our approach r-1 is 0.41913. r-2 of our approach r-2 is 0.13369. r-su4 of our approach r-su4 is 0.17735.
table 4 shows manual evaluation results cov. of textrank cov. is 2.86. read. of textrank read. is 2.34. overall of textrank overall is 2.5. cov. of centroid cov. is 2.83. read. of centroid read. is 2.17. overall of centroid overall is 2.33. cov. of ilp cov. is 2.17. read. of ilp read. is 1.17. overall of ilp overall is 2.27. cov. of clustercmrw cov. is 3.33. read. of clustercmrw read. is 2.34. overall of clustercmrw overall is 2.83. cov. of submodular cov. is 2.51. read. of submodular read. is 2.03. overall of submodular overall is 2.34. cov. of sendivrank cov. is 3.51. read. of sendivrank read. is 2.47. overall of sendivrank overall is 0.86. cov. of our approach cov. is 3.85. read. of our approach read. is 3.32. overall of our approach overall is 3.47.
table 1 shows comparisons of feature weights learned using in-doc or cross-doc coreferent event pairs, euc: euclidean distance, cos: cosine similarity f1 of event word embedding: euc wd is 1.017. f1 of event word embedding: euc cd is 0.207. f1 of event word embedding: cos wd is 1.086. f1 of event word embedding: cos cd is 1.142. f1 of context embedding: euc wd is 0.038. f1 of context embedding: euc cd is 0.422. f1 of context embedding: cos wd is 0.004. f1 of context embedding: cos cd is 3.91. f1 of argument embedding wd is 0.349. f1 of argument embedding cd is 3.27.
table 3 shows withinand cross-document event coreference result on ecb+ corpus. r of lemma r is 39.5. p of lemma p is 73.9. f1 of lemma f1 is 51.4. r of lemma r is 58.1. p of lemma p is 78.2. f1 of lemma f1 is 66.7. r of lemma r is 58.9. p of lemma p is 37.5. f1 of lemma f1 is 46.2. f1 of lemma f1 is 54.8. r of common classifier (wd) r is 46. p of common classifier (wd) p is 72.8. f1 of common classifier (wd) f1 is 56.4. r of common classifier (wd) r is 60.4. p of common classifier (wd) p is 76.8. f1 of common classifier (wd) f1 is 68.4. r of common classifier (wd) r is 59.5. p of common classifier (wd) p is 42.1. f1 of common classifier (wd) f1 is 49.3. f1 of common classifier (wd) f1 is 58. r of common classifier (wd) + 2nd order relations r is 48.8. p of common classifier (wd) + 2nd order relations p is 72.1. f1 of common classifier (wd) + 2nd order relations f1 is 58.2. r of common classifier (wd) + 2nd order relations r is 61.8. p of common classifier (wd) + 2nd order relations p is 78.9. f1 of common classifier (wd) + 2nd order relations f1 is 69.3. r of common classifier (wd) + 2nd order relations r is 59.3. p of common classifier (wd) + 2nd order relations p is 44.1. f1 of common classifier (wd) + 2nd order relations f1 is 50.6. f1 of common classifier (wd) + 2nd order relations f1 is 59.4. r of common classifier (cd) r is 44.9. p of common classifier (cd) p is 64.7. f1 of common classifier (cd) f1 is 53. r of common classifier (cd) r is 66.1. p of common classifier (cd) p is 66.4. f1 of common classifier (cd) f1 is 66.2. r of common classifier (cd) r is 51.9. p of common classifier (cd) p is 46.4. f1 of common classifier (cd) f1 is 49. f1 of common classifier (cd) f1 is 56.1. r of common classifier (cd) + 2nd order relations r is 52.2. p of common classifier (cd) + 2nd order relations p is 58.4. f1 of common classifier (cd) + 2nd order relations f1 is 55.1. r of common classifier (cd) + 2nd order relations r is 70.4. p of common classifier (cd) + 2nd order relations p is 66.2. f1 of common classifier (cd) + 2nd order relations f1 is 68.3. r of common classifier (cd) + 2nd order relations r is 54.1. p of common classifier (cd) + 2nd order relations p is 45.2. f1 of common classifier (cd) + 2nd order relations f1 is 49.2. f1 of common classifier (cd) + 2nd order relations f1 is 57.5. r of wd & cd classifiers r is 49. p of wd & cd classifiers p is 71.9. f1 of wd & cd classifiers f1 is 58.3. r of wd & cd classifiers r is 63.8. p of wd & cd classifiers p is 78.9. f1 of wd & cd classifiers f1 is 70.6. r of wd & cd classifiers r is 59.3. p of wd & cd classifiers p is 48.1. f1 of wd & cd classifiers f1 is 53.1. f1 of wd & cd classifiers f1 is 60.7. r of wd & cd classifiers + 2nd order relations (full model) r is 56.2. p of wd & cd classifiers + 2nd order relations (full model) p is 66.6. f1 of wd & cd classifiers + 2nd order relations (full model) f1 is 61. r of wd & cd classifiers + 2nd order relations (full model) r is 67.5. p of wd & cd classifiers + 2nd order relations (full model) p is 80.4. f1 of wd & cd classifiers + 2nd order relations (full model) f1 is 73.4. r of wd & cd classifiers + 2nd order relations (full model) r is 59. p of wd & cd classifiers + 2nd order relations (full model) p is 54.2. f1 of wd & cd classifiers + 2nd order relations (full model) f1 is 56.5. f1 of wd & cd classifiers + 2nd order relations (full model) f1 is 63.6. r of hddcrp yang et al. (2015) r is 40.6. p of hddcrp yang et al. (2015) p is 78.5. f1 of hddcrp yang et al. (2015) f1 is 53.5. r of hddcrp yang et al. (2015) r is 67.1. p of hddcrp yang et al. (2015) p is 80.3. f1 of hddcrp yang et al. (2015) f1 is 73.1. r of hddcrp yang et al. (2015) r is 68.9. p of hddcrp yang et al. (2015) p is 38.6. f1 of hddcrp yang et al. (2015) f1 is 49.5. f1 of hddcrp yang et al. (2015) f1 is 58.7. r of hdp-lex bejan and harabagiu (2010) r is 43.7. p of hdp-lex bejan and harabagiu (2010) p is 65.6. f1 of hdp-lex bejan and harabagiu (2010) f1 is 52.5. r of hdp-lex bejan and harabagiu (2010) r is 63.5. p of hdp-lex bejan and harabagiu (2010) p is 75.5. f1 of hdp-lex bejan and harabagiu (2010) f1 is 69. r of hdp-lex bejan and harabagiu (2010) r is 60.2. p of hdp-lex bejan and harabagiu (2010) p is 34.8. f1 of hdp-lex bejan and harabagiu (2010) f1 is 44.1. f1 of hdp-lex bejan and harabagiu (2010) f1 is 55.2. r of agglomerative chen et al. (2009) r is 40.2. p of agglomerative chen et al. (2009) p is 73.2. f1 of agglomerative chen et al. (2009) f1 is 51.9. r of agglomerative chen et al. (2009) r is 59.2. p of agglomerative chen et al. (2009) p is 78.3. f1 of agglomerative chen et al. (2009) f1 is 67.4. r of agglomerative chen et al. (2009) r is 65.6. p of agglomerative chen et al. (2009) p is 30.2. f1 of agglomerative chen et al. (2009) f1 is 41.1. f1 of agglomerative chen et al. (2009) f1 is 53.6. r of lemma r is 56.8. p of lemma p is 80.9. f1 of lemma f1 is 66.7. r of lemma r is 35.9. p of lemma p is 76.2. f1 of lemma f1 is 48.8. r of lemma r is 67.4. p of lemma p is 62.9. f1 of lemma f1 is 65.1. f1 of lemma f1 is 60.2. r of common classifier (wd) r is 59.7. p of common classifier (wd) p is 80.5. f1 of common classifier (wd) f1 is 68.6. r of common classifier (wd) r is 44.6. p of common classifier (wd) p is 75. f1 of common classifier (wd) f1 is 55.9. r of common classifier (wd) r is 68.2. p of common classifier (wd) p is 67.7. f1 of common classifier (wd) f1 is 67.9. f1 of common classifier (wd) f1 is 64.2. r of common classifier (wd) + 2nd order relations r is 62.7. p of common classifier (wd) + 2nd order relations p is 79.4. f1 of common classifier (wd) + 2nd order relations f1 is 70. r of common classifier (wd) + 2nd order relations r is 50.3. p of common classifier (wd) + 2nd order relations p is 75.2. f1 of common classifier (wd) + 2nd order relations f1 is 60.3. r of common classifier (wd) + 2nd order relations r is 68.6. p of common classifier (wd) + 2nd order relations p is 70.5. f1 of common classifier (wd) + 2nd order relations f1 is 69.5. f1 of common classifier (wd) + 2nd order relations f1 is 66.6. r of common classifier (cd) r is 65.2. p of common classifier (cd) p is 67.1. f1 of common classifier (cd) f1 is 66.1. r of common classifier (cd) r is 47.6. p of common classifier (cd) p is 53.9. f1 of common classifier (cd) f1 is 50.5. r of common classifier (cd) r is 69.2. p of common classifier (cd) p is 62.1. f1 of common classifier (cd) f1 is 65.5. f1 of common classifier (cd) f1 is 60.7. r of common classifier (cd) + 2nd order relations r is 66.9. p of common classifier (cd) + 2nd order relations p is 69.1. f1 of common classifier (cd) + 2nd order relations f1 is 68. r of common classifier (cd) + 2nd order relations r is 56.7. p of common classifier (cd) + 2nd order relations p is 55.1. f1 of common classifier (cd) + 2nd order relations f1 is 55.9. r of common classifier (cd) + 2nd order relations r is 70.4. p of common classifier (cd) + 2nd order relations p is 63.6. f1 of common classifier (cd) + 2nd order relations f1 is 66.8. f1 of common classifier (cd) + 2nd order relations f1 is 62.8. r of wd & cd classifiers r is 63.8. p of wd & cd classifiers p is 79.9. f1 of wd & cd classifiers f1 is 70.9. r of wd & cd classifiers r is 51.6. p of wd & cd classifiers p is 75.3. f1 of wd & cd classifiers f1 is 61.2. r of wd & cd classifiers r is 68.6. p of wd & cd classifiers p is 70.5. f1 of wd & cd classifiers f1 is 69.5. f1 of wd & cd classifiers f1 is 67.2. r of wd & cd classifiers + 2nd order relations (full model) r is 69.2. p of wd & cd classifiers + 2nd order relations (full model) p is 76. f1 of wd & cd classifiers + 2nd order relations (full model) f1 is 72.4. r of wd & cd classifiers + 2nd order relations (full model) r is 58.5. p of wd & cd classifiers + 2nd order relations (full model) p is 67.3. f1 of wd & cd classifiers + 2nd order relations (full model) f1 is 62.6. r of wd & cd classifiers + 2nd order relations (full model) r is 67.9. p of wd & cd classifiers + 2nd order relations (full model) p is 76.1. f1 of wd & cd classifiers + 2nd order relations (full model) f1 is 71.8. f1 of wd & cd classifiers + 2nd order relations (full model) f1 is 68.9. r of hddcrp yang et al. (2015) r is 67.3. p of hddcrp yang et al. (2015) p is 85.6. f1 of hddcrp yang et al. (2015) f1 is 75.4. r of hddcrp yang et al. (2015) r is 41.7. p of hddcrp yang et al. (2015) p is 74.3. f1 of hddcrp yang et al. (2015) f1 is 53.4. r of hddcrp yang et al. (2015) r is 79. p of hddcrp yang et al. (2015) p is 65.1. f1 of hddcrp yang et al. (2015) f1 is 71.7. f1 of hddcrp yang et al. (2015) f1 is 66.8. r of hdp-lex bejan and harabagiu (2010) r is 67.6. p of hdp-lex bejan and harabagiu (2010) p is 74.7. f1 of hdp-lex bejan and harabagiu (2010) f1 is 71. r of hdp-lex bejan and harabagiu (2010) r is 39.1. p of hdp-lex bejan and harabagiu (2010) p is 50. f1 of hdp-lex bejan and harabagiu (2010) f1 is 43.9. r of hdp-lex bejan and harabagiu (2010) r is 71.4. p of hdp-lex bejan and harabagiu (2010) p is 66.2. f1 of hdp-lex bejan and harabagiu (2010) f1 is 68.7. f1 of hdp-lex bejan and harabagiu (2010) f1 is 61.2. r of agglomerative chen et al. (2009) r is 67.6. p of agglomerative chen et al. (2009) p is 80.7. f1 of agglomerative chen et al. (2009) f1 is 73.5. r of agglomerative chen et al. (2009) r is 39.2. p of agglomerative chen et al. (2009) p is 61.9. f1 of agglomerative chen et al. (2009) f1 is 48. r of agglomerative chen et al. (2009) r is 76. p of agglomerative chen et al. (2009) p is 65.6. f1 of agglomerative chen et al. (2009) f1 is 70.4. f1 of agglomerative chen et al. (2009) f1 is 63.9.
table 3 shows results of quality assessments with 5scale mean opinion scores (mos) and jfk style assessments with binary ratings. quality (mos) of vanilla-sampling quality (mos) is 2.286 ± 0.046. style of vanilla-sampling style is —. quality (mos) of selective-sampling quality (mos) is 2.681 ± 0.049. style of selective-sampling style is 10.42%. quality (mos) of cg-ir quality (mos) is 2.566 ± 0.048. style of cg-ir style is 10.24%. quality (mos) of rank quality (mos) is 2.477 ± 0.048. style of rank style is 21.88%. quality (mos) of multiply quality (mos) is 2.627 ± 0.048. style of multiply style is 13.54%. quality (mos) of finetune quality (mos) is 2.597 ± 0.046. style of finetune style is 20.83%. quality (mos) of finetune-cg-ir quality (mos) is 2.627 ± 0.049. style of finetune-cg-ir style is 20.31%. quality (mos) of finetune-cg-topic quality (mos) is 2.667 ± 0.045. style of finetune-cg-topic style is 21.09%. quality (mos) of singer-songwriter quality (mos) is 2.373 ± 0.045. style of singer-songwriter style is —. quality (mos) of starwars quality (mos) is 2.677 ± 0.048. style of starwars style is —.
table 1 shows results of different strategies to leverage the current label. accuracy of no current label switchboard is 72.93%. accuracy of no current label maptask is 61.27%. accuracy of true current label switchboard is 73.15%. accuracy of true current label maptask is 63.36%. accuracy of predicted current label switchboard is 73.91%. accuracy of predicted current label maptask is 64.53%. accuracy of scheduled sampling switchboard is 74.43%. accuracy of scheduled sampling maptask is 64.50%. accuracy of average embedding switchboard is 75.04%. accuracy of average embedding maptask is 65.09%. accuracy of uncertainty propagation switchboard is 75.61%. accuracy of uncertainty propagation maptask is 65.87%.
table 1 shows performance of three agents on different user types. succ. of rule succ. is 0.322. turn of rule turn is 46.2. reward of rule reward is -24. succ. of rule succ. is 0.24. turn of rule turn is 54.2. reward of rule reward is -42.9. succ. of rule succ. is 0.205. turn of rule turn is 54.3. reward of rule reward is -49.3. succ. of rule+ succ. is 0.535. turn of rule+ turn is 82. reward of rule+ reward is -3.7. succ. of rule+ succ. is 0.385. turn of rule+ turn is 110.5. reward of rule+ reward is -44.95. succ. of rule+ succ. is 0.34. turn of rule+ turn is 108.1. reward of rule+ reward is -51.85. succ. of rl succ. is 0.437. turn of rl turn is 45.6. reward of rl reward is -3.3. succ. of rl succ. is 0.34. turn of rl turn is 52.2. reward of rl reward is -23.8. succ. of rl succ. is 0.348. turn of rl turn is 49.5. reward of rl reward is -21.1. succ. of hrl succ. is 0.632. turn of hrl turn is 43. reward of hrl reward is 33.2. succ. of hrl succ. is 0.6. turn of hrl turn is 44.5. reward of hrl reward is 26.7. succ. of hrl succ. is 0.622. turn of hrl turn is 42.7. reward of hrl reward is 31.7.
table 4 shows overall macro and micro precision/recall. macro prec./rec. of baseline macro prec./rec. is 0.19. macro prec./rec. of baseline micro prec./rec. is 0.19. macro prec./rec. of uur macro prec./rec. is 0.33. macro prec./rec. of uur micro prec./rec. is 0.33.
table 6 shows bucket-wise precision (p)/recall (r) for u u r and the baseline metrics for the two new ground truths. p/r of sb p/r is 0.27. p/r of sb p/r is 0.27. p of sb p is 0.36. r of sb r is 0.25. r of sb r is 0.33. p/r of lb p/r is 0.09. p/r of lb p/r is 0.09. p of lb p is 0.18. r of lb r is 0.08. r of lb r is 0.17. p/r of bl p/r is 0.08. p/r of bl p/r is 0.16. p of bl p is 0.08. r of bl r is 0.28. r of bl r is 0.14. p/r of lm p/r is 0.18. p/r of lm p/r is 0.18. p of lm p is 0.45. r of lm r is 0.14. r of lm r is 0.35. p/r of sm p/r is 0.33. p/r of sm p/r is 0.41. p of sm p is 0.25. r of sm r is 0.41. r of sm r is 0.25.
table 7 shows results obtained for each dimension with the best combination of features for all dimensions (verb + personx + persony + personx persony, boldfaced in table 6) p of cooperative p is 0.73. r of cooperative r is 0.96. f of cooperative f is 0.83. p of cooperative p is 0. r of cooperative r is 0. f of cooperative f is 0. p of cooperative p is 0.6. r of cooperative r is 0.19. f of cooperative f is 0.29. p of cooperative p is 0.66. r of cooperative r is 0.72. f of cooperative f is 0.65. p of equal p is 0.56. r of equal r is 0.1. f of equal f is 0.17. p of equal p is 0. r of equal r is 0. f of equal f is 0. p of equal p is 0.74. r of equal r is 0.97. f of equal f is 0.84. p of equal p is 0.68. r of equal r is 0.74. f of equal f is 0.66. p of intense p is 0.39. r of intense r is 0.3. f of intense f is 0.34. p of intense p is 0. r of intense r is 0. f of intense f is 0. p of intense p is 0.78. r of intense r is 0.85. f of intense f is 0.82. p of intense p is 0.67. r of intense r is 0.71. f of intense f is 0.69. p of pleasure p is 0.4. r of pleasure r is 0.28. f of pleasure f is 0.33. p of pleasure p is 0. r of pleasure r is 0. f of pleasure f is 0. p of pleasure p is 0.87. r of pleasure r is 0.93. f of pleasure f is 0.9. p of pleasure p is 0.79. r of pleasure r is 0.82. f of pleasure f is 0.8. p of active p is 0.69. r of active r is 0.85. f of active f is 0.76. p of active p is 0. r of active r is 0. f of active f is 0. p of active p is 0.68. r of active r is 0.51. f of active f is 0.58. p of active p is 0.67. r of active r is 0.69. f of active f is 0.67. p of intimate p is 0.44. r of intimate r is 0.17. f of intimate f is 0.24. p of intimate p is 0. r of intimate r is 0. f of intimate f is 0. p of intimate p is 0.88. r of intimate r is 0.98. f of intimate f is 0.93. p of intimate p is 0.81. r of intimate r is 0.86. f of intimate f is 0.83. p of temporary p is 0.85. r of temporary r is 0.96. f of temporary f is 0.91. p of temporary p is 0. r of temporary r is 0. f of temporary f is 0. p of temporary p is 0.33. r of temporary r is 0.1. f of temporary f is 0.16. p of temporary p is 0.77. r of temporary r is 0.83. f of temporary f is 0.79. p of concurrent p is 0.72. r of concurrent r is 0.8. f of concurrent f is 0.76. p of concurrent p is 0. r of concurrent r is 0. f of concurrent f is 0. p of concurrent p is 0.77. r of concurrent r is 0.75. f of concurrent f is 0.76. p of concurrent p is 0.71. r of concurrent r is 0.74. f of concurrent f is 0.73. p of spat. near p is 0.66. r of spat. near r is 0.68. f of spat. near f is 0.67. p of spat. near p is 0. r of spat. near r is 0. f of spat. near f is 0. p of spat. near p is 0.73. r of spat. near r is 0.79. f of spat. near f is 0.76. p of spat. near p is 0.66. r of spat. near r is 0.7. f of spat. near f is 0.68. p of average p is 0.69. r of average r is 0.74. f of average f is 0.7. p of average p is 0. r of average r is 0. f of average f is 0. p of average p is 0.77. r of average r is 0.8. f of average f is 0.76. p of average p is 0.71. r of average r is 0.76. f of average f is 0.72.
table 4 shows results obtained on the test set for the argument detection task (l=lexical features) precision of rf+l precision is 0.76. recall of rf+l recall is 0.69. f1 of rf+l f1 is 0.71. precision of lr+l precision is 0.76. recall of lr+l recall is 0.71. f1 of lr+l f1 is 0.73. precision of lr+all features precision is 0.8. recall of lr+all features recall is 0.77. f1 of lr+all features f1 is 0.78.
table 5 shows results obtained by the best model on each category of the test set for the argument detection task p of non-arg p is 0.46. r of non-arg r is 0.6. f1 of non-arg f1 is 0.52. #arguments per category of non-arg #arguments per category is 187. p of arg p is 0.89. r of arg r is 0.82. f1 of arg f1 is 0.85. #arguments per category of arg #arguments per category is 713. p of avg/total p is 0.8. r of avg/total r is 0.77. f1 of avg/total f1 is 0.78. #arguments per category of avg/total #arguments per category is 900.
table 6 shows results obtained on the test set for the factual vs opinion argument classification task (l=lexical features) precision of rf+l precision is 0.75. recall of rf+l recall is 0.68. f1 of rf+l f1 is 0.71. precision of lr+l precision is 0.75. recall of lr+l recall is 0.75. f1 of lr+l f1 is 0.75. precision of lr+all features precision is 0.81. recall of lr+all features recall is 0.79. f1 of lr+all features f1 is 0.8.
table 8 shows results obtained on the test set for the source identification task precision of baseline precision is 0.26. recall of baseline recall is 0.48. f1 of baseline f1 is 0.33. precision of matching+heurist. precision is 0.69. recall of matching+heurist. recall is 0.64. f1 of matching+heurist. f1 is 0.67.
table 2 shows evaluation results of different ordered rules. success rate of r1 r2 r3 success rate is 0.695. #turn of r1 r2 r3 #turn is 4.58. reward of r1 r2 r3 reward is 0.4657. success rate of r1 r4 r2 r3 success rate is 0.749. #turn of r1 r4 r2 r3 #turn is 5.16. reward of r1 r4 r2 r3 reward is 0.491. success rate of r1* r2 r3 success rate is 0.705. #turn of r1* r2 r3 #turn is 4.44. reward of r1* r2 r3 reward is 0.4824. success rate of r1* r4 r2 r3 success rate is 0.753. #turn of r1* r4 r2 r3 #turn is 4.98. reward of r1* r4 r2 r3 reward is 0.5042.
table 3 shows cognate clustering results on the algonquian dataset (in %). found sets of heuristic baseline found sets is 18.9 (9.9). purity of heuristic baseline purity is 96.4. found sets of lexstat found sets is 19.6 (10.5). purity of lexstat purity is 97.1. found sets of semaphor found sets is 63.1(48.2). purity of semaphor purity is 70.3.
table 1 shows we show dramatic improvement on 3 european languages in a low-resource setting. f1 of baseline german is 22.61. f1 of baseline spanish is 45.77. f1 of baseline dutch is 43.1. f1 of baseline avg is 37.27. f1 of previous soa german is 48.12. f1 of previous soa spanish is 60.55. f1 of previous soa dutch is 61.56. f1 of previous soa avg is 56.74. f1 of cheap translation german is 57.53. f1 of cheap translation spanish is 65.18. f1 of cheap translation dutch is 66.5. f1 of cheap translation avg is 62.65.
table 4 shows comparison analysis for each slot type. impact of attention (%) of state_of_death local is 9.8. impact of attention (%) of state_of_death global-kb is -0.4. training data distribution (%) of state_of_death - is 0.9. f1 (%) of state_of_death - is 41.8. wide context distribution (%) of state_of_death - is 66.7. impact of dependency graph (%) of state_of_death - is 44.2. impact of attention (%) of date_of_birth local is 7.3. impact of attention (%) of date_of_birth global-kb is 121.3. training data distribution (%) of date_of_birth - is 1.3. f1 (%) of date_of_birth - is 84.1. wide context distribution (%) of date_of_birth - is 20. impact of dependency graph (%) of date_of_birth - is -81.9. impact of attention (%) of age local is 4.1. impact of attention (%) of age global-kb is -5.3. training data distribution (%) of age - is 1.3. f1 (%) of age - is 98.5. wide context distribution (%) of age - is 15.9. impact of dependency graph (%) of age - is 28.5. impact of attention (%) of per:alternate_names local is -2. impact of attention (%) of per:alternate_names global-kb is 21.2. training data distribution (%) of per:alternate_names - is 1.5. f1 (%) of per:alternate_names - is 36.6. wide context distribution (%) of per:alternate_names - is 41.5. impact of dependency graph (%) of per:alternate_names - is 62. impact of attention (%) of origin local is -0.9. impact of attention (%) of origin global-kb is 7.8. training data distribution (%) of origin - is 1.7. f1 (%) of origin - is 61.5. wide context distribution (%) of origin - is 29.3. impact of dependency graph (%) of origin - is 137.3. impact of attention (%) of country_of_birth local is 16.7. impact of attention (%) of country_of_birth global-kb is 12. training data distribution (%) of country_of_birth - is 1.9. f1 (%) of country_of_birth - is 61.5. wide context distribution (%) of country_of_birth - is 55.6. impact of dependency graph (%) of country_of_birth - is 162.5. impact of attention (%) of city_of_death local is 1.1. impact of attention (%) of city_of_death global-kb is 3.3. training data distribution (%) of city_of_death - is 1.9. f1 (%) of city_of_death - is 61.3. wide context distribution (%) of city_of_death - is 70.3. impact of dependency graph (%) of city_of_death - is 24.4. impact of attention (%) of state_of_headq. local is 9.7. impact of attention (%) of state_of_headq. global-kb is -5.1. training data distribution (%) of state_of_headq. - is 3.1. f1 (%) of state_of_headq. - is 51.7. wide context distribution (%) of state_of_headq. - is 54.8. impact of dependency graph (%) of state_of_headq. - is 95.7. impact of attention (%) of cities_of_residence local is 4.5. impact of attention (%) of cities_of_residence global-kb is 5.7. training data distribution (%) of cities_of_residence - is 3.5. f1 (%) of cities_of_residence - is 57.3. wide context distribution (%) of cities_of_residence - is 77. impact of dependency graph (%) of cities_of_residence - is 40.5. impact of attention (%) of states_of_residence local is -4.3. impact of attention (%) of states_of_residence global-kb is 2.3. training data distribution (%) of states_of_residence - is 3.8. f1 (%) of states_of_residence - is 50.5. wide context distribution (%) of states_of_residence - is 45.9. impact of dependency graph (%) of states_of_residence - is 175.8. impact of attention (%) of country_of_headq. local is 5.6. impact of attention (%) of country_of_headq. global-kb is -0.8. training data distribution (%) of country_of_headq. - is 5.3. f1 (%) of country_of_headq. - is 41.5. wide context distribution (%) of country_of_headq. - is 54.4. impact of dependency graph (%) of country_of_headq. - is 146.3. impact of attention (%) of city_of_headq. local is 1.6. impact of attention (%) of city_of_headq. global-kb is -6.9. training data distribution (%) of city_of_headq. - is 6.7. f1 (%) of city_of_headq. - is 30.3. wide context distribution (%) of city_of_headq. - is 54.9. impact of dependency graph (%) of city_of_headq. - is 39.3. impact of attention (%) of employee_of local is 14.9. impact of attention (%) of employee_of global-kb is 4.9. training data distribution (%) of employee_of - is 7.3. f1 (%) of employee_of - is 65.9. wide context distribution (%) of employee_of - is 54.9. impact of dependency graph (%) of employee_of - is 132.5. impact of attention (%) of countries_of_residence local is 37.7. impact of attention (%) of countries_of_residence global-kb is 8.6. training data distribution (%) of countries_of_residence - is 7.4. f1 (%) of countries_of_residence - is 47.4. wide context distribution (%) of countries_of_residence - is 47.2. impact of dependency graph (%) of countries_of_residence - is 134.9.
table 2 shows development set results on darkode. p of freq p is 41.9. r of freq r is 42.5. f1 of freq f1 is 42.2. p of freq p is 48.4. r of freq r is 33.5. f1 of freq f1 is 39.6. acc. of freq acc. is 45.3. p of dict p is 57.9. r of dict r is 51.1. f1 of dict f1 is 54.3. p of dict p is 65.6. r of dict r is 44. f1 of dict f1 is 52.7. acc. of dict acc. is 60.8. p of ner p is 59.7. r of ner r is 62.2. f1 of ner f1 is 60.9. p of ner p is 60.8. r of ner r is 62.6. f1 of ner f1 is 61.7. acc. of ner acc. is 72.2. p of binary p is 62.4. r of binary r is 76. f1 of binary f1 is 68.5. p of binary p is 58.1. r of binary r is 77.6. f1 of binary f1 is 66.4. acc. of binary acc. is 75.2. p of post p is 82.4. r of post r is 36.1. f1 of post f1 is 50.3. p of post p is 83.5. r of post r is 56.6. f1 of post f1 is 67.5. acc. of post acc. is 82.4. p of human* p is 86.9. r of human* r is 80.4. f1 of human* f1 is 83.5. p of human* p is 87.7. r of human* r is 77.6. f1 of human* f1 is 82.2. acc. of human* acc. is 89.2. p of freq p is 61.8. r of freq r is 28.9. f1 of freq f1 is 39.4. p of freq p is 61.8. r of freq r is 50. f1 of freq f1 is 55.2. acc. of freq acc. is 61.8. p of dict p is 57.9. r of dict r is 61.8. f1 of dict f1 is 59.8. p of dict p is 71.8. r of dict r is 57.5. f1 of dict f1 is 63.8. acc. of dict acc. is 68. p of first p is 73.1. r of first r is 34.2. f1 of first f1 is 46.7. p of first p is 70.3. r of first r is 59.1. f1 of first f1 is 65.4. acc. of first acc. is 73.1. p of ner p is 63.6. r of ner r is 63.3. f1 of ner f1 is 63.4. p of ner p is 69.7. r of ner r is 70.3. f1 of ner f1 is 70. acc. of ner acc. is 76.3. p of binary p is 67. r of binary r is 74.8. f1 of binary f1 is 70.7. p of binary p is 65.5. r of binary r is 82.5. f1 of binary f1 is 73. acc. of binary acc. is 82.4. p of post p is 87.6. r of post r is 41. f1 of post f1 is 55.9. p of post p is 87.6. r of post r is 70.8. f1 of post f1 is 78.3. acc. of post acc. is 87.6. p of human* p is 87.6. r of human* r is 83.2. f1 of human* f1 is 85.3. p of human* p is 91.6. r of human* r is 84.9. f1 of human* f1 is 88.1. acc. of human* acc. is 93.
table 5 shows product token out-of-vocabulary rates on development sets (test set for blackhat and nulled) of various forums with respect to training on darkode and hack forums. % oov of binary (darkode) % oov is 20. rseen of binary (darkode) rseen is 78. roov of binary (darkode) roov is 62. % oov of binary (darkode) % oov is 41. rseen of binary (darkode) rseen is 64. roov of binary (darkode) roov is 47. % oov of binary (darkode) % oov is 42. rseen of binary (darkode) rseen is 69. roov of binary (darkode) roov is 46. % oov of binary (darkode) % oov is 30. rseen of binary (darkode) rseen is 72. roov of binary (darkode) roov is 45. % oov of binary (hf) % oov is 50. rseen of binary (hf) rseen is 76. roov of binary (hf) roov is 40. % oov of binary (hf) % oov is 35. rseen of binary (hf) rseen is 75. roov of binary (hf) roov is 42. % oov of binary (hf) % oov is 51 70. rseen of binary (hf) rseen is 70. roov of binary (hf) roov is 38. % oov of binary (hf) % oov is 33. rseen of binary (hf) rseen is 83. roov of binary (hf) roov is 32.
table 3 shows results on genia. p of lcrf (single) p is 77.1. r of lcrf (single) r is 63.3. f1 of lcrf (single) f1 is 69.5. w/s of lcrf (single) w/s is 81.6. p of lcrf (multiple) p is 75.9. r of lcrf (multiple) r is 66.1. f1 of lcrf (multiple) f1 is 70.6. w/s of lcrf (multiple) w/s is 175.8. p of finkel and manning (2009) p is 75.4. r of finkel and manning (2009) r is 65.9. f1 of finkel and manning (2009) f1 is 70.3. w/s of finkel and manning (2009) w/s is  -. p of lu and roth (2015) p is 74.2. r of lu and roth (2015) r is 66.7. f1 of lu and roth (2015) f1 is 70.3. w/s of lu and roth (2015) w/s is 931.9. p of this work (state) p is 74. r of this work (state) r is 67.7. f1 of this work (state) f1 is 70.7. w/s of this work (state) w/s is 110.8. p of this work (edge) p is 75.4. r of this work (edge) r is 66.8. f1 of this work (edge) f1 is 70.8. w/s of this work (edge) w/s is 389.2.
table 1 shows overall span-level f1 results for keyphrase identification (semeval subtask a) and classification (semeval subtask b). f1 of gupta et.al.(unsupervised) classification (test) is 9.8. f1 of gupta et.al.(unsupervised) identification is 6.4. f1 of tsai et.al. (unsupervised) classification (test) is 11.9. f1 of tsai et.al. (unsupervised) identification is 8. f1 of multitask classification (dev) is 45.5. f1 of best non-neural semeval+ classification (test) is 38. f1 of best non-neural semeval+ identification is 51. f1 of best neural semeval+ classification (test) is 44. f1 of best neural semeval+ identification is 56. f1 of nn-crf (supervised) classification (dev) is 48.1. f1 of nn-crf (supervised) classification (test) is 40.2. f1 of nn-crf (supervised) identification is 52.1. f1 of nn-crf (semi) classification (dev) is 51.9. f1 of nn-crf (semi) classification (test) is 45.3. f1 of nn-crf (semi) identification is 56.9. f1 of nn-crf (semi)* classification (dev) is 52.1. f1 of nn-crf (semi)* classification (test) is 46.6. f1 of nn-crf (semi)* identification is 57.6.
table 2 shows results for ace-2004: f1 is calculated for predicted mentions, and accuracy on goldmentions. f1 of aida f1 is 77.8. f1 of wikifier f1 is 85.1. f1 of vinculum f1 is 88.5. f1 of model c f1 is 88.9. accuracy of model c accuracy is 93.1. f1 of model cdt f1 is 89.8. accuracy of model cdt accuracy is 93.9. f1 of model cdte f1 is 90.7. accuracy of model cdte accuracy is 94.3.
table 2 shows test results (f1 score) on the causeeffect subset(?) of semeval-2010 dataset. f1 score of tymoshenko and giuliano (2010) f1 score is 82.30%. f1 score of tratz and hovy (2010) f1 score is 87.63%. f1 score of rink and harabagiu (2010) f1 score is 89.63%. f1 score of bigru f1 score is 89.89%. f1 score of miwa and bansal (2016) f1 score is 91.57%. f1 score of contextual similarity modeling f1 score is 90.77%. f1 score of relational similarity modeling f1 score is 92.28%.
table 2 shows performance comparison of different outlier document detection methods. map of tfidf-cos map is 5.03. rcl@1% of tfidf-cos rcl@1% is 4.73. rcl@2% of tfidf-cos rcl@2% is 6.72. rcl@5% of tfidf-cos rcl@5% is 14.72. map of p2v-cos map is 22.07. rcl@1% of p2v-cos rcl@1% is 23.45. rcl@2% of p2v-cos rcl@2% is 44.64. rcl@5% of p2v-cos rcl@5% is 66.18. map of uni-kl map is 10.28. rcl@1% of uni-kl rcl@1% is 11.92. rcl@2% of uni-kl rcl@2% is 16.32. rcl@5% of uni-kl rcl@5% is 31.34. map of tm-kl map is 14.51. rcl@1% of tm-kl rcl@1% is 16.5. rcl@2% of tm-kl rcl@2% is 16.5. rcl@5% of tm-kl rcl@5% is 24.67. map of vmf-sf map is 33.7. rcl@1% of vmf-sf rcl@1% is 31.03. rcl@2% of vmf-sf rcl@2% is 44.45. rcl@5% of vmf-sf rcl@5% is 62.6. map of vmf-e map is 36.57. rcl@1% of vmf-e rcl@1% is 35.91. rcl@2% of vmf-e rcl@2% is 49.41. rcl@5% of vmf-e rcl@5% is 67.56. map of vmf-q map is 41.88. rcl@1% of vmf-q rcl@1% is 56.99. rcl@2% of vmf-q rcl@2% is 63.29. rcl@5% of vmf-q rcl@5% is 79.23. map of tfidf-cos map is 8.99. rcl@1% of tfidf-cos rcl@1% is 15.4. rcl@2% of tfidf-cos rcl@2% is 18.75. rcl@5% of tfidf-cos rcl@5% is 30.23. map of p2v-cos map is 7.39. rcl@1% of p2v-cos rcl@1% is 10.51. rcl@2% of p2v-cos rcl@2% is 14.78. rcl@5% of p2v-cos rcl@5% is 24.14. map of uni-kl map is 7.46. rcl@1% of uni-kl rcl@1% is 14.13. rcl@2% of uni-kl rcl@2% is 22.26. rcl@5% of uni-kl rcl@5% is 39.4. map of tm-kl map is 10.09. rcl@1% of tm-kl rcl@1% is 12.04. rcl@2% of tm-kl rcl@2% is 15.37. rcl@5% of tm-kl rcl@5% is 20.24. map of vmf-sf map is 10.69. rcl@1% of vmf-sf rcl@1% is 12.05. rcl@2% of vmf-sf rcl@2% is 22.58. rcl@5% of vmf-sf rcl@5% is 44.51. map of vmf-e map is 10.51. rcl@1% of vmf-e rcl@1% is 12.67. rcl@2% of vmf-e rcl@2% is 25.92. rcl@5% of vmf-e rcl@5% is 45.37. map of vmf-q map is 19.74. rcl@1% of vmf-q rcl@1% is 22.4. rcl@2% of vmf-q rcl@2% is 34.4. rcl@5% of vmf-q rcl@5% is 53.87.
table 9 shows human evaluation on explanation chains generated by symbolic and neural reasoners. accuracy (%) of symb accuracy (%) is 42.5. accuracy (%) of neur accuracy (%) is 57.5.
table 2 shows result of thread-subtitle matching. p@1 of bow p@1 is 0.437. p@3 of bow p@3 is 0.718. p@5 of bow p@5 is 0.806. p@1 of bow p@1 is 0.449. p@3 of bow p@3 is 0.811. p@5 of bow p@5 is 0.909. p@1 of word2vec p@1 is 0.485. p@3 of word2vec p@3 is 0.699. p@5 of word2vec p@5 is 0.816. p@1 of word2vec p@1 is 0.453. p@3 of word2vec p@3 is 0.826. p@5 of word2vec p@5 is 0.89. p@1 of para2vec p@1 is 0.408. p@3 of para2vec p@3 is 0.612. p@5 of para2vec p@5 is 0.728. p@1 of para2vec p@1 is 0.504. p@3 of para2vec p@3 is 0.823. p@5 of para2vec p@5 is 0.894. p@1 of hdv p@1 is 0.466. p@3 of hdv p@3 is 0.621. p@5 of hdv p@5 is 0.777. p@1 of hdv p@1 is 0.496. p@3 of hdv p@3 is 0.819. p@5 of hdv p@5 is 0.913. p@1 of cdr p@1 is 0.505. p@3 of cdr p@3 is 0.689. p@5 of cdr p@5 is 0.786. p@1 of cdr p@1 is 0.52. p@3 of cdr p@3 is 0.854. p@5 of cdr p@5 is 0.941.
table 2 shows results of ablation tests for the coarsegrained classifier. precision of all precision is 0.862. recall of all recall is 0.641. f1 of all f1 is 0.735. precision of all - unigrams precision is 0.731. recall of all - unigrams recall is 0.487. f1 of all - unigrams f1 is 0.585. precision of all - bigrams precision is 0.885. recall of all - bigrams recall is 0.59. f1 of all - bigrams f1 is 0.708. precision of all - rel. location precision is 0.889. recall of all - rel. location recall is 0.615. f1 of all - rel. location f1 is 0.727. precision of all - topic models precision is 0.852. recall of all - topic models recall is 0.59. f1 of all - topic models f1 is 0.697. precision of all - productions precision is 0.957. recall of all - productions recall is 0.564. f1 of all - productions f1 is 0.71. precision of all - nonterminals precision is 0.913. recall of all - nonterminals recall is 0.538. f1 of all - nonterminals f1 is 0.677. precision of all - max. depth precision is 0.857. recall of all - max. depth recall is 0.615. f1 of all - max. depth f1 is 0.716. precision of all - avg. depth precision is 0.857. recall of all - avg. depth recall is 0.615. f1 of all - avg. depth f1 is 0.716. precision of phrase inclusion - baseline precision is 0.425. recall of phrase inclusion - baseline recall is 0.797. f1 of phrase inclusion - baseline f1 is 0.554. precision of paragraph vec. - 50 dimensions precision is 0.667. recall of paragraph vec. - 50 dimensions recall is 0.211. f1 of paragraph vec. - 50 dimensions f1 is 0.32. precision of paragraph vec. - 100 dimensions precision is 0.667. recall of paragraph vec. - 100 dimensions recall is 0.158. f1 of paragraph vec. - 100 dimensions f1 is 0.255.
table 4 shows experiment results on the development and test data of english switchboard data. p of crf p is 93.9. r of crf r is 78.3. f1 of crf f1 is 85.4. p of crf p is 91.7. r of crf r is 75.1. f1 of crf f1 is 82.6. p of bi-lstm p is 94.1. r of bi-lstm r is 79.3. f1 of bi-lstm f1 is 86.1. p of bi-lstm p is 91.7. r of bi-lstm r is 80.6. f1 of bi-lstm f1 is 85.8. p of greedy p is 91.4. r of greedy r is 83.7. f1 of greedy f1 is 87.4. p of greedy p is 91.1. r of greedy r is 83.3. f1 of greedy f1 is 87.1. p of greedy + beam p is 93.6. r of greedy + beam r is 83.6. f1 of greedy + beam f1 is 88.3. p of greedy + beam p is 92.8. r of greedy + beam r is 82.7. f1 of greedy + beam f1 is 87.5. p of greedy + scheduled p is 92.3. r of greedy + scheduled r is 84.3. f1 of greedy + scheduled f1 is 88.1. p of greedy + scheduled p is 91.1. r of greedy + scheduled r is 84.1. f1 of greedy + scheduled f1 is 87.5.
table 6 shows test result of our transition-based model using dps files for training. p of our p is 93.1. r of our r is 83.5. f1 of our f1 is 88.1. p of bi-lstm p is 92.4. r of bi-lstm r is 82. f1 of bi-lstm f1 is 86.9. p of m3n * (qian and liu, 2013) p is 90.6. r of m3n * (qian and liu, 2013) r is 78.7. f1 of m3n * (qian and liu, 2013) f1 is 84.2. p of crf p is 91.8. r of crf r is 77.2. f1 of crf f1 is 83.9.
table 7 shows performance on chinese annotated data the result of m3n∗ comes from our experiments with the toolkit4 released by qian and liu (2013), which use the same data set and pre-processing. p of our p is 68.9. r of our r is 40.4. f1 of our f1 is 50.9. p of our p is 77.2. r of our r is 37.7. f1 of our f1 is 50.6. p of bi-lstm p is 60.1. r of bi-lstm r is 41.3. f1 of bi-lstm f1 is 48.9. p of bi-lstm p is 65.3. r of bi-lstm r is 38.2. f1 of bi-lstm f1 is 48.2. p of crf p is 73.7. r of crf r is 33.5. f1 of crf f1 is 46.1. p of crf p is 77.7. r of crf r is 32. f1 of crf f1 is 45.3.
table 6 shows error-type performance before and after re-ranking on the fce test set (largest impact highlighted in bold; bottom part of the table displays negative effects on performance). f0.5 of m:adv f0.5 is 25. f0.5 of m:adv f0.5 is 31.25. f0.5 of m:verb f0.5 is 25.42. f0.5 of m:verb f0.5 is 29.85. f0.5 of r:noun:num f0.5 is 56.6. f0.5 of r:noun:num f0.5 is 62.5. f0.5 of r:noun:poss f0.5 is 35.71. f0.5 of r:noun:poss f0.5 is 55.56. f0.5 of r:other f0.5 is 34.99. f0.5 of r:other f0.5 is 38.75. f0.5 of r:pron f0.5 is 26.88. f0.5 of r:pron f0.5 is 33.33. f0.5 of r:verb:form f0.5 is 53.62. f0.5 of r:verb:form f0.5 is 58.06. f0.5 of r:verb:sva f0.5 is 58.38. f0.5 of r:verb:sva f0.5 is 69.4. f0.5 of r:verb:tense f0.5 is 31.94. f0.5 of r:verb:tense f0.5 is 36.29. f0.5 of u:adv f0.5 is 13.51. f0.5 of u:adv f0.5 is 22.73. f0.5 of u:det f0.5 is 46.27. f0.5 of u:det f0.5 is 55.3. f0.5 of u:noun f0.5 is 10.1. f0.5 of u:noun f0.5 is 15.72. f0.5 of u:prep f0.5 is 47.62. f0.5 of u:prep f0.5 is 53.4. f0.5 of u:pron f0.5 is 30.77. f0.5 of u:pron f0.5 is 39.33. f0.5 of u:punct f0.5 is 51.22. f0.5 of u:punct f0.5 is 58.38. f0.5 of u:verb:tense f0.5 is 28.41. f0.5 of u:verb:tense f0.5 is 41.67. f0.5 of m:prep f0.5 is 43.69. f0.5 of m:prep f0.5 is 39.43. f0.5 of m:verb:form f0.5 is 50. f0.5 of m:verb:form f0.5 is 38.46. f0.5 of r:adj f0.5 is 45.45. f0.5 of r:adj f0.5 is 37.67. f0.5 of r:contr f0.5 is 50. f0.5 of r:contr f0.5 is 27.78. f0.5 of r:wo f0.5 is 53.63. f0.5 of r:wo f0.5 is 48.74.
table 1 shows aesw development/test set correction results. correlation of no change dev is 89.68. correlation of no change test is 89.45. correlation of no change dev is 0. correlation of no change test is 0. correlation of smt-diffs+m2 dev is 90.44. correlation of smt-diffs+m2 dev is 38.55. correlation of smt-diffs+bleu dev is 90.9. correlation of smt-diffs+bleu dev is 37.66. correlation of word+bi-diffs dev is 91.18. correlation of word+bi-diffs dev is 38.88. correlation of char+bi-diffs dev is 91.28. correlation of char+bi-diffs dev is 40.11. correlation of smt+bleu dev is 90.95. correlation of smt+bleu test is 90.7. correlation of smt+bleu dev is 38.99. correlation of smt+bleu test is 38.31. correlation of word+bi dev is 91.34. correlation of word+bi test is 91.05. correlation of word+bi dev is 43.61. correlation of word+bi test is 42.78. correlation of charcnn dev is 91.23. correlation of charcnn test is 90.96. correlation of charcnn dev is 42.02. correlation of charcnn test is 41.21. correlation of char+bi dev is 91.46. correlation of char+bi test is 91.22. correlation of char+bi dev is 44.67. correlation of char+bi test is 44.62. correlation of word+dom dev is 91.25. correlation of word+dom dev is 43.12. correlation of word+bi+dom dev is 91.45. correlation of word+bi+dom dev is 44.33. correlation of charcnn+bi+dom dev is 91.15. correlation of charcnn+bi+dom dev is 40.79. correlation of charcnn+dom dev is 91.35. correlation of charcnn+dom dev is 43.94. correlation of char+bi+dom dev is 91.64. correlation of char+bi+dom test is 91.39. correlation of char+bi+dom dev is 47.25. correlation of char+bi+dom test is 46.72.
table 1 shows sentence-level formality quantifying evaluation (spearman’s rho) among different models with different vector spaces. rho of simdiff lsa is 0.66. rho of simdiff w2v is 0.654. rho of svm lsa is 0.657. rho of svm w2v is 0.585. rho of pca lsa is 0.656. rho of pca w2v is 0.663. rho of densifier lsa is 0.664. rho of densifier w2v is 0.644. rho of baseline lsa is 0.54. rho of baseline w2v is 0.54.
table 1 shows evaluation of translation quality. bleu of moses mt05 is 33.08. bleu of moses mt06 is 32.69. bleu of moses mt08 is 23.78. bleu of moses ave. is 28.24. bleu of moses delta is –. bleu of nematus mt05 is 34.35. bleu of nematus mt06 is 35.75. bleu of nematus mt08 is 25.39. bleu of nematus ave. is 30.57. bleu of nematus delta is –. bleu of +initenc mt05 is 36.05. bleu of +initenc mt06 is 36.44†. bleu of +initenc mt08 is 26.65†. bleu of +initenc ave. is 31.55. bleu of +initenc delta is +0.98. bleu of +initdec mt05 is 36.27. bleu of +initdec mt06 is 36.69†. bleu of +initdec mt08 is 27.11†. bleu of +initdec ave. is 31.90. bleu of +initdec delta is +1.33. bleu of +initenc+dec mt05 is 36.34. bleu of +initenc+dec mt06 is 36.82†. bleu of +initenc+dec mt08 is 27.18†. bleu of +initenc+dec ave. is 32.00. bleu of +initenc+dec delta is +1.43. bleu of +auxi mt05 is 35.26. bleu of +auxi mt06 is 36.47†. bleu of +auxi mt08 is 26.12†. bleu of +auxi ave. is 31.30. bleu of +auxi delta is +0.73. bleu of +gating auxi mt05 is 36.64. bleu of +gating auxi mt06 is 37.63†. bleu of +gating auxi mt08 is 26.85†. bleu of +gating auxi ave. is 32.24. bleu of +gating auxi delta is +1.67. bleu of +initenc+dec+gating auxi mt05 is 36.89. bleu of +initenc+dec+gating auxi mt06 is 37.76†. bleu of +initenc+dec+gating auxi mt08 is 27.57†. bleu of +initenc+dec+gating auxi ave. is 32.67. bleu of +initenc+dec+gating auxi delta is +2.10.
table 6 shows preordering results for english → japanese. frs of nakagawa (2015) frs is 81.6. frs of small ff frs is 75.2. size of small ff size is 0.5mb. frs of small ff + pos tags frs is 81.3. size of small ff + pos tags size is 1.3mb. frs of small ff + tagger input fts. frs is 76.6. size of small ff + tagger input fts. size is 3.7mb.
table 5 shows model performance on the politifact validation set. f1 of majority baseline text is 0.39. f1 of majority baseline text is 0.6. f1 of naive bayes text is 0.44. f1 of naive bayes +liwc is 0.58. f1 of naive bayes text is 0.16. f1 of naive bayes +liwc is 0.21. f1 of maxent text is 0.55. f1 of maxent +liwc is 0.58. f1 of maxent text is 0.2. f1 of maxent +liwc is 0.21. f1 of lstm text is 0.58. f1 of lstm +liwc is 0.57. f1 of lstm text is 0.21. f1 of lstm +liwc is 0.22.
table 1 shows topics evaluation: accuracy in word intrusion task. acc.@4 of vanilla-lda acc.@4 is 0.22. acc.@8 of vanilla-lda acc.@8 is 0.35. acc.@4 of key concept-lda acc.@4 is 0.29. acc.@8 of key concept-lda acc.@8 is 0.36. acc.@4 of graph-based clusters acc.@4 is 0.46. acc.@8 of graph-based clusters acc.@8 is 0.44. acc.@4 of k-means clusters acc.@4 is 0.72. acc.@8 of k-means clusters acc.@8 is 0.67. acc.@4 of key concept clusters acc.@4 is 0.86. acc.@8 of key concept clusters acc.@8 is 0.67.
table 2 shows number of violated constraints, mean amplified bias, and test performance before and after calibration using rba. viol. of crf viol. is 154. amp. bias of crf amp. bias is 0.05. perf.(%) of crf perf.(%) is 24.07. viol. of crf + rba viol. is 107. amp. bias of crf + rba amp. bias is 0.024. perf.(%) of crf + rba perf.(%) is 23.97.
table 3 shows results on the test sets of the trustpilot dataset, +demo setting. main of germany main is 85.1. priv. of germany priv. is 32.2. main of germany main is -0.6. priv. of germany priv. is -0.3. main of germany main is -1.3. priv. of germany priv. is 0.6. main of germany main is -0.8. priv. of germany priv. is 1.9. main of baseline main is 78.6. priv. of baseline priv. is 36.9. main of baseline main is . priv. of baseline priv. is . main of baseline main is . priv. of baseline priv. is . main of baseline main is . priv. of baseline priv. is . main of denmark main is 82.6. priv. of denmark priv. is 28.1. main of denmark main is -0.2. priv. of denmark priv. is 4.4. main of denmark main is -0.1. priv. of denmark priv. is 6. main of denmark main is -0.3. priv. of denmark priv. is 7.6. main of baseline main is 70.4. priv. of baseline priv. is 40. main of baseline main is . priv. of baseline priv. is . main of baseline main is . priv. of baseline priv. is . main of baseline main is . priv. of baseline priv. is . main of france main is 75.1. priv. of france priv. is 41.1. main of france main is -0.8. priv. of france priv. is 0.7. main of france main is -1.4. priv. of france priv. is -6.4. main of france main is -1.5. priv. of france priv. is -18.2. main of baseline main is 69.2. priv. of baseline priv. is 44.4. main of baseline main is . priv. of baseline priv. is . main of baseline main is . priv. of baseline priv. is . main of baseline main is . priv. of baseline priv. is . main of uk main is 87. priv. of uk priv. is 39.3. main of uk main is -0.5. priv. of uk priv. is 0.9. main of uk main is -0.2. priv. of uk priv. is 0.2. main of uk main is -0.1. priv. of uk priv. is 0.3. main of baseline main is 77.1. priv. of baseline priv. is 42.2. main of baseline main is . priv. of baseline priv. is . main of baseline main is . priv. of baseline priv. is . main of baseline main is . priv. of baseline priv. is . main of us main is 85. priv. of us priv. is 33.9. main of us main is -0.1. priv. of us priv. is 2.6. main of us main is -0.2. priv. of us priv. is 1.8. main of us main is 0.7. priv. of us priv. is 2.2. main of baseline main is 79.4. priv. of baseline priv. is 36.4. main of baseline main is . priv. of baseline priv. is . main of baseline main is . priv. of baseline priv. is . main of baseline main is . priv. of baseline priv. is .
table 4 shows results on the test sets of the trustpilot dataset, raw setting. main of germany main is 85.5. priv. of germany priv. is 32.1. main of germany main is 0.3. priv. of germany priv. is 0.5. main of germany main is -0.8. priv. of germany priv. is 0.9. main of germany main is -1.7. priv. of germany priv. is 2.2. main of baseline main is 78.6. priv. of baseline priv. is 36.9. main of baseline main is . priv. of baseline priv. is . main of baseline main is . priv. of baseline priv. is . main of baseline main is . priv. of baseline priv. is . main of denmark main is 82.3. priv. of denmark priv. is 37.3. main of denmark main is -0.6. priv. of denmark priv. is 0.6. main of denmark main is -0.1. priv. of denmark priv. is -0.3. main of denmark main is -0.2. priv. of denmark priv. is -0.1. main of baseline main is 70.4. priv. of baseline priv. is 40. main of baseline main is . priv. of baseline priv. is . main of baseline main is . priv. of baseline priv. is . main of baseline main is . priv. of baseline priv. is . main of france main is 72.7. priv. of france priv. is 40.6. main of france main is 1.8. priv. of france priv. is -0.1. main of france main is 1.9. priv. of france priv. is -0.4. main of france main is -0.3. priv. of france priv. is -0.1. main of baseline main is 69.2. priv. of baseline priv. is 44.4. main of baseline main is . priv. of baseline priv. is . main of baseline main is . priv. of baseline priv. is . main of baseline main is . priv. of baseline priv. is . main of uk main is 86.9. priv. of uk priv. is 40.1. main of uk main is -0.2. priv. of uk priv. is 1. main of uk main is 0. priv. of uk priv. is 1.2. main of uk main is 0. priv. of uk priv. is 0. main of baseline main is 77.1. priv. of baseline priv. is 42.2. main of baseline main is . priv. of baseline priv. is . main of baseline main is . priv. of baseline priv. is . main of baseline main is . priv. of baseline priv. is . main of us main is 84.5. priv. of us priv. is 36.1. main of us main is -1.1. priv. of us priv. is 0.2. main of us main is 0.5. priv. of us priv. is 0.1. main of us main is 0.3. priv. of us priv. is 0.5. main of baseline main is 79.4. priv. of baseline priv. is 36.4. main of baseline main is . priv. of baseline priv. is . main of baseline main is . priv. of baseline priv. is . main of baseline main is . priv. of baseline priv. is .
table 4 shows results of different adversarial configurations. accuracy of no adversary baseline sentiment is 67.4. accuracy of no adversary baseline race is 14.5. accuracy of no adversary baseline mention is 77.5. accuracy of no adversary baseline gender is 10.1. accuracy of no adversary baseline mention is 74.7. accuracy of no adversary baseline age is 9.4. accuracy of standard adversary sentiment is 64.7. accuracy of standard adversary race is 6.0. accuracy of standard adversary delta is 5.0. accuracy of standard adversary mention is 75.6. accuracy of standard adversary gender is 8.5. accuracy of standard adversary delta is 8.0. accuracy of standard adversary mention is 72.5. accuracy of standard adversary age is 7.3. accuracy of standard adversary delta is 6.9. accuracy of adv-capacity sentiment is 64.1. accuracy of adv-capacity race is 6.7. accuracy of adv-capacity delta is 5.2. accuracy of adv-capacity mention is 73.8. accuracy of adv-capacity gender is 8.1. accuracy of adv-capacity delta is 6.7. accuracy of adv-capacity mention is 71.4. accuracy of adv-capacity age is 4.3. accuracy of adv-capacity delta is 4.1. accuracy of adv-capacity sentiment is 63.4. accuracy of adv-capacity race is 7.1. accuracy of adv-capacity delta is 4.9. accuracy of adv-capacity mention is 75.2. accuracy of adv-capacity gender is 8.9. accuracy of adv-capacity delta is 7.0. accuracy of adv-capacity mention is 71.6. accuracy of adv-capacity age is 6.3. accuracy of adv-capacity delta is 4.0. accuracy of adv-capacity sentiment is 65.2. accuracy of adv-capacity race is 8.1. accuracy of adv-capacity delta is 6.9. accuracy of adv-capacity mention is 76.1. accuracy of adv-capacity gender is 6.7. accuracy of adv-capacity delta is 6.4. accuracy of adv-capacity mention is 71.9. accuracy of adv-capacity age is 6.0. accuracy of adv-capacity delta is 5.7. accuracy of adv-capacity sentiment is 63.9. accuracy of adv-capacity race is 6.2. accuracy of adv-capacity delta is 3.7. accuracy of adv-capacity mention is 74.5. accuracy of adv-capacity gender is 5.6. accuracy of adv-capacity delta is 1.6. accuracy of adv-capacity mention is 73.0. accuracy of adv-capacity age is 10.2. accuracy of adv-capacity delta is 9.6. accuracy of adv-capacity sentiment is 65.0. accuracy of adv-capacity race is 7.1. accuracy of adv-capacity delta is 4.8. accuracy of adv-capacity mention is 75.7. accuracy of adv-capacity gender is 5.4. accuracy of adv-capacity delta is 4.2. accuracy of adv-capacity mention is 71.9. accuracy of adv-capacity age is 9.8. accuracy of adv-capacity delta is 7.3. accuracy of lambda sentiment is 63.9. accuracy of lambda race is 6.8. accuracy of lambda delta is 6.2. accuracy of lambda mention is 75.6. accuracy of lambda gender is 7.8. accuracy of lambda delta is 6.8. accuracy of lambda mention is 73.1. accuracy of lambda age is 4.8. accuracy of lambda delta is 3.4. accuracy of lambda sentiment is 64.9. accuracy of lambda race is 7.4. accuracy of lambda delta is 5.4. accuracy of lambda mention is 75.6. accuracy of lambda gender is 4.9. accuracy of lambda delta is 2.4. accuracy of lambda mention is 72.5. accuracy of lambda age is 6.8. accuracy of lambda delta is 5.8. accuracy of lambda sentiment is 64.2. accuracy of lambda race is 7.3. accuracy of lambda delta is 5.9. accuracy of lambda mention is 76.0. accuracy of lambda gender is -7.2. accuracy of lambda delta is 6.7. accuracy of lambda mention is 72.1. accuracy of lambda age is 8.5. accuracy of lambda delta is 7.7. accuracy of lambda sentiment is 65.8. accuracy of lambda race is 10.2. accuracy of lambda delta is 10.1. accuracy of lambda mention is 73.7. accuracy of lambda gender is 6.4. accuracy of lambda delta is 6.1. accuracy of lambda mention is 72.5. accuracy of lambda age is -6.3. accuracy of lambda delta is 5.2. accuracy of lambda sentiment is 50.0. accuracy of lambda mention is 73.6. accuracy of lambda gender is 6.5. accuracy of lambda delta is 5.7. accuracy of lambda mention is 69.0. accuracy of lambda age is 3.2. accuracy of lambda delta is 2.9. accuracy of ensemble sentiment is 62.4. accuracy of ensemble race is 7.4. accuracy of ensemble delta is 5.4. accuracy of ensemble mention is 74.8. accuracy of ensemble gender is 6.4. accuracy of ensemble delta is 5.0. accuracy of ensemble mention is 72.8. accuracy of ensemble age is 8.8. accuracy of ensemble delta is 8.3. accuracy of ensemble sentiment is 66.5. accuracy of ensemble race is 6.5. accuracy of ensemble delta is 5.0. accuracy of ensemble mention is 75.3. accuracy of ensemble gender is 4.9. accuracy of ensemble delta is 3.1. accuracy of ensemble mention is 72.1. accuracy of ensemble age is 6.7. accuracy of ensemble delta is 6.0. accuracy of ensemble sentiment is 63.8. accuracy of ensemble race is 4.8. accuracy of ensemble delta is 2.6. accuracy of ensemble mention is 74.3. accuracy of ensemble gender is 4.1. accuracy of ensemble delta is 3.0. accuracy of ensemble mention is 70.1. accuracy of ensemble age is 5.7. accuracy of ensemble delta is 5.4.
table 1 shows results on the prediction task (test set). precision of prolocal precision is 77.4. recall of prolocal recall is 22.9. f1 of prolocal f1 is 35.3. precision of qrn precision is 55.5. recall of qrn recall is 31.3. f1 of qrn f1 is 40.0. precision of entnet precision is 50.2. recall of entnet recall is 33.5. f1 of entnet f1 is 40.2. precision of proglobal precision is 46.7. recall of proglobal recall is 52.4. f1 of proglobal f1 is 49.4. precision of prostruct precision is 74.2. recall of prostruct recall is 42.1. f1 of prostruct f1 is 53.7.
table 2 shows wikipage retrieval evaluation on dev. rate of 1 rate is 25.31. acc ceiling of 1 acc ceiling is 50.21. rate of 1 rate is 76.58. acc ceiling of 1 acc ceiling is 84.38. rate of 5 rate is 55.3. acc ceiling of 5 acc ceiling is 70.2. rate of 5 rate is 89.63. acc ceiling of 5 acc ceiling is 93.08. rate of 10 rate is 65.86. acc ceiling of 10 acc ceiling is 77.24. rate of 10 rate is 91.19. acc ceiling of 10 acc ceiling is 94.12. rate of 25 rate is 75.92. acc ceiling of 25 acc ceiling is 83.95. rate of 25 rate is 92.81. acc ceiling of 25 acc ceiling is 95.2. rate of 50 rate is 82.49. acc ceiling of 50 acc ceiling is 90.13. rate of 50 rate is 93.36. acc ceiling of 50 acc ceiling is 95.57. rate of 100 rate is 86.59. acc ceiling of 100 acc ceiling is 91.06. rate of 100 rate is 94.19. acc ceiling of 100 acc ceiling is 96.12.
table 3 shows performance on dev and test of fever. noscoreev of - noscoreev is 41.86. scoreev of - scoreev is 19.04. recall of - recall is 44.22. precision of - precision is 10.44. f1 of - f1 is 16.89. noscoreev of - noscoreev is 52.09. scoreev of - scoreev is 32.57. recall of - recall is 44.22. precision of - precision is 10.44. f1 of - f1 is 16.89. noscoreev of coarse & coarse noscoreev is . scoreev of coarse & coarse scoreev is . recall of coarse & coarse recall is . precision of coarse & coarse precision is . f1 of coarse & coarse f1 is . noscoreev of pipeline noscoreev is 35.72. scoreev of pipeline scoreev is 22.26. recall of pipeline recall is 53.75. precision of pipeline precision is 29.42. f1 of pipeline f1 is 33.80. noscoreev of diff-cnn noscoreev is 39.22. scoreev of diff-cnn scoreev is 21.04. recall of diff-cnn recall is 46.88. precision of diff-cnn precision is 43.01. f1 of diff-cnn f1 is 44.86. noscoreev of share-cnn noscoreev is 72.32. scoreev of share-cnn scoreev is 50.12. recall of share-cnn recall is 45.55. precision of share-cnn precision is 40.77. f1 of share-cnn f1 is 43.03. noscoreev of coarse & fine (single) noscoreev is 75.65. scoreev of coarse & fine (single) scoreev is 52.65. recall of coarse & fine (single) recall is 45.81. precision of coarse & fine (single) precision is 42.53. f1 of coarse & fine (single) f1 is 44.11. noscoreev of coarse & fine (two) noscoreev is 78.77. scoreev of coarse & fine (two) scoreev is 53.64. recall of coarse & fine (two) recall is 45.78. precision of coarse & fine (two) precision is 39.23. f1 of coarse & fine (two) f1 is 42.25. noscoreev of fine & sent-wise noscoreev is 71.02. scoreev of fine & sent-wise scoreev is 53.43. recall of fine & sent-wise recall is 52.70. precision of fine & sent-wise precision is 48.31. f1 of fine & sent-wise f1 is 50.40. noscoreev of fine & coarse noscoreev is 71.48. scoreev of fine & coarse scoreev is 53.17. recall of fine & coarse recall is 52.75. precision of fine & coarse precision is 47.30. f1 of fine & coarse f1 is 49.87. noscoreev of fine & fine (two) noscoreev is 78.90. scoreev of fine & fine (two) scoreev is 56.16. recall of fine & fine (two) recall is 53.81. precision of fine & fine (two) precision is 47.73. f1 of fine & fine (two) f1 is 50.59. noscoreev of - noscoreev is 50.91. scoreev of - scoreev is 31.87. recall of - recall is 45.89. precision of - precision is 10.79. f1 of - f1 is 17.47. noscoreev of - noscoreev is 75.99. scoreev of - scoreev is 54.33. recall of - recall is 49.91. precision of - precision is 44.68. f1 of - f1 is 47.15.
table 2 shows performance on the coco karpathy test split. meteor of hardatt (xu et al. 2015) meteor is 0.230. bleu-4 of hardatt (xu et al. 2015) bleu-4 is 0.250. meteor of att-fcn (you et al. 2016) meteor is 0.243. bleu-4 of att-fcn (you et al. 2016) bleu-4 is 0.304. cider of sca-cnn (chen et al. 2017) cider is 0.952. meteor of sca-cnn (chen et al. 2017) meteor is 0.250. rouge-l of sca-cnn (chen et al. 2017) rouge-l is 0.531. bleu-4 of sca-cnn (chen et al. 2017) bleu-4 is 0.311. spice of lstm-a (yao et al. 2017) spice is 0.186. cider of lstm-a (yao et al. 2017) cider is 1.002. meteor of lstm-a (yao et al. 2017) meteor is 0.254. rouge-l of lstm-a (yao et al. 2017) rouge-l is 0.540. bleu-4 of lstm-a (yao et al. 2017) bleu-4 is 0.326. cider of scn-lstm (gan et al. 2017) cider is 1.012. meteor of scn-lstm (gan et al. 2017) meteor is 0.257. bleu-4 of scn-lstm (gan et al. 2017) bleu-4 is 0.330. cider of skeleton (wang et al. 2017) cider is 1.069. meteor of skeleton (wang et al. 2017) meteor is 0.268. rouge-l of skeleton (wang et al. 2017) rouge-l is 0.552. bleu-4 of skeleton (wang et al. 2017) bleu-4 is 0.336. spice of adaatt (lu et al. 2017) spice is 0.195. cider of adaatt (lu et al. 2017) cider is 1.085. meteor of adaatt (lu et al. 2017) meteor is 0.266. rouge-l of adaatt (lu et al. 2017) rouge-l is 0.549. bleu-4 of adaatt (lu et al. 2017) bleu-4 is 0.332. spice of nbt (lu et al. 2018) spice is 0.201. cider of nbt (lu et al. 2018) cider is 1.072. meteor of nbt (lu et al. 2018) meteor is 0.271. bleu-4 of nbt (lu et al. 2018) bleu-4 is 0.347. cider of drl (ren et al. 2017b) cider is 0.937. meteor of drl (ren et al. 2017b) meteor is 0.251. rouge-l of drl (ren et al. 2017b) rouge-l is 0.525. bleu-4 of drl (ren et al. 2017b) bleu-4 is 0.304. cider of td-m-att (chen et al. 2018) cider is 1.116. meteor of td-m-att (chen et al. 2018) meteor is 0.268. rouge-l of td-m-att (chen et al. 2018) rouge-l is 0.555. bleu-4 of td-m-att (chen et al. 2018) bleu-4 is 0.336. cider of scst (rennie et al. 2017) cider is 1.140. meteor of scst (rennie et al. 2017) meteor is 0.267. rouge-l of scst (rennie et al. 2017) rouge-l is 0.557. bleu-4 of scst (rennie et al. 2017) bleu-4 is 0.342. spice of sr-pl (liu et al. 2018) spice is 0.210. cider of sr-pl (liu et al. 2018) cider is 1.171. meteor of sr-pl (liu et al. 2018) meteor is 0.274. rouge-l of sr-pl (liu et al. 2018) rouge-l is 0.570. bleu-4 of sr-pl (liu et al. 2018) bleu-4 is 0.358. spice of up-down (anderson et al. 2018) spice is 0.214. cider of up-down (anderson et al. 2018) cider is 1.201. meteor of up-down (anderson et al. 2018) meteor is 0.277. rouge-l of up-down (anderson et al. 2018) rouge-l is 0.569. bleu-4 of up-down (anderson et al. 2018) bleu-4 is 0.363. spice of simnet spice is 0.220. cider of simnet cider is 1.135. meteor of simnet meteor is 0.283. rouge-l of simnet rouge-l is 0.564. bleu-4 of simnet bleu-4 is 0.332.
table 6 shows performance on the online coco evaluation server. bleu-1 of hardatt (xu et al. 2015) c5 is 0.705. bleu-1 of hardatt (xu et al. 2015) c40 is 0.881. bleu-2 of hardatt (xu et al. 2015) c5 is 0.528. bleu-2 of hardatt (xu et al. 2015) c40 is 0.779. bleu-3 of hardatt (xu et al. 2015) c5 is 0.383. bleu-3 of hardatt (xu et al. 2015) c40 is 0.658. bleu-4 of hardatt (xu et al. 2015) c5 is 0.277. bleu-4 of hardatt (xu et al. 2015) c40 is 0.537. meteor of hardatt (xu et al. 2015) c5 is 0.241. meteor of hardatt (xu et al. 2015) c40 is 0.322. rouge-l of hardatt (xu et al. 2015) c5 is 0.516. rouge-l of hardatt (xu et al. 2015) c40 is 0.654. cider of hardatt (xu et al. 2015) c5 is 0.865. cider of hardatt (xu et al. 2015) c40 is 0.893. bleu-1 of att-fcn (you et al. 2016) c5 is 0.731. bleu-1 of att-fcn (you et al. 2016) c40 is 0.900. bleu-2 of att-fcn (you et al. 2016) c5 is 0.565. bleu-2 of att-fcn (you et al. 2016) c40 is 0.815. bleu-3 of att-fcn (you et al. 2016) c5 is 0.424. bleu-3 of att-fcn (you et al. 2016) c40 is 0.709. bleu-4 of att-fcn (you et al. 2016) c5 is 0.316. bleu-4 of att-fcn (you et al. 2016) c40 is 0.599. meteor of att-fcn (you et al. 2016) c5 is 0.250. meteor of att-fcn (you et al. 2016) c40 is 0.335. rouge-l of att-fcn (you et al. 2016) c5 is 0.535. rouge-l of att-fcn (you et al. 2016) c40 is 0.682. cider of att-fcn (you et al. 2016) c5 is 0.943. cider of att-fcn (you et al. 2016) c40 is 0.958. bleu-1 of sca-cnn (chen et al. 2017) c5 is 0.712. bleu-1 of sca-cnn (chen et al. 2017) c40 is 0.894. bleu-2 of sca-cnn (chen et al. 2017) c5 is 0.542. bleu-2 of sca-cnn (chen et al. 2017) c40 is 0.802. bleu-3 of sca-cnn (chen et al. 2017) c5 is 0.404. bleu-3 of sca-cnn (chen et al. 2017) c40 is 0.691. bleu-4 of sca-cnn (chen et al. 2017) c5 is 0.302. bleu-4 of sca-cnn (chen et al. 2017) c40 is 0.579. meteor of sca-cnn (chen et al. 2017) c5 is 0.244. meteor of sca-cnn (chen et al. 2017) c40 is 0.331. rouge-l of sca-cnn (chen et al. 2017) c5 is 0.524. rouge-l of sca-cnn (chen et al. 2017) c40 is 0.674. cider of sca-cnn (chen et al. 2017) c5 is 0.912. cider of sca-cnn (chen et al. 2017) c40 is 0.921. bleu-1 of lstm-a (yao et al. 2017) c5 is 0.739. bleu-1 of lstm-a (yao et al. 2017) c40 is 0.919. bleu-2 of lstm-a (yao et al. 2017) c5 is 0.575. bleu-2 of lstm-a (yao et al. 2017) c40 is 0.842. bleu-3 of lstm-a (yao et al. 2017) c5 is 0.436. bleu-3 of lstm-a (yao et al. 2017) c40 is 0.740. bleu-4 of lstm-a (yao et al. 2017) c5 is 0.330. bleu-4 of lstm-a (yao et al. 2017) c40 is 0.632. meteor of lstm-a (yao et al. 2017) c5 is 0.256. meteor of lstm-a (yao et al. 2017) c40 is 0.350. rouge-l of lstm-a (yao et al. 2017) c5 is 0.542. rouge-l of lstm-a (yao et al. 2017) c40 is 0.700. cider of lstm-a (yao et al. 2017) c5 is 0.984. cider of lstm-a (yao et al. 2017) c40 is 1.003. bleu-1 of scn-lstm (gan et al. 2017) c5 is 0.740. bleu-1 of scn-lstm (gan et al. 2017) c40 is 0.917. bleu-2 of scn-lstm (gan et al. 2017) c5 is 0.575. bleu-2 of scn-lstm (gan et al. 2017) c40 is 0.839. bleu-3 of scn-lstm (gan et al. 2017) c5 is 0.436. bleu-3 of scn-lstm (gan et al. 2017) c40 is 0.739. bleu-4 of scn-lstm (gan et al. 2017) c5 is 0.331. bleu-4 of scn-lstm (gan et al. 2017) c40 is 0.631. meteor of scn-lstm (gan et al. 2017) c5 is 0.257. meteor of scn-lstm (gan et al. 2017) c40 is 0.348. rouge-l of scn-lstm (gan et al. 2017) c5 is 0.543. rouge-l of scn-lstm (gan et al. 2017) c40 is 0.696. cider of scn-lstm (gan et al. 2017) c5 is 1.003. cider of scn-lstm (gan et al. 2017) c40 is 1.013. bleu-1 of adaatt (lu et al. 2017) c5 is 0.748. bleu-1 of adaatt (lu et al. 2017) c40 is 0.920. bleu-2 of adaatt (lu et al. 2017) c5 is 0.584. bleu-2 of adaatt (lu et al. 2017) c40 is 0.845. bleu-3 of adaatt (lu et al. 2017) c5 is 0.444. bleu-3 of adaatt (lu et al. 2017) c40 is 0.744. bleu-4 of adaatt (lu et al. 2017) c5 is 0.336. bleu-4 of adaatt (lu et al. 2017) c40 is 0.637. meteor of adaatt (lu et al. 2017) c5 is 0.264. meteor of adaatt (lu et al. 2017) c40 is 0.359. rouge-l of adaatt (lu et al. 2017) c5 is 0.550. rouge-l of adaatt (lu et al. 2017) c40 is 0.705. cider of adaatt (lu et al. 2017) c5 is 1.042. cider of adaatt (lu et al. 2017) c40 is 1.059. bleu-1 of td-m-att (chen et al. 2018) c5 is 0.757. bleu-1 of td-m-att (chen et al. 2018) c40 is 0.913. bleu-2 of td-m-att (chen et al. 2018) c5 is 0.591. bleu-2 of td-m-att (chen et al. 2018) c40 is 0.836. bleu-3 of td-m-att (chen et al. 2018) c5 is 0.441. bleu-3 of td-m-att (chen et al. 2018) c40 is 0.726. bleu-4 of td-m-att (chen et al. 2018) c5 is 0.324. bleu-4 of td-m-att (chen et al. 2018) c40 is 0.609. meteor of td-m-att (chen et al. 2018) c5 is 0.259. meteor of td-m-att (chen et al. 2018) c40 is 0.342. rouge-l of td-m-att (chen et al. 2018) c5 is 0.547. rouge-l of td-m-att (chen et al. 2018) c40 is 0.689. cider of td-m-att (chen et al. 2018) c5 is 1.059. cider of td-m-att (chen et al. 2018) c40 is 1.090. bleu-1 of scst (rennie et al. 2017) c5 is 0.781. bleu-1 of scst (rennie et al. 2017) c40 is 0.937. bleu-2 of scst (rennie et al. 2017) c5 is 0.619. bleu-2 of scst (rennie et al. 2017) c40 is 0.860. bleu-3 of scst (rennie et al. 2017) c5 is 0.470. bleu-3 of scst (rennie et al. 2017) c40 is 0.759. bleu-4 of scst (rennie et al. 2017) c5 is 0.352. bleu-4 of scst (rennie et al. 2017) c40 is 0.645. meteor of scst (rennie et al. 2017) c5 is 0.270. meteor of scst (rennie et al. 2017) c40 is 0.355. rouge-l of scst (rennie et al. 2017) c5 is 0.563. rouge-l of scst (rennie et al. 2017) c40 is 0.707. cider of scst (rennie et al. 2017) c5 is 1.147. cider of scst (rennie et al. 2017) c40 is 1.167. bleu-1 of up-down (anderson et al. 2018) c5 is 0.802. bleu-1 of up-down (anderson et al. 2018) c40 is 0.952. bleu-2 of up-down (anderson et al. 2018) c5 is 0.641. bleu-2 of up-down (anderson et al. 2018) c40 is 0.888. bleu-3 of up-down (anderson et al. 2018) c5 is 0.491. bleu-3 of up-down (anderson et al. 2018) c40 is 0.794. bleu-4 of up-down (anderson et al. 2018) c5 is 0.369. bleu-4 of up-down (anderson et al. 2018) c40 is 0.685. meteor of up-down (anderson et al. 2018) c5 is 0.276. meteor of up-down (anderson et al. 2018) c40 is 0.367. rouge-l of up-down (anderson et al. 2018) c5 is 0.571. rouge-l of up-down (anderson et al. 2018) c40 is 0.724. cider of up-down (anderson et al. 2018) c5 is 1.179. cider of up-down (anderson et al. 2018) c40 is 1.205. bleu-1 of simnet c5 is 0.766. bleu-1 of simnet c40 is 0.941. bleu-2 of simnet c5 is 0.605. bleu-2 of simnet c40 is 0.874. bleu-3 of simnet c5 is 0.462. bleu-3 of simnet c40 is 0.778. bleu-4 of simnet c5 is 0.350. bleu-4 of simnet c40 is 0.671. meteor of simnet c5 is 0.267. meteor of simnet c40 is 0.362. rouge-l of simnet c5 is 0.558. rouge-l of simnet c40 is 0.716. cider of simnet c5 is 1.087. cider of simnet c40 is 1.111.
table 1 shows performance comparisons of different methods on didemo. r@1 of mfp iou=1 is 19.40. r@5 of mfp iou=1 is 66.38. miou of mfp - is 26.65. r@1 of mcn-vgg16 iou=1 is 13.10. r@5 of mcn-vgg16 iou=1 is 44.82. miou of mcn-vgg16 - is 25.13. r@1 of mcn-flow iou=1 is 18.35. r@5 of mcn-flow iou=1 is 56,25. miou of mcn-flow - is 31.46. r@1 of mcn-fusion iou=1 is 19.88. r@5 of mcn-fusion iou=1 is 62.39. miou of mcn-fusion - is 33.51. r@1 of mcn-fusion+tef iou=1 is 28.10. r@5 of mcn-fusion+tef iou=1 is 78.21. miou of mcn-fusion+tef - is 41.08. r@1 of tgn-vgg16 iou=1 is 24.28. r@5 of tgn-vgg16 iou=1 is 71.43. miou of tgn-vgg16 - is 38.62. r@1 of tgn-flow iou=1 is 27.52. r@5 of tgn-flow iou=1 is 76.94. miou of tgn-flow - is 42.84. r@1 of tgn-fusion iou=1 is 28.23. r@5 of tgn-fusion iou=1 is 79.26. miou of tgn-fusion - is 42.97.
table 2 shows ner results for named entities on the original weiboner dataset (peng and dredze, 2015). p(%) of crf (peng and dredze 2015) p(%) is 56.98. r(%) of crf (peng and dredze 2015) r(%) is 25.26. f1(%) of crf (peng and dredze 2015) f1(%) is 35.00. p(%) of crf+word (peng and dredze 2015) p(%) is 64.94. r(%) of crf+word (peng and dredze 2015) r(%) is 25.77. f1(%) of crf+word (peng and dredze 2015) f1(%) is 36.90. p(%) of crf+character (peng and dredze 2015) p(%) is 57.89. r(%) of crf+character (peng and dredze 2015) r(%) is 34.02. f1(%) of crf+character (peng and dredze 2015) f1(%) is 42.86. p(%) of crf+character+position (peng and dredze 2015) p(%) is 57.26. r(%) of crf+character+position (peng and dredze 2015) r(%) is 34.53. f1(%) of crf+character+position (peng and dredze 2015) f1(%) is 43.09. p(%) of joint(cp) (main) (peng and dredze 2015) p(%) is 57.98. r(%) of joint(cp) (main) (peng and dredze 2015) r(%) is 35.57. f1(%) of joint(cp) (main) (peng and dredze 2015) f1(%) is 44.09. p(%) of pipeline seg.repr.+ner (peng and dredze 2016) p(%) is 64.22. r(%) of pipeline seg.repr.+ner (peng and dredze 2016) r(%) is 36.08. f1(%) of pipeline seg.repr.+ner (peng and dredze 2016) f1(%) is 46.20. p(%) of jointly train char.emb (peng and dredze 2016) p(%) is 63.16. r(%) of jointly train char.emb (peng and dredze 2016) r(%) is 37.11. f1(%) of jointly train char.emb (peng and dredze 2016) f1(%) is 46.75. p(%) of jointly train lstm hidden (peng and dredze 2016) p(%) is 63.03. r(%) of jointly train lstm hidden (peng and dredze 2016) r(%) is 38.66. f1(%) of jointly train lstm hidden (peng and dredze 2016) f1(%) is 47.92. p(%) of jointly train lstm+emb (main) (peng and dredze 2016) p(%) is 63.33. r(%) of jointly train lstm+emb (main) (peng and dredze 2016) r(%) is 39.18. f1(%) of jointly train lstm+emb (main) (peng and dredze 2016) f1(%) is 48.41. p(%) of bilstm+crf+adversarial+self-attention p(%) is 55.72. r(%) of bilstm+crf+adversarial+self-attention r(%) is 50.68. f1(%) of bilstm+crf+adversarial+self-attention f1(%) is 53.08.
table 3 shows experimental results on the updated weiboner dataset (he and sun, 2017a). p(%) of peng and dredze (2015) p(%) is 74.78. r(%) of peng and dredze (2015) r(%) is 39.81. f1(%) of peng and dredze (2015) f1(%) is 51.96. p(%) of peng and dredze (2015) p(%) is 71.92. r(%) of peng and dredze (2015) r(%) is 53.03. f1(%) of peng and dredze (2015) f1(%) is 61.05. f1(%) of peng and dredze (2015) f1(%) is 56.05. p(%) of peng and dredze (2016) p(%) is 66.67. r(%) of peng and dredze (2016) r(%) is 47.22. f1(%) of peng and dredze (2016) f1(%) is 55.28. p(%) of peng and dredze (2016) p(%) is 74.48. r(%) of peng and dredze (2016) r(%) is 54.55. f1(%) of peng and dredze (2016) f1(%) is 62.97. f1(%) of peng and dredze (2016) f1(%) is 58.99. p(%) of he and sun (2017a) p(%) is 66.93. r(%) of he and sun (2017a) r(%) is 40.67. f1(%) of he and sun (2017a) f1(%) is 50.6. p(%) of he and sun (2017a) p(%) is 66.46. r(%) of he and sun (2017a) r(%) is 53.57. f1(%) of he and sun (2017a) f1(%) is 59.32. f1(%) of he and sun (2017a) f1(%) is 54.82. p(%) of he and sun (2017b) p(%) is 61.68. r(%) of he and sun (2017b) r(%) is 48.82. f1(%) of he and sun (2017b) f1(%) is 54.5. p(%) of he and sun (2017b) p(%) is 74.13. r(%) of he and sun (2017b) r(%) is 53.54. f1(%) of he and sun (2017b) f1(%) is 62.17. f1(%) of he and sun (2017b) f1(%) is 58.23. p(%) of bilstm+crf+adv+self-attention p(%) is 59.51. r(%) of bilstm+crf+adv+self-attention r(%) is 50. f1(%) of bilstm+crf+adv+self-attention f1(%) is 54.34. p(%) of bilstm+crf+adv+self-attention p(%) is 71.43. r(%) of bilstm+crf+adv+self-attention r(%) is 47.9. f1(%) of bilstm+crf+adv+self-attention f1(%) is 57.35. f1(%) of bilstm+crf+adv+self-attention f1(%) is 58.7.
table 4 shows results on sighanner dataset. p(%) of chen et al. (2006) p(%) is 91.22. r(%) of chen et al. (2006) r(%) is 81.71. f1(%) of chen et al. (2006) f1(%) is 86.2. p(%) of zhou et al. (2006) p(%) is 88.94. r(%) of zhou et al. (2006) r(%) is 84.2. f1(%) of zhou et al. (2006) f1(%) is 86.51. p(%) of luo and yang (2016) p(%) is 91.3. r(%) of luo and yang (2016) r(%) is 87.22. f1(%) of luo and yang (2016) f1(%) is 89.21. p(%) of bilstm+crf+adversarial+self-attention p(%) is 91.73. r(%) of bilstm+crf+adversarial+self-attention r(%) is 89.58. f1(%) of bilstm+crf+adversarial+self-attention f1(%) is 90.64.
table 5 shows comparison between our proposed model and simplified models on sighanner dataset and original weiboner dataset. p(%) of bilstm+crf p(%) is 89.84. r(%) of bilstm+crf r(%) is 88.42. f1(%) of bilstm+crf f1(%) is 89.13. p(%) of bilstm+crf p(%) is 58.99. r(%) of bilstm+crf r(%) is 44.93. f1(%) of bilstm+crf f1(%) is 51.01. p(%) of bilstm+crf+transfer p(%) is 90.60. r(%) of bilstm+crf+transfer r(%) is 89.19. f1(%) of bilstm+crf+transfer f1(%) is 89.89. p(%) of bilstm+crf+transfer p(%) is 60.00. r(%) of bilstm+crf+transfer r(%) is 46.03. f1(%) of bilstm+crf+transfer f1(%) is 52.09. p(%) of bilstm+crf+adversarial p(%) is 90.52. r(%) of bilstm+crf+adversarial r(%) is 89.56. f1(%) of bilstm+crf+adversarial f1(%) is 90.04. p(%) of bilstm+crf+adversarial p(%) is 61.94. r(%) of bilstm+crf+adversarial r(%) is 45.48. f1(%) of bilstm+crf+adversarial f1(%) is 52.45. p(%) of bilstm+crf+self-attention p(%) is 90.62. r(%) of bilstm+crf+self-attention r(%) is 88.81. f1(%) of bilstm+crf+self-attention f1(%) is 89.71. p(%) of bilstm+crf+self-attention p(%) is 57.81. r(%) of bilstm+crf+self-attention r(%) is 47.67. f1(%) of bilstm+crf+self-attention f1(%) is 52.25. p(%) of bilstm+crf+adversarial+self-attention p(%) is 91.73. r(%) of bilstm+crf+adversarial+self-attention r(%) is 89.58. f1(%) of bilstm+crf+adversarial+self-attention f1(%) is 90.64. p(%) of bilstm+crf+adversarial+self-attention p(%) is 55.72. r(%) of bilstm+crf+adversarial+self-attention r(%) is 50.68. f1(%) of bilstm+crf+adversarial+self-attention f1(%) is 53.08.
table 2 shows tagging accuracies (%) on ud test sets. acc. of ncrf acc. is 93.4. acc. of ncrf acc. is 90.4. acc. of ncrf acc. is 88.4. acc. of ncrf acc. is 91.2. acc. of ncrf acc. is 86.6. acc. of ncrf acc. is 86.1. acc. of ncrf-ae acc. is 93.7. ulδ of ncrf-ae ulδ is +0.2. acc. of ncrf-ae acc. is 90.8. ulδ of ncrf-ae ulδ is +0.2. acc. of ncrf-ae acc. is 89.1. ulδ of ncrf-ae ulδ is +0.3. acc. of ncrf-ae acc. is 91.7. ulδ of ncrf-ae ulδ is +0.5. acc. of ncrf-ae acc. is 87.8. ulδ of ncrf-ae ulδ is +1.1. acc. of ncrf-ae acc. is 87.9. ulδ of ncrf-ae ulδ is +1.2. acc. of bigru baseline acc. is 95.9. acc. of bigru baseline acc. is 92.6. acc. of bigru baseline acc. is 92.2. acc. of bigru baseline acc. is 94.7. acc. of bigru baseline acc. is 95.2. acc. of bigru baseline acc. is 95.6. acc. of vsl-g acc. is 96.1. ulδ of vsl-g ulδ is +0.0. acc. of vsl-g acc. is 92.8. ulδ of vsl-g ulδ is +0.0. acc. of vsl-g acc. is 92.3. ulδ of vsl-g ulδ is +0.0. acc. of vsl-g acc. is 94.8. ulδ of vsl-g ulδ is +0.1. acc. of vsl-g acc. is 95.3. ulδ of vsl-g ulδ is +0.0. acc. of vsl-g acc. is 95.6. ulδ of vsl-g ulδ is +0.1. acc. of vsl-gg-flat acc. is 96.1. ulδ of vsl-gg-flat ulδ is +0.0. acc. of vsl-gg-flat acc. is 93.0. ulδ of vsl-gg-flat ulδ is +0.1. acc. of vsl-gg-flat acc. is 92.4. ulδ of vsl-gg-flat ulδ is +0.1. acc. of vsl-gg-flat acc. is 95.0. ulδ of vsl-gg-flat ulδ is +0.1. acc. of vsl-gg-flat acc. is 95.5. ulδ of vsl-gg-flat ulδ is +0.1. acc. of vsl-gg-flat acc. is 95.8. ulδ of vsl-gg-flat ulδ is +0.1. acc. of vsl-gg-hier acc. is 96.4. ulδ of vsl-gg-hier ulδ is +0.1. acc. of vsl-gg-hier acc. is 93.3. ulδ of vsl-gg-hier ulδ is +0.1. acc. of vsl-gg-hier acc. is 92.8. ulδ of vsl-gg-hier ulδ is +0.1. acc. of vsl-gg-hier acc. is 95.3. ulδ of vsl-gg-hier ulδ is +0.2. acc. of vsl-gg-hier acc. is 95.9. ulδ of vsl-gg-hier ulδ is +0.1. acc. of vsl-gg-hier acc. is 96.3. ulδ of vsl-gg-hier ulδ is +0.2.
table 3 shows twitter and ner dev results (%), ud averaged test accuracies (%) for two choices of attaching the classification loss to latent variables in the vslgg-hier model. acc. of classifier on y acc. is 91.6. ulδ of classifier on y ulδ is +0.3. acc. of classifier on y acc. is 88.4. ulδ of classifier on y ulδ is +0.2. acc. of classifier on y acc. is 95.0. ulδ of classifier on y ulδ is +0.1. acc. of classifier on z acc. is 91.1. ulδ of classifier on z ulδ is +0.2. acc. of classifier on z acc. is 87.8. ulδ of classifier on z ulδ is +0.1. acc. of classifier on z acc. is 94.4. ulδ of classifier on z ulδ is +0.0.
table 4 shows results using bilingual lexicons with varying sizes (40,000, 10,000, 2,000, 1,000, 500, 250) and three languages. qvec of multicca monolingual is 10.8. qvec of multicca multilingual is 8.5. qvec-cca of multicca monolingual is 63.8. qvec-cca of multicca multilingual is 43.9. qvec of multicluster monolingual is 10.8. qvec of multicluster multilingual is 9.1. qvec-cca of multicluster monolingual is 63.6. qvec-cca of multicluster multilingual is 45.8. qvec of corrnet w monolingual is 14.8. qvec of corrnet w multilingual is 11.3. qvec-cca of corrnet w monolingual is 63.6. qvec-cca of corrnet w multilingual is 43.4. qvec of corrnet w+n+c+l monolingual is 16.2. qvec of corrnet w+n+c+l multilingual is 12.4. qvec-cca of corrnet w+n+c+l monolingual is 67.3. qvec-cca of corrnet w+n+c+l multilingual is 45.4. qvec of multicca monolingual is 9.8. qvec of multicca multilingual is 6.5. qvec-cca of multicca monolingual is 63.6. qvec-cca of multicca multilingual is 42.3. qvec of multicluster monolingual is 10.6. qvec of multicluster multilingual is 9.5. qvec-cca of multicluster monolingual is 62.4. qvec-cca of multicluster multilingual is 44.7. qvec of corrnet w monolingual is 14.8. qvec of corrnet w multilingual is 11.3. qvec-cca of corrnet w monolingual is 63.4. qvec-cca of corrnet w multilingual is 43.0. qvec of corrnet w+n+c+l monolingual is 15.7. qvec of corrnet w+n+c+l multilingual is 12.4. qvec-cca of corrnet w+n+c+l monolingual is 68.0. qvec-cca of corrnet w+n+c+l multilingual is 45.1. qvec of multicca monolingual is 9.9. qvec of multicca multilingual is 6.2. qvec-cca of multicca monolingual is 63.6. qvec-cca of multicca multilingual is 40.9. qvec of multicluster monolingual is 10.5. qvec of multicluster multilingual is 9.3. qvec-cca of multicluster monolingual is 62.5. qvec-cca of multicluster multilingual is 44.8. qvec of corrnet w monolingual is 14.5. qvec of corrnet w multilingual is 7.1. qvec-cca of corrnet w monolingual is 62.0. qvec-cca of corrnet w multilingual is 39.2. qvec of corrnet w+n+c+l monolingual is 14.5. qvec of corrnet w+n+c+l multilingual is 11.4. qvec-cca of corrnet w+n+c+l monolingual is 68.0. qvec-cca of corrnet w+n+c+l multilingual is 44.8. qvec of multicca monolingual is 12.3. qvec of multicca multilingual is 6.9. qvec-cca of multicca monolingual is 63.5. qvec-cca of multicca multilingual is 38.2. qvec of multicluster monolingual is 10.5. qvec of multicluster multilingual is 9.3. qvec-cca of multicluster monolingual is 62.5. qvec-cca of multicluster multilingual is 44.8. qvec of corrnet w monolingual is 13.7. qvec of corrnet w multilingual is 9.4. qvec-cca of corrnet w monolingual is 63.0. qvec-cca of corrnet w multilingual is 40.0. qvec of corrnet w+n+c+l monolingual is 13.6. qvec of corrnet w+n+c+l multilingual is 10.5. qvec-cca of corrnet w+n+c+l monolingual is 66.4. qvec-cca of corrnet w+n+c+l multilingual is 43.0. qvec of multicca monolingual is 12.3. qvec of multicca multilingual is 5.5. qvec-cca of multicca monolingual is 63.5. qvec-cca of multicca multilingual is 36.0. qvec of multicluster monolingual is 10.5. qvec of multicluster multilingual is 9.3. qvec-cca of multicluster monolingual is 62.6. qvec-cca of multicluster multilingual is 44.7. qvec of corrnet w monolingual is 13.3. qvec of corrnet w multilingual is 9.1. qvec-cca of corrnet w monolingual is 62.8. qvec-cca of corrnet w multilingual is 39.4. qvec of corrnet w+n+c+l monolingual is 13.4. qvec of corrnet w+n+c+l multilingual is 9.5. qvec-cca of corrnet w+n+c+l monolingual is 66.2. qvec-cca of corrnet w+n+c+l multilingual is 42.7. qvec of multicca monolingual is 12.3. qvec of multicca multilingual is 5.3. qvec-cca of multicca monolingual is 63.5. qvec-cca of multicca multilingual is 35.0. qvec of multicluster monolingual is 10.5. qvec of multicluster multilingual is 9.2. qvec-cca of multicluster monolingual is 62.7. qvec-cca of multicluster multilingual is 44.9. qvec of corrnet w monolingual is 13.8. qvec of corrnet w multilingual is 9.3. qvec-cca of corrnet w monolingual is 62.5. qvec-cca of corrnet w multilingual is 39.3. qvec of corrnet w+n+c+l monolingual is 13.9. qvec of corrnet w+n+c+l multilingual is 9.8. qvec-cca of corrnet w+n+c+l monolingual is 65.9. qvec-cca of corrnet w+n+c+l multilingual is 42.2.
table 7 shows comparison on cross-lingual direct transfer: name tagging performance (f-score, %) when the tagger was trained on 1-2 source languages and tested on a target language. f-score of tig - is 15.5. f-score of tig - is 29.7. f-score of tig w is 28.3. f-score of tig w+n+c+l is 33.7. f-score of amh - is 11.1. f-score of amh - is 24.7. f-score of amh w is 12.8. f-score of amh w+n+c+l is 23.3. f-score of uig - is 4.8. f-score of uig - is 9.1. f-score of uig w is 13.3. f-score of uig w+n+c+l is 15.5. f-score of uig - is 0.4. f-score of uig - is 11.4. f-score of uig w is 19.8. f-score of uig w+n+c+l is 25.0. f-score of uig - is 8.3. f-score of uig - is 10.5. f-score of uig w is 17.3. f-score of uig w+n+c+l is 23.3. f-score of tur - is 17.6. f-score of tur - is 21.4. f-score of tur w is 18.3. f-score of tur w+n+c+l is 22.4. f-score of tur - is 6.9. f-score of tur - is 12.8. f-score of tur w is 13.2. f-score of tur w+n+c+l is 10.7. f-score of tur - is 20.4. f-score of tur - is 23.3. f-score of tur w is 14.5. f-score of tur w+n+c+l is 27.0.
table 4 shows results on the hypernym discovery task. map of bestuns map is 2.4. mrr of bestuns mrr is 5.5. p@5 of bestuns p@5 is 2.5. map of bestuns map is 3.9. mrr of bestuns mrr is 8.7. p@5 of bestuns p@5 is 3.9. map of vecmap map is 6.4. mrr of vecmap mrr is 16.5. p@5 of vecmap p@5 is 6.0. map of vecmap map is 4.5. mrr of vecmap mrr is 10.6. p@5 of vecmap p@5 is 4.3. map of vecmapμ map is 6.1. mrr of vecmapμ mrr is 15.4. p@5 of vecmapμ p@5 is 5.7. map of vecmapμ map is 5.6. mrr of vecmapμ mrr is 13.3. p@5 of vecmapμ p@5 is 5.4. map of muse map is 5.9. mrr of muse mrr is 14.1. p@5 of muse p@5 is 5.5. map of muse map is 4.9. mrr of muse mrr is 11.1. p@5 of muse p@5 is 4.7. map of museμ map is 6.2. mrr of museμ mrr is 14.8. p@5 of museμ p@5 is 5.8. map of museμ map is 5.1. mrr of museμ mrr is 11.7. p@5 of museμ p@5 is 4.9. map of vecmap map is 7.3. mrr of vecmap mrr is 18.2. p@5 of vecmap p@5 is 7.0. map of vecmap map is 6.1. mrr of vecmap mrr is 14.0. p@5 of vecmap p@5 is 5.8. map of vecmapμ map is 7.0. mrr of vecmapμ mrr is 17.6. p@5 of vecmapμ p@5 is 6.6. map of vecmapμ map is 6.8. mrr of vecmapμ mrr is 16.2. p@5 of vecmapμ p@5 is 6.4. map of muse map is 6.4. mrr of muse mrr is 15.9. p@5 of muse p@5 is 6.1. map of muse map is 5.3. mrr of muse mrr is 12.0. p@5 of muse p@5 is 5.0. map of museμ map is 6.9. mrr of museμ mrr is 16.9. p@5 of museμ p@5 is 6.6. map of museμ map is 6.0. mrr of museμ mrr is 13.4. p@5 of museμ p@5 is 5.7. map of vecmap map is 7.9. mrr of vecmap mrr is 19.2. p@5 of vecmap p@5 is 7.6. map of vecmap map is 7.0. mrr of vecmap mrr is 16.4. p@5 of vecmap p@5 is 6.6. map of vecmapμ map is 7.8. mrr of vecmapμ mrr is 19.2. p@5 of vecmapμ p@5 is 7.4. map of vecmapμ map is 7.5. mrr of vecmapμ mrr is 18.1. p@5 of vecmapμ p@5 is 7.0. map of muse map is 7.2. mrr of muse mrr is 17.3. p@5 of muse p@5 is 6.9. map of muse map is 6.2. mrr of muse mrr is 13.8. p@5 of muse p@5 is 5.8. map of museμ map is 7.8. mrr of museμ mrr is 18.8. p@5 of museμ p@5 is 7.5. map of museμ map is 6.5. mrr of museμ mrr is 14.2. p@5 of museμ p@5 is 6.3. map of vecmap map is 8.0. mrr of vecmap mrr is 19.1. p@5 of vecmap p@5 is 7.7. map of vecmap map is 8.2. mrr of vecmap mrr is 19.1. p@5 of vecmap p@5 is 7.5. map of vecmapμ map is 8.2. mrr of vecmapμ mrr is 19.9. p@5 of vecmapμ p@5 is 7.9. map of vecmapμ map is 8.7. mrr of vecmapμ mrr is 20.7. p@5 of vecmapμ p@5 is 8.1. map of muse map is 7.2. mrr of muse mrr is 17.2. p@5 of muse p@5 is 6.8. map of muse map is 7.2. mrr of muse mrr is 15.8. p@5 of muse p@5 is 7.0. map of museμ map is 8.3. mrr of museμ mrr is 19.5. p@5 of museμ p@5 is 8.0. map of museμ map is 7.6. mrr of museμ mrr is 17.0. p@5 of museμ p@5 is 7.2.
table 1 shows evaluation results of cross-lingual lexical sememe prediction with different seed lexicon sizes. map of 1000 map is 27.57. f1 score of 1000 f1 score is 16.08. map of 2000 map is 33.79. f1 score of 2000 f1 score is 22.33. map of 4000 map is 35.78. f1 score of 4000 f1 score is 25.74. map of 6000 map is 38.29. f1 score of 6000 f1 score is 28.71. map of 1000 map is 38.12. f1 score of 1000 f1 score is 18.55. map of 2000 map is 33.78. f1 score of 2000 f1 score is 23.64. map of 4000 map is 38.30. f1 score of 4000 f1 score is 27.74. map of 6000 map is 41.23. f1 score of 6000 f1 score is 30.64. map of 1000 map is 31.78. f1 score of 1000 f1 score is 18.22. map of 2000 map is 37.70. f1 score of 2000 f1 score is 24.31. map of 4000 map is 40.77. f1 score of 4000 f1 score is 29.33. map of 6000 map is 43.16. f1 score of 6000 f1 score is 32.49.
table 1 shows the main results of ch-en translation. bleu of baseline 03 is 41.01. bleu of baseline 04 is 42.94. bleu of baseline 05 is 40.31. bleu of baseline 06 is 40.57. bleu of baseline 08 is 30.96. bleu of baseline avg. is 39.16. bleu of arthur(test) 03 is 41.34. bleu of arthur(test) 04 is 43.31. bleu of arthur(test) 05 is 40.79. bleu of arthur(test) 06 is 40.84. bleu of arthur(test) 08 is 31.11. bleu of arthur(test) avg. is 39.48. bleu of arthur(train+test) 03 is 41.88. bleu of arthur(train+test) 04 is 43.75. bleu of arthur(train+test) 05 is 41.16. bleu of arthur(train+test) 06 is 41.63. bleu of arthur(train+test) 08 is 31.47. bleu of arthur(train+test) avg. is 39.98. bleu of baseline(sub-word) 03 is 43.93. bleu of baseline(sub-word) 04 is 44.74. bleu of baseline(sub-word) 05 is 42.46. bleu of baseline(sub-word) 06 is 43.01. bleu of baseline(sub-word) 08 is 32.53. bleu of baseline(sub-word) avg. is 41.33. bleu of baseline+mem 03 is 42.74. bleu of baseline+mem 04 is 43.94. bleu of baseline+mem 05 is 42.15. bleu of baseline+mem 06 is 41.94. bleu of baseline+mem 08 is 31.86. bleu of baseline+mem avg. is 40.53. bleu of baseline+mem △ is +1.37. bleu of arthur(train+test)+mem 03 is 43.04. bleu of arthur(train+test)+mem 04 is 44.65. bleu of arthur(train+test)+mem 05 is 42.19. bleu of arthur(train+test)+mem 06 is 42.59. bleu of arthur(train+test)+mem 08 is 32.05. bleu of arthur(train+test)+mem avg. is 40.90. bleu of arthur(train+test)+mem △ is +0.92. bleu of baseline(sub-word)+mem 03 is 44.98. bleu of baseline(sub-word)+mem 04 is 45.51. bleu of baseline(sub-word)+mem 05 is 43.93. bleu of baseline(sub-word)+mem 06 is 43.95. bleu of baseline(sub-word)+mem 08 is 33.33. bleu of baseline(sub-word)+mem avg. is 42.34. bleu of baseline(sub-word)+mem △ is +1.01.
table 4 shows news commentary v8 experiment results. bleu of seq2seq bleu is 16.61. ribes of seq2seq ribes is 73.8. bleu of seq2seq bleu is 11.22. ribes of seq2seq ribes is 69.6. bleu of seq2seq bleu is 12.03. ribes of seq2seq ribes is 69.6. bleu of str2tree bleu is 16.13. bleu of str2tree bleu is 11.65. bleu of str2tree bleu is 11.94. bleu of nmt+rnng bleu is 16.41. ribes of nmt+rnng ribes is 75.0. bleu of nmt+rnng bleu is 12.06. ribes of nmt+rnng ribes is 70.4. bleu of nmt+rnng bleu is 12.46. ribes of nmt+rnng ribes is 71.0. bleu of seq2drnn bleu is 16.90. ribes of seq2drnn ribes is 75.1. bleu of seq2drnn bleu is 11.84. ribes of seq2drnn ribes is 67.3. bleu of seq2drnn bleu is 12.04. ribes of seq2drnn ribes is 69.7. bleu of seq2drnn+sync bleu is 17.21. ribes of seq2drnn+sync ribes is 75.8. bleu of seq2drnn+sync bleu is 12.11. ribes of seq2drnn+sync ribes is 70.3. bleu of seq2drnn+sync bleu is 12.96. ribes of seq2drnn+sync ribes is 71.1.
table 2 shows perplexity of source data as assigned by a language model (5-gram kneser–ney). perplexity of human data perplexity is 75.34. perplexity of beam perplexity is 72.42. perplexity of sampling perplexity is 500.17. perplexity of top10 perplexity is 87.15. perplexity of beam+noise perplexity is 2823.73.
table 6 shows bleu on newstest2014 for wmt english-german (en–de) and english-french (en–fr). bleu of a. gehring et al. (2017) en?de is 25.2. bleu of a. gehring et al. (2017) en?fr is 40.5. bleu of b. vaswani et al. (2017) en?de is 28.4. bleu of b. vaswani et al. (2017) en?fr is 41. bleu of c. ahmed et al. (2017) en?de is 28.9. bleu of c. ahmed et al. (2017) en?fr is 41.4. bleu of d. shaw et al. (2018) en?de is 29.2. bleu of d. shaw et al. (2018) en?fr is 41.5. bleu of deepl en?de is 33.3. bleu of deepl en?fr is 45.9. bleu of our result en?de is 35. bleu of our result en?fr is 45.6. bleu of detok. sacrebleu en?de is 33.8. bleu of detok. sacrebleu en?fr is 43.8.
table 2 shows comparing different approaches on the news 2015 dataset using acc@1 as the evaluation metric. acc@1 of seq2seq w/ att (u) hi is 35.5. acc@1 of seq2seq w/ att (u) kn is 33.4. acc@1 of seq2seq w/ att (u) bn is 46.1. acc@1 of seq2seq w/ att (u) ta is 17.2. acc@1 of seq2seq w/ att (u) he is 20.3. acc@1 of seq2seq w/ att (u) avg. is 30.5. acc@1 of p&r (u) hi is 37.4. acc@1 of p&r (u) kn is 31.6. acc@1 of p&r (u) bn is 45.4. acc@1 of p&r (u) ta is 20.2. acc@1 of p&r (u) he is 18.7. acc@1 of p&r (u) avg. is 30.7. acc@1 of directl+ (u) hi is 38.9. acc@1 of directl+ (u) kn is 34.7. acc@1 of directl+ (u) bn is 48.4. acc@1 of directl+ (u) ta is 19.9. acc@1 of directl+ (u) he is 16.8. acc@1 of directl+ (u) avg. is 31.7. acc@1 of rpi-isi (u) hi is 40.3. acc@1 of rpi-isi (u) kn is 29.8. acc@1 of rpi-isi (u) bn is 49.4. acc@1 of rpi-isi (u) ta is 20.2. acc@1 of rpi-isi (u) he is 21.5. acc@1 of rpi-isi (u) avg. is 32.2. acc@1 of ours(u) hi is 42.8. acc@1 of ours(u) kn is 38.9. acc@1 of ours(u) bn is 52.4. acc@1 of ours(u) ta is 20.5. acc@1 of ours(u) he is 23.4. acc@1 of ours(u) avg. is 35.6. acc@1 of rpi-isi + el hi is 44.8. acc@1 of rpi-isi + el kn is 37.6. acc@1 of rpi-isi + el bn is 52.0. acc@1 of rpi-isi + el ta is 29.0. acc@1 of rpi-isi + el he is 37.2. acc@1 of rpi-isi + el avg. is 40.1. acc@1 of ours(dc) hi is 51.8. acc@1 of ours(dc) kn is 43.3. acc@1 of ours(dc) bn is 56.6. acc@1 of ours(dc) ta is 28.0. acc@1 of ours(dc) he is 36.1. acc@1 of ours(dc) avg. is 43.2. acc@1 of seq2seq w/ att (u) hi is 17.0. acc@1 of seq2seq w/ att (u) kn is 13.6. acc@1 of seq2seq w/ att (u) bn is 14.5. acc@1 of seq2seq w/ att (u) ta is 6.0. acc@1 of seq2seq w/ att (u) he is 9.5. acc@1 of seq2seq w/ att (u) avg. is 12.1. acc@1 of p&r (u) hi is 21.1. acc@1 of p&r (u) kn is 16.6. acc@1 of p&r (u) bn is 34.2. acc@1 of p&r (u) ta is 9.4. acc@1 of p&r (u) he is 13.0. acc@1 of p&r (u) avg. is 18.9. acc@1 of directl+ (u) hi is 26.6. acc@1 of directl+ (u) kn is 25.3. acc@1 of directl+ (u) bn is 35.5. acc@1 of directl+ (u) ta is 11.8. acc@1 of directl+ (u) he is 10.7. acc@1 of directl+ (u) avg. is 22.0. acc@1 of rpi-isi (u) hi is 29.1. acc@1 of rpi-isi (u) kn is 27.7. acc@1 of rpi-isi (u) bn is 37.7. acc@1 of rpi-isi (u) ta is 11.5. acc@1 of rpi-isi (u) he is 16.2. acc@1 of rpi-isi (u) avg. is 24.4. acc@1 of ours(u) + boot. hi is 40.1. acc@1 of ours(u) + boot. kn is 35.1. acc@1 of ours(u) + boot. bn is 50.3. acc@1 of ours(u) + boot. ta is 17.8. acc@1 of ours(u) + boot. he is 22.8. acc@1 of ours(u) + boot. avg. is 33.0.
table 3 shows acc@1 for native and foreign words for four languages (§7.2). acc@1 of hindi native is 45.1. acc@1 of hindi foreign is 31.4. acc@1 of hindi ratio is 1.44. acc@1 of bengali native is 63.1. acc@1 of bengali foreign is 20.1. acc@1 of bengali ratio is 3.14. acc@1 of kannada native is 42.6. acc@1 of kannada foreign is 23.1. acc@1 of kannada ratio is 1.84. acc@1 of tamil native is 24.3. acc@1 of tamil foreign is 05.2. acc@1 of tamil ratio is 4.67.
table 3 shows performance for en-pt on rare words (rare), and the en-pt muse dataset, which as shown in figure 3 contains a lot of frequent words. accuracy of norma-linear rare is 57.67. accuracy of norma-linear muse is 72.60. accuracy of norma-highway-nn rare is 49.33. accuracy of norma-highway-nn muse is 71.73. accuracy of 1 layer-nn rare is 48.67. accuracy of 1 layer-nn muse is 72.13. accuracy of 1 layer-highway-nn rare is 49.33. accuracy of 1 layer-highway-nn muse is 72.10. accuracy of artetxe et al . 2018 rare is 47.00. accuracy of artetxe et al . 2018 muse is 77.73. accuracy of lazaridou et al 2015 rare is 48.00. accuracy of lazaridou et al 2015 muse is 72.27.
table 3 shows comparison with previous works on chinese-english translation task. bleu of rnnsearch mt06 is 37.76. bleu of rnnsearch mt05 is 36.89. bleu of rnnsearch mt08 is 27.57. bleu of rnnsearch mt02 is 34.41. bleu of rnnsearch mt04 is 38.40. bleu of rnnsearch mt05 is 32.90. bleu of rnnsearch mt08 is 31.86. bleu of transformer mt06 is 48.09. bleu of transformer mt02 is 48.63. bleu of transformer mt03 is 47.54. bleu of transformer mt04 is 47.79. bleu of transformer mt05 is 48.34. bleu of transformer mt08 is 38.31. bleu of transformer all is 45.97. bleu of transformer mt06 is 48.14. bleu of transformer mt02 is 48.97. bleu of transformer mt03 is 48.05. bleu of transformer mt04 is 47.91. bleu of transformer mt05 is 48.53. bleu of transformer mt08 is 38.38. bleu of transformer all is 46.37. bleu of transformer mt06 is 46.69. bleu of transformer mt02 is 50.96. bleu of transformer mt03 is 50.21. bleu of transformer mt04 is 49.73. bleu of transformer mt05 is 49.46. bleu of transformer mt08 is 39.69. bleu of transformer all is 47.93.
table 5 shows subjective evaluation of the comparison between the original transformer model and our model. percentage of human 1 > is 24%. percentage of human 1 = is 45%. percentage of human 1 < is 31%. percentage of human 2 > is 20%. percentage of human 2 = is 55%. percentage of human 2 < is 25%. percentage of human 3 > is 12%. percentage of human 3 = is 52%. percentage of human 3 < is 36%. percentage of overall > is 19%. percentage of overall = is 51%. percentage of overall < is 31%.
table 1 shows performance on squad dev set with the piqa constraint (top), and without the constraint (bottom). f1 (%) of tf-idf f1 (%) is 15.0. em (%) of tf-idf em (%) is 3.9. f1 (%) of lstm f1 (%) is 57.2. em (%) of lstm em (%) is 46.8. f1 (%) of lstm+sa f1 (%) is 59.8. em (%) of lstm+sa em (%) is 49.0. f1 (%) of lstm+elmo f1 (%) is 60.9. em (%) of lstm+elmo em (%) is 50.9. f1 (%) of lstm+sa+elmo f1 (%) is 62.7. em (%) of lstm+sa+elmo em (%) is 52.7. f1 (%) of rajpurkar et al. (2016) f1 (%) is 51.0. em (%) of rajpurkar et al. (2016) em (%) is 40.0. f1 (%) of yu et al. (2018) f1 (%) is 89.3. em (%) of yu et al. (2018) em (%) is 82.5.
table 1 shows word similarity and analogy results (ρ× 100 and analogy accuracy). accuracy of glove - is 66.8. accuracy of glove - is 35.0. accuracy of glove - is 59.3. accuracy of glove - is 44.1. accuracy of glove - is 74.7. accuracy of glove - is 69.9. accuracy of glove sem is 76.0. accuracy of glove syn is 75.3. accuracy of glove + co - is 69.7. accuracy of glove + co - is 38.0. accuracy of glove + co - is 63.8. accuracy of glove + co - is 45.1. accuracy of glove + co - is 77.6. accuracy of glove + co - is 71.3. accuracy of glove + co sem is 78.6. accuracy of glove + co syn is 75.0. accuracy of sgns - is 71.1. accuracy of sgns - is 40.7. accuracy of sgns - is 67.1. accuracy of sgns - is 52.8. accuracy of sgns - is 78.1. accuracy of sgns - is 70.4. accuracy of sgns sem is 67.2. accuracy of sgns syn is 77.3. accuracy of swivel - is 73.1. accuracy of swivel - is 39.9. accuracy of swivel - is 66.4. accuracy of swivel - is 53.4. accuracy of swivel - is 79.1. accuracy of swivel - is 71.7. accuracy of swivel sem is 78.6. accuracy of swivel syn is 78.0. accuracy of swivel + co - is 74.0. accuracy of swivel + co - is 41.2. accuracy of swivel + co - is 66.3. accuracy of swivel + co - is 53.6. accuracy of swivel + co - is 79.8. accuracy of swivel + co - is 72.5. accuracy of swivel + co sem is 79.4. accuracy of swivel + co syn is 78.1.
table 5 shows the breakdown of performance on the vua sequence labeling test set by pos tags. # of verb # is 20k. % metaphor of verb % metaphor is 18.1. p of verb p is 68.1. r of verb r is 71.9. f1. of verb f1. is 69.9. # of noun # is 20k. % metaphor of noun % metaphor is 13.6. p of noun p is 59.9. r of noun r is 60.8. f1. of noun f1. is 60.4. # of adp # is 13k. % metaphor of adp % metaphor is 28.0. p of adp p is 86.8. r of adp r is 89.0. f1. of adp f1. is 87.9. # of adj # is 9k. % metaphor of adj % metaphor is 11.5. p of adj p is 56.1. r of adj r is 60.6. f1. of adj f1. is 58.3. # of part # is 3k. % metaphor of part % metaphor is 10.1. p of part p is 57.1. r of part r is 59.1. f1. of part f1. is 58.1.
table 6 shows model performances for the verb classification task. p of lexical baseline p is 39.1. r of lexical baseline r is 26.7. f1 of lexical baseline f1 is 31.3. acc. of lexical baseline acc. is 43.6. p of lexical baseline p is 72.4. r of lexical baseline r is 55.7. f1 of lexical baseline f1 is 62.9. acc. of lexical baseline acc. is 71.4. p of lexical baseline p is 67.9. r of lexical baseline r is 40.7. f1 of lexical baseline f1 is 50.9. acc. of lexical baseline acc. is 76.4. maf1 of lexical baseline maf1 is 48.9. maf1 of klebanov (2016) maf1 is 60.0. p of rei (2017) p is 73.6. r of rei (2017) r is 76.1. f1 of rei (2017) f1 is 74.2. acc. of rei (2017) acc. is 74.8. r of koper (2017) r is . f1 of koper (2017) f1 is 75.0. f1 of koper (2017) f1 is 62.0. p of wu (2018) ensemble p is 60.0. r of wu (2018) ensemble r is 76.3. f1 of wu (2018) ensemble f1 is 67.2. p of cls p is 75.3. r of cls r is 84.3. f1 of cls f1 is 79.1. acc. of cls acc. is 78.5. p of cls p is 68.7. r of cls r is 74.6. f1 of cls f1 is 72.0. acc. of cls acc. is 73.7. p of cls p is 53.4. r of cls r is 65.6. f1 of cls f1 is 58.9. acc. of cls acc. is 69.1. maf1 of cls maf1 is 53.4. p of seq p is 79.1. r of seq r is 73.5. f1 of seq f1 is 75.6. acc. of seq acc. is 77.2. p of seq p is 70.7. r of seq r is 71.6. f1 of seq f1 is 71.1. acc. of seq acc. is 74.6. p of seq p is 68.2. r of seq r is 71.3. f1 of seq f1 is 69.7. acc. of seq acc. is 81.4. maf1 of seq maf1 is 66.4.
table 2 shows experimental results on chinese-english dataset. accuracy (%) of 0 accuracy (%) is 0.05. accuracy (%) of 0 accuracy (%) is 0.09. accuracy (%) of 50 accuracy (%) is 0.29. accuracy (%) of 100 accuracy (%) is 21.79. accuracy (%) of 0 accuracy (%) is 43.31. accuracy (%) of 0 accuracy (%) is 51.37.
table 3 shows performance of sentiment classifiers on opt. acc% of opt acc% is 63.20. acc% of opt acc% is 63.60. acc% of opt acc% is 59.60. acc% of opt acc% is 67.60. acc% of opt acc% is 55.20. acc% of opt acc% is 80.19.
table 1 shows the results of human annotations (c = consistency, l = logic, e = emotion). c of s2s c is 1.301. l of s2s l is 0.776. e of s2s e is 0.197. c of s2s c is 1.368. l of s2s l is 0.924. e of s2s e is 0.285. c of s2s c is 1.341. l of s2s l is 0.757. e of s2s e is 0.217. c of s2s c is 1.186. l of s2s l is 0.723. e of s2s e is 0.076. c of s2s c is 1.393. l of s2s l is 0.928. e of s2s e is 0.237. c of s2s c is 1.245. l of s2s l is 0.782. e of s2s e is 0.215. c of s2s c is 1.205. l of s2s l is 0.535. e of s2s e is 0.113. c of s2s c is 1.368. l of s2s l is 0.680. e of s2s e is 0.236. c of s2s-aw c is 1.348. l of s2s-aw l is 1.063. e of s2s-aw e is 0.231. c of s2s-aw c is 1.437. l of s2s-aw l is 1.097. e of s2s-aw e is 0.237. c of s2s-aw c is 1.418. l of s2s-aw l is 1.125. e of s2s-aw e is 0.276. c of s2s-aw c is 1.213. l of s2s-aw l is 0.916. e of s2s-aw e is 0.105. c of s2s-aw c is 1.423. l of s2s-aw l is 1.196. e of s2s-aw e is 0.293. c of s2s-aw c is 1.260. l of s2s-aw l is 1.105. e of s2s-aw e is 0.272. c of s2s-aw c is 1.198. l of s2s-aw l is 0.860. e of s2s-aw e is 0.182. c of s2s-aw c is 1.488. l of s2s-aw l is 1.145. e of s2s-aw e is 0.253. c of e-scba c is 1.375. l of e-scba l is 1.123. e of e-scba e is 0.476. c of e-scba c is 1.476. l of e-scba l is 1.286. e of e-scba e is 0.615. c of e-scba c is 1.437. l of e-scba l is 1.173. e of e-scba e is 0.545. c of e-scba c is 1.197. l of e-scba l is 0.902. e of e-scba e is 0.245. c of e-scba c is 1.497. l of e-scba l is 1.268. e of e-scba e is 0.525. c of e-scba c is 1.268. l of e-scba l is 1.124. e of e-scba e is 0.453. c of e-scba c is 1.110. l of e-scba l is 0.822. e of e-scba e is 0.347. c of e-scba c is 1.637. l of e-scba l is 1.289. e of e-scba e is 0.603.
table 3 shows results of classification evaluations. precision of co precision is 0.62. recall of co recall is 0.62. f1 of co f1 is 0.62. precision of co+ct precision is 0.60. recall of co+ct recall is 0.61. f1 of co+ct f1 is 0.61. precision of all precision is 0.61. recall of all recall is 0.61. f1 of all f1 is 0.62. precision of co precision is 0.60. recall of co recall is 0.58. f1 of co f1 is 0.59. precision of co+ct precision is 0.61. recall of co+ct recall is 0.59. f1 of co+ct f1 is 0.60. precision of all precision is 0.61. recall of all recall is 0.57. f1 of all f1 is 0.58. precision of co precision is 0.65. recall of co recall is 0.63. f1 of co f1 is 0.64. precision of co+ct precision is 0.68. recall of co+ct recall is 0.64. f1 of co+ct f1 is 0.65. precision of all precision is 0.67. recall of all recall is 0.67. f1 of all f1 is 0.67. precision of co+t precision is 0.65. recall of co+t recall is 0.64. f1 of co+t f1 is 0.65. precision of co+ct+t precision is 0.70. recall of co+ct+t recall is 0.66. f1 of co+ct+t f1 is 0.68. precision of all+t precision is 0.74. recall of all+t recall is 0.67. f1 of all+t f1 is 0.70.
table 3 shows human evaluation results of the aem model and the seq2seq model. fluency of seq2seq fluency is 6.97. coherence of seq2seq coherence is 3.51. g-score of seq2seq g-score is 4.95. fluency of aem fluency is 8.11. coherence of aem coherence is 4.18. g-score of aem g-score is 5.82. fluency of seq2seq+attention fluency is 5.11. coherence of seq2seq+attention coherence is 3.30. g-score of seq2seq+attention g-score is 4.10. fluency of aem+attention fluency is 7.92. coherence of aem+attention coherence is 4.97. g-score of aem+attention g-score is 6.27.
table 2 shows accuracy results over the test set asr transcripts, for w2v and skip-thought (st). accuracy (%) of all-yes accuracy (%) is 39.8. accuracy (%) of w2v title-speech accuracy (%) is 49.8. accuracy (%) of w2v arg-speech accuracy (%) is 57.6. accuracy (%) of w2v title-sentence accuracy (%) is 55.8. accuracy (%) of w2v arg-sentence accuracy (%) is 64.6. accuracy (%) of st arg-sentence accuracy (%) is 60.2.
table 1 shows results of our model compared with prior published results. meteor of krause et al. (template) meteor is 14.31. cider of krause et al. (template) cider is 12.15. bleu-1 of krause et al. (template) bleu-1 is 37.47. bleu-2 of krause et al. (template) bleu-2 is 21.02. bleu-3 of krause et al. (template) bleu-3 is 12.30. bleu-4 of krause et al. (template) bleu-4 is 7.38. meteor of krause et al. (flat w/o object detector) meteor is 12.82. cider of krause et al. (flat w/o object detector) cider is 11.06. bleu-1 of krause et al. (flat w/o object detector) bleu-1 is 34.04. bleu-2 of krause et al. (flat w/o object detector) bleu-2 is 19.95. bleu-3 of krause et al. (flat w/o object detector) bleu-3 is 12.20. bleu-4 of krause et al. (flat w/o object detector) bleu-4 is 7.71. meteor of krause et al. (flat) meteor is 13.54. cider of krause et al. (flat) cider is 11.14. bleu-1 of krause et al. (flat) bleu-1 is 37.30. bleu-2 of krause et al. (flat) bleu-2 is 21.70. bleu-3 of krause et al. (flat) bleu-3 is 13.07. bleu-4 of krause et al. (flat) bleu-4 is 8.07. meteor of krause et al. (hierarchical) meteor is 15.95. cider of krause et al. (hierarchical) cider is 13.52. bleu-1 of krause et al. (hierarchical) bleu-1 is 41.90. bleu-2 of krause et al. (hierarchical) bleu-2 is 24.11. bleu-3 of krause et al. (hierarchical) bleu-3 is 14.23. bleu-4 of krause et al. (hierarchical) bleu-4 is 8.69. meteor of liang et al. (w/o discriminator) meteor is 16.57. cider of liang et al. (w/o discriminator) cider is 15.07. bleu-1 of liang et al. (w/o discriminator) bleu-1 is 41.86. bleu-2 of liang et al. (w/o discriminator) bleu-2 is 24.33. bleu-3 of liang et al. (w/o discriminator) bleu-3 is 14.56. bleu-4 of liang et al. (w/o discriminator) bleu-4 is 8.99. meteor of liang et al. meteor is 17.12. cider of liang et al. cider is 16.87. bleu-1 of liang et al. bleu-1 is 41.99. bleu-2 of liang et al. bleu-2 is 24.86. bleu-3 of liang et al. bleu-3 is 14.89. bleu-4 of liang et al. bleu-4 is 9.03. meteor of ours (xe training w/o rep. penalty) meteor is 13.66. cider of ours (xe training w/o rep. penalty) cider is 12.89. bleu-1 of ours (xe training w/o rep. penalty) bleu-1 is 32.78. bleu-2 of ours (xe training w/o rep. penalty) bleu-2 is 19.00. bleu-3 of ours (xe training w/o rep. penalty) bleu-3 is 11.40. bleu-4 of ours (xe training w/o rep. penalty) bleu-4 is 6.89. meteor of ours (xe training w/ rep. penalty) meteor is 15.17. cider of ours (xe training w/ rep. penalty) cider is 22.68. bleu-1 of ours (xe training w/ rep. penalty) bleu-1 is 35.68. bleu-2 of ours (xe training w/ rep. penalty) bleu-2 is 22.40. bleu-3 of ours (xe training w/ rep. penalty) bleu-3 is 14.04. bleu-4 of ours (xe training w/ rep. penalty) bleu-4 is 8.70. meteor of ours (scst training w/o rep. penalty) meteor is 13.63. cider of ours (scst training w/o rep. penalty) cider is 13.77. bleu-1 of ours (scst training w/o rep. penalty) bleu-1 is 29.67. bleu-2 of ours (scst training w/o rep. penalty) bleu-2 is 16.45. bleu-3 of ours (scst training w/o rep. penalty) bleu-3 is 9.74. bleu-4 of ours (scst training w/o rep. penalty) bleu-4 is 5.88. meteor of ours (scst training w/ rep. penalty) meteor is 17.86. cider of ours (scst training w/ rep. penalty) cider is 30.63. bleu-1 of ours (scst training w/ rep. penalty) bleu-1 is 43.54. bleu-2 of ours (scst training w/ rep. penalty) bleu-2 is 27.44. bleu-3 of ours (scst training w/ rep. penalty) bleu-3 is 17.33. bleu-4 of ours (scst training w/ rep. penalty) bleu-4 is 10.58.
table 1 shows correlation results with the manual metrics of pyramid, responsiveness, and readability using the correlation metrics of pearson r, spearman ρ, and kendall τ. pearson of c s iiith3 pearson is 0.965. spearman of c s iiith3 spearman is 0.903. kendall of c s iiith3 kendall is 0.758. pearson of c s iiith3 pearson is 0.933. spearman of c s iiith3 spearman is 0.781. kendall of c s iiith3 kendall is 0.596. pearson of c s iiith3 pearson is 0.731. spearman of c s iiith3 spearman is 0.358. kendall of c s iiith3 kendall is 0.242. pearson of demokritosgr1 pearson is 0.974. spearman of demokritosgr1 spearman is 0.897. kendall of demokritosgr1 kendall is 0.747. pearson of demokritosgr1 pearson is 0.947. spearman of demokritosgr1 spearman is 0.845. kendall of demokritosgr1 kendall is 0.675. pearson of demokritosgr1 pearson is 0.794. spearman of demokritosgr1 spearman is 0.497. kendall of demokritosgr1 kendall is 0.359. pearson of catolicasc1 pearson is 0.967. spearman of catolicasc1 spearman is 0.902. kendall of catolicasc1 kendall is 0.735. pearson of catolicasc1 pearson is 0.950. spearman of catolicasc1 spearman is 0.837. kendall of catolicasc1 kendall is 0.666. pearson of catolicasc1 pearson is 0.819. spearman of catolicasc1 spearman is 0.494. kendall of catolicasc1 kendall is 0.366. pearson of rouge-1 pearson is 0.966. spearman of rouge-1 spearman is 0.909. kendall of rouge-1 kendall is 0.747. pearson of rouge-1 pearson is 0.935. spearman of rouge-1 spearman is 0.818. kendall of rouge-1 kendall is 0.633. pearson of rouge-1 pearson is 0.790. spearman of rouge-1 spearman is 0.391. kendall of rouge-1 kendall is 0.285. pearson of rouge-2 pearson is 0.961. spearman of rouge-2 spearman is 0.894. kendall of rouge-2 kendall is 0.745. pearson of rouge-2 pearson is 0.942. spearman of rouge-2 spearman is 0.790. kendall of rouge-2 kendall is 0.610. pearson of rouge-2 pearson is 0.752. spearman of rouge-2 spearman is 0.398. kendall of rouge-2 kendall is 0.293. pearson of rouge-su4 pearson is 0.981. spearman of rouge-su4 spearman is 0.894. kendall of rouge-su4 kendall is 0.737. pearson of rouge-su4 pearson is 0.955. spearman of rouge-su4 spearman is 0.790. kendall of rouge-su4 kendall is 0.602. pearson of rouge-su4 pearson is 0.784. spearman of rouge-su4 spearman is 0.395. kendall of rouge-su4 kendall is 0.293. pearson of rouge-we-1 pearson is 0.949. spearman of rouge-we-1 spearman is 0.914. kendall of rouge-we-1 kendall is 0.753. pearson of rouge-we-1 pearson is 0.916. spearman of rouge-we-1 spearman is 0.819. kendall of rouge-we-1 kendall is 0.631. pearson of rouge-we-1 pearson is 0.785. spearman of rouge-we-1 spearman is 0.431. kendall of rouge-we-1 kendall is 0.322. pearson of rouge-we-2 pearson is 0.977. spearman of rouge-we-2 spearman is 0.898. kendall of rouge-we-2 kendall is 0.744. pearson of rouge-we-2 pearson is 0.953. spearman of rouge-we-2 spearman is 0.797. kendall of rouge-we-2 kendall is 0.615. pearson of rouge-we-2 pearson is 0.782. spearman of rouge-we-2 spearman is 0.414. kendall of rouge-we-2 kendall is 0.304. pearson of rouge-we-su4 pearson is 0.978. spearman of rouge-we-su4 spearman is 0.881. kendall of rouge-we-su4 kendall is 0.720. pearson of rouge-we-su4 pearson is 0.954. spearman of rouge-we-su4 spearman is 0.787. kendall of rouge-we-su4 kendall is 0.597. pearson of rouge-we-su4 pearson is 0.793. spearman of rouge-we-su4 spearman is 0.407. kendall of rouge-we-su4 kendall is 0.302. pearson of rouge-g-1 pearson is 0.971. spearman of rouge-g-1 spearman is 0.915. kendall of rouge-g-1 kendall is 0.758. pearson of rouge-g-1 pearson is 0.944. spearman of rouge-g-1 spearman is 0.825. kendall of rouge-g-1 kendall is 0.638. pearson of rouge-g-1 pearson is 0.791. spearman of rouge-g-1 spearman is 0.434. kendall of rouge-g-1 kendall is 0.330. pearson of rouge-g-2 pearson is 0.983. spearman of rouge-g-2 spearman is 0.926. kendall of rouge-g-2 kendall is 0.774. pearson of rouge-g-2 pearson is 0.956. spearman of rouge-g-2 spearman is 0.869. kendall of rouge-g-2 kendall is 0.713. pearson of rouge-g-2 pearson is 0.790. spearman of rouge-g-2 spearman is 0.516. kendall of rouge-g-2 kendall is 0.385. pearson of rouge-g-su4 pearson is 0.979. spearman of rouge-g-su4 spearman is 0.898. kendall of rouge-g-su4 kendall is 0.741. pearson of rouge-g-su4 pearson is 0.957. spearman of rouge-g-su4 spearman is 0.814. kendall of rouge-g-su4 kendall is 0.616. pearson of rouge-g-su4 pearson is 0.823. spearman of rouge-g-su4 spearman is 0.445. kendall of rouge-g-su4 kendall is 0.334.
table 1 shows results for amr-to-text bleu of our model (unguided nlg) bleu is 21.1. bleu of neuralamr (konstas et al. 2017) bleu is 22.0. bleu of tsp (song et al. 2016) bleu is 22.4. bleu of treetostr (flanigan et al. 2016) bleu is 23.0.
table 2 shows bleu and rouge results for guided and unguided models using test dataset. bleu of guided nlg (oracle) bleu is 61.3. r-1 of guided nlg (oracle) r-1 is 79.4. r-2 of guided nlg (oracle) r-2 is 63.7. r-l of guided nlg (oracle) r-l is 76.4. bleu of guided nlg bleu is 45.8. r-1 of guided nlg r-1 is 70.7. r-2 of guided nlg r-2 is 49.5. r-l of guided nlg r-l is 64.9. bleu of unguided nlg bleu is 29.6. r-1 of unguided nlg r-1 is 68.6. r-2 of unguided nlg r-2 is 39.6. r-l of unguided nlg r-l is 61.3.
table 3 shows evaluation results on atis where accori and accpara denote the accuracy on the original and paraphrased development set of atis, respectively. accori of word order accori is 84.8. accpara of word order accpara is 78.7. diff. of word order diff. is -6.1. accori of dep accori is 83.5. accpara of dep accpara is 80.1. diff. of dep diff. is -3.4. accori of cons accori is 82.9. accpara of cons accpara is 77.3. diff. of cons diff. is -5.6. accori of dep + cons accori is 84.0. accpara of dep + cons accpara is 80.7. diff. of dep + cons diff. is -3.3. accori of word order + dep accori is 85.2. accpara of word order + dep accpara is 82.3. diff. of word order + dep diff. is -2.9. accori of word order + cons accori is 84.9. accpara of word order + cons accpara is 79.9. diff. of word order + cons diff. is -5.0. accori of word order + dep + cons accori is 86.0. accpara of word order + dep + cons accpara is 83.5. diff. of word order + dep + cons diff. is -2.5.
table 2 shows results compared to baselines. acc of seq2seq acc is 0.0. bleu of seq2seq bleu is 55.0. acc of seq2seq acc is 13.9. bleu of seq2seq bleu is 67.3. acc of yn17 acc is 16.2. bleu of yn17 bleu is 75.8. acc of yn17 acc is 71.6. bleu of yn17 bleu is 84.5. acc of asn acc is 18.2. bleu of asn bleu is 77.6. acc of asn + supatt acc is 22.7. bleu of asn + supatt bleu is 79.2. acc of recode acc is 19.6. bleu of recode bleu is 78.4. acc of recode acc is 72.8. bleu of recode bleu is 84.7.
table 1 shows results on the wikisql (above) and stackoverﬂow (below). bleu-4 of template bleu-4 is 15.71. grammar. of template grammar. is 1.50. bleu-4 of seq2seq bleu-4 is 20.91. grammar. of seq2seq grammar. is 2.54. correct. of seq2seq correct. is 62.1%. bleu-4 of seq2seq + copy bleu-4 is 24.12. grammar. of seq2seq + copy grammar. is 2.65. correct. of seq2seq + copy correct. is 64.5%. bleu-4 of tree2seq bleu-4 is 26.67. grammar. of tree2seq grammar. is 2.70. correct. of tree2seq correct. is 66.8%. bleu-4 of graph2seq-pge bleu-4 is 38.97. grammar. of graph2seq-pge grammar. is 3.81. correct. of graph2seq-pge correct. is 79.2%. bleu-4 of graph2seq-nge bleu-4 is 34.28. grammar. of graph2seq-nge grammar. is 3.26. correct. of graph2seq-nge correct. is 75.3%. bleu-4 of (iyer et al. 2016) bleu-4 is 18.4. grammar. of (iyer et al. 2016) grammar. is 3.16. correct. of (iyer et al. 2016) correct. is 64.2%. bleu-4 of graph2seq-pge bleu-4 is 23.3. grammar. of graph2seq-pge grammar. is 3.23. correct. of graph2seq-pge correct. is 70.2%. bleu-4 of graph2seq-nge bleu-4 is 21.9. grammar. of graph2seq-nge grammar. is 2.97. correct. of graph2seq-nge correct. is 65.1%.
table 6 shows breakdown by property of binary classification f1 on spr1. micro f1 of crf micro f1 is 81.7. macro f1 of crf macro f1 is 65.9. micro f1 of spr1 micro f1 is 82.2. macro f1 of spr1 macro f1 is 69.3. micro f1 of mt:spr1 micro f1 is 83.3. macro f1 of mt:spr1 macro f1 is 71.1. micro f1 of spr1+2 micro f1 is 83.3. macro f1 of spr1+2 macro f1 is 70.4.
table 1 shows main results in terms of f1 score (%). f1 of finkel and manning (2009) genia is 70.3. f1 of finkel and manning (2009) w/s is 38. f1 of lu and roth (2015) ace04 is 62.8. f1 of lu and roth (2015) ace05 is 62.5. f1 of lu and roth (2015) genia is 70.3. f1 of lu and roth (2015) w/s is 454. f1 of muis and lu (2017) ace04 is 64.5. f1 of muis and lu (2017) ace05 is 63.1. f1 of muis and lu (2017) genia is 70.8. f1 of muis and lu (2017) w/s is 263. f1 of katiyar and cardie (2018) ace04 is 72.7. f1 of katiyar and cardie (2018) ace05 is 70.5. f1 of katiyar and cardie (2018) genia is 73.6. f1 of ju et al. (2018) ace05 is 72.2. f1 of ju et al. (2018) genia is 74.7. f1 of ours ace04 is 73.3. f1 of ours ace05 is 73.0. f1 of ours genia is 73.9. f1 of ours w/s is 1445. f1 of - char-level lstm ace04 is 72.3. f1 of - char-level lstm ace05 is 71.9. f1 of - char-level lstm genia is 72.1. f1 of - char-level lstm w/s is 1546. f1 of - pre-trained embeddings ace04 is 71.3. f1 of - pre-trained embeddings ace05 is 71.5. f1 of - pre-trained embeddings genia is 72.0. f1 of - pre-trained embeddings w/s is 1452. f1 of - dropout layer ace04 is 71.7. f1 of - dropout layer ace05 is 72.0. f1 of - dropout layer genia is 72.7. f1 of - dropout layer w/s is 1440.
table 1 shows results on the wikilinksned dev and test sets. accuracy on test (%) of eshel et al. (2017) accuracy on test (%) is 73.0. accuracy on dev (%) of eshel et al. (2017) accuracy on dev (%) is . accuracy on test (%) of eshel system release accuracy on test (%) is 72.2. accuracy on dev (%) of eshel system release accuracy on dev (%) is . accuracy on test (%) of gru+attn accuracy on test (%) is 74.5. accuracy on dev (%) of gru+attn accuracy on dev (%) is . accuracy on test (%) of gru+attn+feats accuracy on test (%) is 75.8. accuracy on dev (%) of gru+attn+feats accuracy on dev (%) is . accuracy on test (%) of gru accuracy on test (%) is . accuracy on dev (%) of gru accuracy on dev (%) is 73.4. accuracy on test (%) of gru+attn accuracy on test (%) is . accuracy on dev (%) of gru+attn accuracy on dev (%) is 74.4. accuracy on test (%) of gru+attn+feats accuracy on test (%) is . accuracy on dev (%) of gru+attn+feats accuracy on dev (%) is 74.9. accuracy on test (%) of gru+attn+cnn accuracy on test (%) is . accuracy on dev (%) of gru+attn+cnn accuracy on dev (%) is 73.8.
table 2 shows validation & test results on all datasets. accuracy of ga reader (chu et al. 2017) test is 49.00. accuracy of mage (48) (dhingra et al. 2017) val is 51.10. accuracy of mage (48) (dhingra et al. 2017) test is 51.60. accuracy of mage (64) (dhingra et al. 2017) val is 52.10. accuracy of mage (64) (dhingra et al. 2017) test is 51.10. accuracy of ga + c-gru (dhingra et al. 2018) test is 55.69. accuracy of attsum val is 56.03. accuracy of attsum test is 55.60. accuracy of attsum + l1 val is 58.35. accuracy of attsum + l1 test is 56.86. accuracy of attsum + l2 val is 58.08. accuracy of attsum + l2 test is 57.29. accuracy of attsum-feat val is 59.62. accuracy of attsum-feat test is 59.05. accuracy of attsum-feat + l1 val is 60.22. accuracy of attsum-feat + l1 test is 59.23. accuracy of attsum-feat + l2 val is 60.13. accuracy of attsum-feat + l2 test is 58.47. accuracy of ga reader (dhingra et al. 2016) val is 78.50. accuracy of ga reader (dhingra et al. 2016) test is 74.90. accuracy of epireader (trischler et al. 2016) val is 75.30. accuracy of epireader (trischler et al. 2016) test is 69.70. accuracy of dim reader (liu et al. 2017) val is 77.10. accuracy of dim reader (liu et al. 2017) test is 72.20. accuracy of aoa (cui et al. 2016) val is 77.80. accuracy of aoa (cui et al. 2016) test is 72.0. accuracy of aoa + reranker (cui et al. 2016) val is 79.60. accuracy of aoa + reranker (cui et al. 2016) test is 74.0. accuracy of attsum val is 74.35. accuracy of attsum test is 69.96. accuracy of attsum + l1 val is 76.20. accuracy of attsum + l1 test is 72.16. accuracy of attsum + l2 val is 76.80. accuracy of attsum + l2 test is 72.60. accuracy of attsum-feat val is 77.80. accuracy of attsum-feat test is 72.36. accuracy of attsum-feat + l1 val is 78.40. accuracy of attsum-feat + l1 test is 74.36. accuracy of attsum-feat + l2 val is 79.40. accuracy of attsum-feat + l2 test is 72.40.
table 3 shows ablation results on validation sets, see text for definitions of the numeric columns and models. accuracy of attsum all is 56.03. accuracy of attsum entity is 75.17. accuracy of attsum speaker is 74.81. accuracy of attsum quote is 73.31. accuracy of attsum + l1 all is 58.35. accuracy of attsum + l1 entity is 78.51. accuracy of attsum + l1 speaker is 78.38. accuracy of attsum + l1 quote is 79.42. accuracy of attsum + l2 all is 58.08. accuracy of attsum + l2 entity is 78.17. accuracy of attsum + l2 speaker is 77.96. accuracy of attsum + l2 quote is 76.76. accuracy of attsum-feat all is 59.62. accuracy of attsum-feat entity is 79.40. accuracy of attsum-feat speaker is 80.34. accuracy of attsum-feat quote is 79.68. accuracy of attsum-feat + l1 all is 60.22. accuracy of attsum-feat + l1 entity is 82.00. accuracy of attsum-feat + l1 speaker is 82.98. accuracy of attsum-feat + l1 quote is 81.67. accuracy of attsum-feat + l2 all is 60.14. accuracy of attsum-feat + l2 entity is 82.06. accuracy of attsum-feat + l2 speaker is 83.06. accuracy of attsum-feat + l2 quote is 82.60. accuracy of attsum all is 74.35. accuracy of attsum entity is 76.28. accuracy of attsum speaker is 75.08. accuracy of attsum quote is 74.96. accuracy of attsum + l1 all is 76.20. accuracy of attsum + l1 entity is 78.03. accuracy of attsum + l1 speaker is 76.98. accuracy of attsum + l1 quote is 77.33. accuracy of attsum + l2 all is 76.80. accuracy of attsum + l2 entity is 77.45. accuracy of attsum + l2 speaker is 76.27. accuracy of attsum + l2 quote is 76.48. accuracy of attsum-feat all is 77.80. accuracy of attsum-feat entity is 80.58. accuracy of attsum-feat speaker is 79.84. accuracy of attsum-feat quote is 79.61. accuracy of attsum-feat + l1 all is 78.40. accuracy of attsum-feat + l1 entity is 80.44. accuracy of attsum-feat + l1 speaker is 79.68. accuracy of attsum-feat + l1 quote is 79.78. accuracy of attsum-feat + l2 all is 79.40. accuracy of attsum-feat + l2 entity is 82.41. accuracy of attsum-feat + l2 speaker is 81.51. accuracy of attsum-feat + l2 quote is 81.39.
table 2 shows results of human evaluation. sentiment of cae sentiment is 6.55. content of cae content is 4.46. fluency of cae fluency is 5.98. sentiment of mae sentiment is 6.64. content of mae content is 4.43. fluency of mae fluency is 5.36. sentiment of smae sentiment is 6.57. content of smae content is 5.98. fluency of smae fluency is 6.69.
table 1 shows results on the germeval data, aspect + sentiment task. f1-score of pipeline lstm + word2vec development set is .350. f1-score of pipeline lstm + word2vec synchronic test set is .297. f1-score of pipeline lstm + word2vec diachronic test set is .342. f1-score of end-to-end lstm + word2vec development set is .378. f1-score of end-to-end lstm + word2vec synchronic test set is .315. f1-score of end-to-end lstm + word2vec diachronic test set is .383. f1-score of pipeline cnn + word2vec development set is .350. f1-score of pipeline cnn + word2vec synchronic test set is .298. f1-score of pipeline cnn + word2vec diachronic test set is .343. f1-score of end-to-end cnn + word2vec development set is .400. f1-score of end-to-end cnn + word2vec synchronic test set is .319. f1-score of end-to-end cnn + word2vec diachronic test set is .388. f1-score of pipeline lstm + glove development set is .350. f1-score of pipeline lstm + glove synchronic test set is .297. f1-score of pipeline lstm + glove diachronic test set is .342. f1-score of end-to-end lstm + glove development set is .378. f1-score of end-to-end lstm + glove synchronic test set is .315. f1-score of end-to-end lstm + glove diachronic test set is .384. f1-score of pipeline cnn + glove development set is .350. f1-score of pipeline cnn + glove synchronic test set is .298. f1-score of pipeline cnn + glove diachronic test set is .342. f1-score of end-to-end cnn + glove development set is .415. f1-score of end-to-end cnn + glove synchronic test set is .315. f1-score of end-to-end cnn + glove diachronic test set is .390. f1-score of pipeline lstm + fasttext development set is .350. f1-score of pipeline lstm + fasttext synchronic test set is .297. f1-score of pipeline lstm + fasttext diachronic test set is .342. f1-score of end-to-end lstm + fasttext development set is .378. f1-score of end-to-end lstm + fasttext synchronic test set is .315. f1-score of end-to-end lstm + fasttext diachronic test set is .384. f1-score of pipeline cnn + fasttext development set is .342. f1-score of pipeline cnn + fasttext synchronic test set is .295. f1-score of pipeline cnn + fasttext diachronic test set is .342. f1-score of end-to-end cnn + fasttext development set is .511. f1-score of end-to-end cnn + fasttext synchronic test set is .423. f1-score of end-to-end cnn + fasttext diachronic test set is .465. f1-score of majority class baseline synchronic test set is .315. f1-score of majority class baseline diachronic test set is .384. f1-score of germeval baseline synchronic test set is .322. f1-score of germeval baseline diachronic test set is .389. f1-score of germeval best submission synchronic test set is .354. f1-score of germeval best submission diachronic test set is .401.
table 2 shows micro-averaged f1-score for the prediction of aspect categories only (i.e. f1-score of end-to-end lstm + word2vec development set is .517. f1-score of end-to-end lstm + word2vec synchronic test set is .442. f1-score of end-to-end lstm + word2vec diachronic test set is .455. f1-score of end-to-end cnn + word2vec development set is .521. f1-score of end-to-end cnn + word2vec synchronic test set is .436. f1-score of end-to-end cnn + word2vec diachronic test set is .470. f1-score of end-to-end lstm + glove development set is .517. f1-score of end-to-end lstm + glove synchronic test set is .442. f1-score of end-to-end lstm + glove diachronic test set is .456. f1-score of end-to-end cnn + glove development set is .537. f1-score of end-to-end cnn + glove synchronic test set is .457. f1-score of end-to-end cnn + glove diachronic test set is .480. f1-score of end-to-end lstm + fasttext development set is .517. f1-score of end-to-end lstm + fasttext synchronic test set is .442. f1-score of end-to-end lstm + fasttext diachronic test set is .456. f1-score of end-to-end cnn + fasttext development set is .623. f1-score of end-to-end cnn + fasttext synchronic test set is .523. f1-score of end-to-end cnn + fasttext diachronic test set is .557. f1-score of majority class baseline synchronic test set is .442. f1-score of majority class baseline diachronic test set is .456. f1-score of germeval baseline synchronic test set is .481. f1-score of germeval baseline diachronic test set is .495. f1-score of germeval best submission synchronic test set is .482. f1-score of germeval best submission diachronic test set is .460.
table 2 shows evaluation results of hsldas in comparison with sommeliers’ performance. f1 scores of hslda1 monovarietal training set is 71.1. f1 scores of hslda1 monovarietal testing set is 68.4. f1 scores of hslda2 blend training set is 62.5. f1 scores of hslda2 blend testing set is 59.1. f1 scores of hslda3 balanced training set is 59.8. f1 scores of hslda3 balanced testing set is 56.4. f1 scores of sommeliers training set is na. f1 scores of sommeliers testing set is 62.1.
table 3 shows emotion detection results using 10-fold cross validation. pr of convlexlstm pr is 92.3. re of convlexlstm re is 94.3. f1 of convlexlstm f1 is 93.2. pr of convlexlstm pr is 90.4. re of convlexlstm re is 89.3. f1 of convlexlstm f1 is 89.8. pr of convlstm pr is 86.6. re of convlstm re is 88.4. f1 of convlstm f1 is 87.4. pr of convlstm pr is 87.0. re of convlstm re is 83.0. f1 of convlstm f1 is 85.0. pr of cnn pr is 85.0. re of cnn re is 84.0. f1 of cnn f1 is 84.5. pr of cnn pr is 82.2. re of cnn re is 82.8. f1 of cnn f1 is 82.5. pr of lstm pr is 86.0. re of lstm re is 86.6. f1 of lstm f1 is 86.3. pr of lstm pr is 85.0. re of lstm re is 83.0. f1 of lstm f1 is 84.0. pr of seven-lexicon pr is 63.4. re of seven-lexicon re is 87.3. f1 of seven-lexicon f1 is 73.45. pr of seven-lexicon pr is 60.0. re of seven-lexicon re is 85.1. f1 of seven-lexicon f1 is 70.37. pr of c-convlstm pr is 86.2. re of c-convlstm re is 87.0. f1 of c-convlstm f1 is 86.6. pr of c-convlstm pr is 85.0. re of c-convlstm re is 82.0. f1 of c-convlstm f1 is 83.47. pr of swat pr is 66.0. re of swat re is 68.0. f1 of swat f1 is 67.0. pr of swat pr is 65.5. re of swat re is 66.7. f1 of swat f1 is 66.0. pr of emosvm pr is 81.0. re of emosvm re is 82.0. f1 of emosvm f1 is 81.5. pr of emosvm pr is 82.0. re of emosvm re is 80.0. f1 of emosvm f1 is 81.0. pr of convlexlstm pr is 93.7. re of convlexlstm re is 91.1. f1 of convlexlstm f1 is 92.3. pr of convlexlstm pr is 88.0. re of convlexlstm re is 90.9. f1 of convlexlstm f1 is 89.4. pr of convlstm pr is 89.0. re of convlstm re is 87.8. f1 of convlstm f1 is 88.4. pr of convlstm pr is 81.0. re of convlstm re is 87.5. f1 of convlstm f1 is 84.0. pr of cnn pr is 83.2. re of cnn re is 83.6. f1 of cnn f1 is 83.4. pr of cnn pr is 81.7. re of cnn re is 80.5. f1 of cnn f1 is 81.0. pr of lstm pr is 87.4. re of lstm re is 85.8. f1 of lstm f1 is 86.6. pr of lstm pr is 83.2. re of lstm re is 83.6. f1 of lstm f1 is 83.4. pr of seven-lexicon pr is 61.0. re of seven-lexicon re is 84.9. f1 of seven-lexicon f1 is 70.99. pr of seven-lexicon pr is 61.0. re of seven-lexicon re is 83.3. f1 of seven-lexicon f1 is 70.42. pr of c-convlstm pr is 85.0. re of c-convlstm re is 83.6. f1 of c-convlstm f1 is 84.3. pr of c-convlstm pr is 83.7. re of c-convlstm re is 82.1. f1 of c-convlstm f1 is 82.9. pr of swat pr is 65.0. re of swat re is 66.0. f1 of swat f1 is 65.5. pr of swat pr is 64.0. re of swat re is 65.0. f1 of swat f1 is 64.5. pr of emosvm pr is 80.5. re of emosvm re is 81.7. f1 of emosvm f1 is 81.0. pr of emosvm pr is 79.0. re of emosvm re is 78.0. f1 of emosvm f1 is 78.5.
table 4 shows prediction results (pearson r, using unigrams + topics) using full 10% data vs. r of user to county income is .82. r of user to county educat. is .88. r of user to county life satis. is .47. r of user to county heart disease is .75. r of nuser−tweets income is 1.350b. r of nuser−tweets educat. is 1.350b. r of nuser−tweets life satis. is 1.356b. r of nuser−tweets heart disease is 1.360b. r of tweet to county (all) income is .72. r of tweet to county (all) educat. is .81. r of tweet to county (all) life satis. is .36. r of tweet to county (all) heart disease is .71. r of county (all) income is .73. r of county (all) educat. is .82. r of county (all) life satis. is .31. r of county (all) heart disease is .72. r of nall−tweets income is 1.621b. r of nall−tweets educat. is 1.621b. r of nall−tweets life satis. is 1.628b. r of nall−tweets heart disease is 1.634b.
table 5 shows 1% sample prediction results (pearson r) using topics + unigrams. r of tweet to county income is .71. r of tweet to county income is .62. r of tweet to county educat. is .77. r of tweet to county educat. is .71. r of tweet to county life satis. is .35. r of tweet to county life satis. is .32. r of tweet to county heart disease is .64. r of tweet to county heart disease is .63. r of county income is .70. r of county income is .60. r of county educat. is .76. r of county educat. is .67. r of county life satis. is .32. r of county life satis. is .28. r of county heart disease is .62. r of county heart disease is .62. r of user to county income is .76. r of user to county income is .70. r of user to county educat. is .79. r of user to county educat. is .74. r of user to county life satis. is .39. r of user to county life satis. is .28. r of user to county heart disease is .66. r of user to county heart disease is .66. r of nuser−tweets income is 127m. r of nuser−tweets income is 130m. r of nuser−tweets educat. is 127m. r of nuser−tweets educat. is 130m. r of nuser−tweets life satis. is 127m. r of nuser−tweets life satis. is 130m. r of nuser−tweets heart disease is 127m. r of nuser−tweets heart disease is 131m. r of county (all) income is .75. r of county (all) income is .67. r of county (all) educat. is .83. r of county (all) educat. is .77. r of county (all) life satis. is .37. r of county (all) life satis. is .34. r of county (all) heart disease is .68. r of county (all) heart disease is .66. r of nall−tweets income is 191m. r of nall−tweets income is 195m. r of nall−tweets educat. is 191m. r of nall−tweets educat. is 195m. r of nall−tweets life satis. is 191m. r of nall−tweets life satis. is 197m. r of nall−tweets heart disease is 191m. r of nall−tweets heart disease is 198m. r of ncounties income is 949. r of ncounties income is 1750. r of ncounties educat. is 949. r of ncounties educat. is 1750. r of ncounties life satis. is 954. r of ncounties life satis. is 1952. r of ncounties heart disease is 960. r of ncounties heart disease is 2041.
table 3 shows language modeling perplexity on ptb test set (lower is better). perplexity of 24m dev. is 73.3. perplexity of 24m test is 71.4. perplexity of 24m dev. is 78.8. perplexity of 24m test is 76.2. perplexity of 10m dev. is 73.1. perplexity of 10m test is 69.2. perplexity of 10m dev. is 75.1. perplexity of 10m test is 71.7. perplexity of 10m dev. is 72.5. perplexity of 10m test is 69.5. perplexity of 10m dev. is 69.5. perplexity of 10m test is 66.3. perplexity of 24m dev. is 68.7. perplexity of 24m test is 65.2. perplexity of 24m dev. is 70.8. perplexity of 24m test is 66.9. perplexity of 24m dev. is 70.0. perplexity of 24m test is 67.0. perplexity of 24m dev. is 66.0. perplexity of 24m test is 63.1.
table 1 shows overall performance comparing to the state-of-the-art methods with golden-standard entities. p of cross-event p is n/a. r of cross-event r is n/a. f1 of cross-event f1 is n/a. p of cross-event p is 68.7. r of cross-event r is 68.9. f1 of cross-event f1 is 68.8. p of cross-event p is 50.9. r of cross-event r is 49.7. f1 of cross-event f1 is 50.3. p of cross-event p is 45.1. r of cross-event r is 44.1. f1 of cross-event f1 is 44.6. p of jointbeam p is 76.9. r of jointbeam r is 65.0. f1 of jointbeam f1 is 70.4. p of jointbeam p is 73.7. r of jointbeam r is 62.3. f1 of jointbeam f1 is 67.5. p of jointbeam p is 69.8. r of jointbeam r is 47.9. f1 of jointbeam f1 is 56.8. p of jointbeam p is 64.7. r of jointbeam r is 44.4. f1 of jointbeam f1 is 52.7. p of dmcnn p is 80.4. r of dmcnn r is 67.7. f1 of dmcnn f1 is 73.5. p of dmcnn p is 75.6. r of dmcnn r is 63.6. f1 of dmcnn f1 is 69.1. p of dmcnn p is 68.8. r of dmcnn r is 51.9. f1 of dmcnn f1 is 59.1. p of dmcnn p is 62.2. r of dmcnn r is 46.9. f1 of dmcnn f1 is 53.5. p of psl p is n/a. r of psl r is n/a. f1 of psl f1 is n/a. p of psl p is 75.3. r of psl r is 64.4. f1 of psl f1 is 69.4. p of psl p is n/a. r of psl r is n/a. f1 of psl f1 is n/a. p of psl p is n/a. r of psl r is n/a. f1 of psl f1 is n/a. p of jrnn p is 68.5. r of jrnn r is 75.7. f1 of jrnn f1 is 71.9. p of jrnn p is 66.0. r of jrnn r is 73.0. f1 of jrnn f1 is 69.3. p of jrnn p is 61.4. r of jrnn r is 64.2. f1 of jrnn f1 is 62.8. p of jrnn p is 54.2. r of jrnn r is 56.7. f1 of jrnn f1 is 55.4. p of dbrnn p is n/a. r of dbrnn r is n/a. f1 of dbrnn f1 is n/a. p of dbrnn p is 74.1. r of dbrnn r is 69.8. f1 of dbrnn f1 is 71.9. p of dbrnn p is 71.3. r of dbrnn r is 64.5. f1 of dbrnn f1 is 67.7. p of dbrnn p is 66.2. r of dbrnn r is 52.8. f1 of dbrnn f1 is 58.7. p of jmee p is 80.2. r of jmee r is 72.1. f1 of jmee f1 is 75.9. p of jmee p is 76.3. r of jmee r is 71.3. f1 of jmee f1 is 73.7. p of jmee p is 71.4. r of jmee r is 65.6. f1 of jmee f1 is 68.4. p of jmee p is 66.8. r of jmee r is 54.9. f1 of jmee f1 is 60.3.
table 2 shows performance of different ed systems. accuracy of lstm+softmax 1/1 is 74.7. accuracy of lstm+softmax 1/n is 44.6. accuracy of lstm+softmax all is 66.8. accuracy of lstm+crf 1/1 is 75.1. accuracy of lstm+crf 1/n is 49.5. accuracy of lstm+crf all is 68.5. accuracy of lstm+tlstm 1/1 is 76.8. accuracy of lstm+tlstm 1/n is 51.2. accuracy of lstm+tlstm all is 70.2. accuracy of lstm+htlstm 1/1 is 77.9. accuracy of lstm+htlstm 1/n is 57.3. accuracy of lstm+htlstm all is 72.4. accuracy of lstm+htlstm+bias 1/1 is 78.4. accuracy of lstm+htlstm+bias 1/n is 59.5. accuracy of lstm+htlstm+bias all is 73.3.
table 5 shows experimental results involving analyzing pps as valency patterns. uas of baseline uas is 87.59. las of baseline las is 83.64. core p of baseline core p is 80.87. core r of baseline core r is 81.31. core f of baseline core f is 81.08. func. p of baseline func. p is 91.99. func. r of baseline func. r is 92.43. func. f of baseline func. f is 92.20. pp p of baseline pp p is 77.29. pp r of baseline pp r is 77.99. pp f of baseline pp f is 77.62. uas of pp mtl uas is 87.67. las of pp mtl las is 83.70. core p of pp mtl core p is 80.61. core r of pp mtl core r is 81.23. core f of pp mtl core f is 80.91. func. p of pp mtl func. p is 92.03. func. r of pp mtl func. r is 92.50. func. f of pp mtl func. f is 92.26. pp p of pp mtl pp p is 78.30. pp r of pp mtl pp r is 78.38. pp f of pp mtl pp f is 78.32. uas of pp mtl + joint decoding uas is 87.68. las of pp mtl + joint decoding las is 83.69. core p of pp mtl + joint decoding core p is 79.93. core r of pp mtl + joint decoding core r is 81.50. core f of pp mtl + joint decoding core f is 80.69. func. p of pp mtl + joint decoding func. p is 91.92. func. r of pp mtl + joint decoding func. r is 92.51. func. f of pp mtl + joint decoding func. f is 92.21. pp p of pp mtl + joint decoding pp p is 80.59. pp r of pp mtl + joint decoding pp r is 77.68. pp f of pp mtl + joint decoding pp f is 79.04. uas of core + pp mtl uas is 87.70. las of core + pp mtl las is 83.77. core p of core + pp mtl core p is 81.62. core r of core + pp mtl core r is 81.81. core f of core + pp mtl core f is 81.71. func. p of core + pp mtl func. p is 91.93. func. r of core + pp mtl func. r is 92.52. func. f of core + pp mtl func. f is 92.22. pp p of core + pp mtl pp p is 77.93. pp r of core + pp mtl pp r is 78.25. pp f of core + pp mtl pp f is 78.08. uas of core + pp mtl + joint decoding uas is 87.80. las of core + pp mtl + joint decoding las is 83.91. core p of core + pp mtl + joint decoding core p is 84.18. core r of core + pp mtl + joint decoding core r is 81.97. core f of core + pp mtl + joint decoding core f is 83.05. func. p of core + pp mtl + joint decoding func. p is 91.68. func. r of core + pp mtl + joint decoding func. r is 92.65. func. f of core + pp mtl + joint decoding func. f is 92.16. pp p of core + pp mtl + joint decoding pp p is 79.71. pp r of core + pp mtl + joint decoding pp r is 78.03. pp f of core + pp mtl + joint decoding pp f is 78.83. uas of core + func. + pp mtl uas is 87.67. las of core + func. + pp mtl las is 83.75. core p of core + func. + pp mtl core p is 81.35. core r of core + func. + pp mtl core r is 81.68. core f of core + func. + pp mtl core f is 81.50. func. p of core + func. + pp mtl func. p is 92.18. func. r of core + func. + pp mtl func. r is 92.61. func. f of core + func. + pp mtl func. f is 92.39. pp p of core + func. + pp mtl pp p is 77.99. pp r of core + func. + pp mtl pp r is 78.22. pp f of core + func. + pp mtl pp f is 78.08. uas of core + func. + pp mtl + joint decoding uas is 87.81. las of core + func. + pp mtl + joint decoding las is 83.94. core p of core + func. + pp mtl + joint decoding core p is 83.88. core r of core + func. + pp mtl + joint decoding core r is 81.97. core f of core + func. + pp mtl + joint decoding core f is 82.90. func. p of core + func. + pp mtl + joint decoding func. p is 92.78. func. r of core + func. + pp mtl + joint decoding func. r is 92.63. func. f of core + func. + pp mtl + joint decoding func. f is 92.70. pp p of core + func. + pp mtl + joint decoding pp p is 79.54. pp r of core + func. + pp mtl + joint decoding pp r is 78.11. pp f of core + func. + pp mtl + joint decoding pp f is 78.78.
table 5 shows human accuracy on test set based on different sources. human accuracy on test. of question human accuracy on test. is 31.84. human accuracy on test. of video and question human accuracy on test. is 61.73. human accuracy on test. of subtitle and question human accuracy on test. is 72.88. human accuracy on test. of video subtitle and question human accuracy on test. is 89.41.
table 6 shows comparison of different model performance on tempo hl on the test set. r@1 of frequeny prior r@1 is 19.43. miou of frequeny prior miou is 25.44. r@1 of frequeny prior r@1 is 29.31. miou of frequeny prior miou is 51.92. r@1 of frequeny prior r@1 is 0.00. miou of frequeny prior miou is 0.00. r@1 of frequeny prior r@1 is 0.00. miou of frequeny prior miou is 7.84. r@1 of frequeny prior r@1 is 4.74. miou of frequeny prior miou is 12.27. r@1 of frequeny prior r@1 is 10.69.  r@5 of frequeny prior  r@5 is 37.56. miou of frequeny prior miou is 19.50. r@1 of mcn r@1 is 26.07. miou of mcn miou is 39.92. r@1 of mcn r@1 is 26.79. miou of mcn miou is 51.40. r@1 of mcn r@1 is 14.93. miou of mcn miou is 34.28. r@1 of mcn r@1 is 18.55. miou of mcn miou is 47.92. r@1 of mcn r@1 is 10.70. miou of mcn miou is 35.47. r@1 of mcn r@1 is 19.4.  r@5 of mcn  r@5 is 70.88. miou of mcn miou is 41.80. r@1 of tall + tef r@1 is 21.79. miou of tall + tef miou is 33.55. r@1 of tall + tef r@1 is 25.91. miou of tall + tef miou is 49.26. r@1 of tall + tef r@1 is 14.43. miou of tall + tef miou is 32.62. r@1 of tall + tef r@1 is 2.52. miou of tall + tef miou is 31.13. r@1 of tall + tef r@1 is 8.1. miou of tall + tef miou is 28.14. r@1 of tall + tef r@1 is 14.55.  r@5 of tall + tef  r@5 is 60.69. miou of tall + tef miou is 34.94. r@1 of mllc - global r@1 is 27.01. miou of mllc - global miou is 41.72. r@1 of mllc - global r@1 is 27.42. miou of mllc - global miou is 52.22. r@1 of mllc - global r@1 is 14.10. miou of mllc - global miou is 34.33. r@1 of mllc - global r@1 is 18.40. miou of mllc - global miou is 49.17. r@1 of mllc - global r@1 is 10.86. miou of mllc - global miou is 35.36. r@1 of mllc - global r@1 is 19.56.  r@5 of mllc - global  r@5 is 71.23. miou of mllc - global miou is 42.56. r@1 of mllc - b/a r@1 is 26.47. miou of mllc - b/a miou is 40.39. r@1 of mllc - b/a r@1 is 31.95. miou of mllc - b/a miou is 55.89. r@1 of mllc - b/a r@1 is 14.93. miou of mllc - b/a miou is 34.78. r@1 of mllc - b/a r@1 is 17.36. miou of mllc - b/a miou is 47.52. r@1 of mllc - b/a r@1 is 11.32. miou of mllc - b/a miou is 35.52. r@1 of mllc - b/a r@1 is 20.40.  r@5 of mllc - b/a  r@5 is 70.97. miou of mllc - b/a miou is 42.82. r@1 of mllc (ours) r@1 is 27.38. miou of mllc (ours) miou is 42.45. r@1 of mllc (ours) r@1 is 32.33. miou of mllc (ours) miou is 56.91. r@1 of mllc (ours) r@1 is 14.43. miou of mllc (ours) miou is 37.33. r@1 of mllc (ours) r@1 is 19.58. miou of mllc (ours) miou is 50.39. r@1 of mllc (ours) r@1 is 10.39. miou of mllc (ours) miou is 35.95. r@1 of mllc (ours) r@1 is 20.82.  r@5 of mllc (ours)  r@5 is 71.68. miou of mllc (ours) miou is 44.57. r@1 of mllc (ours) context sup. test r@1 is 27.39. miou of mllc (ours) context sup. test miou is 42.25. r@1 of mllc (ours) context sup. test r@1 is 52.58. miou of mllc (ours) context sup. test miou is 80.37. r@1 of mllc (ours) context sup. test r@1 is 36.48. miou of mllc (ours) context sup. test miou is 75.79. r@1 of mllc (ours) context sup. test r@1 is 36.05. miou of mllc (ours) context sup. test miou is 70.51. r@1 of mllc (ours) context sup. test r@1 is 10.39. miou of mllc (ours) context sup. test miou is 35.87. r@1 of mllc (ours) context sup. test r@1 is 32.58.  r@5 of mllc (ours) context sup. test  r@5 is 79.86. miou of mllc (ours) context sup. test miou is 60.96.
table 2 shows results of domain specific named entity recognition. p of word2vec p is 76.12. r of word2vec r is 69.80. f1 of word2vec f1 is 72.82. p of word2vec p is 73.13. r of word2vec r is 54.79. f1 of word2vec f1 is 62.64. p of word2vec p is 75.22. r of word2vec r is 75.37. f1 of word2vec f1 is 74.39. p of glove p is 75.83. r of glove r is 67.04. f1 of glove f1 is 71.14. p of glove p is 72.58. r of glove r is 53.35. f1 of glove f1 is 61.50. p of glove p is 75.76. r of glove r is 72.33. f1 of glove f1 is 74.01. p of n2v p is 76.81. r of n2v r is 66.8. f1 of n2v f1 is 71.46. p of n2v p is 73.91. r of n2v r is 54.21. f1 of n2v f1 is 62.54. p of n2v p is 72.45. r of n2v r is 74.37. f1 of n2v f1 is 73.30. p of sum p is 77.06. r of sum r is 69.01. f1 of sum f1 is 72.81. p of sum p is 74.36. r of sum r is 58.58. f1 of sum f1 is 62.25. p of sum p is 74.89. r of sum r is 74.02. f1 of sum f1 is 74.45. p of darep p is 79.03. r of darep r is 67.95. f1 of darep f1 is 73.07. p of darep p is 77.18. r of darep r is 54.19. f1 of darep f1 is 63.67. p of darep p is 78.76. r of darep r is 75.60. f1 of darep f1 is 77.15. p of cre p is 80.04. r of cre r is 67.90. f1 of cre f1 is 73.47. p of cre p is 76.74. r of cre r is 56.98. f1 of cre f1 is 65.40. p of cre p is 78.98. r of cre r is 76.63. f1 of cre f1 is 77.79. p of mem2vec p is 81.23. r of mem2vec r is 67.90. f1 of mem2vec f1 is 73.96. p of mem2vec p is 76.70. r of mem2vec r is 57.81. f1 of mem2vec f1 is 65.92. p of mem2vec p is 79.56. r of mem2vec r is 76.63. f1 of mem2vec f1 is 78.06.
table 2 shows sentiment classification accuracy results on the binary sst task. accuracy of const. tree lstm (tai et al. 2015) sst is 88.0. accuracy of dmn (kumar et al. 2016) sst is 88.6. accuracy of dcg (looks et al. 2017) sst is 89.4. accuracy of nse (munkhdalai and yu 2017) sst is 89.7. accuracy of glove bilstm-max (4.1m) sst is 88.0±.1. accuracy of fasttext bilstm-max (4.1m) sst is 86.7±.3. accuracy of naive baseline (5.4m) sst is 88.5±.4. accuracy of unweighted dme (4.1m) sst is 89.0±.2. accuracy of dme (4.1m) sst is 88.7±.6. accuracy of cdme (4.1m) sst is 89.2±.4. accuracy of cdme*-softmax (4.6m) sst is 89.3±.5. accuracy of cdme*-sigmoid (4.6m) sst is 89.8±.4.
table 3 shows image and caption retrieval results (r@1 and r@10) on flickr30k dataset, compared to vse++ baseline (faghri et al., 2017). r@1 of vse++ 1 is 32.3. r@10 of vse++ 10 is 72.1. r@1 of vse++ 1 is 43.7. r@10 of vse++ 10 is 82.1. r@1 of fasttext (15m) 1 is 35.6. r@10 of fasttext (15m) 10 is 74.7. r@1 of fasttext (15m) 1 is 47.1. r@10 of fasttext (15m) 10 is 82.7. r@1 of imagenet (29m) 1 is 25.6. r@10 of imagenet (29m) 10 is 63.1. r@1 of imagenet (29m) 1 is 36.6. r@10 of imagenet (29m) 10 is 72.2. r@1 of naive (32m) 1 is 34.4. r@10 of naive (32m) 10 is 73.9. r@1 of naive (32m) 1 is 46.4. r@10 of naive (32m) 10 is 82.2. r@1 of unweighted dme (15m) 1 is 35.9. r@10 of unweighted dme (15m) 10 is 75.0. r@1 of unweighted dme (15m) 1 is 48.9. r@10 of unweighted dme (15m) 10 is 83.7. r@1 of dme (15m) 1 is 36.5. r@10 of dme (15m) 10 is 75.5. r@1 of dme (15m) 1 is 49.7. r@10 of dme (15m) 10 is 83.6. r@1 of cdme (15m) 1 is 36.5. r@10 of cdme (15m) 10 is 75.6. r@1 of cdme (15m) 1 is 49.0. r@10 of cdme (15m) 10 is 83.8.
table 2 shows results on word similarity task. semantic/taxonomic similarity of cnn 98% is 0.49. semantic/taxonomic similarity of cnn 39% is 0.41. semantic/taxonomic similarity of cnn 44% is 0.49. semantic/taxonomic similarity of cnn 72% is 0.54. semantic/taxonomic similarity of cnn 73% is 0.46. general relatedness of cnn 54% is 0.54. general relatedness of cnn 53% is 0.20. general relatedness of cnn 26% is 0.18. visual similarity of cnn 98% is 0.53. rel+sim of cnn 39% is 0.28. semantic/taxonomic similarity of vae 98% is 0.65. semantic/taxonomic similarity of vae 39% is 0.43. semantic/taxonomic similarity of vae 44% is 0.51. semantic/taxonomic similarity of vae 72% is 0.56. semantic/taxonomic similarity of vae 73% is 0.55. general relatedness of vae 54% is 0.62. general relatedness of vae 53% is 0.22. general relatedness of vae 26% is 0.40. visual similarity of vae 98% is 0.62. rel+sim of vae 39% is 0.37. semantic/taxonomic similarity of sgns 100% is 0.50. semantic/taxonomic similarity of sgns 98% is 0.50. semantic/taxonomic similarity of sgns 100% is 0.33. semantic/taxonomic similarity of sgns 39% is 0.35. semantic/taxonomic similarity of sgns 100% is 0.66. semantic/taxonomic similarity of sgns 44% is 0.66. semantic/taxonomic similarity of sgns 100% is 0.60. semantic/taxonomic similarity of sgns 72% is 0.55. semantic/taxonomic similarity of sgns 100% is 0.60. semantic/taxonomic similarity of sgns 73% is 0.52. general relatedness of sgns 100% is 0.65. general relatedness of sgns 54% is 0.67. general relatedness of sgns 100% is 0.56. general relatedness of sgns 53% is 0.51. general relatedness of sgns 100% is 0.65. general relatedness of sgns 26% is 0.63. visual similarity of sgns 100% is 0.38. visual similarity of sgns 98% is 0.38. rel+sim of sgns 100% is 061. rel+sim of sgns 39% is 0.60. semantic/taxonomic similarity of cnn⊕sgns 98% is 0.67. semantic/taxonomic similarity of cnn⊕sgns 39% is 0.48. semantic/taxonomic similarity of cnn⊕sgns 44% is 0.65. semantic/taxonomic similarity of cnn⊕sgns 72% is 0.60. semantic/taxonomic similarity of cnn⊕sgns 73% is 0.55. general relatedness of cnn⊕sgns 54% is 0.74. general relatedness of cnn⊕sgns 53% is 0.44. general relatedness of cnn⊕sgns 26% is 0.51. visual similarity of cnn⊕sgns 98% is 0.63. rel+sim of cnn⊕sgns 39% is 0.56. semantic/taxonomic similarity of vae⊕sgns 98% is 0.70. semantic/taxonomic similarity of vae⊕sgns 39% is 0.51. semantic/taxonomic similarity of vae⊕sgns 44% is 0.67. semantic/taxonomic similarity of vae⊕sgns 72% is 0.61. semantic/taxonomic similarity of vae⊕sgns 73% is 0.60. general relatedness of vae⊕sgns 54% is 0.76. general relatedness of vae⊕sgns 53% is 0.45. general relatedness of vae⊕sgns 26% is 0.55. visual similarity of vae⊕sgns 98% is 0.63. rel+sim of vae⊕sgns 39% is 0.56. semantic/taxonomic similarity of v-sgns 100% is 0.58. semantic/taxonomic similarity of v-sgns 98% is 0.58. semantic/taxonomic similarity of v-sgns 100% is 0.29. semantic/taxonomic similarity of v-sgns 39% is 0.30. semantic/taxonomic similarity of v-sgns 100% is 0.66. semantic/taxonomic similarity of v-sgns 44% is 0.71. semantic/taxonomic similarity of v-sgns 100% is 0.73. semantic/taxonomic similarity of v-sgns 72% is 0.73. semantic/taxonomic similarity of v-sgns 100% is 0.69. semantic/taxonomic similarity of v-sgns 73% is 0.69. general relatedness of v-sgns 100% is 0.64. general relatedness of v-sgns 54% is 0.65. general relatedness of v-sgns 100% is 0.51. general relatedness of v-sgns 53% is 0.52. general relatedness of v-sgns 100% is 0.60. general relatedness of v-sgns 26% is 0.65. visual similarity of v-sgns 100% is 0.42. visual similarity of v-sgns 98% is 0.42. rel+sim of v-sgns 100% is 0.59. rel+sim of v-sgns 39% is 0.64. semantic/taxonomic similarity of iv-sgns(linear) 100% is 0.49. semantic/taxonomic similarity of iv-sgns(linear) 98% is 0.50. semantic/taxonomic similarity of iv-sgns(linear) 100% is 0.31. semantic/taxonomic similarity of iv-sgns(linear) 39% is 0.33. semantic/taxonomic similarity of iv-sgns(linear) 100% is 0.55. semantic/taxonomic similarity of iv-sgns(linear) 44% is 0.61. semantic/taxonomic similarity of iv-sgns(linear) 100% is 0.58. semantic/taxonomic similarity of iv-sgns(linear) 72% is 0.56. semantic/taxonomic similarity of iv-sgns(linear) 100% is 0.59. semantic/taxonomic similarity of iv-sgns(linear) 73% is 0.65. general relatedness of iv-sgns(linear) 100% is 0.60. general relatedness of iv-sgns(linear) 54% is 0.62. general relatedness of iv-sgns(linear) 100% is 0.41. general relatedness of iv-sgns(linear) 53% is 0.38. general relatedness of iv-sgns(linear) 100% is 0.57. general relatedness of iv-sgns(linear) 26% is 0.71. visual similarity of iv-sgns(linear) 100% is 0.36. visual similarity of iv-sgns(linear) 98% is 0.37. rel+sim of iv-sgns(linear) 100% is 0.46. rel+sim of iv-sgns(linear) 39% is 0.51. semantic/taxonomic similarity of iv-sgns(nonlinear) 100% is 0.44. semantic/taxonomic similarity of iv-sgns(nonlinear) 98% is 0.44. semantic/taxonomic similarity of iv-sgns(nonlinear) 100% is 0.30. semantic/taxonomic similarity of iv-sgns(nonlinear) 39% is 0.32. semantic/taxonomic similarity of iv-sgns(nonlinear) 100% is 0.53. semantic/taxonomic similarity of iv-sgns(nonlinear) 44% is 0.59. semantic/taxonomic similarity of iv-sgns(nonlinear) 100% is 0.54. semantic/taxonomic similarity of iv-sgns(nonlinear) 72% is 0.53. semantic/taxonomic similarity of iv-sgns(nonlinear) 100% is 0.59. semantic/taxonomic similarity of iv-sgns(nonlinear) 73% is 0.63. general relatedness of iv-sgns(nonlinear) 100% is 0.57. general relatedness of iv-sgns(nonlinear) 54% is 0.59. general relatedness of iv-sgns(nonlinear) 100% is 0.40. general relatedness of iv-sgns(nonlinear) 53% is 0.37. general relatedness of iv-sgns(nonlinear) 100% is 0.56. general relatedness of iv-sgns(nonlinear) 26% is 0.71. visual similarity of iv-sgns(nonlinear) 100% is 0.32. visual similarity of iv-sgns(nonlinear) 98% is 0.33. rel+sim of iv-sgns(nonlinear) 100% is 0.44. rel+sim of iv-sgns(nonlinear) 39% is 0.48. semantic/taxonomic similarity of pixie+ 100% is 0.63. semantic/taxonomic similarity of pixie+ 98% is 0.63. semantic/taxonomic similarity of pixie+ 100% is 0.35. semantic/taxonomic similarity of pixie+ 39% is 0.48. semantic/taxonomic similarity of pixie+ 100% is 0.63. semantic/taxonomic similarity of pixie+ 44% is 0.72. semantic/taxonomic similarity of pixie+ 100% is 0.65. semantic/taxonomic similarity of pixie+ 72% is 0.60. semantic/taxonomic similarity of pixie+ 100% is 0.62. semantic/taxonomic similarity of pixie+ 73% is 0.62. general relatedness of pixie+ 100% is 0.64. general relatedness of pixie+ 54% is 0.73. general relatedness of pixie+ 100% is 0.46. general relatedness of pixie+ 53% is 0.56. general relatedness of pixie+ 100% is 0.55. general relatedness of pixie+ 26% is 0.55. visual similarity of pixie+ 100% is 0.54. visual similarity of pixie+ 98% is 0.54. rel+sim of pixie+ 100% is 0.50. rel+sim of pixie+ 39% is 0.59. semantic/taxonomic similarity of pixie⊕ 100% is 0.71. semantic/taxonomic similarity of pixie⊕ 98% is 0.71. semantic/taxonomic similarity of pixie⊕ 100% is 0.39. semantic/taxonomic similarity of pixie⊕ 39% is 0.53. semantic/taxonomic similarity of pixie⊕ 100% is 0.68. semantic/taxonomic similarity of pixie⊕ 44% is 0.71. semantic/taxonomic similarity of pixie⊕ 100% is 0.73. semantic/taxonomic similarity of pixie⊕ 72% is 0.73. semantic/taxonomic similarity of pixie⊕ 100% is 0.69. semantic/taxonomic similarity of pixie⊕ 73% is 0.71. general relatedness of pixie⊕ 100% is 0.68. general relatedness of pixie⊕ 54% is 0.76. general relatedness of pixie⊕ 100% is 0.52. general relatedness of pixie⊕ 53% is 0.59. general relatedness of pixie⊕ 100% is 0.60. general relatedness of pixie⊕ 26% is 0.59. visual similarity of pixie⊕ 100% is 0.60. visual similarity of pixie⊕ 98% is 0.61. rel+sim of pixie⊕ 100% is 0.58. rel+sim of pixie⊕ 39% is 0.65.
table 6 shows results for image (i) ↔ sentence (s) retrieval. accuracy of sgns k=1 is 23.1. accuracy of sgns k=5 is 49.0. accuracy of sgns k=10 is 61.6. accuracy of sgns k=1 is 16.6. accuracy of sgns k=5 is 41.0. accuracy of sgns k=10 is 53.8. accuracy of v-sgns k=1 is 21.9. accuracy of v-sgns k=5 is 51.7. accuracy of v-sgns k=10 is 64.2. accuracy of v-sgns k=1 is 16.2. accuracy of v-sgns k=5 is 42.0. accuracy of v-sgns k=10 is 54.8. accuracy of iv-sgns (linear) k=1 is 22.7. accuracy of iv-sgns (linear) k=5 is 50.5. accuracy of iv-sgns (linear) k=10 is 61.7. accuracy of iv-sgns (linear) k=1 is 17.1. accuracy of iv-sgns (linear) k=5 is 42.6. accuracy of iv-sgns (linear) k=10 is 55.4. accuracy of pixie+ k=1 is 24.2. accuracy of pixie+ k=5 is 52.5. accuracy of pixie+ k=10 is 65.4. accuracy of pixie+ k=1 is 17.5. accuracy of pixie+ k=5 is 43.8. accuracy of pixie+ k=10 is 56.2. accuracy of pixie⊕ k=1 is 25.7. accuracy of pixie⊕ k=5 is 55.7. accuracy of pixie⊕ k=10 is 67.7. accuracy of pixie⊕ k=1 is 18.4. accuracy of pixie⊕ k=5 is 44.9. accuracy of pixie⊕ k=10 is 56.9.
table 3 shows performance (%correct, %wrong, %abstained) of the different odd-man-out solvers on the c of 1b+2b c is 76.7. w of 1b+2b w is 13.9. a of 1b+2b a is 9.4. c of 1b+2b c is 42.6. w of 1b+2b w is 17.8. a of 1b+2b a is 39.6. c of 1b+2b c is 55.5. w of 1b+2b w is 18.8. a of 1b+2b a is 25.6. c of 100b c is 61.9. w of 100b w is 25.2. a of 100b a is 12.9. c of 100b c is 40.1. w of 100b w is 14.9. a of 100b a is 45.0. c of 100b c is 46.3. w of 100b w is 28.8. a of 100b a is 24.9. c of 840b c is 60.9. w of 840b w is 23.8. a of 840b a is 15.4. c of 840b c is 32.2. w of 840b w is 14.4. a of 840b a is 53.5. c of 840b c is 47.1. w of 840b w is 28.4. a of 840b a is 24.6. c of 42b c is 57.4. w of 42b w is 29.2. a of 42b a is 13.4. c of 42b c is 30.7. w of 42b w is 17.8. a of 42b a is 51.5. c of 42b c is 40.1. w of 42b w is 36.3. a of 42b a is 23.7. c of 6b c is 54.5. w of 6b w is 24.3. a of 6b a is 21.3. c of 6b c is 29.2. w of 6b w is 10.9. a of 6b a is 59.9. c of 6b c is 42.7. w of 6b w is 29.0. a of 6b a is 28.4. c of 1b c is 35.2. w of 1b w is 25.7. a of 1b a is 39.1. c of 1b c is 18.3. w of 1b w is 14.4. a of 1b a is 67.3. c of 1b c is 32.6. w of 1b w is 27.3. a of 1b a is 40.2. c of 100b c is 22.3. w of 100b w is 28.7. a of 100b a is 49.0. c of 100b c is 34.2. w of 100b w is 14.9. a of 100b a is 51.0. c of 100b c is 9.9. w of 100b w is 11.3. a of 100b a is 78.3. c of - c is 40.6. w of - w is 13.4. a of - a is 46.0. c of - c is 0.5. w of - w is 0.0. a of - a is 99.5. c of - c is 22.0. w of - w is 15.1. a of - a is 63.0.
table 3 shows experimental results on simile sentence classification. p of baseline1 p is 0.6523. r of baseline1 r is 0.4752. f1 of baseline1 f1 is 0.5498. p of baseline2 p is 0.7661. r of baseline2 r is 0.7832. f1 of baseline2 f1 is 0.7745. p of singletask (sc) p is 0.7751. r of singletask (sc) r is 0.8895. f1 of singletask (sc) f1 is 0.8284. p of multitask (sc+ce) p is 0.8056. r of multitask (sc+ce) r is 0.8886. f1 of multitask (sc+ce) f1 is 0.8450. p of multitask (sc+lm) p is 0.8021. r of multitask (sc+lm) r is 0.9105. f1 of multitask (sc+lm) f1 is 0.8525. p of multitask (sc+ce+lm) p is 0.8084. r of multitask (sc+ce+lm) r is 0.9220. f1 of multitask (sc+ce+lm) f1 is 0.8615.
table 4 shows experimental results on component extraction. p of rule based p is 0.4094. r of rule based r is 0.1805. f1 of rule based f1 is 0.2505. p of crf p is 0.5619. r of crf r is 0.5907. f1 of crf f1 is 0.5760. p of crf p is 0.3157. r of crf r is 0.3698. f1 of crf f1 is 0.3406. p of singletask (ce) p is 0.7297. r of singletask (ce) r is 0.7854. f1 of singletask (ce) f1 is 0.7564. p of singletask (ce) p is 0.5580. r of singletask (ce) r is 0.6489. f1 of singletask (ce) f1 is 0.5998. p of randomforest → crf p is 0.4591. r of randomforest → crf r is 0.4980. f1 of randomforest → crf f1 is 0.4778. p of singlesc → singlece p is 0.5720. r of singlesc → singlece r is 0.7074. f1 of singlesc → singlece f1 is 0.6325. p of multitask (ce+sc) p is 0.5409. r of multitask (ce+sc) r is 0.6400. f1 of multitask (ce+sc) f1 is 0.5861. p of multitask (ce+lm) p is 0.7530. r of multitask (ce+lm) r is 0.7876. f1 of multitask (ce+lm) f1 is 0.7699. p of multitask (ce+lm) p is 0.5741. r of multitask (ce+lm) r is 0.7015. f1 of multitask (ce+lm) f1 is 0.6306. p of multitask (ce+sc+lm) p is 0.5599. r of multitask (ce+sc+lm) r is 0.6989. f1 of multitask (ce+sc+lm) f1 is 0.6211. p of optimized pipeline p is 0.6160. r of optimized pipeline r is 0.7361. f1 of optimized pipeline f1 is 0.6707.
table 1 shows results of our models (top) and previously proposed systems (bottom) on the trec-qa test set. map of word-level attention map is 0.764. mrr of word-level attention mrr is 0.842. map of simple span alignment map is 0.772. mrr of simple span alignment mrr is 0.851. map of simple span alignment + external parser map is 0.780. mrr of simple span alignment + external parser mrr is 0.846. map of structured alignment (shared parameters) map is 0.780. mrr of structured alignment (shared parameters) mrr is 0.860. map of structured alignment (separated parameters) map is 0.786. mrr of structured alignment (separated parameters) mrr is 0.860. map of qa-lstm (tan et al. 2016b) map is 0.730. mrr of qa-lstm (tan et al. 2016b) mrr is 0.824. map of attentive pooling network (santos et al. 2016) map is 0.753. mrr of attentive pooling network (santos et al. 2016) mrr is 0.851. map of pairwise word interaction (he and lin 2016) map is 0.777. mrr of pairwise word interaction (he and lin 2016) mrr is 0.836. map of lexical decomposition and composition (wang et al. 2016) map is 0.771. mrr of lexical decomposition and composition (wang et al. 2016) mrr is 0.845. map of noise-contrastive estimation (rao et al. 2016) map is 0.801. mrr of noise-contrastive estimation (rao et al. 2016) mrr is 0.877. map of bimpm (wang et al. 2017b) map is 0.802. mrr of bimpm (wang et al. 2017b) mrr is 0.875.
table 2 shows performance comparison (accuracy) on multinli and scitail. accuracy of majority match is 36.5. accuracy of majority mismatch is 35.6. accuracy of majority - is 60.3. accuracy of ngram# - is 70.6. accuracy of cbow♭ match is 65.2. accuracy of cbow♭ mismatch is 64.8. accuracy of bilstm♭ match is 69.8. accuracy of bilstm♭ mismatch is 69.4. accuracy of esim#♭ match is 72.4. accuracy of esim#♭ mismatch is 72.1. accuracy of esim#♭ - is 70.6. accuracy of decompatt# - - is 72.3. accuracy of dgem# - is 70.8. accuracy of dgem + edge# - is 77.3. accuracy of esim† match is 76.3. accuracy of esim† mismatch is 75.8. accuracy of esim + read† match is 77.8. accuracy of esim + read† mismatch is 77.0. accuracy of cafe match is 78.7. accuracy of cafe mismatch is 77.9. accuracy of cafe - is 83.3. accuracy of cafe ensemble match is 80.2. accuracy of cafe ensemble mismatch is 79.0.
table 2 shows performance on snli dataset. accuracy of handcrafted features (bowman et al. 2015) train is 99.7. accuracy of handcrafted features (bowman et al. 2015) test is 78.2. accuracy of lstm with attention (rocktaschel et al. 2015) train is 85.3. accuracy of lstm with attention (rocktaschel et al. 2015) test is 83.5. accuracy of match-lstm (wang and jiang 2016) train is 92.0. accuracy of match-lstm (wang and jiang 2016) test is 86.1. accuracy of decomposable attention model (parikh et al. 2016) train is 90.5. accuracy of decomposable attention model (parikh et al. 2016) test is 86.8. accuracy of bimpm (zhiguo wang 2017) train is 90.9. accuracy of bimpm (zhiguo wang 2017) test is 87.5. accuracy of nti-slstm-lstm (munkhdalai and yu 2017) train is 88.5. accuracy of nti-slstm-lstm (munkhdalai and yu 2017) test is 87.3. accuracy of re-read lstm (sha et al. 2016) train is 90.7. accuracy of re-read lstm (sha et al. 2016) test is 87.5. accuracy of diin (gong et al. 2017) train is 91.2. accuracy of diin (gong et al. 2017) test is 88.0. accuracy of esim (chen et al. 2017a) train is 92.6. accuracy of esim (chen et al. 2017a) test is 88.0. accuracy of cin train is 93.2. accuracy of cin test is 88.0. accuracy of esim (chen et al. 2017a) (ensemble) train is 93.5. accuracy of esim (chen et al. 2017a) (ensemble) test is 88.6. accuracy of bimpm (zhiguo wang 2017) (ensemble) train is 93.2. accuracy of bimpm (zhiguo wang 2017) (ensemble) test is 88.8. accuracy of diin (gong et al. 2017) (ensemble) train is 92.3. accuracy of diin (gong et al. 2017) (ensemble) test is 88.9. accuracy of cin (ensemble) train is 94.3. accuracy of cin (ensemble) test is 89.1.
table 3 shows performance on multinli test set. accuracy of bilstm (williams et al. 2017) match is 67.0. accuracy of bilstm (williams et al. 2017) mismatch is 67.6. accuracy of inneratt (balazs et al. 2017) match is 72.1. accuracy of inneratt (balazs et al. 2017) mismatch is 72.1. accuracy of esim (chen et al. 2017a) match is 72.3. accuracy of esim (chen et al. 2017a) mismatch is 72.1. accuracy of gated-att bilstm (chen et al. 2017b) match is 73.2. accuracy of gated-att bilstm (chen et al. 2017b) mismatch is 73.6. accuracy of esim (chen et al. 2017a) match is 76.3. accuracy of esim (chen et al. 2017a) mismatch is 75.8. accuracy of cin match is 77.0. accuracy of cin mismatch is 77.6.
table 4 shows performance on quora question pair dataset. accuracy of siamese-cnn test is 79.60. accuracy of multi-perspective cnn test is 81.38. accuracy of siamese-lstm test is 82.58. accuracy of multi-perspective-lstm test is 83.21. accuracy of l.d.c test is 85.55. accuracy of bimpm (zhiguo wang 2017) test is 88.17. accuracy of cin test is 88.62.
table 1 shows evaluation of results on the test set. s metric of pipeline precision is 35.08. s metric of pipeline recall is 30.10. s metric of pipeline f1 is 32.39. bleu inst of pipeline - is 15.03. mae spr of pipeline - is n/a. mae fact of pipeline - is n/a. s metric of variant (a) precision is 39.31. s metric of variant (a) recall is 32.93. s metric of variant (a) f1 is 35.84. bleu inst of variant (a) - is 16.74. mae spr of variant (a) - is 0.75. mae fact of variant (a) - is 1.11. s metric of variant (b) precision is 42.76. s metric of variant (b) recall is 33.20. s metric of variant (b) f1 is 37.38. bleu inst of variant (b) - is 17.71. mae spr of variant (b) - is 0.74. mae fact of variant (b) - is 1.14. s metric of variant (c) precision is 41.74. s metric of variant (c) recall is 33.28. s metric of variant (c) f1 is 37.03. bleu inst of variant (c) - is 18.01. mae spr of variant (c) - is 0.80. mae fact of variant (c) - is 1.14. s metric of our model precision is 45.33. s metric of our model recall is 33.88. s metric of our model f1 is 38.78. bleu inst of our model - is 19.61. mae spr of our model - is 0.71. mae fact of our model - is 1.06.
table 4 shows the performance of minv+nn and models without soft label on all the idioms in the two corpora avg. ff ig of gibbs avg. ff ig is 0.58 (0.31 ? 0.78). avg.acc of gibbs avg.acc is 0.57 (0.4 ? 0.78). avg. ff ig of em avg. ff ig is 0.56 (0.31 ? 0.71). avg.acc of em avg.acc is 0.6 (0.42 ? 0.77). avg. ff ig of minv+nn avg. ff ig is 0.68 (0.41 ? 0.83). avg.acc of minv+nn avg.acc is 0.67 (0.55 ? 0.86).
table 1 shows results on development set (all metrics except mr are x100). mr of rule mr is 13396. mrr of rule mrr is 35.26. h@10 of rule h@10 is 35.27. h@1 of rule h@1 is 35.23. mr of distmult mr is 1111. mrr of distmult mrr is 43.29. h@10 of distmult h@10 is 50.73. h@1 of distmult h@1 is 39.67. mr of bilin mr is 738. mrr of bilin mrr is 45.36. h@10 of bilin h@10 is 52.93. h@1 of bilin h@1 is 41.37. mr of transe mr is 2231. mrr of transe mrr is 46.07. h@10 of transe h@10 is 55.65. h@1 of transe h@1 is 41.41. mr of m3gm mr is 2231. mrr of m3gm mrr is 47.94. h@10 of m3gm h@10 is 57.72. h@1 of m3gm h@1 is 43.26. mr of m3gmαr mr is 2231. mrr of m3gmαr mrr is 48.30. h@10 of m3gmαr h@10 is 57.59. h@1 of m3gmαr h@1 is 43.78.
table 2 shows main results on test set. mr of rule mr is 13396. mrr of rule mrr is 35.26. h@10 of rule h@10 is 35.26. h@1 of rule h@1 is 35.26. mr of complex† mr is 5261. mrr of complex† mrr is 44. h@10 of complex† h@10 is 51. h@1 of complex† h@1 is 41. mr of conve† mr is 5277. mrr of conve† mrr is 46. h@10 of conve† h@10 is 48. h@1 of conve† h@1 is 39. mr of convkb† mr is 2554. mrr of convkb† mrr is 24.8. h@10 of convkb† h@10 is 52.5. h@1 of convkb† h@1 is . mr of transe mr is 2195. mrr of transe mrr is 46.59. h@10 of transe h@10 is 55.55. h@1 of transe h@1 is 42.26. mr of m3gmαr mr is 2193. mrr of m3gmαr mrr is 49.83. h@10 of m3gmαr h@10 is 59.02. h@1 of m3gmαr h@1 is 45.37.
table 5 shows comparsion results of sentence selection. rouge-1 of summarunner-abs rouge-1 is 37.5. rouge-2 of summarunner-abs rouge-2 is 14.5. rouge-l of summarunner-abs rouge-l is 33.4. rouge-1 of summarunner rouge-1 is 39.6. rouge-2 of summarunner rouge-2 is 16.2. rouge-l of summarunner rouge-l is 35.3. rouge-1 of ourextractive rouge-1 is 40.41. rouge-2 of ourextractive rouge-2 is 18.30. rouge-l of ourextractive rouge-l is 36.30. rouge-1 of – dists rouge-1 is 37.06. rouge-2 of – dists rouge-2 is 16.55. rouge-l of – dists rouge-l is 33.23. rouge-1 of – dists&gatef rouge-1 is 36.25. rouge-2 of – dists&gatef rouge-2 is 16.22. rouge-l of – dists&gatef rouge-l is 32.59.
table 1 shows comparison of summarization datasets with respect to overall corpus size, size of training, validation, and test set, average document (source) and summary (target) length (in terms of words and sentences), and vocabulary size on both on source and target. # docs of cnn train is 90266. # docs of cnn val is 1220. # docs of cnn test is 1093. avg. document length of cnn words is 760.50. avg. document length of cnn sentences is 33.98. avg. summary length of cnn words is 45.70. avg. summary length of cnn sentences is 3.59. vocabulary size of cnn document is 343,516. vocabulary size of cnn summary is 89,051. # docs of dailymail train is 196961. # docs of dailymail val is 12148. # docs of dailymail test is 10397. avg. document length of dailymail words is 653.33. avg. document length of dailymail sentences is 29.33. avg. summary length of dailymail words is 54.65. avg. summary length of dailymail sentences is 3.86. vocabulary size of dailymail document is 563,663. vocabulary size of dailymail summary is 179,966. # docs of ny times train is 589284. # docs of ny times val is 32736. # docs of ny times test is 32739. avg. document length of ny times words is 800.04. avg. document length of ny times sentences is 35.55. avg. summary length of ny times words is 45.54. avg. summary length of ny times sentences is 2.44. vocabulary size of ny times document is 1,399,358. vocabulary size of ny times summary is 294,011. # docs of xsum train is 204045. # docs of xsum val is 11332. # docs of xsum test is 11334. avg. document length of xsum words is 431.07. avg. document length of xsum sentences is 19.77. avg. summary length of xsum words is 23.26. avg. summary length of xsum sentences is 1.00. vocabulary size of xsum document is 399,147. vocabulary size of xsum summary is 81,092.
table 2 shows corpus bias towards extractive methods in the cnn, dailymail, ny times, and xsum datasets. unigrams of cnn unigrams is 16.75. bigrams of cnn bigrams is 54.33. trigrams of cnn trigrams is 72.42. 4-grams of cnn 4-grams is 80.37. r1 of cnn r1 is 29.15. r2 of cnn r2 is 11.13. rl of cnn rl is 25.95. r1 of cnn r1 is 50.38. r2 of cnn r2 is 28.55. rl of cnn rl is 46.58. unigrams of dailymail unigrams is 17.03. bigrams of dailymail bigrams is 53.78. trigrams of dailymail trigrams is 72.14. 4-grams of dailymail 4-grams is 80.28. r1 of dailymail r1 is 40.68. r2 of dailymail r2 is 18.36. rl of dailymail rl is 37.25. r1 of dailymail r1 is 55.12. r2 of dailymail r2 is 30.55. rl of dailymail rl is 51.24. unigrams of ny times unigrams is 22.64. bigrams of ny times bigrams is 55.59. trigrams of ny times trigrams is 71.93. 4-grams of ny times 4-grams is 80.16. r1 of ny times r1 is 31.85. r2 of ny times r2 is 15.86. rl of ny times rl is 23.75. r1 of ny times r1 is 52.08. r2 of ny times r2 is 31.59. rl of ny times rl is 46.72. unigrams of xsum unigrams is 35.76. bigrams of xsum bigrams is 83.45. trigrams of xsum trigrams is 95.50. 4-grams of xsum 4-grams is 98.49. r1 of xsum r1 is 16.30. r2 of xsum r2 is 1.61. rl of xsum rl is 11.95. r1 of xsum r1 is 29.79. r2 of xsum r2 is 8.81. rl of xsum rl is 22.65.
table 4 shows rouge results on xsum test set. r1 of random r1 is 15.16. r2 of random r2 is 1.78. r3 of random rl is 11.27. r1 of lead r1 is 16.30. r2 of lead r2 is 1.60. r3 of lead rl is 11.95. r1 of ext-oracle r1 is 29.79. r2 of ext-oracle r2 is 8.81. r3 of ext-oracle rl is 22.66. r1 of seq2seq r1 is 28.42. r2 of seq2seq r2 is 8.77. r3 of seq2seq rl is 22.48. r1 of ptgen r1 is 29.70. r2 of ptgen r2 is 9.21. r3 of ptgen rl is 23.24. r1 of ptgen-covg r1 is 28.10. r2 of ptgen-covg r2 is 8.02. r3 of ptgen-covg rl is 21.72. r1 of convs2s r1 is 31.27. r2 of convs2s r2 is 11.07. r3 of convs2s rl is 25.23. r1 of t-convs2ss (enct) r1 is 31.71. r2 of t-convs2ss (enct) r2 is 11.38. r3 of t-convs2ss (enct) rl is 25.56. r1 of t-convs2s (enct dectd) r1 is 31.71. r2 of t-convs2s (enct dectd) r2 is 11.34. r3 of t-convs2s (enct dectd) rl is 25.61. r1 of t-convs2s (enc(t td)) r1 is 31.61. r2 of t-convs2s (enc(t td)) r2 is 11.30. r3 of t-convs2s (enc(t td)) rl is 25.51. r1 of t-convs2s (enc(t td) dectd) r1 is 31.89. r2 of t-convs2s (enc(t td) dectd) r2 is 11.54. r3 of t-convs2s (enc(t td) dectd) rl is 25.75.
table 3 shows rouge-2 recall across sentence extractors when using fixed pretrained embeddings or when embeddings are updated during training. rouge-2 of fixed cnn/dm is 25.6. rouge-2 of fixed nyt is 35.7. rouge-2 of fixed duc is 22.8. rouge-2 of fixed reddit is 13.6. rouge-2 of fixed ami is 5.5. rouge-2 of fixed  pubmed is 17.7. rouge-2 of learn cnn/dm is 25.3 (0.3). rouge-2 of learn nyt is 35.7 (0.0). rouge-2 of learn duc is 22.9 (-0.1). rouge-2 of learn reddit is 13.8 (-0.2). rouge-2 of learn ami is 5.8 (-0.3). rouge-2 of learn  pubmed is 16.9 (0.8). rouge-2 of fixed cnn/dm is 25.3. rouge-2 of fixed nyt is 35.6. rouge-2 of fixed duc is 23.1. rouge-2 of fixed reddit is 13.6. rouge-2 of fixed ami is 6.1. rouge-2 of fixed  pubmed is 17.7. rouge-2 of learn cnn/dm is 24.9 (0.4). rouge-2 of learn nyt is 35.4 (0.2). rouge-2 of learn duc is 23.0 (0.1). rouge-2 of learn reddit is 13.4 (0.2). rouge-2 of learn ami is 6.2 (-0.1). rouge-2 of learn  pubmed is 16.4 (1.3). rouge-2 of fixed cnn/dm is 25.4. rouge-2 of fixed nyt is 35.4. rouge-2 of fixed duc is 22.3. rouge-2 of fixed reddit is 13.4. rouge-2 of fixed ami is 5.6. rouge-2 of fixed  pubmed is 17.2. rouge-2 of learn cnn/dm is 25.1 (0.3). rouge-2 of learn nyt is 35.2 (0.2). rouge-2 of learn duc is 22.2 (0.1). rouge-2 of learn reddit is 12.6 (0.8). rouge-2 of learn ami is 5.8 (-0.2). rouge-2 of learn  pubmed is 16.8 (0.4).
table 5 shows rouge-2 recall using models trained on in-order and shufﬂed documents. rouge-2 of in-order cnn/dm is 25.6. rouge-2 of in-order nyt is 35.7. rouge-2 of in-order duc is 22.8. rouge-2 of in-order reddit is 13.6. rouge-2 of in-order ami is 5.5. rouge-2 of in-order pubmed is 17.7. rouge-2 of shuffled cnn/dm is 21.7 (3.9). rouge-2 of shuffled nyt is 25.6 (10.1). rouge-2 of shuffled duc is 21.2 (1.6). rouge-2 of shuffled reddit is 13.5 (0.1). rouge-2 of shuffled ami is 6.0 (-0.5). rouge-2 of shuffled pubmed is 14.9 (2.8).
table 2 shows comparison of sample precision and absolute recall (all instances and unique entity tuples) in test extraction on pmc. prec. of peng 2017 prec. is 0.64. abs. rec. of peng 2017 abs. rec. is 6768. unique of peng 2017 unique is 2738. prec. of dpl + emb prec. is 0.74. abs. rec. of dpl + emb abs. rec. is 8478. unique of dpl + emb unique is 4821. prec. of dpl prec. is 0.73. abs. rec. of dpl abs. rec. is 7666. unique of dpl unique is 4144. prec. of dpl -ds prec. is 0.29. abs. rec. of dpl -ds abs. rec. is 7555. unique of dpl -ds unique is 4912. prec. of dpl -dp prec. is 0.67. abs. rec. of dpl -dp abs. rec. is 4826. unique of dpl -dp unique is 2629. prec. of dpl -dp (entity) prec. is 0.70. abs. rec. of dpl -dp (entity) abs. rec. is 7638. unique of dpl -dp (entity) unique is 4074. prec. of dpl -ji prec. is 0.72. abs. rec. of dpl -ji abs. rec. is 7418. unique of dpl -ji unique is 4011.
table 5 shows comparison of gene entity linking results on a balanced test set. acc. of string match acc. is 0.18. f1 of string match f1 is 0.31. prec. of string match prec. is 0.18. rec. of string match rec. is 1.00. acc. of ds acc. is 0.64. f1 of ds f1 is 0.71. prec. of ds prec. is 0.62. rec. of ds rec. is 0.83. acc. of ds + dp acc. is 0.66. f1 of ds + dp f1 is 0.71. prec. of ds + dp prec. is 0.62. rec. of ds + dp rec. is 0.83. acc. of ds + dp + ji acc. is 0.70. f1 of ds + dp + ji f1 is 0.76. prec. of ds + dp + ji prec. is 0.68. rec. of ds + dp + ji rec. is 0.86.
table 5 shows the quality of the coreference chains on the conll-2012 test set. p of - p is 89.78. r of - r is 73.88. f1 of - f1 is 81.06. p of - p is 83.93. r of - r is 59.22. f1 of - f1 is 69.44. p of - p is 73.87. r of - r is 60.57. f1 of - f1 is 66.56. avg.f1 of - - is 72.35. p of avg. p is 88.27. r of avg. r is 86.00. f1 of avg. f1 is 87.12. p of avg. p is 73.92. r of avg. r is 70.81. f1 of avg. f1 is 72.33. p of avg. p is 70.62. r of avg. r is 76.73. f1 of avg. f1 is 73.55. avg.f1 of avg. - is 77.67. p of s.d. p is 0.38. r of s.d. r is 0.35. f1 of s.d. f1 is 0.36. p of s.d. p is 0.83. r of s.d. r is 0.52. f1 of s.d. f1 is 0.62. p of s.d. p is 0.49. r of s.d. r is 0.60. f1 of s.d. f1 is 0.50. avg.f1 of s.d. - is 0.47. p of avg. p is 90.92. r of avg. r is 91.97. f1 of avg. f1 is 91.44. p of avg. p is 75.51. r of avg. r is 80.14. f1 of avg. f1 is 77.75. p of avg. p is 81.98. r of avg. r is 78.81. f1 of avg. f1 is 80.37. avg.f1 of avg. - is 83.19. p of s.d. p is 0.48. r of s.d. r is 0.36. f1 of s.d. f1 is 0.41. p of s.d. p is 1.20. r of s.d. r is 0.67. f1 of s.d. f1 is 0.93. p of s.d. p is 0.74. r of s.d. r is 1.16. f1 of s.d. f1 is 0.93. avg.f1 of s.d. - is 0.75. p of avg. p is 81.99. r of avg. r is 79.01. f1 of avg. f1 is 80.47. p of avg. p is 65.91. r of avg. r is 62.64. f1 of avg. f1 is 64.23. p of avg. p is 60.61. r of avg. r is 68.00. f1 of avg. f1 is 64.09. avg.f1 of avg. - is 69.59. p of s.d. p is 0.32. r of s.d. r is 0.43. f1 of s.d. f1 is 0.38. p of s.d. p is 0.45. r of s.d. r is 0.51. f1 of s.d. f1 is 0.40. p of s.d. p is 0.39. r of s.d. r is 0.24. f1 of s.d. f1 is 0.28. avg.f1 of s.d. - is 0.32. p of avg. p is 87.90. r of avg. r is 88.24. f1 of avg. f1 is 88.07. p of avg. p is 70.67. r of avg. r is 73.91. f1 of avg. f1 is 72.25. p of avg. p is 74.62. r of avg. r is 73.63. f1 of avg. f1 is 74.12. avg.f1 of avg. - is 78.15. p of s.d. p is 0.47. r of s.d. r is 0.44. f1 of s.d. f1 is 0.45. p of s.d. p is 0.96. r of s.d. r is 0.65. f1 of s.d. f1 is 0.77. p of s.d. p is 0.75. r of s.d. r is 0.93. f1 of s.d. f1 is 0.82. avg.f1 of s.d. - is 0.66. p of avg. p is 91.84. r of avg. r is 88.28. f1 of avg. f1 is 90.02. p of avg. p is 80.94. r of avg. r is 74.19. f1 of avg. f1 is 77.41. p of avg. p is 75.13. r of avg. r is 84.93. f1 of avg. f1 is 79.73. avg.f1 of avg. - is 82.39. p of s.d. p is 0.36. r of s.d. r is 0.49. f1 of s.d. f1 is 0.42. p of s.d. p is 0.61. r of s.d. r is 0.84. f1 of s.d. f1 is 0.66. p of s.d. p is 0.88. r of s.d. r is 0.55. f1 of s.d. f1 is 0.72. avg.f1 of s.d. - is 0.58. p of avg. p is 97.42. r of avg. r is 97.20. f1 of avg. f1 is 97.31. p of avg. p is 91.61. r of avg. r is 91.53. f1 of avg. f1 is 91.57. p of avg. p is 93.87. r of avg. r is 94.58. f1 of avg. f1 is 94.23. avg.f1 of avg. - is 94.37. p of s.d. p is 0.27. r of s.d. r is 0.28. f1 of s.d. f1 is 0.27. p of s.d. p is 1.05. r of s.d. r is 1.28. f1 of s.d. f1 is 1.15. p of s.d. p is 0.67. r of s.d. r is 0.61. f1 of s.d. f1 is 0.63. avg.f1 of s.d. - is 0.68. p of avg. p is 86.86. r of avg. r is 81.70. f1 of avg. f1 is 84.20. p of avg. p is 74.26. r of avg. r is 65.42. f1 of avg. f1 is 69.56. p of avg. p is 65.45. r of avg. r is 78.51. f1 of avg. f1 is 71.39. avg.f1 of avg. - is 75.05. p of s.d. p is 0.49. r of s.d. r is 0.54. f1 of s.d. f1 is 0.51. p of s.d. p is 0.63. r of s.d. r is 0.48. f1 of s.d. f1 is 0.52. p of s.d. p is 0.67. r of s.d. r is 0.55. f1 of s.d. f1 is 0.60. avg.f1 of s.d. - is 0.53. p of avg. p is 94.86. r of avg. r is 94.09. f1 of avg. f1 is 94.47. p of avg. p is 85.24. r of avg. r is 84.42. f1 of avg. f1 is 84.83. p of avg. p is 87.52. r of avg. r is 89.91. f1 of avg. f1 is 88.70. avg.f1 of avg. - is 89.33. p of s.d. p is 0.32. r of s.d. r is 0.36. f1 of s.d. f1 is 0.34. p of s.d. p is 0.70. r of s.d. r is 0.75. f1 of s.d. f1 is 0.71. p of s.d. p is 0.60. r of s.d. r is 0.49. f1 of s.d. f1 is 0.54. avg.f1 of s.d. - is 0.52.
table 6 shows results of using np head plus modifications in different word representations for bridging anaphora resolution compared to the best results of two models from hou et al. acc of pairwise model iii acc is 36.35. acc of mln model ii acc is 41.32. acc of glove gigawiki14 acc is 20.52. acc of glove giga acc is 20.81. acc of embeddings pp acc is 31.67. acc of embeddings bridging acc is 39.52.
table 8 shows results of different systems for bridging anaphora resolution in isnotes. acc of schulte im walde (1998) acc is 13.68. acc of poesio et al. (2004) acc is 18.85. acc of pairwise model iii acc is 36.35. acc of mln model ii acc is 41.32. acc of mln model ii + embeddings pp (np head + noun pre-modifiers) acc is 45.85. acc of embeddings bridging (np head + modifiers) acc is 39.52. acc of mln model ii + embeddings bridging (np head + modifiers) acc is 46.46.
table 9 shows results of resolving bridging anaphors in other corpora. # of anaphors of referential, including comparative anaphora # of anaphors is 452. acc of referential, including comparative anaphora acc is 27.43. # of anaphors of referential, excluding comparative anaphora # of anaphors is 344. acc of referential, excluding comparative anaphora acc is 29.94. # of anaphors of mostly lexical, some referential # of anaphors is 2,325. acc of mostly lexical, some referential acc is 31.44. # of anaphors of mostly lexical, some referential # of anaphors is 639. acc of mostly lexical, some referential acc is 32.39.
table 2 shows results for the reverse dictionary task, compared with the highest numbers reported by hill et al. acc-10 of onelook (hill et al. 2016) acc-10 is 0.89. acc-100 of onelook (hill et al. 2016) acc-100 is 0.91. acc-10 of rnn cosine (hill et al. 2016) acc-10 is 0.48. acc-100 of rnn cosine (hill et al. 2016) acc-100 is 0.73. acc-10 of std lstm (150 dim.) + tf vec. acc-10 is 0.86. acc-100 of std lstm (150 dim.) + tf vec. acc-100 is 0.96. acc-10 of std lstm (k × 150 dim.) + tf vec. acc-10 is 0.93. acc-100 of std lstm (k × 150 dim.) + tf vec. acc-100 is 0.98. acc-10 of ms-lstm +tf vectors acc-10 is 0.95. acc-100 of ms-lstm +tf vectors acc-100 is 0.99. acc-10 of ms-lstm +tf vectors + anchors acc-10 is 0.96. acc-100 of ms-lstm +tf vectors + anchors acc-100 is 0.99. acc-10 of rnn w2v cosine (hill et al. 2016) acc-10 is 0.44. acc-100 of rnn w2v cosine (hill et al. 2016) acc-100 is 0.69. acc-10 of bow w2v cosine (hill et al. 2016) acc-10 is 0.46. acc-100 of bow w2v cosine (hill et al. 2016) acc-100 is 0.71. acc-10 of std lstm (150 dim.) + tf vec. acc-10 is 0.72. acc-100 of std lstm (150 dim.) + tf vec. acc-100 is 0.88. acc-10 of std lstm (k × 150 dim.) + tf vec. acc-10 is 0.77. acc-100 of std lstm (k × 150 dim.) + tf vec. acc-100 is 0.90. acc-10 of ms-lstm + tf vectors acc-10 is 0.79. acc-100 of ms-lstm + tf vectors acc-100 is 0.90. acc-10 of ms-lstm + tf vectors + anchors acc-10 is 0.80. acc-100 of ms-lstm + tf vectors + anchors acc-100 is 0.91.
table 3 shows results for the cora dataset. accuracy of plsa (hofmann 1999) accuracy is 0.68. accuracy of netplsa (mei et al. 2008) accuracy is 0.85. accuracy of tadw (yang et al. 2015) accuracy is 0.87. accuracy of linear svm + deepwalk vectors accuracy is 0.85. accuracy of linear svm + tf vectors accuracy is 0.88. accuracy of planetoid (yang et al. 2016) accuracy is 0.76. accuracy of gcn (kipf and welling 2017) accuracy is 0.81. accuracy of gat (veličkovic et al. 2018) accuracy is 0.83. accuracy of linear svm + deepwalk vectors accuracy is 0.72. accuracy of linear svm + tf vectors accuracy is 0.82.
table 3 shows experimental results on instanceof triple classification(%). accuracy of transe accuracy is 82.6. precision of transe precision is 83.6. recall of transe recall is 81.0. f1-score of transe f1-score is 82.3. accuracy of transe accuracy is 71.0. precision of transe precision is 81.4. recall of transe recall is 54.4. f1-score of transe f1-score is 65.2. accuracy of transh accuracy is 82.9. precision of transh precision is 83.7. recall of transh recall is 81.7. f1-score of transh f1-score is 82.7. accuracy of transh accuracy is 70.1. precision of transh precision is 80.4. recall of transh recall is 53.2. f1-score of transh f1-score is 64.0. accuracy of transr accuracy is 80.6. precision of transr precision is 79.4. recall of transr recall is 82.5. f1-score of transr f1-score is 80.9. accuracy of transr accuracy is 70.9. precision of transr precision is 73.0. recall of transr recall is 66.3. f1-score of transr f1-score is 69.5. accuracy of transd accuracy is 83.2. precision of transd precision is 84.4. recall of transd recall is 81.5. f1-score of transd f1-score is 82.9. accuracy of transd accuracy is 72.5. precision of transd precision is 73.1. recall of transd recall is 71.4. f1-score of transd f1-score is 72.2. accuracy of hole accuracy is 82.3. precision of hole precision is 86.3. recall of hole recall is 76.7. f1-score of hole f1-score is 81.2. accuracy of hole accuracy is 74.2. precision of hole precision is 81.4. recall of hole recall is 62.7. f1-score of hole f1-score is 70.9. accuracy of distmult accuracy is 83.9. precision of distmult precision is 86.8. recall of distmult recall is 80.1. f1-score of distmult f1-score is 83.3. accuracy of distmult accuracy is 70.5. precision of distmult precision is 86.1. recall of distmult recall is 49.0. f1-score of distmult f1-score is 62.4. accuracy of complex accuracy is 83.3. precision of complex precision is 84.8. recall of complex recall is 81.1. f1-score of complex f1-score is 82.9. accuracy of complex accuracy is 70.2. precision of complex precision is 84.4. recall of complex recall is 49.5. f1-score of complex f1-score is 62.4. accuracy of transc (unif) accuracy is 80.2. precision of transc (unif) precision is 81.6. recall of transc (unif) recall is 80.0. f1-score of transc (unif) f1-score is 79.7. accuracy of transc (unif) accuracy is 85.5. precision of transc (unif) precision is 88.3. recall of transc (unif) recall is 81.8. f1-score of transc (unif) f1-score is 85.0. accuracy of transc (bern) accuracy is 79.7. precision of transc (bern) precision is 83.2. recall of transc (bern) recall is 74.4. f1-score of transc (bern) f1-score is 78.6. accuracy of transc (bern) accuracy is 85.3. precision of transc (bern) precision is 86.1. recall of transc (bern) recall is 84.2. f1-score of transc (bern) f1-score is 85.2.
table 4 shows the predicted mean rank (lower the better) for temporal scoping. predicted mean rank of tans (equation 1) yago11k is 14.0. predicted mean rank of tans (equation 1) wikidata12k is 29.3. predicted mean rank of tdns (equation 2) yago11k is 9.88. predicted mean rank of tdns (equation 2) wikidata12k is 17.6.
table 2 shows entity linking performance of different methods on yelp-el. accuracy (mean�std) of directlink accuracy (mean±std) is 0.6684±0.008. accuracy (mean�std) of elt accuracy (mean±std) is 0.8451±0.012. accuracy (mean�std) of ssregu accuracy (mean±std) is 0.7970±0.013. accuracy (mean�std) of linkyelp accuracy (mean±std) is 0.9034±0.014.
table 2 shows [biomedical domain] ner performance comparison. pre of gold annotations pre is 88.84. rec of gold annotations rec is 85.16. f1 of gold annotations f1 is 86.96. pre of gold annotations pre is 86.11. rec of gold annotations rec is 85.49. f1 of gold annotations f1 is 85.80. pre of regex design + special case tuning pre is 86.11. rec of regex design + special case tuning rec is 82.39. f1 of regex design + special case tuning f1 is 84.21. pre of regex design + special case tuning pre is 81.6. rec of regex design + special case tuning rec is 80.1. f1 of regex design + special case tuning f1 is 80.8. pre of regex design pre is 84.98. rec of regex design rec is 83.49. f1 of regex design f1 is 84.23. pre of regex design pre is 64.7. rec of regex design rec is 69.7. f1 of regex design f1 is 67.1. pre of none pre is 93.93. rec of none rec is 58.35. f1 of none f1 is 71.98. pre of none pre is 90.59. rec of none rec is 56.15. f1 of none f1 is 69.32. pre of none pre is 88.27. rec of none rec is 76.75. f1 of none f1 is 82.11. pre of none pre is 79.85. rec of none rec is 67.71. f1 of none f1 is 73.28. pre of none pre is 88.96. rec of none rec is 81.00. f1 of none f1 is 84.8. pre of none pre is 79.42. rec of none rec is 71.98. f1 of none f1 is 75.52.
table 3 shows evaluation of coarse entity-typing (§4.2): we compare two supervised entity-typers with our system. f1 of ontonotes per is 98.4. f1 of ontonotes loc is 91.9. f1 of ontonotes org is 97.7. f1 of ontonotes per is 83.7. f1 of ontonotes loc is 70.1. f1 of ontonotes org is 68.3. f1 of ontonotes per is 82.5. f1 of ontonotes loc is 76.9. f1 of ontonotes org is 86.7. f1 of conll per is 94.4. f1 of conll loc is 59.1. f1 of conll org is 87.8. f1 of conll per is 95.6. f1 of conll loc is 92.9. f1 of conll org is 90.5. f1 of conll per is 90.8. f1 of conll loc is 90.8. f1 of conll org is 90.9. f1 of × per is 88.4. f1 of × loc is 70.0. f1 of × org is 85.6. f1 of × per is 90.1. f1 of × loc is 80.1. f1 of × org is 73.9. f1 of × per is 87.8. f1 of × loc is 90.9. f1 of × org is 91.2.
table 6 shows ablation study of different ways in which concepts are generated in our system (§4.5). acc. of zoe (ours) acc. is 58.8. f1ma of zoe (ours) f1ma is 74.8. f1mi of zoe (ours) f1mi is 71.3. acc. of zoe (ours) acc. is 61.8. f1ma of zoe (ours) f1ma is 74.6. f1mi of zoe (ours) f1mi is 74.9. acc. of zoe (ours) acc. is 50.7. f1ma of zoe (ours) f1ma is 66.9. f1mi of zoe (ours) f1mi is 60.8. acc. of no surface-based concepts acc. is -8.8. f1ma of no surface-based concepts f1ma is -7.5. f1mi of no surface-based concepts f1mi is -9.2. acc. of no surface-based concepts acc. is -12.9. f1ma of no surface-based concepts f1ma is -7.0. f1mi of no surface-based concepts f1mi is -8.6. acc. of no surface-based concepts acc. is -1.8. f1ma of no surface-based concepts f1ma is -1.2. f1mi of no surface-based concepts f1mi is -0.1. acc. of no context-based concepts acc. is -39.3. f1ma of no context-based concepts f1ma is -42.1. f1mi of no context-based concepts f1mi is -25.4. acc. of no context-based concepts acc. is -36.4. f1ma of no context-based concepts f1ma is -31.0. f1mi of no context-based concepts f1mi is -13.9. acc. of no context-based concepts acc. is -10.0. f1ma of no context-based concepts f1ma is -12.3. f1mi of no context-based concepts f1mi is -7.4.
table 3 shows selected results of the baseline models on follow-up question generation. bleu-1 of first sent. bleu-1 is 0.221. bleu-2 of first sent. bleu-2 is 0.144. bleu-3 of first sent. bleu-3 is 0.119. bleu-4 of first sent. bleu-4 is 0.106. bleu-1 of nmt-copy bleu-1 is 0.339. bleu-2 of nmt-copy bleu-2 is 0.206. bleu-3 of nmt-copy bleu-3 is 0.139. bleu-4 of nmt-copy bleu-4 is 0.102. bleu-1 of bidaf bleu-1 is 0.450. bleu-2 of bidaf bleu-2 is 0.375. bleu-3 of bidaf bleu-3 is 0.338. bleu-4 of bidaf bleu-4 is 0.312. bleu-1 of rule-based bleu-1 is 0.533. bleu-2 of rule-based bleu-2 is 0.437. bleu-3 of rule-based bleu-3 is 0.379. bleu-4 of rule-based bleu-4 is 0.344.
table 4 shows results of entailment models on sharc. micro acc. of random micro acc. is 0.330. macro acc. of random macro acc. is 0.326. micro acc. of surface lr micro acc. is 0.682. macro acc. of surface lr macro acc. is 0.333. micro acc. of dam (snli) micro acc. is 0.479. macro acc. of dam (snli) macro acc. is 0.362. micro acc. of dam (sharc) micro acc. is 0.492. macro acc. of dam (sharc) macro acc. is 0.322.
table 2 shows comparison among different choices for the loss function with multiple answers on the development set rouge-l of single answer rouge-l is 48.93. rouge-l of lmin rouge-l is 49.05. delta of lmin δ is +0.12. rouge-l of lavg rouge-l is 49.67. delta of lavg δ is +0.74. rouge-l of lwavg rouge-l is 49.77. delta of lwavg δ is +0.84.
table 4 shows performance of our model and competing models on the dureader test set rouge-l of bidaf (he et al. 2017) rouge-l is 39.0. bleu-4 of bidaf (he et al. 2017) bleu-4 is 31.8. rouge-l of match-lstm (he et al. 2017) rouge-l is 39.2. bleu-4 of match-lstm (he et al. 2017) bleu-4 is 31.9. rouge-l of pr+bidaf (wang et al. 2018b) rouge-l is 41.81. bleu-4 of pr+bidaf (wang et al. 2018b) bleu-4 is 37.55. rouge-l of pe+bidaf (ours) rouge-l is 45.93. bleu-4 of pe+bidaf (ours) bleu-4 is 38.86. rouge-l of v-net (wang et al. 2018b) rouge-l is 44.18. bleu-4 of v-net (wang et al. 2018b) bleu-4 is 40.97. rouge-l of our complete model rouge-l is 51.09. bleu-4 of our complete model bleu-4 is 43.76. rouge-l of human rouge-l is 57.4. bleu-4 of human bleu-4 is 56.1.
table 1 shows results for short questions (clevrgen): performance of our model compared to baseline models on the short questions test set. accuracy of lstm (no kg) boolean questions is 50.7. accuracy of lstm (no kg) entity set questions is 14.4. accuracy of lstm (no kg) relation questions is 17.5. accuracy of lstm (no kg) overall is 27.2. accuracy of lstm boolean questions is 88.5. accuracy of lstm entity set questions is 99.9. accuracy of lstm relation questions is 15.7. accuracy of lstm overall is 84.9. accuracy of bi-lstm boolean questions is 85.3. accuracy of bi-lstm entity set questions is 99.6. accuracy of bi-lstm relation questions is 14.9. accuracy of bi-lstm overall is 83.6. accuracy of tree-lstm boolean questions is 82.2. accuracy of tree-lstm entity set questions is 97.0. accuracy of tree-lstm relation questions is 15.7. accuracy of tree-lstm overall is 81.2. accuracy of tree-lstm (unsup.) boolean questions is 85.4. accuracy of tree-lstm (unsup.) entity set questions is 99.4. accuracy of tree-lstm (unsup.) relation questions is 16.1. accuracy of tree-lstm (unsup.) overall is 83.6. accuracy of relation network boolean questions is 85.6. accuracy of relation network entity set questions is 89.7. accuracy of relation network relation questions is 97.6. accuracy of relation network overall is 89.4. accuracy of our model (pre-parsed) boolean questions is 94.8. accuracy of our model (pre-parsed) entity set questions is 93.4. accuracy of our model (pre-parsed) relation questions is 70.5. accuracy of our model (pre-parsed) overall is 90.8. accuracy of our model boolean questions is 99.9. accuracy of our model entity set questions is 100. accuracy of our model relation questions is 100. accuracy of our model overall is 99.9.
table 3 shows results for human queries (genx) our model outperforms lstm and semantic parsing models on complex human-generated queries, showing it is robust to work on natural language. accuracy of lstm (no kg) accuracy is 0.0. accuracy of lstm accuracy is 64.9. accuracy of bi-lstm accuracy is 64.6. accuracy of tree-lstm accuracy is 43.5. accuracy of tree-lstm (unsup.) accuracy is 67.7. accuracy of sempre accuracy is 48.1. accuracy of our model (pre-parsed) accuracy is 67.1. accuracy of our model accuracy is 73.7.
table 7 shows comparison to the soa on semeval-2016 mrr of kelp [#1] (filice et al. 2016) mrr is 86.42. map of kelp [#1] (filice et al. 2016) map is 79.19. mrr of conv-kn [#2] (barron-cedeno et al. 2016) mrr is 84.93. map of conv-kn [#2] (barron-cedeno et al. 2016) map is 77.6. mrr of ctkc +vqf (tymoshenko et al. 2016b) mrr is 86.26. map of ctkc +vqf (tymoshenko et al. 2016b) map is 78.78. mrr of hyperqa (tay et al. 2018) mrr is n/a. map of hyperqa (tay et al. 2018) map is 79.5. mrr of ai-cnn (zhang et al. 2017) mrr is n/a. map of ai-cnn (zhang et al. 2017) map is 80.14. mrr of our model (v+bcr+ecr+e+sst) mrr is 86.52. map of our model (v+bcr+ecr+e+sst) map is 79.79.
table 1 shows results on tacred. p of lr (zhang+2017) p is 73.5. r of lr (zhang+2017) r is 49.9. f1 of lr (zhang+2017) f1 is 59.4. p of sdp-lstm (xu+2015b) p is 66.3. r of sdp-lstm (xu+2015b) r is 52.7. f1 of sdp-lstm (xu+2015b) f1 is 58.7. p of tree-lstm (tai+2015) p is 66. r of tree-lstm (tai+2015) r is 59.2. f1 of tree-lstm (tai+2015) f1 is 62.4. p of pa-lstm (zhang+2017) p is 65.7. r of pa-lstm (zhang+2017) r is 64.5. f1 of pa-lstm (zhang+2017) f1 is 65.1. p of gcn p is 69.8. r of gcn r is 59. f1 of gcn f1 is 64. p of c-gcn p is 69.9. r of c-gcn r is 63.3. f1 of c-gcn f1 is 66.4. p of gcn + pa-lstm p is 71.7. r of gcn + pa-lstm r is 63. f1 of gcn + pa-lstm f1 is 67.1. p of c-gcn + pa-lstm p is 71.3. r of c-gcn + pa-lstm r is 65.4. f1 of c-gcn + pa-lstm f1 is 68.2.
table 4 shows main results: the performance of question answering and supporting fact prediction in the two benchmark settings. em of dev em is 44.44. f1 of dev f1 is 58.28. em of dev em is 21.95. f1 of dev f1 is 66.66. em of dev em is 11.56. f1 of dev f1 is 40.86. em of test em is 45.46. f1 of test f1 is 58.99. em of test em is 22.24. f1 of test f1 is 66.62. em of test em is 12.04. f1 of test f1 is 41.37. em of dev em is 24.68. f1 of dev f1 is 34.36. em of dev em is 5.28. f1 of dev f1 is 40.98. em of dev em is 2.54. f1 of dev f1 is 17.73. em of test em is 25.23. f1 of test f1 is 34.40. em of test em is 5.07. f1 of test f1 is 40.69. em of test em is 2.63. f1 of test f1 is 17.85.
table 9 shows retrieval performance comparison on full wiki setting for train-medium, dev and test with 1,000 random samples each. map of train-medium map is 41.89. mean rank of train-medium mean rank is 288.19. corans rank of train-medium corans rank is 82.76. map of dev map is 42.79. mean rank of dev mean rank is 304.30. corans rank of dev corans rank is 97.93. map of test map is 45.92. mean rank of test mean rank is 286.20. corans rank of test corans rank is 74.85.
table 3 shows results on the english out-of-domain test set. f1 of lei et al. (2015) f1 is 75.6. f1 of fitzgerald et al. (2015) f1 is 75.2. p of roth and lapata (2016) p is 76.9. r of roth and lapata (2016) r is 73.8. f1 of roth and lapata (2016) f1 is 75.3. p of marcheggiani et al. (2017) p is 79.4. r of marcheggiani et al. (2017) r is 76.2. f1 of marcheggiani et al. (2017) f1 is 77.7. p of marcheggiani and titov (2017) p is 78.5. r of marcheggiani and titov (2017) r is 75.9. f1 of marcheggiani and titov (2017) f1 is 77.2. p of he et al. (2018) p is 81.9. r of he et al. (2018) r is 76.9. f1 of he et al. (2018) f1 is 79.3. p of cai et al. (2018) p is 79.8. r of cai et al. (2018) r is 78.3. f1 of cai et al. (2018) f1 is 79.0. p of ours (syn-gcn) p is 80.6. r of ours (syn-gcn) r is 79.0. f1 of ours (syn-gcn) f1 is 79.8. p of ours (sa-lstm) p is 81.0. r of ours (sa-lstm) r is 78.2. f1 of ours (sa-lstm) f1 is 79.6. p of ours (tree-lstm) p is 80.4. r of ours (tree-lstm) r is 78.7. f1 of ours (tree-lstm) f1 is 79.5. p of bjorkelund et al. (2010) p is 77.9. r of bjorkelund et al. (2010) r is 73.6. f1 of bjorkelund et al. (2010) f1 is 75.7. f1 of fitzgerald et al. (2015) f1 is 75.2. p of roth and lapata (2016) p is 78.6. r of roth and lapata (2016) r is 73.8. f1 of roth and lapata (2016) f1 is 76.1. f1 of fitzgerald et al. (2015) f1 is 75.5. p of roth and lapata (2016) p is 79.7. r of roth and lapata (2016) r is 73.6. f1 of roth and lapata (2016) f1 is 76.5. p of marcheggiani and titov (2017) p is 80.8. r of marcheggiani and titov (2017) r is 77.1. f1 of marcheggiani and titov (2017) f1 is 78.9.
table 6 shows comparison of models with deep encoder and m&t encoder (marcheggiani and titov, 2017) on the english test set. p of baseline (syntax-agnostic) p is 89.5. r of baseline (syntax-agnostic) r is 87.9. f1 of baseline (syntax-agnostic) f1 is 88.7. p of syn-gcn p is 90.3. r of syn-gcn r is 89.3. f1 of syn-gcn f1 is 89.8. p of sa-lstm p is 90.8. r of sa-lstm r is 88.6. f1 of sa-lstm f1 is 89.7. p of tree-lstm p is 90.0. r of tree-lstm r is 88.8. f1 of tree-lstm f1 is 89.4. p of syn-gcn (m&t encoder) p is 89.2. r of syn-gcn (m&t encoder) r is 88.0. f1 of syn-gcn (m&t encoder) f1 is 88.6. p of sa-lstm (m&t encoder) p is 89.8. r of sa-lstm (m&t encoder) r is 88.8. f1 of sa-lstm (m&t encoder) f1 is 89.3. p of tree-lstm (m&t encoder) p is 90.0. r of tree-lstm (m&t encoder) r is 87.8. f1 of tree-lstm (m&t encoder) f1 is 88.9.
table 7 shows results on english test set, in terms of labeled attachment score for syntactic dependencies (las), semantic precision (p), semantic recall (r), semantic labeled f1 score (sem-f1), the ratio sem-f1/las. las of zhao et al. (2009c) [srl-only] las is 86.0. sem-f1 of zhao et al. (2009c) [srl-only] sem-f1 is 85.4. sem-f1/las of zhao et al. (2009c) [srl-only] sem-f1/las is 99.3. las of zhao et al. (2009a) [joint] las is 89.2. sem-f1 of zhao et al. (2009a) [joint] sem-f1 is 86.2. sem-f1/las of zhao et al. (2009a) [joint] sem-f1/las is 96.6. las of bjorkelund et al.(2010) las is 89.8. p of bjorkelund et al.(2010) p is 87.1. r of bjorkelund et al.(2010) r is 84.5. sem-f1 of bjorkelund et al.(2010) sem-f1 is 85.8. sem-f1/las of bjorkelund et al.(2010) sem-f1/las is 95.6. las of lei et al. (2015) las is 90.4. sem-f1 of lei et al. (2015) sem-f1 is 86.6. sem-f1/las of lei et al. (2015) sem-f1/las is 95.8. las of roth and lapata (2016) las is 89.8. p of roth and lapata (2016) p is 88.1. r of roth and lapata (2016) r is 85.3. sem-f1 of roth and lapata (2016) sem-f1 is 86.7. sem-f1/las of roth and lapata (2016) sem-f1/las is 96.5. las of marcheggiani and titov (2017) las is 90.34*. p of marcheggiani and titov (2017) p is 89.1. r of marcheggiani and titov (2017) r is 86.8. sem-f1 of marcheggiani and titov (2017) sem-f1 is 88.0. sem-f1/las of marcheggiani and titov (2017) sem-f1/las is 97.41. las of he et al. (2018) [conll-2009 predicted] las is 86.0. p of he et al. (2018) [conll-2009 predicted] p is 89.7. r of he et al. (2018) [conll-2009 predicted] r is 89.3. sem-f1 of he et al. (2018) [conll-2009 predicted] sem-f1 is 89.5. sem-f1/las of he et al. (2018) [conll-2009 predicted] sem-f1/las is 104.0. las of he et al. (2018) [gold syntax] las is 100. p of he et al. (2018) [gold syntax] p is 91.0. r of he et al. (2018) [gold syntax] r is 89.7. sem-f1 of he et al. (2018) [gold syntax] sem-f1 is 90.3. sem-f1/las of he et al. (2018) [gold syntax] sem-f1/las is 90.3. las of our syn-gcn (conll-2009 predicted) las is 86.0. p of our syn-gcn (conll-2009 predicted) p is 90.5. r of our syn-gcn (conll-2009 predicted) r is 88.5. sem-f1 of our syn-gcn (conll-2009 predicted) sem-f1 is 89.5. sem-f1/las of our syn-gcn (conll-2009 predicted) sem-f1/las is 104.07. las of our syn-gcn (biaffine parser) las is 90.22. p of our syn-gcn (biaffine parser) p is 90.3. r of our syn-gcn (biaffine parser) r is 89.3. sem-f1 of our syn-gcn (biaffine parser) sem-f1 is 89.8. sem-f1/las of our syn-gcn (biaffine parser) sem-f1/las is 99.53. las of our syn-gcn (bist parser) las is 90.05. p of our syn-gcn (bist parser) p is 90.3. r of our syn-gcn (bist parser) r is 89.1. sem-f1 of our syn-gcn (bist parser) sem-f1 is 89.7. sem-f1/las of our syn-gcn (bist parser) sem-f1/las is 99.61. las of our syn-gcn (gold syntax) las is 100.0. p of our syn-gcn (gold syntax) p is 91.0. r of our syn-gcn (gold syntax) r is 90.0. sem-f1 of our syn-gcn (gold syntax) sem-f1 is 90.5. sem-f1/las of our syn-gcn (gold syntax) sem-f1/las is 90.50.
table 3 shows evaluation of different dfs orderings, in labeled f1 score, across the different tasks. f1 of random dm is 86.1. f1 of random pas is 87.7. f1 of random psd is 78.4. f1 of random avg. is 84.1. f1 of sentence order dm is 87.2. f1 of sentence order pas is 90.3. f1 of sentence order psd is 79.9. f1 of sentence order avg. is 85.8. f1 of closest words dm is 87.5. f1 of closest words pas is 89.8. f1 of closest words psd is 79.7. f1 of closest words avg. is 85.8. f1 of smaller-first dm is 87.9. f1 of smaller-first pas is 90.9. f1 of smaller-first psd is 80.3. f1 of smaller-first avg. is 86.2.
table 4 shows evaluation of our model (labeled f1 score) versus the current state of the art. f1 of peng et al. (2017a) dm is 90.4. f1 of peng et al. (2017a) pas is 92.7. f1 of peng et al. (2017a) psd is 78.5. f1 of peng et al. (2017a) avg. is 87.2. f1 of single dm is 70.1. f1 of single pas is 73.6. f1 of single psd is 63.6. f1 of single avg. is 69.1. f1 of mtl primary dm is 82.4. f1 of mtl primary pas is 87.2. f1 of mtl primary psd is 71.4. f1 of mtl primary avg. is 80.3. f1 of mtl primary+aux dm is 87.9. f1 of mtl primary+aux pas is 90.9. f1 of mtl primary+aux psd is 80.3. f1 of mtl primary+aux avg. is 86.2.
table 5 shows performance (labeled f1 score) of our model versus the state of the art, when reducing the amount of overlap in the training data to 10%. f1 of peng et al. (2017a) dm is 86.8. f1 of peng et al. (2017a) pas is 90.5. f1 of peng et al. (2017a) psd is 77.3. f1 of peng et al. (2017a) avg. is 84.9. f1 of mtl primary+aux dm is 87.1. f1 of mtl primary+aux pas is 89.6. f1 of mtl primary+aux psd is 79.1. f1 of mtl primary+aux avg. is 85.3.
table 3 shows the intrinsic evaluation results. alignment f1 (on hand-align) of jamr alignment f1 (on hand-align) is 90.6. oracle�s smatch (on dev. dataset) of jamr oracle’s smatch (on dev. dataset) is 91.7. alignment f1 (on hand-align) of our alignment f1 (on hand-align) is 95.2. oracle�s smatch (on dev. dataset) of our oracle’s smatch (on dev. dataset) is 94.7.
table 4 shows the parsing results. accuracy of + jamr aligner newswire is 71.3. accuracy of + jamr aligner all is 65.9. accuracy of + our aligner newswire is 73.1. accuracy of + our aligner all is 67.6. accuracy of + jamr aligner newswire is 68.4. accuracy of + jamr aligner all is 64.6. accuracy of + our aligner newswire is 68.8. accuracy of + our aligner all is 65.1.
table 3 shows the accuracy@k scores of all methods in bilingual lexicon induction on lex-c. accuracy of mikolov et al. (2013) bg-en is 44.80. accuracy of mikolov et al. (2013) en-bg is 48.47. accuracy of mikolov et al. (2013) ca-en is 57.73. accuracy of mikolov et al. (2013) en-ca is 66.20. accuracy of mikolov et al. (2013) sv-en is 43.73. accuracy of mikolov et al. (2013) en-sv is 63.73. accuracy of mikolov et al. (2013) lv-en is 26.53. accuracy of mikolov et al. (2013) en-lv is 28.93. accuracy of zhang et al. (2016) bg-en is 50.60. accuracy of zhang et al. (2016) en-bg is 39.73. accuracy of zhang et al. (2016) ca-en is 63.40. accuracy of zhang et al. (2016) en-ca is 58.73. accuracy of zhang et al. (2016) sv-en is 50.87. accuracy of zhang et al. (2016) en-sv is 53.93. accuracy of zhang et al. (2016) lv-en is 34.53. accuracy of zhang et al. (2016) en-lv is 22.87. accuracy of xing et al. (2015) bg-en is 50.33. accuracy of xing et al. (2015) en-bg is 40.00. accuracy of xing et al. (2015) ca-en is 63.40. accuracy of xing et al. (2015) en-ca is 58.53. accuracy of xing et al. (2015) sv-en is 51.13. accuracy of xing et al. (2015) en-sv is 53.73. accuracy of xing et al. (2015) lv-en is 34.27. accuracy of xing et al. (2015) en-lv is 21.60. accuracy of shigeto et al. (2015) bg-en is 61.00. accuracy of shigeto et al. (2015) en-bg is 33.80. accuracy of shigeto et al. (2015) ca-en is 69.33. accuracy of shigeto et al. (2015) en-ca is 53.60. accuracy of shigeto et al. (2015) sv-en is 61.27. accuracy of shigeto et al. (2015) en-sv is 41.67. accuracy of shigeto et al. (2015) lv-en is 42.20. accuracy of shigeto et al. (2015) en-lv is 13.87. accuracy of artetxe et al. (2016) bg-en is 53.27. accuracy of artetxe et al. (2016) en-bg is 43.40. accuracy of artetxe et al. (2016) ca-en is 65.27. accuracy of artetxe et al. (2016) en-ca is 60.87. accuracy of artetxe et al. (2016) sv-en is 54.07. accuracy of artetxe et al. (2016) en-sv is 55.93. accuracy of artetxe et al. (2016) lv-en is 35.80. accuracy of artetxe et al. (2016) en-lv is 26.47. accuracy of artetxe et al. (2017) bg-en is 47.27. accuracy of artetxe et al. (2017) en-bg is 34.40. accuracy of artetxe et al. (2017) ca-en is 61.27. accuracy of artetxe et al. (2017) en-ca is 56.73. accuracy of artetxe et al. (2017) sv-en is 38.07. accuracy of artetxe et al. (2017) en-sv is 44.20. accuracy of artetxe et al. (2017) lv-en is 24.07. accuracy of artetxe et al. (2017) en-lv is 12.20. accuracy of conneau et al. (2017) bg-en is 26.47. accuracy of conneau et al. (2017) en-bg is 13.87. accuracy of conneau et al. (2017) ca-en is 41.00. accuracy of conneau et al. (2017) en-ca is 33.07. accuracy of conneau et al. (2017) sv-en is 24.27. accuracy of conneau et al. (2017) en-sv is 24.47. accuracy of ours bg-en is 50.33. accuracy of ours en-bg is 34.27. accuracy of ours ca-en is 58.60. accuracy of ours en-ca is 54.60. accuracy of ours sv-en is 48.13. accuracy of ours en-sv is 50.47. accuracy of ours lv-en is 27.73. accuracy of ours en-lv is 13.53.
table 4 shows the accuracy@k scores of all methods in bilingual lexicon induction on lex-c. accuracy of mikolov et al. (2013) de-en is 61.93. accuracy of mikolov et al. (2013) en-de is 73.07. accuracy of mikolov et al. (2013) es-en is 74.00. accuracy of mikolov et al. (2013) en-es is 80.73. accuracy of mikolov et al. (2013) fr-en is 71.33. accuracy of mikolov et al. (2013) en-fr is 82.20. accuracy of mikolov et al. (2013) it-en is 68.93. accuracy of mikolov et al. (2013) en-it is 77.60. accuracy of zhang et al. (2016) de-en is 67.67. accuracy of zhang et al. (2016) en-de is 69.87. accuracy of zhang et al. (2016) es-en is 77.27. accuracy of zhang et al. (2016) en-es is 78.53. accuracy of zhang et al. (2016) fr-en is 76.07. accuracy of zhang et al. (2016) en-fr is 78.20. accuracy of zhang et al. (2016) it-en is 72.40. accuracy of zhang et al. (2016) en-it is 73.40. accuracy of xing et al. (2015) de-en is 67.73. accuracy of xing et al. (2015) en-de is 69.53. accuracy of xing et al. (2015) es-en is 77.20. accuracy of xing et al. (2015) en-es is 78.60. accuracy of xing et al. (2015) fr-en is 76.33. accuracy of xing et al. (2015) en-fr is 78.67. accuracy of xing et al. (2015) it-en is 72.00. accuracy of xing et al. (2015) en-it is 73.33. accuracy of shigeto et al. (2015) de-en is 71.07. accuracy of shigeto et al. (2015) en-de is 63.73. accuracy of shigeto et al. (2015) es-en is 81.07. accuracy of shigeto et al. (2015) en-es is 74.53. accuracy of shigeto et al. (2015) fr-en is 79.93. accuracy of shigeto et al. (2015) en-fr is 73.13. accuracy of shigeto et al. (2015) it-en is 76.47. accuracy of shigeto et al. (2015) en-it is 68.13. accuracy of artetxe et al. (2016) de-en is 69.13. accuracy of artetxe et al. (2016) en-de is 72.13. accuracy of artetxe et al. (2016) es-en is 78.27. accuracy of artetxe et al. (2016) en-es is 80.07. accuracy of artetxe et al. (2016) fr-en is 77.73. accuracy of artetxe et al. (2016) en-fr is 79.20. accuracy of artetxe et al. (2016) it-en is 73.60. accuracy of artetxe et al. (2016) en-it is 74.47. accuracy of artetxe et al. (2017) de-en is 68.07. accuracy of artetxe et al. (2017) en-de is 69.20. accuracy of artetxe et al. (2017) es-en is 75.60. accuracy of artetxe et al. (2017) en-es is 78.20. accuracy of artetxe et al. (2017) fr-en is 74.47. accuracy of artetxe et al. (2017) en-fr is 77.67. accuracy of artetxe et al. (2017) it-en is 70.53. accuracy of artetxe et al. (2017) en-it is 71.67. accuracy of conneau et al. (2017) de-en is 69.87. accuracy of conneau et al. (2017) en-de is 71.53. accuracy of conneau et al. (2017) es-en is 78.53. accuracy of conneau et al. (2017) en-es is 79.40. accuracy of conneau et al. (2017) fr-en is 77.67. accuracy of conneau et al. (2017) en-fr is 78.33. accuracy of conneau et al. (2017) it-en is 74.60. accuracy of conneau et al. (2017) en-it is 75.80. accuracy of ours de-en is 67.00. accuracy of ours en-de is 69.33. accuracy of ours es-en is 77.80. accuracy of ours en-es is 79.53. accuracy of ours fr-en is 75.47. accuracy of ours en-fr is 77.93. accuracy of ours it-en is 72.60. accuracy of ours en-it is 73.47.
table 5 shows performance (measured using pearson correlation) of all the methods in cross-lingual semantic word similarity prediction on the benchmark data from conneau et al. correlation of mikolov et al. (2013) de-en is 0.71. correlation of mikolov et al. (2013) es-en is 0.72. correlation of mikolov et al. (2013) fa-en is 0.68. correlation of mikolov et al. (2013) it-en is 0.71. correlation of zhang et al. (2016) de-en is 0.71. correlation of zhang et al. (2016) es-en is 0.71. correlation of zhang et al. (2016) fa-en is 0.69. correlation of zhang et al. (2016) it-en is 0.71. correlation of xing et al. (2015) de-en is 0.72. correlation of xing et al. (2015) es-en is 0.71. correlation of xing et al. (2015) fa-en is 0.69. correlation of xing et al. (2015) it-en is 0.72. correlation of shigeto et al. (2015) de-en is 0.72. correlation of shigeto et al. (2015) es-en is 0.72. correlation of shigeto et al. (2015) fa-en is 0.69. correlation of shigeto et al. (2015) it-en is 0.71. correlation of artetxe et al. (2016) de-en is 0.73. correlation of artetxe et al. (2016) es-en is 0.72. correlation of artetxe et al. (2016) fa-en is 0.70. correlation of artetxe et al. (2016) it-en is 0.73. correlation of artetxe et al. (2017) de-en is 0.70. correlation of artetxe et al. (2017) es-en is 0.70. correlation of artetxe et al. (2017) fa-en is 0.67. correlation of artetxe et al. (2017) it-en is 0.71. correlation of conneau et al. (2017) de-en is 0.71. correlation of conneau et al. (2017) es-en is 0.71. correlation of conneau et al. (2017) fa-en is 0.68. correlation of conneau et al. (2017) it-en is 0.71. correlation of ours de-en is 0.71. correlation of ours es-en is 0.71. correlation of ours fa-en is 0.67. correlation of ours it-en is 0.71.
table 7 shows linking accuracy of the zero-shot (z-s) approach on different datasets. accuracy of xelms (z-s w/ prior) (es) is 80.3. accuracy of xelms (z-s w/ prior)  (zh) is 83.9. accuracy of xelms (z-s w/ prior)  (avg) is 43.5. accuracy of xelms (z-s w/ prior)  (avg) is 88.1. accuracy of xelms (z-s w/o prior) (es) is 53.5. accuracy of xelms (z-s w/o prior)  (zh) is 55.9. accuracy of xelms (z-s w/o prior)  (avg) is 41.1. accuracy of xelms (z-s w/o prior)  (avg) is 86.0. accuracy of sota (es) is 83.9. accuracy of sota  (zh) is 85.9. accuracy of sota  (avg) is 54.7. accuracy of sota  (avg) is 89.4.
table 5 shows correlation results of trn13, trn14, trn15 and d with tst13, tst14, tst15. c (%) of trn13 : tst13 c (%) is 16.2. c (%) of trn14 : tst14 c (%) is 53.9. c (%) of trn15 : tst15 c (%) is 46.7. c (%) of d : tst13 c (%) is 74.1. c (%) of d : tst14 c (%) is 80.6. c (%) of d : tst15 c (%) is 84.2.
table 7 shows the performance of chinese spelling error detection with bilstm on tst13,tst14,tst15 (%). p of trn p is 24.4. r of trn r is 27.3. f1 of trn f1 is 25.8. p of trn p is 49.8. r of trn r is 51.5. f1 of trn f1 is 50.6. p of trn p is 40.1. r of trn r is 43.2. f1 of trn f1 is 41.6. p of d-10k p is 33.3. r of d-10k r is 39.6. f1 of d-10k f1 is 36.1. p of d-10k p is 31.1. r of d-10k r is 35.1. f1 of d-10k f1 is 32.9. p of d-10k p is 31.0. r of d-10k r is 37.0. f1 of d-10k f1 is 33.7. p of d-20k p is 41.1. r of d-20k r is 50.2. f1 of d-20k f1 is 45.2. p of d-20k p is 41.1. r of d-20k r is 50.2. f1 of d-20k f1 is 45.2. p of d-20k p is 43.0. r of d-20k r is 54.9. f1 of d-20k f1 is 48.2. p of d-30k p is 47.2. r of d-30k r is 59.1. f1 of d-30k f1 is 52.5. p of d-30k p is 40.9. r of d-30k r is 48.0. f1 of d-30k f1 is 44.2. p of d-30k p is 50.3. r of d-30k r is 62.3. f1 of d-30k f1 is 55.7. p of d-40k p is 53.4. r of d-40k r is 65.0. f1 of d-40k f1 is 58.6. p of d-40k p is 52.3. r of d-40k r is 64.3. f1 of d-40k f1 is 57.7. p of d-40k p is 56.6. r of d-40k r is 66.5. f1 of d-40k f1 is 61.2. p of d-50k p is 54.0. r of d-50k r is 69.3. f1 of d-50k f1 is 60.7. p of d-50k p is 51.9. r of d-50k r is 66.2. f1 of d-50k f1 is 58.2. p of d-50k p is 56.6. r of d-50k r is 69.4. f1 of d-50k f1 is 62.3.
table 7 shows effect of using different tuning sets. accuracy of bohnet et al. (2018) wsj is 98.00±0.12. accuracy of bohnet et al. (2018) nv is 97.98±0.13. accuracy of +elmo wsj is 97.94±0.08. accuracy of +elmo nv is 97.85±0.16. accuracy of +nv data wsj is 97.98±0.11. accuracy of +nv data nv is 97.94±0.14. accuracy of +elmo+nv data wsj is 97.97±0.09. accuracy of +elmo+nv data nv is 97.94±0.13. accuracy of bohnet et al. (2018) wsj is 74.0±1.2. accuracy of bohnet et al. (2018) nv is 76.9±0.6†. accuracy of +elmo wsj is 82.1±0.9. accuracy of +elmo nv is 83.4±0.5†. accuracy of +nv data wsj is 86.4±0.4. accuracy of +nv data nv is 86.8±0.4. accuracy of +elmo+nv data wsj is 88.9±0.3. accuracy of +elmo+nv data nv is 89.3±0.2‡.
table 6 shows las results when case information is added. las of char dev is 91.2. las of char test is 90.6. las of char (multi-task) dev is 91.6. las of char (multi-task) test is 91.0. las of char + predicted case dev is 92.2. las of char + predicted case test is 91.8. las of char + gold case dev is 92.3. las of char + gold case test is 91.9. las of oracle dev is 92.5. las of oracle test is 92.0. las of char dev is 87.5. las of char test is 84.5. las of char (multi-task) dev is 87.9. las of char (multi-task) test is 84.4. las of char + predicted case dev is 87.8. las of char + predicted case test is 86.4. las of char + gold case dev is 90.2. las of char + gold case test is 86.9. las of oracle dev is 89.7. las of oracle test is 86.5. las of char dev is 91.6. las of char test is 92.4. las of char (multi-task) dev is 92.2. las of char (multi-task) test is 92.6. las of char + predicted case dev is 92.5. las of char + predicted case test is 93.3. las of char + gold case dev is 92.8. las of char + gold case test is 93.5. las of oracle dev is 92.6. las of oracle test is 93.3.
table 2 shows f1 score of our proposed models in comparison with state-of-the-art results. f1 of conv-crf+lexicon (collobert et al. 2011) english is 89.59. f1 of conv-crf+lexicon (collobert et al. 2011) chunking is 94.32. f1 of conv-crf+lexicon (collobert et al. 2011) pos is 97.29. f1 of lstm-crf+lexicon (huang et al. 2015) english is 90.10. f1 of lstm-crf+lexicon (huang et al. 2015) chunking is 94.46. f1 of lstm-crf+lexicon (huang et al. 2015) pos is 97.43. f1 of lstm-crf+lexicon+char-cnn (chiu and nichols 2016) english is 90.77. f1 of lstm-softmax+char-lstm (ling et al. 2015) pos is 97.55. f1 of lstm-crf+char-lstm (lample et al. 2016) spanish is 85.75. f1 of lstm-crf+char-lstm (lample et al. 2016) dutch is 81.74. f1 of lstm-crf+char-lstm (lample et al. 2016) english is 90.94. f1 of lstm-crf+char-lstm (lample et al. 2016) german is 78.76. f1 of lstm-crf+char-cnn (ma and hovy 2016) english is 91.21. f1 of lstm-crf+char-cnn (ma and hovy 2016) pos is 97.55. f1 of grm-crf+char-gru (yang et al. 2017) spanish is 84.69. f1 of grm-crf+char-gru (yang et al. 2017) dutch is 85.00. f1 of grm-crf+char-gru (yang et al. 2017) english is 91.20. f1 of grm-crf+char-gru (yang et al. 2017) chunking is 94.66. f1 of grm-crf+char-gru (yang et al. 2017) pos is 97.55. f1 of lstm-crf spanish is 80.33±0.37. f1 of lstm-crf dutch is 79.87±0.28. f1 of lstm-crf english is 88.41±0.22. f1 of lstm-crf german is 73.42±0.39. f1 of lstm-crf chunking is 94.29±0.11. f1 of lstm-crf pos is 96.63±0.08. f1 of lstm-crf+char-lstm spanish is 86.12±0.34. f1 of lstm-crf+char-lstm dutch is 87.13±0.25. f1 of lstm-crf+char-lstm english is 91.13±0.15. f1 of lstm-crf+char-lstm german is 78.31±0.35. f1 of lstm-crf+char-lstm chunking is 94.97±0.09. f1 of lstm-crf+char-lstm pos is 97.49±0.04. f1 of lstm-crf+char-cnn spanish is 85.91±0.38. f1 of lstm-crf+char-cnn dutch is 86.69±0.22. f1 of lstm-crf+char-cnn english is 91.11±0.14. f1 of lstm-crf+char-cnn german is 78.15±0.31. f1 of lstm-crf+char-cnn chunking is 94.91±0.08. f1 of lstm-crf+char-cnn pos is 97.45±0.03. f1 of lstm-crf+char-intnet-9 spanish is 85.71±0.39. f1 of lstm-crf+char-intnet-9 dutch is 87.38±0.27. f1 of lstm-crf+char-intnet-9 english is 91.39±0.16. f1 of lstm-crf+char-intnet-9 german is 79.43±0.33. f1 of lstm-crf+char-intnet-9 chunking is 95.08±0.07. f1 of lstm-crf+char-intnet-9 pos is 97.51±0.04. f1 of lstm-crf+char-intnet-5 spanish is 86.68±0.35. f1 of lstm-crf+char-intnet-5 dutch is 87.81±0.24. f1 of lstm-crf+char-intnet-5 english is 91.64±0.17. f1 of lstm-crf+char-intnet-5 german is 78.58±0.32. f1 of lstm-crf+char-intnet-5 chunking is 95.29±0.08. f1 of lstm-crf+char-intnet-5 pos is 97.58±0.02.
table 4 shows performance of icon on the iemocap dataset. acc. of memnet acc. is 24.4. f1 of memnet f1 is 33.0. acc. of memnet acc. is 60.4. f1 of memnet f1 is 69.3. acc. of memnet acc. is 56.8. f1 of memnet f1 is 55.0. acc. of memnet acc. is 67.1. f1 of memnet f1 is 66.1. acc. of memnet acc. is 65.2. f1 of memnet f1 is 62.3. acc. of memnet acc. is 68.4. f1 of memnet f1 is 63.0. acc. of memnet acc. is 59.9. f1 of memnet f1 is 59.5. acc. of clstm acc. is 25.5. f1 of clstm f1 is 35.6. acc. of clstm acc. is 58.6. f1 of clstm f1 is 69.2. acc. of clstm acc. is 56.5. f1 of clstm f1 is 53.5. acc. of clstm acc. is 70.0. f1 of clstm f1 is 66.3. acc. of clstm acc. is 58.8. f1 of clstm f1 is 61.1. acc. of clstm acc. is 67.4. f1 of clstm f1 is 62.4. acc. of clstm acc. is 59.8. f1 of clstm f1 is 59.0. acc. of tfn acc. is 23.2. f1 of tfn f1 is 33.7. acc. of tfn acc. is 58.0. f1 of tfn f1 is 68.6. acc. of tfn acc. is 56.6. f1 of tfn f1 is 55.1. acc. of tfn acc. is 69.1. f1 of tfn f1 is 64.2. acc. of tfn acc. is 63.1. f1 of tfn f1 is 62.4. acc. of tfn acc. is 65.5. f1 of tfn f1 is 61.2. acc. of tfn acc. is 58.8. f1 of tfn f1 is 58.5. acc. of mfn acc. is 24.0. f1 of mfn f1 is 34.1. acc. of mfn acc. is 65.6. f1 of mfn f1 is 70.5. acc. of mfn acc. is 55.5. f1 of mfn f1 is 52.1. acc. of mfn acc. is 72.3†. f1 of mfn f1 is 66.8. acc. of mfn acc. is 64.3. f1 of mfn f1 is 62.1. acc. of mfn acc. is 67.9. f1 of mfn f1 is 62.5. acc. of mfn acc. is 60.1. f1 of mfn f1 is 59.9. acc. of cmn acc. is 25.7. f1 of cmn f1 is 32.6. acc. of cmn acc. is 66.5. f1 of cmn f1 is 72.9. acc. of cmn acc. is 53.9. f1 of cmn f1 is 56.2. acc. of cmn acc. is 67.6. f1 of cmn f1 is 64.6. acc. of cmn acc. is 69.9. f1 of cmn f1 is 67.9. acc. of cmn acc. is 71.7. f1 of cmn f1 is 63.1. acc. of cmn acc. is 61.9. f1 of cmn f1 is 61.4. acc. of icon acc. is 23.6. f1 of icon f1 is 32.8. acc. of icon acc. is 70.6†. f1 of icon f1 is 74.4†. acc. of icon acc. is 59.9. f1 of icon f1 is 60.6†. acc. of icon acc. is 68.2. f1 of icon f1 is 68.2. acc. of icon acc. is 72.2†. f1 of icon f1 is 68.4. acc. of icon acc. is 71.9. f1 of icon f1 is 66.2†. acc. of icon acc. is 64.0†. f1 of icon f1 is 63.5†.
table 6 shows comparison of the performance of icon on both iemocap and semaine considering different modality combinations. acc. of t acc. is 58.3. f1 of t f1 is 57.9. r of t r is .237. r of t r is .297. r of t r is .260. r of t r is .225. acc. of a acc. is 50.7. f1 of a f1 is 50.9. r of a r is .021. r of a r is .082. r of a r is .250. r of a r is .035. acc. of v acc. is 41.2. f1 of v f1 is 39.8. r of v r is .001. r of v r is .068. r of v r is .251. r of v r is .001. acc. of a+v acc. is 52.0. f1 of a+v f1 is 51.2. r of a+v r is .031. r of a+v r is .122. r of a+v r is .283. r of a+v r is .050. acc. of t+a acc. is 63.8. f1 of t+a f1 is 63.2. r of t+a r is .237. r of t+a r is .310. r of t+a r is .272. r of t+a r is .242. acc. of t+v acc. is 61.4. f1 of t+v f1 is 61.2. r of t+v r is .238. r of t+v r is .293. r of t+v r is .268. r of t+v r is .239. acc. of t+a+v acc. is 64.0. f1 of t+a+v f1 is 63.5. r of t+a+v r is .243. r of t+a+v r is .312. r of t+a+v r is .279. r of t+a+v r is .244.
table 5 shows sense prediction accuracy for motion (left) and non-motion verbs (right) using different image representations. accuracy of random motion is 76.7 ± 0.86. accuracy of random non-motion is 78.5 ± 0.39. accuracy of mfs motion is 76.1. accuracy of mfs non-motion is 80.0. accuracy of cnn motion is 82.3. accuracy of cnn non-motion is 80.0. accuracy of gella–cnn+o motion is 83.0. accuracy of gella–cnn+o non-motion is 80.0. accuracy of gella–cnn+c motion is 82.3. accuracy of gella–cnn+c non-motion is 80.3. accuracy of cnn (reproduced) motion is 83.1. accuracy of cnn (reproduced) non-motion is 79.8 ± 0.53. accuracy of imgobjloc motion is 84.8 ± 0.69. accuracy of imgobjloc non-motion is 80.4 ± 0.57.
table 4 shows results from the human subject study on common ground. accuracy of easy attenton is 0.665. accuracy of easy cvae is 0.776. accuracy of easy cvae+sv is 0.818. accuracy of easy gold is 0.888. accuracy of hard attenton is 0.576. accuracy of hard cvae is 0.718. accuracy of hard cvae+sv is 0.788. accuracy of hard gold is 0.841.
table 3 shows performance of the linear svm regression model and the avg score at different agreements. mean abs err 1 of it-10 mean abs err 1 is 0.77. spearman 1 of it-10 spearman 1 is 0.57. mean abs err 2 of it-10 mean abs err 2 is 0.79. spearman 2 of it-10 spearman 2 is 0.55. mean abs err 3 of it-10 mean abs err 3 is 0.85. spearman 3 of it-10 spearman 3 is 0.55. avg mean abs err of it-10 avg mean abs err is 0.80. avg spearman of it-10 avg spearman is 0.56. mean abs err 1 of it-14 mean abs err 1 is 0.78. spearman 1 of it-14 spearman 1 is 0.64. mean abs err 2 of it-14 mean abs err 2 is 0.80. spearman 2 of it-14 spearman 2 is 0.63. mean abs err 3 of it-14 mean abs err 3 is 0.75. spearman 3 of it-14 spearman 3 is 0.64. avg mean abs err of it-14 avg mean abs err is 0.78. avg spearman of it-14 avg spearman is 0.63. mean abs err 1 of en-10 mean abs err 1 is 0.71. spearman 1 of en-10 spearman 1 is 0.68. mean abs err 2 of en-10 mean abs err 2 is 0.70. spearman 2 of en-10 spearman 2 is 0.67. mean abs err 3 of en-10 mean abs err 3 is 0.77. spearman 3 of en-10 spearman 3 is 0.61. avg mean abs err of en-10 avg mean abs err is 0.72. avg spearman of en-10 avg spearman is 0.65. mean abs err 1 of en-14 mean abs err 1 is 0.68. spearman 1 of en-14 spearman 1 is 0.64. mean abs err 2 of en-14 mean abs err 2 is 0.70. spearman 2 of en-14 spearman 2 is 0.73. mean abs err 3 of en-14 mean abs err 3 is 0.60. spearman 3 of en-14 spearman 3 is 0.71. avg mean abs err of en-14 avg mean abs err is 0.66. avg spearman of en-14 avg spearman is 0.69.
table 1 shows development results for diﬀerent systems using posterior inference on constituents (pioc). rec of best rec is 73.65. prec of best prec is 55.66. f1 of best f1 is 63.40. rec of best w/ pioc rec is 73.59. prec of best w/ pioc prec is 56.41. f1 of best w/ pioc f1 is 63.87. rec of all w/ pioc rec is 72.99. prec of all w/ pioc prec is 59.21. f1 of all w/ pioc f1 is 65.38. rec of all w/ pioc w/o best rec is 73.00. prec of all w/ pioc w/o best prec is 59.06. f1 of all w/ pioc w/o best f1 is 65.29.
table 2 shows recognition results with standard and session-based lstm-lms, measured by word error rates (wer). wer of lstm-lm - is 10.01. wer of lstm-lm swb is 6.88. wer of lstm-lm ch is 12.79. wer of session lstm-lm - is 9.67. wer of session lstm-lm swb is 6.81. wer of session lstm-lm ch is 12.54. wer of session lstm-lm 2nd iteration - is 9.66. wer of session lstm-lm 2nd iteration swb is 6.77. wer of session lstm-lm 2nd iteration ch is 12.56. wer of lstm-lm - is 9.81. wer of lstm-lm swb is 6.89. wer of lstm-lm ch is 13.02. wer of session lstm-lm - is 9.47. wer of session lstm-lm swb is 6.81. wer of session lstm-lm ch is 12.60. wer of session lstm-lm 2nd iteration - is 9.50. wer of session lstm-lm 2nd iteration swb is 6.83. wer of session lstm-lm 2nd iteration ch is 12.73. wer of lstm-lm - is 9.66. wer of lstm-lm swb is 6.63. wer of lstm-lm ch is 12.77. wer of session lstm-lm - is 9.28. wer of session lstm-lm swb is 6.52. wer of session lstm-lm ch is 12.34. wer of lstm-lm + session lstm-lm - is 9.22. wer of lstm-lm + session lstm-lm swb is 6.45. wer of lstm-lm + session lstm-lm ch is 12.11.
table 2 shows multi-label classification results. accuracy of random forest exact match is 35.0. accuracy of random forest hamming is 70.2. accuracy of cnn exact match is 53.7. accuracy of cnn hamming is 80.2. accuracy of rnn exact match is 57.1. accuracy of rnn hamming is 81.5. accuracy of cnn-rnn exact match is 59.2. accuracy of cnn-rnn hamming is 82.3. accuracy of cnn-rnn (bidirec + char) exact match is 62.0. accuracy of cnn-rnn (bidirec + char) hamming is 82.5.
table 2 shows performance comparison of the state-ofthe-art nested ner models on the test dataset. p(%) of exhaustive model p(%) is 93.2. r(%) of exhaustive model r(%) is 64.0. f(%) of exhaustive model f(%) is 77.1. p(%) of ju et al. (2018) p(%) is 78.5. r(%) of ju et al. (2018) r(%) is 71.3. f(%) of ju et al. (2018) f(%) is 74.7. p(%) of katiyar and cardie p(%) is 76.7. r(%) of katiyar and cardie r(%) is 71.1. f(%) of katiyar and cardie f(%) is 73.8. p(%) of muis and lu (2017) p(%) is 75.4. r(%) of muis and lu (2017) r(%) is 66.8. f(%) of muis and lu (2017) f(%) is 70.8. p(%) of lu and roth (2015) p(%) is 72.5. r(%) of lu and roth (2015) r(%) is 65.2. f(%) of lu and roth (2015) f(%) is 68.7. p(%) of finkel and manning p(%) is 75.4. r(%) of finkel and manning r(%) is 65.9. f(%) of finkel and manning f(%) is 70.3.
table 3 shows performances of our model on different entity level on the test dataset. p(%) of single-token p(%) is 91.6. r(%) of single-token r(%) is 58.4. f(%) of single-token f(%) is 69.9. p(%) of multi-token p(%) is 95.9. r(%) of multi-token r(%) is 65.8. f(%) of multi-token f(%) is 77.9. p(%) of top level p(%) is 92.7. r(%) of top level r(%) is 69.8. f(%) of top level f(%) is 79.3. p(%) of nested p(%) is 94.3. r(%) of nested r(%) is 59.3. f(%) of nested f(%) is 72.7. p(%) of all entities p(%) is 93.2. r(%) of all entities r(%) is 64.0. f(%) of all entities f(%) is 77.1.
table 4 shows categorical performances on the genia test dataset. p(%) of dna p(%) is 92.6. r(%) of dna r(%) is 58.7. f(%) of dna f(%) is 71.8. f&m f(%) of dna f&m f(%) is 65.2. p(%) of rna p(%) is 98.8. r(%) of rna r(%) is 57.1. f(%) of rna f(%) is 72.4. f&m f(%) of rna f&m f(%) is 74.7. p(%) of cell line p(%) is 94.6. r(%) of cell line r(%) is 53.1. f(%) of cell line f(%) is 67.9. f&m f(%) of cell line f&m f(%) is 64.0. p(%) of cell type p(%) is 88.4. r(%) of cell type r(%) is 70.0. f(%) of cell type f(%) is 78.1. f&m f(%) of cell type f&m f(%) is 67.1. p(%) of protein p(%) is 94.1. r(%) of protein r(%) is 70.8. f(%) of protein f(%) is 80.8. f&m f(%) of protein f&m f(%) is 73.8.
table 5 shows performance of our model with different maximum region sizes on the development dataset. ratio(%) of size = 3 ratio(%) is 89.6. p(%) of size = 3 p(%) is 92.9. r(%) of size = 3 r(%) is 69.8. f(%) of size = 3 f(%) is 79.5. ratio(%) of size = 6 ratio(%) is 98.9. p(%) of size = 6 p(%) is 93.6. r(%) of size = 6 r(%) is 66.7. f(%) of size = 6 f(%) is 77.5. ratio(%) of size = 8 ratio(%) is 99.4. p(%) of size = 8 p(%) is 93.7. r(%) of size = 8 r(%) is 66.5. f(%) of size = 8 f(%) is 77.6. ratio(%) of size = 10 ratio(%) is 100. p(%) of size = 10 p(%) is 93.5. r(%) of size = 10 r(%) is 67.6. f(%) of size = 10 f(%) is 78.2.
table 6 shows performance of our model with different model architectures on the development dataset. p(%) of bi-lstm p(%) is 94.1. r(%) of bi-lstm r(%) is 65.7. f(%) of bi-lstm f(%) is 77.1. p(%) of bi-lstm + character* p(%) is 93.5. r(%) of bi-lstm + character* r(%) is 67.6. f(%) of bi-lstm + character* f(%) is 78.2. p(%) of boundary* p(%) is 94.1. r(%) of boundary* r(%) is 54.3. f(%) of boundary* f(%) is 68.5. p(%) of inside* p(%) is 93.2. r(%) of inside* r(%) is 46.4. f(%) of inside* f(%) is 61.2. p(%) of boundary+inside* p(%) is 93.5. r(%) of boundary+inside* r(%) is 67.6. f(%) of boundary+inside* f(%) is 78.2.
table 7 shows categorical and overall performances of the jnlpba test dataset. p(%) of dna p(%) is 95.2. r(%) of dna r(%) is 56.8. f(%) of dna f(%) is 71.4. p(%) of rna p(%) is 96.1. r(%) of rna r(%) is 61.4. f(%) of rna f(%) is 75.2. p(%) of cell line p(%) is 86.2. r(%) of cell line r(%) is 44.1. f(%) of cell line f(%) is 58.8. p(%) of cell type p(%) is 96.7. r(%) of cell type r(%) is 61.5. f(%) of cell type f(%) is 75.3. p(%) of protein p(%) is 97.1. r(%) of protein r(%) is 72.2. f(%) of protein f(%) is 82.6. p(%) of overall p(%) is 96.4. r(%) of overall r(%) is 66.8. f(%) of overall f(%) is 78.4.
table 1 shows we reproduce experiments in malouf (2017) using our own implementation of the model. accuracy of finnish nouns our baseline is 99.50. accuracy of finnish nouns malouf (2017) is 99.27 ±0.09. accuracy of french verbs our baseline is 99.88. accuracy of french verbs malouf (2017) is 99.92 ±0.02. accuracy of irish nouns our baseline is 85.11. accuracy of irish nouns malouf (2017) is 85.69 ±1.71. accuracy of khaling verbs our baseline is 99.66. accuracy of khaling verbs malouf (2017) is 99.29 ±0.08. accuracy of maltese verbs our baseline is 98.65. accuracy of maltese verbs malouf (2017) is 98.93 ±0.32. accuracy of p. chinantec verbs our baseline is 91.16. accuracy of p. chinantec verbs malouf (2017) is 91.20 ±0.97. accuracy of russian nouns our baseline is 95.90. accuracy of russian nouns malouf (2017) is 96.34 ±0.96.
table 4 shows overall results for filling in missing forms when the 10,000 most frequent forms are given in the inﬂection tables. accuracy of finnish nouns our system is 63.64 ± 3.24. accuracy of finnish nouns baseline is 25.63 ± 1.63. accuracy of finnish verbs our system is 24.82 ± 1.13. accuracy of finnish verbs baseline is 16.14 ± 1.14. accuracy of french verbs our system is 31.34 ± 1.18. accuracy of french verbs baseline is 14.34 ± 0.87. accuracy of german nouns our system is 18.73 ± 1.26. accuracy of german nouns baseline is 67.16 ± 3.20. accuracy of german verbs our system is 61.21 ± 1.85. accuracy of german verbs baseline is 50.18 ± 2.58. accuracy of latvian nouns our system is 76.90 ± 5.30. accuracy of latvian nouns baseline is 57.28 ± 2.05. accuracy of spanish verbs our system is 27.27 ± 0.72. accuracy of spanish verbs baseline is 16.61 ± 0.70. accuracy of turkish nouns our system is 33.87 ± 2.03. accuracy of turkish nouns baseline is 25.00 ± 2.52.
table 3 shows comparison between the attack success rate and mean percentage of modifications required by the genetic attack and perturb baseline for the two tasks. % success of perturb baseline % success is 52%. % modified of perturb baseline % modified is 19%. % success of genetic attack % success is 97%. % modified of genetic attack % modified is 14.7%. % success of genetic attack % success is 70%. % modified of genetic attack % modified is 23%.
table 3 shows comparison with previous state-of-the-art models. top-1 of google ime top-1 is 62.13. top-5 of google ime top-5 is 72.17. top-10 of google ime top-10 is 74.72. kyss of google ime kyss is 0.6731. top-1 of google ime top-1 is 70.93. top-5 of google ime top-5 is 80.32. top-10 of google ime top-10 is 82.23. kyss of google ime kyss is 0.7535. top-1 of cocat top-1 is 59.15. top-5 of cocat top-5 is 71.85. top-10 of cocat top-10 is 76.78. kyss of cocat kyss is 0.7651. top-1 of cocat top-1 is 61.42. top-5 of cocat top-5 is 73.08. top-10 of cocat top-10 is 78.33. kyss of cocat kyss is 0.7933. top-1 of omwa top-1 is 57.14. top-5 of omwa top-5 is 72.32. top-10 of omwa top-10 is 80.21. kyss of omwa kyss is 0.7389. top-1 of omwa top-1 is 64.42. top-5 of omwa top-5 is 72.91. top-10 of omwa top-10 is 77.93. kyss of omwa kyss is 0.7115. top-1 of basic p2c top-1 is 71.31. top-5 of basic p2c top-5 is 89.12. top-10 of basic p2c top-10 is 90.17. kyss of basic p2c kyss is 0.8845. top-1 of basic p2c top-1 is 70.5. top-5 of basic p2c top-5 is 79.8. top-10 of basic p2c top-10 is 80.1. kyss of basic p2c kyss is 0.8301. top-1 of simple c+ p2c top-1 is 61.28. top-5 of simple c+ p2c top-5 is 71.88. top-10 of simple c+ p2c top-10 is 73.74. kyss of simple c+ p2c kyss is 0.7963. top-1 of simple c+ p2c top-1 is 60.87. top-5 of simple c+ p2c top-5 is 71.23. top-10 of simple c+ p2c top-10 is 75.33. kyss of simple c+ p2c kyss is  0.7883. top-1 of gated c+ p2c top-1 is 73.89. top-5 of gated c+ p2c top-5 is 90.14. top-10 of gated c+ p2c top-10 is 91.22. kyss of gated c+ p2c kyss is 0.8935. top-1 of gated c+ p2c top-1 is 70.98. top-5 of gated c+ p2c top-5 is 80.79. top-10 of gated c+ p2c top-10 is 81.37. kyss of gated c+ p2c kyss is 0.8407.
table 1 shows lm perplexity results on ptb. perplexity of non-tied valid is 95.0. perplexity of non-tied test is 91.1. perplexity of non-tied δ is . perplexity of tied valid is 90.8. perplexity of tied test is 86.6. perplexity of tied δ is -4.5. perplexity of tied+l valid is 89.8. perplexity of tied+l test is 85.8. perplexity of tied+l δ is -5.3. perplexity of non-tied valid is 89.4. perplexity of non-tied test is 85.3. perplexity of non-tied δ is . perplexity of tied+l valid is 83.4. perplexity of tied+l test is 80.3. perplexity of tied+l δ is -5.0. perplexity of non-tied valid is 87.2. perplexity of non-tied test is 83.5. perplexity of non-tied δ is . perplexity of tied valid is 82.0. perplexity of tied test is 78.2. perplexity of tied δ is -5.3. perplexity of tied+l valid is 81.9. perplexity of tied+l test is 78.0. perplexity of tied+l δ is -5.5. perplexity of non-tied valid is 85.8. perplexity of non-tied test is 82.4. perplexity of non-tied δ is . perplexity of tied+l valid is 79.0. perplexity of tied+l test is 76.0. perplexity of tied+l δ is -6.4. perplexity of non-tied valid is 84.3. perplexity of non-tied test is 81.3. perplexity of non-tied δ is . perplexity of tied valid is 79.7. perplexity of tied test is 76.1. perplexity of tied δ is -5.2. perplexity of tied+l valid is 78.7. perplexity of tied+l test is 75.5. perplexity of tied+l δ is -5.8. perplexity of - valid is 77.1. perplexity of - test is 73.9. perplexity of - δ is . perplexity of - valid is 82.2. perplexity of - test is 78.4. perplexity of - δ is . perplexity of - valid is 77.7. perplexity of - test is 74.3. perplexity of - δ is .
table 3 shows performance for variable context sizes k with the han encoder + han decoder. accuracy of 1 zh-en is 17.70. accuracy of 1 es-en is 37.20. accuracy of 1 zh-en is 29.35. accuracy of 1 es-en is 36.20. accuracy of 1 es-en is 22.46. accuracy of 3 zh-en is 17.79. accuracy of 3 es-en is 37.24. accuracy of 3 zh-en is 29.67. accuracy of 3 es-en is 36.23. accuracy of 3 es-en is 22.76. accuracy of 5 zh-en is 17.49. accuracy of 5 es-en is 37.11. accuracy of 5 zh-en is 29.69. accuracy of 5 es-en is 36.22. accuracy of 5 es-en is 22.54. accuracy of 7 zh-en is 17.00. accuracy of 7 es-en is 37.22. accuracy of 7 zh-en is 29.64. accuracy of 7 es-en is 36.21. accuracy of 7 es-en is 22.64.
table 1 shows translation performance of our methods on zh→en/ja and en→de/fr tasks. accuracy of indiv mt03 is 43.59. accuracy of indiv mt04 is 43.95. accuracy of indiv mt05 is 45.34. accuracy of indiv mt06 is 44.05. accuracy of indiv ave is 44.23. accuracy of indiv test is 40.71. accuracy of indiv test is 27.84. accuracy of indiv test is 41.50. accuracy of o2m mt03 is 43.20. accuracy of o2m mt04 is 43.55. accuracy of o2m mt05 is 44.68. accuracy of o2m mt06 is 43.93. accuracy of o2m ave is 43.84. accuracy of o2m test is 42.09. accuracy of o2m test is 26.42. accuracy of o2m test is 41.32. accuracy of o2m + ① mt03 is 43.91. accuracy of o2m + ① mt04 is 44.01. accuracy of o2m + ① mt05 is 45.12. accuracy of o2m + ① mt06 is 44.14. accuracy of o2m + ① ave is 44.30. accuracy of o2m + ① test is 42.54. accuracy of o2m + ① test is 26.78. accuracy of o2m + ① test is 41.56. accuracy of o2m + ① + ② (dyn) mt03 is 44.24. accuracy of o2m + ① + ② (dyn) mt04 is 44.45. accuracy of o2m + ① + ② (dyn) mt05 is 45.43. accuracy of o2m + ① + ② (dyn) mt06 is 44.51. accuracy of o2m + ① + ② (dyn) ave is 44.66. accuracy of o2m + ① + ② (dyn) test is 42.77. accuracy of o2m + ① + ② (dyn) test is 26.98. accuracy of o2m + ① + ② (dyn) test is 41.78. accuracy of o2m + ① + ② (fixed) mt03 is 44.13. accuracy of o2m + ① + ② (fixed) mt04 is 44.57. accuracy of o2m + ① + ② (fixed) mt05 is 45.22. accuracy of o2m + ① + ② (fixed) mt06 is 44.68. accuracy of o2m + ① + ② (fixed) ave is 44.65. accuracy of o2m + ① + ② (fixed) test is 42.70. accuracy of o2m + ① + ② (fixed) test is 26.90. accuracy of o2m + ① + ② (fixed) test is 41.75. accuracy of o2m + ① + ③ mt03 is 44.78. accuracy of o2m + ① + ③ mt04 is 45.23. accuracy of o2m + ① + ③ mt05 is 45.78. accuracy of o2m + ① + ③ mt06 is 45.22. accuracy of o2m + ① + ③ ave is 45.25. accuracy of o2m + ① + ③ test is 42.97. accuracy of o2m + ① + ③ test is 27.11. accuracy of o2m + ① + ③ test is 41.98. accuracy of o2m + ① + ② (dyn)+ 3 mt03 is 44.85. accuracy of o2m + ① + ② (dyn)+ 3 mt04 is 45.51. accuracy of o2m + ① + ② (dyn)+ 3 mt05 is 45.91. accuracy of o2m + ① + ② (dyn)+ 3 mt06 is 45.38. accuracy of o2m + ① + ② (dyn)+ 3 ave is 45.41. accuracy of o2m + ① + ② (dyn)+ 3 test is 43.03. accuracy of o2m + ① + ② (dyn)+ 3 test is 27.23. accuracy of o2m + ① + ② (dyn)+ 3 test is 41.92.
table 1 shows comparison between rcsls, least square error, procrustes and unsupervised approaches in the setting of conneau et al. accuracy of adversarial + refine en-es is 81.7. accuracy of adversarial + refine es-en is 83.3. accuracy of adversarial + refine en-fr is 82.3. accuracy of adversarial + refine fr-en is 82.1. accuracy of adversarial + refine en-de is 74.0. accuracy of adversarial + refine de-en is 72.2. accuracy of adversarial + refine en-ru is 44.0. accuracy of adversarial + refine ru-en is 59.1. accuracy of adversarial + refine en-zh is 32.5. accuracy of adversarial + refine zh-en is 31.4. accuracy of adversarial + refine avg. is 64.3. accuracy of icp + refine en-es is 82.2. accuracy of icp + refine es-en is 83.8. accuracy of icp + refine en-fr is 82.5. accuracy of icp + refine fr-en is 82.5. accuracy of icp + refine en-de is 74.8. accuracy of icp + refine de-en is 73.1. accuracy of icp + refine en-ru is 46.3. accuracy of icp + refine ru-en is 61.6. accuracy of wass. proc. + refine en-es is 82.8. accuracy of wass. proc. + refine es-en is 84.1. accuracy of wass. proc. + refine en-fr is 82.6. accuracy of wass. proc. + refine fr-en is 82.9. accuracy of wass. proc. + refine en-de is 75.4. accuracy of wass. proc. + refine de-en is 73.3. accuracy of wass. proc. + refine en-ru is 43.7. accuracy of wass. proc. + refine ru-en is 59.1. accuracy of least square error en-es is 78.9. accuracy of least square error es-en is 80.7. accuracy of least square error en-fr is 79.3. accuracy of least square error fr-en is 80.7. accuracy of least square error en-de is 71.5. accuracy of least square error de-en is 70.1. accuracy of least square error en-ru is 47.2. accuracy of least square error ru-en is 60.2. accuracy of least square error en-zh is 42.3. accuracy of least square error zh-en is 4.0. accuracy of least square error avg. is 61.5. accuracy of procrustes en-es is 81.4. accuracy of procrustes es-en is 82.9. accuracy of procrustes en-fr is 81.1. accuracy of procrustes fr-en is 82.4. accuracy of procrustes en-de is 73.5. accuracy of procrustes de-en is 72.4. accuracy of procrustes en-ru is 51.7. accuracy of procrustes ru-en is 63.7. accuracy of procrustes en-zh is 42.7. accuracy of procrustes zh-en is 36.7. accuracy of procrustes avg. is 66.8. accuracy of procrustes + refine en-es is 82.4. accuracy of procrustes + refine es-en is 83.9. accuracy of procrustes + refine en-fr is 82.3. accuracy of procrustes + refine fr-en is 83.2. accuracy of procrustes + refine en-de is 75.3. accuracy of procrustes + refine de-en is 73.2. accuracy of procrustes + refine en-ru is 50.1. accuracy of procrustes + refine ru-en is 63.5. accuracy of procrustes + refine en-zh is 40.3. accuracy of procrustes + refine zh-en is 35.5. accuracy of procrustes + refine avg. is 66.9. accuracy of rcsls + spectral en-es is 83.5. accuracy of rcsls + spectral es-en is 85.7. accuracy of rcsls + spectral en-fr is 82.3. accuracy of rcsls + spectral fr-en is 84.1. accuracy of rcsls + spectral en-de is 78.2. accuracy of rcsls + spectral de-en is 75.8. accuracy of rcsls + spectral en-ru is 56.1. accuracy of rcsls + spectral ru-en is 66.5. accuracy of rcsls + spectral en-zh is 44.9. accuracy of rcsls + spectral zh-en is 45.7. accuracy of rcsls + spectral avg. is 70.2. accuracy of rcsls en-es is 84.1. accuracy of rcsls es-en is 86.3. accuracy of rcsls en-fr is 83.3. accuracy of rcsls fr-en is 84.1. accuracy of rcsls en-de is 79.1. accuracy of rcsls de-en is 76.3. accuracy of rcsls en-ru is 57.9. accuracy of rcsls ru-en is 67.2. accuracy of rcsls en-zh is 45.9. accuracy of rcsls zh-en is 46.4. accuracy of rcsls avg. is 71.0.
table 3 shows accuracy on english and italian with the setting of dinu et al. accuracy of adversarial + refine + csls en-it is 45.1. accuracy of adversarial + refine + csls it-en is 38.3. accuracy of mikolov et al. (2013b) en-it is 33.8. accuracy of mikolov et al. (2013b) it-en is 24.9. accuracy of dinu et al. (2014) en-it is 38.5. accuracy of dinu et al. (2014) it-en is 24.6. accuracy of artetxe et al. (2016) en-it is 39.7. accuracy of artetxe et al. (2016) it-en is 33.8. accuracy of smith et al. (2017) en-it is 43.1. accuracy of smith et al. (2017) it-en is 38.0. accuracy of procrustes + csls en-it is 44.9. accuracy of procrustes + csls it-en is 38.5. accuracy of rcsls en-it is 45.5. accuracy of rcsls it-en is 38.0.
table 1 shows ter and bleu scores of our model (mt+ag+lm) v.s. ter of original mt ter is 24.81. bleu of original mt bleu is 62.92. ter of original mt ter is 24.76. bleu of original mt bleu is 62.11. ter of original mt ter is 24.48. bleu of original mt bleu is 62.49. ter of tgt → pe ter is 63.76. bleu of tgt → pe bleu is 21.32. ter of tgt → pe ter is 60.96. bleu of tgt → pe bleu is 22.11. ter of tgt → pe ter is 65.13. bleu of tgt → pe bleu is 18.13. ter of src+tgt → pe ter is 51.41. bleu of src+tgt → pe bleu is 34.04. ter of src+tgt → pe ter is 48.27. bleu of src+tgt → pe bleu is 35.24. ter of src+tgt → pe ter is 50.98. bleu of src+tgt → pe bleu is 31.52. ter of mt+ag ter is 23.74. bleu of mt+ag bleu is 65.95. ter of mt+ag ter is 23.53. bleu of mt+ag bleu is 65.22. ter of mt+ag ter is 23.77. bleu of mt+ag bleu is 64.34. ter of mt+ag+lm ter is 23.36†. bleu of mt+ag+lm bleu is 66.24. ter of mt+ag+lm ter is 23.24†. bleu of mt+ag+lm bleu is 65.53†. ter of mt+ag+lm ter is 23.45†. bleu of mt+ag+lm bleu is 64.65†. ter of tgt → pe ter is 50.91. bleu of tgt → pe bleu is 30.88. ter of tgt → pe ter is 48.62. bleu of tgt → pe bleu is 32.55. ter of tgt → pe ter is 52.07. bleu of tgt → pe bleu is 27.98. ter of src+tgt → pe ter is 30.97. bleu of src+tgt → pe bleu is 53.97. ter of src+tgt → pe ter is 30.20. bleu of src+tgt → pe bleu is 53.92. ter of src+tgt → pe ter is 32.82. bleu of src+tgt → pe bleu is 50.30. ter of mt+ag ter is 22.82. bleu of mt+ag bleu is 66.51. ter of mt+ag ter is 22.87. bleu of mt+ag bleu is 65.67. ter of mt+ag ter is 23.58. bleu of mt+ag bleu is 64.35. ter of mt+ag+lm ter is 22.67. bleu of mt+ag+lm bleu is 67.17†. ter of mt+ag+lm ter is 22.53†. bleu of mt+ag+lm bleu is 66.30†. ter of mt+ag+lm ter is 23.03†. bleu of mt+ag+lm bleu is 65.31†. ter of tgt → pe ter is 57.02. bleu of tgt → pe bleu is 27.87. ter of tgt → pe ter is 55.52. bleu of tgt → pe bleu is 27.8. ter of tgt → pe ter is 60.06. bleu of tgt → pe bleu is 22.78. ter of src+tgt → pe ter is 38.06. bleu of src+tgt → pe bleu is 47.42. ter of src+tgt → pe ter is 36.61. bleu of src+tgt → pe bleu is 47.93. ter of src+tgt → pe ter is 39.86. bleu of src+tgt → pe bleu is 43.52. ter of mt+ag ter is 23.10. bleu of mt+ag bleu is 66.60. ter of mt+ag ter is 22.82. bleu of mt+ag bleu is 66.15. ter of mt+ag ter is 23.14. bleu of mt+ag bleu is 65.19. ter of mt+ag+lm ter is 22.61†. bleu of mt+ag+lm bleu is 67.19†. ter of mt+ag+lm ter is 22.42†. bleu of mt+ag+lm bleu is 66.53†. ter of mt+ag+lm ter is 22.84†. bleu of mt+ag+lm bleu is 65.52*†. ter of tgt → pe ter is 48.89. bleu of tgt → pe bleu is 34.29. ter of tgt → pe ter is 27.12. bleu of tgt → pe bleu is 34.75. ter of tgt → pe ter is 51.13. bleu of tgt → pe bleu is 25.59. ter of src+tgt → pe ter is 28.61. bleu of src+tgt → pe bleu is 57.47. ter of src+tgt → pe ter is 27.79. bleu of src+tgt → pe bleu is 57.61. ter of src+tgt → pe ter is 30.34. bleu of src+tgt → pe bleu is 53.26. ter of mt+ag ter is 22.38. bleu of mt+ag bleu is 67.34. ter of mt+ag ter is 22.14. bleu of mt+ag bleu is 66.53. ter of mt+ag ter is 22.71. bleu of mt+ag bleu is 65.34. ter of mt+ag+lm ter is 21.99*†. bleu of mt+ag+lm bleu is 67.50*. ter of mt+ag+lm ter is 22.07*. bleu of mt+ag+lm bleu is 66.67*. ter of mt+ag+lm ter is 22.58*. bleu of mt+ag+lm bleu is 65.50.
table 2 shows token level identification f1 scores. f1 of exact match eng is 43.4. f1 of exact match amh is 54.4. f1 of exact match ara is 29.3. f1 of exact match ben is 47.7. f1 of exact match fas is 30.5. f1 of exact match hin is 30.9. f1 of exact match som is 46.0. f1 of exact match tgl is 23.7. f1 of exact match avg is 37.5. f1 of capitalization eng is 79.5. f1 of capitalization som is 69.5. f1 of capitalization tgl is 77.6. f1 of srilm eng is 92.8. f1 of srilm amh is 69.9. f1 of srilm ara is 54.7. f1 of srilm ben is 79.4. f1 of srilm fas is 60.8. f1 of srilm hin is 63.8. f1 of srilm som is 84.1. f1 of srilm tgl is 80.5. f1 of srilm avg is 70.5. f1 of skip-gram eng is 76.0. f1 of skip-gram amh is 53.0. f1 of skip-gram ara is 29.7. f1 of skip-gram ben is 41.4. f1 of skip-gram fas is 30.8. f1 of skip-gram hin is 29.0. f1 of skip-gram som is 51.1. f1 of skip-gram tgl is 61.5. f1 of skip-gram avg is 42.4. f1 of cbow eng is 73.7. f1 of cbow amh is 50.0. f1 of cbow ara is 28.1. f1 of cbow ben is 40.6. f1 of cbow fas is 32.6. f1 of cbow hin is 26.5. f1 of cbow som is 56.4. f1 of cbow tgl is 62.5. f1 of cbow avg is 42.4. f1 of log-bilinear eng is 82.8. f1 of log-bilinear amh is 64.5. f1 of log-bilinear ara is 46.1. f1 of log-bilinear ben is 70.8. f1 of log-bilinear fas is 50.4. f1 of log-bilinear hin is 54.8. f1 of log-bilinear som is 78.1. f1 of log-bilinear tgl is 74.9. f1 of log-bilinear avg is 62.8. f1 of cogcompner (ceiling) eng is 96.5. f1 of cogcompner (ceiling) amh is 73.8. f1 of cogcompner (ceiling) ara is 64.9. f1 of cogcompner (ceiling) ben is 80.6. f1 of cogcompner (ceiling) fas is 64.1. f1 of cogcompner (ceiling) hin is 75.9. f1 of cogcompner (ceiling) som is 89.4. f1 of cogcompner (ceiling) tgl is 88.6. f1 of cogcompner (ceiling) avg is 76.8. f1 of lample et al. (2016) (ceiling) eng is 96.4. f1 of lample et al. (2016) (ceiling) amh is 84.4. f1 of lample et al. (2016) (ceiling) ara is 69.8. f1 of lample et al. (2016) (ceiling) ben is 87.6. f1 of lample et al. (2016) (ceiling) fas is 76.4. f1 of lample et al. (2016) (ceiling) hin is 86.3. f1 of lample et al. (2016) (ceiling) som is 90.9. f1 of lample et al. (2016) (ceiling) tgl is 91.2. f1 of lample et al. (2016) (ceiling) avg is 83.8.
table 3 shows ner results on 8 languages show that even a simplistic addition of clm features to a standard ner model boosts performance. accuracy of full eng is 90.94. accuracy of full amh is 73.2. accuracy of full ara is 57.2. accuracy of full ben is 77.7. accuracy of full fas is 61.2. accuracy of full hin is 77.7. accuracy of full som is 81.3. accuracy of full tgl is 83.2. accuracy of full avg is 73.1. accuracy of unseen eng is 86.11. accuracy of unseen amh is 51.9. accuracy of unseen ara is 30.2. accuracy of unseen ben is 57.9. accuracy of unseen fas is 41.4. accuracy of unseen hin is 62.2. accuracy of unseen som is 66.5. accuracy of unseen tgl is 72.8. accuracy of unseen avg is 54.7. accuracy of full eng is 90.88. accuracy of full amh is 67.5. accuracy of full ara is 54.8. accuracy of full ben is 74.5. accuracy of full fas is 57.8. accuracy of full hin is 73.5. accuracy of full som is 82.0. accuracy of full tgl is 80.9. accuracy of full avg is 70.1. accuracy of unseen eng is 84.40. accuracy of unseen amh is 42.7. accuracy of unseen ara is 25.0. accuracy of unseen ben is 51.9. accuracy of unseen fas is 31.5. accuracy of unseen hin is 53.9. accuracy of unseen som is 67.2. accuracy of unseen tgl is 68.3. accuracy of unseen avg is 48.6. accuracy of full eng is 91.21. accuracy of full amh is 71.3. accuracy of full ara is 59.1. accuracy of full ben is 75.5. accuracy of full fas is 59.0. accuracy of full hin is 74.2. accuracy of full som is 82.1. accuracy of full tgl is 78.5. accuracy of full avg is 71.4. accuracy of unseen eng is 85.20. accuracy of unseen amh is 48.4. accuracy of unseen ara is 32.0. accuracy of unseen ben is 54.0. accuracy of unseen fas is 31.2. accuracy of unseen hin is 55.4. accuracy of unseen som is 68.0. accuracy of unseen tgl is 65.2. accuracy of unseen avg is 50.6.
table 4 shows comparison of f1 scores (weighted average by support (the number of true instances for each label)) between our model and the best published methods. f1 of marco lui (lui 2012) - is 82.0. f1 of bi-ann (dernoncourt et al. 2016) 20k is 90.0. f1 of bi-ann (dernoncourt et al. 2016) 200k is 91.6. f1 of bi-ann (dernoncourt et al. 2016) - is 82.7. f1 of hsln-cnn 20k is 92.2. f1 of hsln-cnn 200k is 92.8. f1 of hsln-cnn - is 84.7. f1 of hsln-rnn 20k is 92.6. f1 of hsln-rnn 200k is 93.9. f1 of hsln-rnn - is 84.3.
table 9 shows comparison of performance with different choices of word embeddings for our hsln-rnn model trained on the pubmed 20k dataset (reported on f1-scores on the test set). dimension of glove-wiki dimension is 200. f1-scores of glove-wiki p.m. 20k is 92.0. dimension of fasttext-wiki dimension is 300. f1-scores of fasttext-wiki p.m. 20k is 92.2. dimension of fasttext-p.m.+mimic dimension is 300. f1-scores of fasttext-p.m.+mimic p.m. 20k is 92.0. dimension of word2vec-news dimension is 300. f1-scores of word2vec-news p.m. 20k is 92.2. dimension of word2vec-wiki dimension is 200. f1-scores of word2vec-wiki p.m. 20k is 92.1. dimension of word2vec-wiki+p.m. dimension is 200. f1-scores of word2vec-wiki+p.m. p.m. 20k is 92.6.
table 2 shows mimic ii results across frequent (s), few-shot (f), and zero-shot (z) groups. r@5 of random r@5 is 0.000. r@10 of random r@10 is 0.000. r@5 of random r@5 is 0.000. r@10 of random r@10 is 0.000. r@5 of random r@5 is 0.011. r@10 of random r@10 is 0.032. r@5 of random r@5 is 0.000. r@10 of random r@10 is 0.000. r@5 of logistic (vani et al. 2017) * r@5 is 0.137. r@10 of logistic (vani et al. 2017) * r@10 is 0.247. r@5 of logistic (vani et al. 2017) * r@5 is 0.001. r@10 of logistic (vani et al. 2017) * r@10 is 0.003. r@5 of cnn (baumel et al. 2018) * r@5 is 0.138. r@10 of cnn (baumel et al. 2018) * r@10 is 0.250. r@5 of cnn (baumel et al. 2018) * r@5 is 0.050. r@10 of cnn (baumel et al. 2018) * r@10 is 0.082. r@5 of acnn (mullenbach et al. 2018) * r@5 is 0.138. r@10 of acnn (mullenbach et al. 2018) * r@10 is 0.255. r@5 of acnn (mullenbach et al. 2018) * r@5 is 0.046. r@10 of acnn (mullenbach et al. 2018) * r@10 is 0.081. r@5 of match-cnn (rios and kavuluru, 2018) r@5 is 0.137. r@10 of match-cnn (rios and kavuluru, 2018) r@10 is 0.247. r@5 of match-cnn (rios and kavuluru, 2018) r@5 is 0.031. r@10 of match-cnn (rios and kavuluru, 2018) r@10 is 0.042. r@5 of eszsl + w2v r@5 is 0.074. r@10 of eszsl + w2v r@10 is 0.119. r@5 of eszsl + w2v r@5 is 0.008. r@10 of eszsl + w2v r@10 is 0.017. r@5 of eszsl + w2v r@5 is 0.080. r@10 of eszsl + w2v r@10 is 0.172. r@5 of eszsl + w2v r@5 is 0.020. r@10 of eszsl + w2v r@10 is 0.041. r@5 of eszsl + w2v 2 r@5 is 0.050. r@10 of eszsl + w2v 2 r@10 is 0.086. r@5 of eszsl + w2v 2 r@5 is 0.025. r@10 of eszsl + w2v 2 r@10 is 0.044. r@5 of eszsl + w2v 2 r@5 is 0.103. r@10 of eszsl + w2v 2 r@10 is 0.189. r@5 of eszsl + w2v 2 r@5 is 0.043. r@10 of eszsl + w2v 2 r@10 is 0.076. r@5 of eszsl + grals r@5 is 0.135. r@10 of eszsl + grals r@10 is 0.238. r@5 of eszsl + grals r@5 is 0.081. r@10 of eszsl + grals r@10 is 0.123. r@5 of eszsl + grals r@5 is 0.085. r@10 of eszsl + grals r@10 is 0.136. r@5 of eszsl + grals r@5 is 0.095. r@10 of eszsl + grals r@10 is 0.152. r@5 of zacnn r@5 is 0.135. r@10 of zacnn r@10 is 0.245. r@5 of zacnn r@5 is 0.103. r@10 of zacnn r@10 is 0.149. r@5 of zacnn r@5 is 0.147. r@10 of zacnn r@10 is 0.221. r@5 of zacnn r@5 is 0.128. r@10 of zacnn r@10 is 0.205. r@5 of zagcnn r@5 is 0.135. r@10 of zagcnn r@10 is 0.247. r@5 of zagcnn r@5 is 0.130. r@10 of zagcnn r@10 is 0.185. r@5 of zagcnn r@5 is 0.269. r@10 of zagcnn r@10 is 0.362. r@5 of zagcnn r@5 is 0.160. r@10 of zagcnn r@10 is 0.246.
table 3 shows human evaluation results. fluency of base fluency is 3.28. coherence of base coherence is 2.77. meaning of base meaning is 2.63. overall quality of base overall quality is 2.58. fluency of mem fluency is 3.23. coherence of mem coherence is 2.88. meaning of mem meaning is 2.68. overall quality of mem overall quality is 2.68. fluency of mrl fluency is 4.05. coherence of mrl coherence is 3.81. meaning of mrl meaning is 3.68. overall quality of mrl overall quality is 3.60. fluency of gt fluency is 4.14. coherence of gt coherence is 4.11. meaning of gt meaning is 4.16. overall quality of gt overall quality is 3.97.
table 6 shows triple classification results. accuracy of ctransr wn11 is 85.7. accuracy of ctransr fb15k is 84.4. accuracy of transd wn11 is 86.4. accuracy of transd fb13 is 89.1. accuracy of transd fb15k is 88.2. accuracy of transd avg is 87.9. accuracy of transg wn11 is 87.4. accuracy of transg fb13 is 87.3. accuracy of transg fb15k is 88.5. accuracy of transg avg is 87.7. accuracy of transe wn11 is 75.9. accuracy of transe fb13 is 81.5. accuracy of transe fb15k is 78.7. accuracy of transe avg is 78.7. accuracy of transh wn11 is 78.8. accuracy of transh fb13 is 83.3. accuracy of transh fb15k is 81.1. accuracy of transh avg is 81.1. accuracy of distmult wn11 is 87.1. accuracy of distmult fb13 is 86.2. accuracy of distmult fb15k is 86.3. accuracy of distmult avg is 86.5. accuracy of transe-hrs wn11 is 86.8. accuracy of transe-hrs fb13 is 88.4. accuracy of transe-hrs fb15k is 87.6. accuracy of transe-hrs avg is 87.6. accuracy of transh-hrs wn11 is 87.6. accuracy of transh-hrs fb13 is 88.9. accuracy of transh-hrs fb15k is 88.7. accuracy of transh-hrs avg is 88.4. accuracy of distmult-hrs wn11 is 88.9. accuracy of distmult-hrs fb13 is 89.0. accuracy of distmult-hrs fb15k is 89.1. accuracy of distmult-hrs avg is 89.0.
table 4 shows per-relation breakdown showing performance of each model on different relations. mrr of isaffiliatedto mrr is 0.524. hits@1 of isaffiliatedto hits@1 is 0.401. mrr of isaffiliatedto mrr is 0.551. hits@1 of isaffiliatedto hits@1 is 0.467. mrr of isaffiliatedto mrr is 0.572. hits@1 of isaffiliatedto hits@1 is 0.481. mrr of isaffiliatedto mrr is 0.569. hits@1 of isaffiliatedto hits@1 is 0.478. mrr of playsfor mrr is 0.528. hits@1 of playsfor hits@1 is 0.413. mrr of playsfor mrr is 0.554. hits@1 of playsfor hits@1 is 0.471. mrr of playsfor mrr is 0.574. hits@1 of playsfor hits@1 is 0.486. mrr of playsfor mrr is 0.566. hits@1 of playsfor hits@1 is 0.476. mrr of hasgender mrr is 0.798. hits@1 of hasgender hits@1 is 0.596. mrr of hasgender mrr is 0.799. hits@1 of hasgender hits@1 is 0.599. mrr of hasgender mrr is 0.813. hits@1 of hasgender hits@1 is 0.627. mrr of hasgender mrr is 0.842. hits@1 of hasgender hits@1 is 0.683. mrr of isconnectedto mrr is 0.482. hits@1 of isconnectedto hits@1 is 0.367. mrr of isconnectedto mrr is 0.497. hits@1 of isconnectedto hits@1 is 0.379. mrr of isconnectedto mrr is 0.492. hits@1 of isconnectedto hits@1 is 0.384. mrr of isconnectedto mrr is 0.484. hits@1 of isconnectedto hits@1 is 0.372. mrr of ismarriedto mrr is 0.365. hits@1 of ismarriedto hits@1 is 0.207. mrr of ismarriedto mrr is 0.387. hits@1 of ismarriedto hits@1 is 0.221. mrr of ismarriedto mrr is 0.404. hits@1 of ismarriedto hits@1 is 0.296. mrr of ismarriedto mrr is 0.413. hits@1 of ismarriedto hits@1 is 0.326.
table 4 shows results for scientific keyphrase extraction and extraction on semeval 2017 task 10, comparing with previous best systems. f1 of (luan 2017) f1 is 56.9. f1 of (luan 2017) f1 is 45.3. p of best semeval p is 55. r of best semeval r is 54. f1 of best semeval f1 is 55. p of best semeval p is 44. r of best semeval r is 43. f1 of best semeval f1 is 44. p of best semeval p is 36. r of best semeval r is 23. f1 of best semeval f1 is 28. p of best semeval p is 44. r of best semeval r is 41. f1 of best semeval f1 is 43. p of sciie p is 62.2. r of sciie r is 55.4. f1 of sciie f1 is 58.6. p of sciie p is 48.5. r of sciie r is 43.8. f1 of sciie f1 is 46.0. p of sciie p is 40.4. r of sciie r is 21.2. f1 of sciie f1 is 27.8. p of sciie p is 48.1. r of sciie r is 41.8. f1 of sciie f1 is 44.7.
table 2 shows query answering performance compared to state-of-the-art embedding based approaches (top part) and multi-hop reasoning approaches (bottom part). @1 of distmult (yang et al. 2014) @1 is 82.1. @10 of distmult (yang et al. 2014) @10 is 96.7. mrr of distmult (yang et al. 2014) mrr is 86.8. @1 of distmult (yang et al. 2014) @1 is 48.7. @10 of distmult (yang et al. 2014) @10 is 90.4. mrr of distmult (yang et al. 2014) mrr is 61.4. @1 of distmult (yang et al. 2014) @1 is 32.4. @10 of distmult (yang et al. 2014) @10 is 60.0. mrr of distmult (yang et al. 2014) mrr is 41.7. @1 of distmult (yang et al. 2014) @1 is 43.1. @10 of distmult (yang et al. 2014) @10 is 52.4. mrr of distmult (yang et al. 2014) mrr is 46.2. @1 of distmult (yang et al. 2014) @1 is 55.2. @10 of distmult (yang et al. 2014) @10 is 78.3. mrr of distmult (yang et al. 2014) mrr is 64.1. @1 of complex (trouillon et al. 2016) @1 is 89.0. @10 of complex (trouillon et al. 2016) @10 is 99.2. mrr of complex (trouillon et al. 2016) mrr is 93.4. @1 of complex (trouillon et al. 2016) @1 is 81.8. @10 of complex (trouillon et al. 2016) @10 is 98.1. mrr of complex (trouillon et al. 2016) mrr is 88.4. @1 of complex (trouillon et al. 2016) @1 is 32.8. @10 of complex (trouillon et al. 2016) @10 is 61.6. mrr of complex (trouillon et al. 2016) mrr is 42.5. @1 of complex (trouillon et al. 2016) @1 is 41.8. @10 of complex (trouillon et al. 2016) @10 is 48.0. mrr of complex (trouillon et al. 2016) mrr is 43.7. @1 of complex (trouillon et al. 2016) @1 is 64.3. @10 of complex (trouillon et al. 2016) @10 is 86.0. mrr of complex (trouillon et al. 2016) mrr is 72.6. @1 of conve (dettmers et al. 2018) @1 is 93.2. @10 of conve (dettmers et al. 2018) @10 is 99.4. mrr of conve (dettmers et al. 2018) mrr is 95.7. @1 of conve (dettmers et al. 2018) @1 is 79.7. @10 of conve (dettmers et al. 2018) @10 is 98.1. mrr of conve (dettmers et al. 2018) mrr is 87.1. @1 of conve (dettmers et al. 2018) @1 is 34.1. @10 of conve (dettmers et al. 2018) @10 is 62.2. mrr of conve (dettmers et al. 2018) mrr is 43.5. @1 of conve (dettmers et al. 2018) @1 is 40.3. @10 of conve (dettmers et al. 2018) @10 is 54.0. mrr of conve (dettmers et al. 2018) mrr is 44.9. @1 of conve (dettmers et al. 2018) @1 is 67.8. @10 of conve (dettmers et al. 2018) @10 is 88.6. mrr of conve (dettmers et al. 2018) mrr is 76.1. @1 of neurallp (yang et al. 2017) @1 is 64.3. @10 of neurallp (yang et al. 2017) @10 is 96.2. mrr of neurallp (yang et al. 2017) mrr is 77.8. @1 of neurallp (yang et al. 2017) @1 is 47.5. @10 of neurallp (yang et al. 2017) @10 is 91.2. mrr of neurallp (yang et al. 2017) mrr is 61.9. @1 of neurallp (yang et al. 2017) @1 is 16.6. @10 of neurallp (yang et al. 2017) @10 is 34.8. mrr of neurallp (yang et al. 2017) mrr is 22.7. @1 of neurallp (yang et al. 2017) @1 is 37.6. @10 of neurallp (yang et al. 2017) @10 is 65.7. mrr of neurallp (yang et al. 2017) mrr is 46.3. @1 of ntp-λ (rocktaschel et. al. 2017) @1 is 84.3. @10 of ntp-λ (rocktaschel et. al. 2017) @10 is 100. mrr of ntp-λ (rocktaschel et. al. 2017) mrr is 91.2. @1 of ntp-λ (rocktaschel et. al. 2017) @1 is 75.9. @10 of ntp-λ (rocktaschel et. al. 2017) @10 is 87.8. mrr of ntp-λ (rocktaschel et. al. 2017) mrr is 79.3. @1 of minerva (das et al. 2018) @1 is 72.8. @10 of minerva (das et al. 2018) @10 is 96.8. mrr of minerva (das et al. 2018) mrr is 82.5. @1 of minerva (das et al. 2018) @1 is 60.5. @10 of minerva (das et al. 2018) @10 is 92.4. mrr of minerva (das et al. 2018) mrr is 72.0. @1 of minerva (das et al. 2018) @1 is 21.7. @10 of minerva (das et al. 2018) @10 is 45.6. mrr of minerva (das et al. 2018) mrr is 29.3. @1 of minerva (das et al. 2018) @1 is 41.3. @10 of minerva (das et al. 2018) @10 is 51.3. mrr of minerva (das et al. 2018) mrr is 44.8. @1 of minerva (das et al. 2018) @1 is 66.3. @10 of minerva (das et al. 2018) @10 is 83.1. mrr of minerva (das et al. 2018) mrr is 72.5. @1 of ours(complex) @1 is 88.7. @10 of ours(complex) @10 is 98.5. mrr of ours(complex) mrr is 92.9. @1 of ours(complex) @1 is 81.1. @10 of ours(complex) @10 is 98.2. mrr of ours(complex) mrr is 87.8. @1 of ours(complex) @1 is 32.9. @10 of ours(complex) @10 is 54.4. mrr of ours(complex) mrr is 39.3. @1 of ours(complex) @1 is 43.7. @10 of ours(complex) @10 is 54.2. mrr of ours(complex) mrr is 47.2. @1 of ours(complex) @1 is 65.5. @10 of ours(complex) @10 is 83.6. mrr of ours(complex) mrr is 72.2. @1 of ours(conve) @1 is 90.2. @10 of ours(conve) @10 is 99.2. mrr of ours(conve) mrr is 94.0. @1 of ours(conve) @1 is 78.9. @10 of ours(conve) @10 is 98.2. mrr of ours(conve) mrr is 86.5. @1 of ours(conve) @1 is 32.7. @10 of ours(conve) @10 is 56.4. mrr of ours(conve) mrr is 40.7. @1 of ours(conve) @1 is 41.8. @10 of ours(conve) @10 is 51.7. mrr of ours(conve) mrr is 45.0. @1 of ours(conve) @1 is 65.6. @10 of ours(conve) @10 is 84.4. mrr of ours(conve) mrr is 72.7.
table 5 shows mrr evaluation of seen queries vs. accuracy of umls % is 97.2. accuracy of umls ours(conve) is 73.1. accuracy of umls -rs is 67.9 (-7%). accuracy of umls -ad is 61.4 (-16%). accuracy of umls % is 2.8. accuracy of umls ours(conve) is 68.5. accuracy of umls -rs is 61.5 (-10%). accuracy of umls -ad is 58.7 (-14%). accuracy of kinship % is 96.8. accuracy of kinship ours(conve) is 75.1. accuracy of kinship -rs is 66.5 (-11%). accuracy of kinship -ad is 65.8 (-12%). accuracy of kinship % is 3.2. accuracy of kinship ours(conve) is 73.6. accuracy of kinship -rs is 64.3 (-13%). accuracy of kinship -ad is 53.3 (-27%). accuracy of fb15k-237 % is 76.1. accuracy of fb15k-237 ours(conve) is 28.3. accuracy of fb15k-237 -rs is 24.3 (-14%). accuracy of fb15k-237 -ad is 20.6 (-27%). accuracy of fb15k-237 % is 23.9. accuracy of fb15k-237 ours(conve) is 70.9. accuracy of fb15k-237 -rs is 69.1 (-2%). accuracy of fb15k-237 -ad is 63.9 (-10%). accuracy of wn18rr % is 41.8. accuracy of wn18rr ours(conve) is 60.8. accuracy of wn18rr -rs is 62.0 (+2%). accuracy of wn18rr -ad is 53.4 (-12%). accuracy of wn18rr % is 58.2. accuracy of wn18rr ours(conve) is 31.5. accuracy of wn18rr -rs is 33.9 (+7%). accuracy of wn18rr -ad is 28.8 (-9%). accuracy of nell-995 % is 15.3. accuracy of nell-995 ours(conve) is 40.4. accuracy of nell-995 -rs is 45.9 (+14%). accuracy of nell-995 -ad is 42.5 (+5%). accuracy of nell-995 % is 84.7. accuracy of nell-995 ours(conve) is 85.5. accuracy of nell-995 -rs is 84.7 (-1%). accuracy of nell-995 -ad is 84.3 (-1%).
table 1 shows accuracy on pc for sig17+ship (the shared task baseline sig17 with ship), med+pt (med with paradigm transduction), med+pt+ship (med with paradigm transduction and ship), as well as all baselines (bl). accuracy of bl: copy set1 is .0810. accuracy of bl: copy set2 is .0810. accuracy of bl: copy set3 is .0810. accuracy of bl: med set1 is .0004. accuracy of bl: med set2 is .0432. accuracy of bl: med set3 is .4211. accuracy of bl: pt set1 is .0833. accuracy of bl: pt set2 is .0833. accuracy of bl: pt set3 is .0775. accuracy of bl: sig17 set1 is .5012. accuracy of bl: sig17 set2 is .6576. accuracy of bl: sig17 set3 is .7707. accuracy of sig17+ship set1 is .5971. accuracy of sig17+ship set2 is .7355. accuracy of sig17+ship set3 is .8008. accuracy of med+pt set1 is .5808. accuracy of med+pt set2 is .7486. accuracy of med+pt set3 is .8454. accuracy of med+pt+ship set1 is .5793. accuracy of med+pt+ship set2 is .7547. accuracy of med+pt+ship set3 is .8483.
table 3 shows ner results for monolingual experiments. f1 of char-ngrams + lemma + morph turkish is 68.06. f1 of char-ngrams + lemma + morph uyghur is 52.50. f1 of char-ngrams + lemma + morph hindi is 73.15. f1 of char-ngrams + lemma + morph bengali is 52.77. f1 of char-ngrams + lemma turkish is 68.61. f1 of char-ngrams + lemma uyghur is 52.40. f1 of char-ngrams + lemma hindi is 73.37. f1 of char-ngrams + lemma bengali is 52.09. f1 of char-ngrams + morph turkish is 67.97. f1 of char-ngrams + morph uyghur is 47.80. f1 of char-ngrams + morph hindi is 73.46. f1 of char-ngrams + morph bengali is 52.06. f1 of word + lemma turkish is 66.52. f1 of word + lemma uyghur is 46.00. f1 of word + lemma hindi is 71.82. f1 of word + lemma bengali is 50.03. f1 of word + morph turkish is 64.45. f1 of word + morph uyghur is 46.00. f1 of word + morph hindi is 71.52. f1 of word + morph bengali is 49.27. f1 of word + lemma + morph turkish is 68.46. f1 of word + lemma + morph uyghur is 47.70. f1 of word + lemma + morph hindi is 70.51. f1 of word + lemma + morph bengali is 48.16. f1 of char-ngrams turkish is 66.81. f1 of char-ngrams uyghur is 50.80. f1 of char-ngrams hindi is 72.67. f1 of char-ngrams bengali is 52.10. f1 of word turkish is 62.85. f1 of word uyghur is 46.80. f1 of word hindi is 72.04. f1 of word bengali is 49.83. f1 of no embedding turkish is 58.94. f1 of no embedding uyghur is 31.30. f1 of no embedding hindi is 59.89. f1 of no embedding bengali is 21.25.
table 1 shows mean accuracy of 10-fold cross validation in the hype-par setting. accuracy of lr baseline1 is .50. accuracy of lr qq is .64. accuracy of lr skip-gram is .68. accuracy of lr glove is .66. accuracy of lr skip-gram+qq is .72. accuracy of lr glove+qq is .69. accuracy of knn baseline1 is .50. accuracy of knn qq is .63. accuracy of knn skip-gram is .47. accuracy of knn glove is .43. accuracy of knn skip-gram+qq is .52. accuracy of knn glove+qq is .48. accuracy of nb baseline1 is .50. accuracy of nb qq is .66. accuracy of nb skip-gram is .69. accuracy of nb glove is .66. accuracy of nb skip-gram+qq is .69. accuracy of nb glove+qq is .68. accuracy of dt baseline1 is .50. accuracy of dt qq is .60. accuracy of dt skip-gram is .54. accuracy of dt glove is .53. accuracy of dt skip-gram+qq is .55. accuracy of dt glove+qq is .54. accuracy of svm baseline1 is .50. accuracy of svm qq is .64. accuracy of svm skip-gram is .15. accuracy of svm glove is .62. accuracy of svm skip-gram+qq is .63. accuracy of svm glove+qq is .64. accuracy of lda baseline1 is .50. accuracy of lda qq is .61. accuracy of lda skip-gram is .67. accuracy of lda glove is .65. accuracy of lda skip-gram+qq is .68. accuracy of lda glove+qq is .67.
table 4 shows cross-genre classification results on the training set of masc+wiki. macro of crf (friedrich et al. 2016) macro is 66.6. acc of crf (friedrich et al. 2016) acc is 71.8. f1 sta of crf (friedrich et al. 2016) f1 sta is 78.2. f1 eve of crf (friedrich et al. 2016) f1 eve is 77.0. f1 rep of crf (friedrich et al. 2016) f1 rep is 76.8. f1 geni of crf (friedrich et al. 2016) f1 geni is 44.8. f1 gena of crf (friedrich et al. 2016) f1 gena is 27.4. f1 que of crf (friedrich et al. 2016) f1 que is 81.8. f1 imp of crf (friedrich et al. 2016) f1 imp is 70.8. macro of clause-level bi-lstm macro is 69.3. acc of clause-level bi-lstm acc is 73.3. f1 sta of clause-level bi-lstm f1 sta is 79.5. f1 eve of clause-level bi-lstm f1 eve is 78.7. f1 rep of clause-level bi-lstm f1 rep is 82.8. f1 geni of clause-level bi-lstm f1 geni is 47.6. f1 gena of clause-level bi-lstm f1 gena is 31.9. f1 que of clause-level bi-lstm f1 que is 86.9. f1 imp of clause-level bi-lstm f1 imp is 77.7. macro of paragraph-level model macro is 73.2. acc of paragraph-level model acc is 77.2. f1 sta of paragraph-level model f1 sta is 81.5. f1 eve of paragraph-level model f1 eve is 80.1. f1 rep of paragraph-level model f1 rep is 83.2. f1 geni of paragraph-level model f1 geni is 64.7. f1 gena of paragraph-level model f1 gena is 37.2. f1 que of paragraph-level model f1 que is 88.1. f1 imp of paragraph-level model f1 imp is 77.8. macro of paragraph-level model+crf macro is 73.5. acc of paragraph-level model+crf acc is 77.4. f1 sta of paragraph-level model+crf f1 sta is 81.5. f1 eve of paragraph-level model+crf f1 eve is 80.3. f1 rep of paragraph-level model+crf f1 rep is 83.7. f1 geni of paragraph-level model+crf f1 geni is 66.5. f1 gena of paragraph-level model+crf f1 gena is 37.4. f1 que of paragraph-level model+crf f1 que is 88.5. f1 imp of paragraph-level model+crf f1 imp is 76.7.
table 4 shows reply structure recovery results in wikipedia conversation dataset. pnode of naive baseline pnode is 0.3223. rnode of naive baseline rnode is 0.6501. f1node of naive baseline f1node is 0.4310. pnode of hdhp pnode is 0.5598. rnode of hdhp rnode is 0.5834. f1node of hdhp f1node is 0.5714. pnode of hd-gmhp pnode is 0.6433. rnode of hd-gmhp rnode is 0.5468. f1node of hd-gmhp f1node is 0.5911.
table 5 shows timex/event labels generated by stage 1 (bottom). f1 of baseline-simple dev is .64. f1 of baseline-simple test is .68. f1 of baseline-simple dev is .47. f1 of baseline-simple test is .43. f1 of baseline-simple dev is .78. f1 of baseline-simple test is .79. f1 of baseline-simple dev is .39. f1 of baseline-simple test is .39. f1 of baseline-logistic dev is .81. f1 of baseline-logistic test is .79. f1 of baseline-logistic dev is .63. f1 of baseline-logistic test is .54. f1 of baseline-logistic dev is .74. f1 of baseline-logistic test is .74. f1 of baseline-logistic dev is .60. f1 of baseline-logistic test is .63. f1 of neural-basic dev is .78. f1 of neural-basic test is .75. f1 of neural-basic dev is .67. f1 of neural-basic test is .57. f1 of neural-basic dev is .72. f1 of neural-basic test is .74. f1 of neural-basic dev is .60. f1 of neural-basic test is .63. f1 of neural-enriched dev is .80. f1 of neural-enriched test is .78. f1 of neural-enriched dev is .67. f1 of neural-enriched test is .59. f1 of neural-enriched dev is .76. f1 of neural-enriched test is .77. f1 of neural-enriched dev is .63. f1 of neural-enriched test is .65. f1 of neural-attention dev is .83. f1 of neural-attention test is .81. f1 of neural-attention dev is .76. f1 of neural-attention test is .70. f1 of neural-attention dev is .79. f1 of neural-attention test is .79. f1 of neural-attention dev is .66. f1 of neural-attention test is .68. f1 of baseline-simple dev is .39. f1 of baseline-simple test is .40. f1 of baseline-simple dev is .26. f1 of baseline-simple test is .25. f1 of baseline-simple dev is .44. f1 of baseline-simple test is .47. f1 of baseline-simple dev is .27. f1 of baseline-simple test is .25. f1 of baseline-logistic dev is .36. f1 of baseline-logistic test is .34. f1 of baseline-logistic dev is .24. f1 of baseline-logistic test is .22. f1 of baseline-logistic dev is .43. f1 of baseline-logistic test is .49. f1 of baseline-logistic dev is .33. f1 of baseline-logistic test is .37. f1 of neural-basic dev is .37. f1 of neural-basic test is .36. f1 of neural-basic dev is .21. f1 of neural-basic test is .23. f1 of neural-basic dev is .42. f1 of neural-basic test is .45. f1 of neural-basic dev is .33. f1 of neural-basic test is .35. f1 of neural-enriched dev is .51. f1 of neural-enriched test is .52. f1 of neural-enriched dev is .32. f1 of neural-enriched test is .35. f1 of neural-enriched dev is .44. f1 of neural-enriched test is .49. f1 of neural-enriched dev is .33. f1 of neural-enriched test is .37. f1 of neural-attention dev is .54. f1 of neural-attention test is .54. f1 of neural-attention dev is .36. f1 of neural-attention test is .39. f1 of neural-attention dev is .44. f1 of neural-attention test is .49. f1 of neural-attention dev is .35. f1 of neural-attention test is .39.
table 2 shows comparison on datasets with the baselines. rmse of offset rmse is 0.979. mae of offset mae is 0.769. rmse of offset rmse is 1.247. mae of offset mae is 0.882. rmse of offset rmse is 1.389. mae of offset mae is 0.933. rmse of offset rmse is 1.401. mae of offset mae is 0.928. rmse of nmf rmse is 0.948. mae of nmf mae is 0.671. rmse of nmf rmse is 1.059. mae of nmf mae is 0.761. rmse of nmf rmse is 1.135. mae of nmf mae is 0.794. rmse of nmf rmse is 1.297. mae of nmf mae is 0.904. rmse of svd++ rmse is 0.922. mae of svd++ mae is 0.669. rmse of svd++ rmse is 1.026. mae of svd++ mae is 0.760. rmse of svd++ rmse is 1.049. mae of svd++ mae is 0.745. rmse of svd++ rmse is 1.194. mae of svd++ mae is 0.847. rmse of urp rmse is 1.006. mae of urp mae is 0.764. rmse of urp rmse is 1.126. mae of urp mae is 0.860. rmse of rmr rmse is 1.005. mae of rmr mae is 0.741. rmse of rmr rmse is 1.123. mae of rmr mae is 0.822. rmse of hft rmse is 0.924. mae of hft mae is 0.659. rmse of hft rmse is 1.040. mae of hft mae is 0.757. rmse of hft rmse is 0.997. mae of hft mae is 0.735. rmse of hft rmse is 1.110. mae of hft mae is 0.807. rmse of deepconn rmse is 0.943. mae of deepconn mae is 0.667. rmse of deepconn rmse is 1.045. mae of deepconn mae is 0.746. rmse of deepconn rmse is 1.014. mae of deepconn mae is 0.743. rmse of deepconn rmse is 1.109. mae of deepconn mae is 0.797. rmse of nrt rmse is 0.985. mae of nrt mae is 0.702. rmse of nrt rmse is 1.107. mae of nrt mae is 0.806. rmse of lrmm(+f) rmse is 0.886. mae of lrmm(+f) mae is 0.624. rmse of lrmm(+f) rmse is 0.988. mae of lrmm(+f) mae is 0.708. rmse of lrmm(+f) rmse is 0.983. mae of lrmm(+f) mae is 0.716. rmse of lrmm(+f) rmse is 1.052. mae of lrmm(+f) mae is 0.766. rmse of lrmm(-u) rmse is 0.936. mae of lrmm(-u) mae is 0.719. rmse of lrmm(-u) rmse is 1.058. mae of lrmm(-u) mae is 0.782. rmse of lrmm(-u) rmse is 1.086. mae of lrmm(-u) mae is 0.821. rmse of lrmm(-u) rmse is 1.138. mae of lrmm(-u) mae is 0.900. rmse of lrmm(-o) rmse is 0.931. mae of lrmm(-o) mae is 0.680. rmse of lrmm(-o) rmse is 1.039. mae of lrmm(-o) mae is 0.805. rmse of lrmm(-o) rmse is 1.074. mae of lrmm(-o) mae is 0.855. rmse of lrmm(-o) rmse is 1.101. mae of lrmm(-o) mae is 0.864. rmse of lrmm(-m) rmse is 0.887. mae of lrmm(-m) mae is 0.625. rmse of lrmm(-m) rmse is 0.989. mae of lrmm(-m) mae is 0.710. rmse of lrmm(-m) rmse is 0.991. mae of lrmm(-m) mae is 0.725. rmse of lrmm(-m) rmse is 1.053. mae of lrmm(-m) mae is 0.766. rmse of lrmm(-v) rmse is 0.886. mae of lrmm(-v) mae is 0.624. rmse of lrmm(-v) rmse is 0.989. mae of lrmm(-v) mae is 0.708. rmse of lrmm(-v) rmse is 0.991. mae of lrmm(-v) mae is 0.725. rmse of lrmm(-v) rmse is 1.052. mae of lrmm(-v) mae is 0.766.
table 3 shows the performance of training with missing modality imputation. rmse of lrmm(+f) rmse is 0.997. mae of lrmm(+f) mae is 0.790. rmse of lrmm(+f) rmse is 1.131. mae of lrmm(+f) mae is 0.912. rmse of lrmm(-u) rmse is 0.998. mae of lrmm(-u) mae is 0.795. rmse of lrmm(-u) rmse is 1.132. mae of lrmm(-u) mae is 0.914. rmse of lrmm(-o) rmse is 0.999. mae of lrmm(-o) mae is 0.796. rmse of lrmm(-o) rmse is 1.133. mae of lrmm(-o) mae is 0.917. rmse of lrmm(-m) rmse is 0.998. mae of lrmm(-m) mae is 0.797. rmse of lrmm(-m) rmse is 1.133. mae of lrmm(-m) mae is 0.913. rmse of lrmm(-v) rmse is 0.997. mae of lrmm(-v) mae is 0.791. rmse of lrmm(-v) rmse is 1.132. mae of lrmm(-v) mae is 0.913.
table 1 shows experimental results for ncg-idf@k and prec@k scores for different methods. ncg-idf @1 of - ncg-idf @1 is 14.86. ncg-idf @3 of - ncg-idf @3 is 14.50. ncg-idf @5 of - ncg-idf @5 is 14.61. ncg-idf @7 of - ncg-idf @7 is 14.56. prec @1 of - prec @1 is 36.76. prec @3 of - prec @3 is 32.69. prec @5 of - prec @5 is 31.12. prec @7 of - prec @7 is 30.23. model size of - model size is 28b. ncg-idf @1 of base ncg-idf @1 is 20.12. ncg-idf @3 of base ncg-idf @3 is 19.95. ncg-idf @5 of base ncg-idf @5 is 19.43. ncg-idf @7 of base ncg-idf @7 is 18.91. prec @1 of base prec @1 is 46.92. prec @3 of base prec @3 is 42.89. prec @5 of base prec @5 is 40.29. prec @7 of base prec @7 is 38.15. model size of base model size is 4.7gb. ncg-idf @1 of svd ncg-idf @1 is 19.99. ncg-idf @3 of svd ncg-idf @3 is 19.79. ncg-idf @5 of svd ncg-idf @5 is 19.20. ncg-idf @7 of svd ncg-idf @7 is 18.72. prec @1 of svd prec @1 is 46.68. prec @3 of svd prec @3 is 42.60. prec @5 of svd prec @5 is 39.88. prec @7 of svd prec @7 is 37.81. model size of svd model size is 0.1gb. ncg-idf @1 of base ncg-idf @1 is 22.92. ncg-idf @3 of base ncg-idf @3 is 22.77. ncg-idf @5 of base ncg-idf @5 is 22.17. ncg-idf @7 of base ncg-idf @7 is 21.86. prec @1 of base prec @1 is 51.50. prec @3 of base prec @3 is 47.81. prec @5 of base prec @5 is 44.97. prec @7 of base prec @7 is 43.19. model size of base model size is 4.7gb. ncg-idf @1 of svd ncg-idf @1 is 21.56. ncg-idf @3 of svd ncg-idf @3 is 21.00. ncg-idf @5 of svd ncg-idf @5 is 20.65. ncg-idf @7 of svd ncg-idf @7 is 20.31. prec @1 of svd prec @1 is 48.79. prec @3 of svd prec @3 is 44.44. prec @5 of svd prec @5 is 42.22. prec @7 of svd prec @7 is 40.54. model size of svd model size is 0.1gb. ncg-idf @1 of base ncg-idf @1 is 22.92. ncg-idf @3 of base ncg-idf @3 is 20.86. ncg-idf @5 of base ncg-idf @5 is 19.45. ncg-idf @7 of base ncg-idf @7 is 18.42. prec @1 of base prec @1 is 23.95. prec @3 of base prec @3 is 21.17. prec @5 of base prec @5 is 19.33. prec @7 of base prec @7 is 18.01. model size of base model size is 4.7gb. ncg-idf @1 of svd ncg-idf @1 is 18.22. ncg-idf @3 of svd ncg-idf @3 is 18.22. ncg-idf @5 of svd ncg-idf @5 is 18.03. ncg-idf @7 of svd ncg-idf @7 is 17.89. prec @1 of svd prec @1 is 26.49. prec @3 of svd prec @3 is 25.36. prec @5 of svd prec @5 is 24.31. prec @7 of svd prec @7 is 23.49. model size of svd model size is 0.1gb. ncg-idf @1 of base ncg-idf @1 is 31.30. ncg-idf @3 of base ncg-idf @3 is 31.12. ncg-idf @5 of base ncg-idf @5 is 31.07. ncg-idf @7 of base ncg-idf @7 is 31.02. prec @1 of base prec @1 is 54.85. prec @3 of base prec @3 is 52.62. prec @5 of base prec @5 is 51.09. prec @7 of base prec @7 is 49.79. model size of base model size is 1.8gb. ncg-idf @1 of idf ncg-idf @1 is 32.95. ncg-idf @3 of idf ncg-idf @3 is 32.24. ncg-idf @5 of idf ncg-idf @5 is 32.00. ncg-idf @7 of idf ncg-idf @7 is 31.62. prec @1 of idf prec @1 is 50.95. prec @3 of idf prec @3 is 49.07. prec @5 of idf prec @5 is 47.63. prec @7 of idf prec @7 is 46.57. model size of idf model size is 1.8gb. ncg-idf @1 of base ncg-idf @1 is 33.13. ncg-idf @3 of base ncg-idf @3 is 32.20. ncg-idf @5 of base ncg-idf @5 is 31.93. ncg-idf @7 of base ncg-idf @7 is 31.75. prec @1 of base prec @1 is 58.72. prec @3 of base prec @3 is 53.60. prec @5 of base prec @5 is 52.05. prec @7 of base prec @7 is 50.35. model size of base model size is 1.8gb. ncg-idf @1 of idf ncg-idf @1 is 33.89. ncg-idf @3 of idf ncg-idf @3 is 33.42. ncg-idf @5 of idf ncg-idf @5 is 33.02. ncg-idf @7 of idf ncg-idf @7 is 32.76. prec @1 of idf prec @1 is 51.99. prec @3 of idf prec @3 is 49.54. prec @5 of idf prec @5 is 48.40. prec @7 of idf prec @7 is 47.24. model size of idf model size is 1.8gb. ncg-idf @1 of - ncg-idf @1 is 35.31. ncg-idf @3 of - ncg-idf @3 is 34.39. ncg-idf @5 of - ncg-idf @5 is 33.74. ncg-idf @7 of - ncg-idf @7 is 33.15. prec @1 of - prec @1 is 69.98. prec @3 of - prec @3 is 63.78. prec @5 of - prec @5 is 60.16. prec @7 of - prec @7 is 57.37. model size of - model size is 150gb. ncg-idf @1 of - ncg-idf @1 is 36.19. ncg-idf @3 of - ncg-idf @3 is 34.94. ncg-idf @5 of - ncg-idf @5 is 34.12. ncg-idf @7 of - ncg-idf @7 is 33.31. prec @1 of - prec @1 is 55.09. prec @3 of - prec @3 is 51.51. prec @5 of - prec @5 is 49.50. prec @7 of - prec @7 is 47.85. model size of - model size is 150gb.
table 8 shows seu sentiment classification accuracy. accuracy of tseu la is 0.702. accuracy of tseu lad is 0.723. accuracy of tseu law is 0.733. accuracy of tseu l is 0.750. accuracy of tseu(d) la is 0.692. accuracy of tseu(d) lad is 0.715. accuracy of tseu(d) law is 0.716. accuracy of tseu(d) l is 0.735.
table 2 shows the performance comparisons of different methods on the three datasets, where the results of baseline methods are retrieved from published papers. acc of majority acc is 0.5350. macro-f1 of majority macro-f1 is 0.3333. acc of majority acc is 0.6500. macro-f1 of majority macro-f1 is 0.3333. acc of majority acc is 0.5000. macro-f1 of majority macro-f1 is 0.3333. acc of feature-svm acc is 0.7049. acc of feature-svm acc is 0.8016. acc of feature-svm acc is 0.6340. macro-f1 of feature-svm macro-f1 is 0.6330. acc of atae-lstm acc is 0.6870. acc of atae-lstm acc is 0.7720. acc of td-lstm acc is 0.7183. macro-f1 of td-lstm macro-f1 is 0.6843. acc of td-lstm acc is 0.7800. macro-f1 of td-lstm macro-f1 is 0.6673. acc of td-lstm acc is 0.6662. macro-f1 of td-lstm macro-f1 is 0.6401. acc of ian acc is 0.7210. acc of ian acc is 0.7860. acc of memnet acc is 0.7237. acc of memnet acc is 0.8032. acc of memnet acc is 0.6850. macro-f1 of memnet macro-f1 is 0.6691. acc of bilstm-att-g acc is 0.7312. macro-f1 of bilstm-att-g macro-f1 is 0.6980. acc of bilstm-att-g acc is 0.7973. macro-f1 of bilstm-att-g macro-f1 is 0.6925. acc of bilstm-att-g acc is 0.7038. macro-f1 of bilstm-att-g macro-f1 is 0.6837. acc of ram acc is 0.7449. macro-f1 of ram macro-f1 is 0.7135. acc of ram acc is 0.8023. macro-f1 of ram macro-f1 is 0.7080. acc of ram acc is 0.6936. macro-f1 of ram macro-f1 is 0.6730. acc of mgan acc is 0.7539. macro-f1 of mgan macro-f1 is 0.7247. acc of mgan acc is 0.8125. macro-f1 of mgan macro-f1 is 0.7194. acc of mgan acc is 0.7254. macro-f1 of mgan macro-f1 is 0.7081.
table 3 shows the performance comparisons of mgan variants. acc of mgan-c acc is 0.7273. macro-f1 of mgan-c macro-f1 is 0.6933. acc of mgan-c acc is 0.8054. macro-f1 of mgan-c macro-f1 is 0.7099. acc of mgan-c acc is 0.7153. macro-f1 of mgan-c macro-f1 is 0.6952. acc of mgan-f acc is 0.7398. macro-f1 of mgan-f macro-f1 is 0.7082. acc of mgan-f acc is 0.8000. macro-f1 of mgan-f macro-f1 is 0.7092. acc of mgan-f acc is 0.7110. macro-f1 of mgan-f macro-f1 is 0.6918. acc of mgan-cf acc is 0.7445. macro-f1 of mgan-cf macro-f1 is 0.7121. acc of mgan-cf acc is 0.8089. macro-f1 of mgan-cf macro-f1 is 0.7135. acc of mgan-cf acc is 0.7254*. macro-f1 of mgan-cf macro-f1 is 0.7081*. acc of mgan acc is 0.7539. macro-f1 of mgan macro-f1 is 0.7247. acc of mgan acc is 0.8125. macro-f1 of mgan macro-f1 is 0.7194. acc of mgan acc is 0.7254. macro-f1 of mgan macro-f1 is 0.7081.
table 1 shows experimental results on test datasets semeval2013 and semeval2014. acc of nbow-mlp acc is 65.18. f1 of nbow-mlp f1 is 60.94. acc of nbow-mlp acc is 85.44. f1 of nbow-mlp f1 is 82.30. acc of nbow-mlp acc is 65.68. f1 of nbow-mlp f1 is 60.35. acc of nbow-mlp acc is 89.44. f1 of nbow-mlp f1 is 81.60. acc of nbow-mlp acc is 76.44. f1 of nbow-mlp f1 is 71.30. acc of cnn acc is 71.41. f1 of cnn f1 is 68.23. acc of cnn acc is 85.74. f1 of cnn f1 is 82.60. acc of cnn acc is 70.05. f1 of cnn f1 is 66.22. acc of cnn acc is 89.86. f1 of cnn f1 is 82.09. acc of cnn acc is 79.27. f1 of cnn f1 is 74.79. acc of bilstm acc is 72.06. f1 of bilstm f1 is 70.00. acc of bilstm acc is 85.89. f1 of bilstm f1 is 82.79. acc of bilstm acc is 71.62. f1 of bilstm f1 is 68.34. acc of bilstm acc is 90.20. f1 of bilstm f1 is 83.09. acc of bilstm acc is 79.94. f1 of bilstm f1 is 76.06. acc of at-bilstm acc is 72.21. f1 of at-bilstm f1 is 69.89. acc of at-bilstm acc is 86.13. f1 of at-bilstm f1 is 83.22. acc of at-bilstm acc is 71.83. f1 of at-bilstm f1 is 68.01. acc of at-bilstm acc is 90.20. f1 of at-bilstm f1 is 83.46. acc of at-bilstm acc is 80.09. f1 of at-bilstm f1 is 76.15. acc of lexicon rnn acc is 69.97. f1 of lexicon rnn f1 is 68.69. acc of lexicon rnn acc is 86.43. f1 of lexicon rnn f1 is 83.54. acc of lexicon rnn acc is 70.75. f1 of lexicon rnn f1 is 67.06. acc of lexicon rnn acc is 91.13. f1 of lexicon rnn f1 is 84.60. acc of lexicon rnn acc is 79.57. f1 of lexicon rnn f1 is 75.97. acc of aglr acc is 73.27. f1 of aglr f1 is 71.79. acc of aglr acc is 86.72. f1 of aglr f1 is 84.18. acc of aglr acc is 73.29. f1 of aglr f1 is 70.48. acc of aglr acc is 90.37. f1 of aglr f1 is 84.15. acc of aglr acc is 80.91. f1 of aglr f1 is 77.65.
table 3 shows comparisons against top semeval systems. fpn of tweets top system is 69.02. fpn of tweets ours is 70.10. fpn of tweets top system is 70.96. fpn of tweets ours is 71.11. fpn of sarcasm top system is 56.50. fpn of sarcasm ours is 58.87. fpn of livejournal top system is 69.44. fpn of livejournal ours is 72.52. fpn of tweets top system is 63.30. fpn of tweets ours is 61.90. fpn of tweets (acc) top system is 64.60. fpn of tweets (acc) ours is 66.60.
table 2 shows the task and event settings performance. f1-task of uos-iti f1-task is 0.830. f1-event of uos-iti f1-event is 0.224. f1-task of mcg-ict f1-task is 0.942. f1-event of mcg-ict f1-event is 0.756. f1-task of certh-unitn f1-task is 0.911. f1-event of certh-unitn f1-event is 0.693. f1-task of tfg f1-task is 0.908. f1-event of tfg f1-event is 0.822. f1-task of bfg f1-task is 0.810. f1-event of bfg f1-event is 0.739. f1-task of combo f1-task is 0.899. f1-event of combo f1-event is 0.816.
table 5 shows rumor verification performance on the ccmr baidu, where indicates there is no webpage in that event. accuracy of hurricane sandy random is 0.247. accuracy of hurricane sandy transfer is 0.287. accuracy of boston marathon bombing random is 0.230. accuracy of boston marathon bombing transfer is 0.284. accuracy of sochi olympics random is 0.555. accuracy of sochi olympics transfer is 0.752. accuracy of mh flight 370 random is 0.407. accuracy of mh flight 370 transfer is 0.536. accuracy of bring back our girls random is 0.500. accuracy of bring back our girls transfer is 0.923. accuracy of columbian chemicals random is 0.000. accuracy of columbian chemicals transfer is 0.100. accuracy of passport hoax random is 0.000. accuracy of passport hoax transfer is 0.000. accuracy of rock elephant random is 0.000. accuracy of rock elephant transfer is 0.500. accuracy of underwater bedroom random is 0.577. accuracy of underwater bedroom transfer is 0.972. accuracy of pig fish random is 0.375. accuracy of pig fish transfer is 1.00. accuracy of solar eclipse random is 0.571. accuracy of solar eclipse transfer is 0.889. accuracy of girl with samurai boots random is 0.559. accuracy of girl with samurai boots transfer is 0.925. accuracy of nepal earthquake random is 0.227. accuracy of nepal earthquake transfer is 0.211. accuracy of garissa attack random is 0.125. accuracy of garissa attack transfer is 0.059. accuracy of avg random is 0.312. accuracy of avg transfer is 0.531.
table 1 shows rationale performance relative to human annotations. mse of sigmoid predictor mse is 0.029. acc. of sigmoid predictor acc. is 0.94. f1 of sigmoid predictor f1 is 0.74. mse of rnn predictor mse is 0.018. acc. of rnn predictor acc. is 0.95. f1 of rnn predictor f1 is 0.78. f1 of mean human performance f1 is 0.55. pr. of mean human performance pr. is 0.62. rec. of mean human performance rec. is 0.57. f1 of mean human performance f1 is 0.72. pr. of mean human performance pr. is 0.78. rec. of mean human performance rec. is 0.69. acc. of mean human performance acc. is --. f1 of sigmoid predictor + feature importance f1 is 0.20. pr. of sigmoid predictor + feature importance pr. is 0.62. rec. of sigmoid predictor + feature importance rec. is 0.12. f1 of sigmoid predictor + feature importance f1 is 0.64. pr. of sigmoid predictor + feature importance pr. is 0.59. rec. of sigmoid predictor + feature importance rec. is 0.70. mse of sigmoid predictor + feature importance mse is 0.029. acc. of sigmoid predictor + feature importance acc. is 0.94. f1 of sigmoid predictor + feature importance f1 is 0.74. f1 of rnn predictor + sigmoid generator f1 is 0.29. pr. of rnn predictor + sigmoid generator pr. is 0.22. rec. of rnn predictor + sigmoid generator rec. is 0.45. f1 of rnn predictor + sigmoid generator f1 is 0.31. pr. of rnn predictor + sigmoid generator pr. is 0.19. rec. of rnn predictor + sigmoid generator rec. is 0.92. mse of rnn predictor + sigmoid generator mse is 0.038. acc. of rnn predictor + sigmoid generator acc. is 0.91. f1 of rnn predictor + sigmoid generator f1 is 0.70. f1 of rnn predictor + lime f1 is 0.33. pr. of rnn predictor + lime pr. is 0.29. rec. of rnn predictor + lime rec. is 0.39. f1 of rnn predictor + lime f1 is 0.4. pr. of rnn predictor + lime pr. is 0.25. rec. of rnn predictor + lime rec. is 0.96. mse of rnn predictor + lime mse is 0.018. acc. of rnn predictor + lime acc. is 0.95. f1 of rnn predictor + lime f1 is 0.78. f1 of lei2016 f1 is 0.44. pr. of lei2016 pr. is 0.38. rec. of lei2016 rec. is 0.52. f1 of lei2016 f1 is 0.51. pr. of lei2016 pr. is 0.38. rec. of lei2016 rec. is 0.83. mse of lei2016 mse is 0.021. acc. of lei2016 acc. is 0.95. f1 of lei2016 f1 is 0.77. f1 of lei2016 + bias f1 is 0.49. pr. of lei2016 + bias pr. is 0.48. rec. of lei2016 + bias rec. is 0.49. f1 of lei2016 + bias f1 is 0.60. pr. of lei2016 + bias pr. is 0.46. rec. of lei2016 + bias rec. is 0.86. mse of lei2016 + bias mse is 0.02. acc. of lei2016 + bias acc. is 0.95. f1 of lei2016 + bias f1 is 0.77. f1 of lei2016 + bias + inverse (ean) f1 is 0.53. pr. of lei2016 + bias + inverse (ean) pr. is 0.48. rec. of lei2016 + bias + inverse (ean) rec. is 0.58. f1 of lei2016 + bias + inverse (ean) f1 is 0.61. pr. of lei2016 + bias + inverse (ean) pr. is 0.47. rec. of lei2016 + bias + inverse (ean) rec. is 0.87. mse of lei2016 + bias + inverse (ean) mse is 0.021. acc. of lei2016 + bias + inverse (ean) acc. is 0.95. f1 of lei2016 + bias + inverse (ean) f1 is 0.77.
table 7 shows results on classifying vague sentences. p (%) of baseline (majority) p (%) is 25.77. r (%) of baseline (majority) r (%) is 50.77. f (%) of baseline (majority) f (%) is 34.19. p (%) of lstm p (%) is 47.79. r (%) of lstm r (%) is 50.06. f (%) of lstm f (%) is 47.88. p (%) of cnn p (%) is 49.66. r (%) of cnn r (%) is 52.51. f (%) of cnn f (%) is 50.18. p (%) of ac-gan (full model) p (%) is 51.00. r (%) of ac-gan (full model) r (%) is 53.50. f (%) of ac-gan (full model) f (%) is 50.42. p (%) of ac-gan (vagueness only) p (%) is 52.90. r (%) of ac-gan (vagueness only) r (%) is 54.64. f (%) of ac-gan (vagueness only) f (%) is 52.34.
table 1 shows precision, recall, and f1 scores of our model mvdam on the test set compared with several baselines. p of - p is 34.53. r of - r is 34.59. f1 of - f1 is 34.53. p of title p is 59.53. r of title r is 59.42. f1 of title f1 is 59.12. p of title p is 59.26. r of title r is 59.40. f1 of title f1 is 59.24. p of network p is 68.28. r of network r is 56.54. f1 of network f1 is 55.10. p of content p is 69.85. r of content r is 68.72. f1 of content f1 is 68.92. p of title network p is 69.87. r of title network r is 69.71. f1 of title network f1 is 69.66. p of title content p is 70.84. r of title content r is 70.19. f1 of title content f1 is 69.54. p of title network content p is 80.10. r of title network content r is 79.56. f1 of title network content f1 is 79.67.
table 4 shows results for factuality and bias prediction. macro-f1 of - macro-f1 is 22.47. acc. of - acc. is 50.84. mae of - mae is 0.73. maem of - maem is 1.00. macro-f1 of - macro-f1 is 5.65. acc. of - acc. is 24.67. mae of - mae is 1.39. maem of - maem is 1.71. macro-f1 of 1 macro-f1 is 22.46. acc. of 1 acc. is 50.75. mae of 1 mae is 0.73. maem of 1 maem is 1.00. macro-f1 of 1 macro-f1 is 7.76. acc. of 1 acc. is 25.70. mae of 1 mae is 1.38. maem of 1 maem is 1.71. macro-f1 of 12 macro-f1 is 39.30. acc. of 12 acc. is 53.28. mae of 12 mae is 0.68. maem of 12 maem is 0.81. macro-f1 of 12 macro-f1 is 13.50. acc. of 12 acc. is 23.64. mae of 12 mae is 1.65. maem of 12 maem is 2.06. macro-f1 of 1 macro-f1 is 30.72. acc. of 1 acc. is 52.91. mae of 1 mae is 0.69. maem of 1 maem is 0.92. macro-f1 of 1 macro-f1 is 5.65. acc. of 1 acc. is 24.67. mae of 1 mae is 1.39. maem of 1 maem is 1.71. macro-f1 of 1 macro-f1 is 30.72. acc. of 1 acc. is 52.91. mae of 1 mae is 0.69. maem of 1 maem is 0.92. macro-f1 of 1 macro-f1 is 5.65. acc. of 1 acc. is 24.67. mae of 1 mae is 1.39. maem of 1 maem is 1.71. macro-f1 of 1 macro-f1 is 30.72. acc. of 1 acc. is 52.91. mae of 1 mae is 0.69. maem of 1 maem is 0.92. macro-f1 of 1 macro-f1 is 5.65. acc. of 1 acc. is 24.67. mae of 1 mae is 1.39. maem of 1 maem is 1.71. macro-f1 of 1 macro-f1 is 36.73. acc. of 1 acc. is 52.72. mae of 1 mae is 0.69. maem of 1 maem is 0.82. macro-f1 of 1 macro-f1 is 9.44. acc. of 1 acc. is 24.86. mae of 1 mae is 1.54. maem of 1 maem is 1.85. macro-f1 of 2 macro-f1 is 39.98. acc. of 2 acc. is 54.60. mae of 2 mae is 0.66. maem of 2 maem is 0.72. macro-f1 of 2 macro-f1 is 10.16. acc. of 2 acc. is 25.61. mae of 2 mae is 1.51. maem of 2 maem is 1.97. macro-f1 of 300 macro-f1 is 44.79. acc. of 300 acc. is 51.41. mae of 300 mae is 0.65. maem of 300 maem is 0.70. macro-f1 of 300 macro-f1 is 19.08. acc. of 300 acc. is 25.33. mae of 300 mae is 1.73. maem of 300 maem is 2.04. macro-f1 of 5 macro-f1 is 46.88. acc. of 5 acc. is 57.22. mae of 5 mae is 0.57. maem of 5 maem is 0.66. macro-f1 of 5 macro-f1 is 18.34. acc. of 5 acc. is 24.86. mae of 5 mae is 1.62. maem of 5 maem is 2.01. macro-f1 of 308 macro-f1 is 48.23. acc. of 308 acc. is 54.78. mae of 308 mae is 0.59. maem of 308 maem is 0.64. macro-f1 of 308 macro-f1 is 21.38. acc. of 308 acc. is 27.77. mae of 308 mae is 1.58. maem of 308 maem is 1.83. macro-f1 of 1 macro-f1 is 43.53. acc. of 1 acc. is 59.10. mae of 1 mae is 0.57. maem of 1 maem is 0.63. macro-f1 of 1 macro-f1 is 14.33. acc. of 1 acc. is 26.83. mae of 1 mae is 1.63. maem of 1 maem is 2.14. macro-f1 of 300 macro-f1 is 43.95. acc. of 300 acc. is 51.04. mae of 300 mae is 0.60. maem of 300 maem is 0.65. macro-f1 of 300 macro-f1 is 15.10. acc. of 300 acc. is 22.96. mae of 300 mae is 1.86. maem of 300 maem is 2.25. macro-f1 of 300 macro-f1 is 46.36. acc. of 300 acc. is 53.70. mae of 300 mae is 0.65. maem of 300 maem is 0.61. macro-f1 of 300 macro-f1 is 25.64. acc. of 300 acc. is 32.16. mae of 300 mae is 1.70. maem of 300 maem is 2.10. macro-f1 of 300 macro-f1 is 46.39. acc. of 300 acc. is 51.14. mae of 300 mae is 0.71. maem of 300 maem is 0.65. macro-f1 of 300 macro-f1 is 19.79. acc. of 300 acc. is 26.85. mae of 300 mae is 1.68. maem of 300 maem is 1.99. macro-f1 of 300 macro-f1 is 51.88. acc. of 300 acc. is 58.91. mae of 300 mae is 0.54. maem of 300 maem is 0.52. macro-f1 of 300 macro-f1 is 30.02. acc. of 300 acc. is 37.43. mae of 300 mae is 1.47. maem of 300 maem is 1.98. macro-f1 of 300 macro-f1 is 55.29. acc. of 300 acc. is 62.10. mae of 300 mae is 0.51. maem of 300 maem is 0.50. macro-f1 of 300 macro-f1 is 30.92. acc. of 300 acc. is 38.61. mae of 300 mae is 1.51. maem of 300 maem is 2.01. macro-f1 of 301 macro-f1 is 55.52. acc. of 301 acc. is 62.29. mae of 301 mae is 0.50. maem of 301 maem is 0.49. macro-f1 of 301 macro-f1 is 28.66. acc. of 301 acc. is 35.93. mae of 301 mae is 1.51. maem of 301 maem is 2.00. macro-f1 of 141 macro-f1 is 53.20. acc. of 141 acc. is 59.57. mae of 141 mae is 0.51. maem of 141 maem is 0.58. macro-f1 of 141 macro-f1 is 30.91. acc. of 141 acc. is 37.52. mae of 141 mae is 1.29. maem of 141 maem is 1.53. macro-f1 of 141 macro-f1 is 58.02. acc. of 141 acc. is 64.35. mae of 141 mae is 0.43. maem of 141 maem is 0.51. macro-f1 of 141 macro-f1 is 36.63. acc. of 141 acc. is 41.74. mae of 141 mae is 1.15. maem of 141 maem is 1.43.
table 1 shows r2 (variance explained) of residualized factor adaptation (rfa) versus baseline models. r2 of hd - is 0.585. r2 of hd controls only is 0.423. r2 of hd added-controls is 0.590. r2 of hd rc is 0.620. r2 of hd fa is 0.628. r2 of hd rfa is 0.638. r2 of hd controls only is 0.515. r2 of hd added-controls is 0.597. r2 of hd rc is 0.630. r2 of hd fa is 0.636. r2 of hd rfa is 0.657*. r2 of fp - is 0.602. r2 of fp controls only is 0.434. r2 of fp added-controls is 0.606. r2 of fp rc is 0.619. r2 of fp fa is 0.647. r2 of fp rfa is 0.647. r2 of fp controls only is 0.609. r2 of fp added-controls is 0.632. r2 of fp rc is 0.657. r2 of fp fa is 0.685. r2 of fp rfa is 0.680. r2 of ls - is 0.214. r2 of ls controls only is 0.148. r2 of ls added-controls is 0.219. r2 of ls rc is 0.292. r2 of ls fa is 0.308. r2 of ls rfa is 0.338. r2 of ls controls only is 0.326. r2 of ls added-controls is 0.352. r2 of ls rc is 0.376. r2 of ls fa is 0.353. r2 of ls rfa is 0.396*. r2 of ip - is 0.245. r2 of ip controls only is 0.072. r2 of ip added-controls is 0.243. r2 of ip rc is 0.266. r2 of ip fa is 0.274. r2 of ip rfa is 0.307. r2 of ip controls only is 0.240. r2 of ip added-controls is 0.226. r2 of ip rc is 0.330. r2 of ip fa is 0.344. r2 of ip rfa is 0.402*. r2 of fc - is 0.153. r2 of fc controls only is 0.128. r2 of fc added-controls is 0.156. r2 of fc rc is 0.197. r2 of fc fa is 0.218. r2 of fc rfa is 0.238. r2 of fc controls only is 0.160. r2 of fc added-controls is 0.161. r2 of fc rc is 0.209. r2 of fc fa is 0.240. r2 of fc rfa is 0.276*. r2 of avg. - is 0.360. r2 of avg. controls only is 0.241. r2 of avg. added-controls is 0.362. r2 of avg. rc is 0.398. r2 of avg. fa is 0.415. r2 of avg. rfa is 0.434. r2 of avg. controls only is 0.370. r2 of avg. added-controls is 0.394. r2 of avg. rc is 0.440. r2 of avg. fa is 0.452. r2 of avg. rfa is 0.482*.
table 2 shows results: content features accuracy of in-domain binary is 91.07. accuracy of in-domain families is 83.51. accuracy of in-domain nli is 70.26. accuracy of out-of-domain binary is 81.49. accuracy of out-of-domain families is 65.37. accuracy of out-of-domain nli is 35.99.
table 4 shows results: grammar and spelling features accuracy of in-domain binary is 72.93. accuracy of in-domain families is 55.59. accuracy of in-domain nli is 26.74. accuracy of out-of-domain binary is 70.24. accuracy of out-of-domain families is 47.23. accuracy of out-of-domain nli is 14.15.
table 5 shows results: centrality features accuracy of in-domain binary is 57.92. accuracy of in-domain families is 32.39. accuracy of in-domain nli is 5.75. accuracy of out-of-domain binary is 56.29. accuracy of out-of-domain families is 30.70. accuracy of out-of-domain nli is 5.60.
table 3 shows bleu scores. bleu of left 0 is 10.17. bleu of left 1 is 10.71. bleu of left δ is 0.54. bleu of left 0 is 9.41. bleu of left 1 is 10.41. bleu of left δ is 1.00. bleu of right 0 is 8.39. bleu of right 1 is 9.25. bleu of right δ is 0.86. bleu of right 0 is 7.83. bleu of right 1 is 8.45. bleu of right δ is 0.62. bleu of left 0 is 7.90. bleu of left 1 is 9.43. bleu of left δ is 1.53. bleu of left 0 is 7.11. bleu of left 1 is 10.71. bleu of left δ is 3.60. bleu of right 0 is 6.60. bleu of right 1 is 8.36. bleu of right δ is 1.76. bleu of right 0 is 6.45. bleu of right 1 is 8.37. bleu of right δ is 1.92. bleu of left 0 is 7.41. bleu of left 1 is 9.11. bleu of left δ is 1.70. bleu of left 0 is 7.01. bleu of left 1 is 9.83. bleu of left δ is 2.82. bleu of right 0 is 5.91. bleu of right 1 is 8.55. bleu of right δ is 2.64. bleu of right 0 is 5.77. bleu of right 1 is 7.54. bleu of right δ is 1.77.
table 2 shows results of variance reduction of gradient estimation. bleu of rl  en-de is 27.23. bleu of rl en-zh is 34.47. bleu of rl zh-en is 24.72. bleu of rl (baseline function)  en-de is 27.25. bleu of rl (baseline function) en-zh is 34.43. bleu of rl (baseline function) zh-en is 24.73.
table 3 shows results with source monolingual data. bleu of [b] (mle) valid is 22.32. bleu of [b] (mle) test is 24.29. bleu of [b] (mle) + [b] (rl) valid is 22.87. bleu of [b] (mle) + [b] (rl) test is 25.04. bleu of [b] (mle) + [ms] (rl) valid is 23.03. bleu of [b] (mle) + [ms] (rl) test is 25.22. bleu of [b & ms] (mle) valid is 24.31. bleu of [b & ms] (mle) test is 25.31. bleu of [b & ms] (mle) + [b & ms] (rl) valid is 24.58. bleu of [b & ms] (mle) + [b & ms] (rl) test is 25.60.
table 5 shows results of sequential approach for monolingual data. bleu of [b & ms] (mle) valid is 24.31. bleu of [b & ms] (mle) test is 25.31. bleu of [b & ms] (mle) + [b & ms] (rl) valid is 24.58. bleu of [b & ms] (mle) + [b & ms] (rl) test is 25.60. bleu of [b & ms] (mle) + [mt] (rl) valid is 24.61. bleu of [b & ms] (mle) + [mt] (rl) test is 25.72. bleu of [b & mt] (mle) valid is 24.14. bleu of [b & mt] (mle) test is 25.24. bleu of [b & mt] (mle) + [b & mt] (rl) valid is 24.41. bleu of [b & mt] (mle) + [b & mt] (rl) test is 25.58. bleu of [b & mt] (mle) + [ms] (rl) valid is 24.75. bleu of [b & mt] (mle) + [ms] (rl) test is 25.92.
table 3 shows results of the proposed method in comparison to supervised systems (bleu). bleu of nmt (transformer) en-fr is 41.8. bleu of nmt (transformer) en-de is 28.4. bleu of wmt best fr-en is 35.0. bleu of wmt best en-fr is 35.8. bleu of wmt best de-en is 29.0. bleu of wmt best en-de is 20.6. bleu of wmt best de-en is 40.2. bleu of wmt best en-de is 34.2. bleu of supervised smt (europarl) fr-en is 30.61. bleu of supervised smt (europarl) en-fr is 30.82. bleu of supervised smt (europarl) de-en is 20.83. bleu of supervised smt (europarl) en-de is 16.60. bleu of supervised smt (europarl) de-en is 26.38. bleu of supervised smt (europarl) en-de is 22.12. bleu of + w/o lexical reord. fr-en is 30.54. bleu of + w/o lexical reord. en-fr is 30.33. bleu of + w/o lexical reord. de-en is 20.37. bleu of + w/o lexical reord. en-de is 16.34. bleu of + w/o lexical reord. de-en is 25.99. bleu of + w/o lexical reord. en-de is 22.20. bleu of + constrained vocab. fr-en is 30.04. bleu of + constrained vocab. en-fr is 30.10. bleu of + constrained vocab. de-en is 19.91. bleu of + constrained vocab. en-de is 16.32. bleu of + constrained vocab. de-en is 25.66. bleu of + constrained vocab. en-de is 21.53. bleu of + unsup. tuning fr-en is 29.32. bleu of + unsup. tuning en-fr is 29.46. bleu of + unsup. tuning de-en is 17.75. bleu of + unsup. tuning en-de is 15.45. bleu of + unsup. tuning de-en is 23.35. bleu of + unsup. tuning en-de is 19.86. bleu of proposed system fr-en is 25.87. bleu of proposed system en-fr is 26.22. bleu of proposed system de-en is 17.43. bleu of proposed system en-de is 14.08. bleu of proposed system de-en is 23.05. bleu of proposed system en-de is 18.23.
table 4 shows experimental results for the identification of aspect segments (top) and the retrieval of salient segments (bottom) on oposum’s six product domains and overall (avg). f1 of majority l. bags is 37.9. f1 of majority b/t h/s is 39.8. f1 of majority boots is 37.1. f1 of majority keyb/s is 43.2. f1 of majority tvs is 41.7. f1 of majority vac/s is 41.6. f1 of majority avg is 40.2. f1 of abae l. bags is 38.1. f1 of abae b/t h/s is 37.6. f1 of abae boots is 35.2. f1 of abae keyb/s is 38.6. f1 of abae tvs is 39.5. f1 of abae vac/s is 38.1. f1 of abae avg is 37.9. f1 of abaeinit l. bags is 41.6. f1 of abaeinit b/t h/s is 48.5. f1 of abaeinit boots is 41.2. f1 of abaeinit keyb/s is 41.3. f1 of abaeinit tvs is 45.7. f1 of abaeinit vac/s is 40.6. f1 of abaeinit avg is 43.2. f1 of mate l. bags is 46.2. f1 of mate b/t h/s is 52.2. f1 of mate boots is 45.6. f1 of mate keyb/s is 43.5. f1 of mate tvs is 48.8. f1 of mate vac/s is 42.3. f1 of mate avg is 46.4. f1 of mate+mt l. bags is 48.6. f1 of mate+mt b/t h/s is 54.5. f1 of mate+mt boots is 46.4. f1 of mate+mt keyb/s is 45.3. f1 of mate+mt tvs is 51.8. f1 of mate+mt vac/s is 47.7. f1 of mate+mt avg is 49.1. f1 of milnet l. bags is 21.8 / 40.0. f1 of milnet b/t h/s is 19.8 / 36.7. f1 of milnet boots is 17.0 / 39.3. f1 of milnet keyb/s is 14.1 / 28.0. f1 of milnet tvs is 14.3 / 36.0. f1 of milnet vac/s is 14.6 / 31.3. f1 of milnet avg is 16.9 / 35.2. f1 of abaeinit l. bags is 19.9 / 48.5. f1 of abaeinit b/t h/s is 27.5 / 49.7. f1 of abaeinit boots is 13.8 / 28.1. f1 of abaeinit keyb/s is 19.0 / 44.9. f1 of abaeinit tvs is 16.8 / 42.4. f1 of abaeinit vac/s is 16.1 / 34.0. f1 of abaeinit avg is 18.8 / 41.3. f1 of mate l. bags is 23.0 / 57.1. f1 of mate b/t h/s is 30.9 / 50.7. f1 of mate boots is 15.4 / 31.9. f1 of mate keyb/s is 21.0 / 43.1. f1 of mate tvs is 18.7 / 44.7. f1 of mate vac/s is 19.9 / 44.0. f1 of mate avg is 21.5 / 45.2. f1 of mate+mt l. bags is 26.3 / 60.8. f1 of mate+mt b/t h/s is 37.5 / 66.7. f1 of mate+mt boots is 17.3 / 33.6. f1 of mate+mt keyb/s is 20.9 / 44.9. f1 of mate+mt tvs is 23.6 / 48.0. f1 of mate+mt vac/s is 22.4 / 43.9. f1 of mate+mt avg is 24.7 / 49.6. f1 of milnet+abaeinit l. bags is 27.1 / 56.0. f1 of milnet+abaeinit b/t h/s is 33.5 / 66.5. f1 of milnet+abaeinit boots is 19.3 / 34.8. f1 of milnet+abaeinit keyb/s is 22.4 / 51.7. f1 of milnet+abaeinit tvs is 19.0 / 43.7. f1 of milnet+abaeinit vac/s is 20.8 / 43.5. f1 of milnet+abaeinit avg is 23.7 / 49.4. f1 of milnet+mate l. bags is 28.2 / 54.7. f1 of milnet+mate b/t h/s is 36.0 / 66.5. f1 of milnet+mate boots is 21.7 / 39.3. f1 of milnet+mate keyb/s is 24.0 / 52.0. f1 of milnet+mate tvs is 20.8 / 46.1. f1 of milnet+mate vac/s is 23.5 / 49.3. f1 of milnet+mate avg is 25.7 / 51.3. f1 of milnet+mate+mt l. bags is 32.1 / 69.2. f1 of milnet+mate+mt b/t h/s is 40.0 / 74.7. f1 of milnet+mate+mt boots is 23.3 / 40.4. f1 of milnet+mate+mt keyb/s is 24.8 / 56.4. f1 of milnet+mate+mt tvs is 23.8 / 52.8. f1 of milnet+mate+mt vac/s is 26.0 / 53.1. f1 of milnet+mate+mt avg is 28.3 / 57.8.
table 5 shows summarization results on oposum. rouge-1 of random rouge-1 is 35.1. rouge-2 of random rouge-2 is 11.3. rouge-l of random rouge-l is 34.3. rouge-1 of lead rouge-1 is 35.5. rouge-2 of lead rouge-2 is 15.2. rouge-l of lead rouge-l is 34.8. rouge-1 of sumbasic rouge-1 is 34.0. rouge-2 of sumbasic rouge-2 is 11.2. rouge-l of sumbasic rouge-l is 32.6. rouge-1 of lexrank rouge-1 is 37.7. rouge-2 of lexrank rouge-2 is 14.1. rouge-l of lexrank rouge-l is 36.6. rouge-1 of opinosis rouge-1 is 36.8. rouge-2 of opinosis rouge-2 is 14.3. rouge-l of opinosis rouge-l is 35.7. rouge-1 of opinosis+mate+mt rouge-1 is 38.7. rouge-2 of opinosis+mate+mt rouge-2 is 15.8. rouge-l of opinosis+mate+mt rouge-l is 37.4. rouge-1 of milnet+mate+mt rouge-1 is 43.5. rouge-2 of milnet+mate+mt rouge-2 is 21.7. rouge-l of milnet+mate+mt rouge-l is 42.8. rouge-1 of milnet+mate+mt+rd rouge-1 is 44.1. rouge-2 of milnet+mate+mt+rd rouge-2 is 21.8. rouge-l of milnet+mate+mt+rd rouge-l is 43.3. rouge-1 of inter-annotator agreement rouge-1 is 54.7. rouge-2 of inter-annotator agreement rouge-2 is 36.6. rouge-l of inter-annotator agreement rouge-l is 53.9.
table 2 shows substitution ranking evaluation on english lexical simplification shared-task of semeval 2012. p@1 of biran et al. (2011) p@1 is 51.3. pearson of biran et al. (2011) pearson is 0.505. p@1 of jauhar & specia (2012) p@1 is 60.2. pearson of jauhar & specia (2012) pearson is 0.575. p@1 of kajiwara et al. (2013) p@1 is 60.4. pearson of kajiwara et al. (2013) pearson is 0.649. p@1 of horn et al. (2014) p@1 is 63.9. pearson of horn et al. (2014) pearson is 0.673. p@1 of glavaš & štajner (2015) p@1 is 63.2. pearson of glavaš & štajner (2015) pearson is 0.644. p@1 of boundary ranker p@1 is 65.3. pearson of boundary ranker pearson is 0.677. p@1 of paetzold & specia (2017) p@1 is 65.6. pearson of paetzold & specia (2017) pearson is 0.679. p@1 of nrrall p@1 is 65.4. pearson of nrrall pearson is 0.682. p@1 of nrrall+binning p@1 is 66.6. pearson of nrrall+binning pearson is 0.702*. p@1 of nrrall+binning+wc p@1 is 67.3*. pearson of nrrall+binning+wc pearson is 0.714*.
table 4 shows cross-validation accuracy and precision of our neural readability ranking (nrr) model used to create simpleppdb++, in comparison to the simpleppdb and other baselines. acc. of google ngram frequency acc. is 49.4. p+1 of google ngram frequency p+1 is 53.7. p-1 of google ngram frequency p-1 is 54.0. acc. of number of syllables acc. is 50.1. p+1 of number of syllables p+1 is 53.8. p-1 of number of syllables p-1 is 53.3. acc. of character & word length acc. is 56.2. p+1 of character & word length p+1 is 55.7. p-1 of character & word length p-1 is 56.1. acc. of w2v acc. is 60.4. p+1 of w2v p+1 is 54.9. p-1 of w2v p-1 is 53.1. acc. of simpleppdb acc. is 62.1. p+1 of simpleppdb p+1 is 57.6. p-1 of simpleppdb p-1 is 57.8. acc. of nrrall acc. is 59.4. p+1 of nrrall p+1 is 61.8. p-1 of nrrall p-1 is 57.7. acc. of nrrall+binning acc. is 64.1. p+1 of nrrall+binning p+1 is 62.1. p-1 of nrrall+binning p-1 is 59.8. acc. of nrrall+binning+wc acc. is 65.3*. p+1 of nrrall+binning+wc p+1 is 65.0*. p-1 of nrrall+binning+wc p-1 is 61.8*.
table 5 shows substitution generation evaluation with mean average precision, precision@1 and the average number of paraphrases generated per target for each method. map of glavas map is 22.8. p@1 of glavas p@1 is 13.5. #pps of wordnet #pps is 6.63. map of wordnet map is 62.2. p@1 of wordnet p@1 is 50.6. #pps of kauchak #pps is 4.39. map of kauchak map is 76.4†. p@1 of kauchak p@1 is 68.9. #pps of simpleppdb #pps is 8.77. map of simpleppdb map is 67.8. p@1 of simpleppdb p@1 is 78.0. #pps of simpleppdb++ #pps is 9.52. map of simpleppdb++ map is 69.1. p@1 of simpleppdb++ p@1 is 80.2.
table 6 shows evaluation on two datasets for english complex word identification. g-score of length g-score is 47.8. f-score of length f-score is 10.7. accuracy of length accuracy is 33.2. g-score of length g-score is 70.8. f-score of length f-score is 65.9. accuracy of length accuracy is 67.7. g-score of senses g-score is 57.9. f-score of senses f-score is 12.5. accuracy of senses accuracy is 43.6. g-score of senses g-score is 67.7. f-score of senses f-score is 62.3. accuracy of senses accuracy is 54.1. g-score of simplewiki g-score is 69.7. f-score of simplewiki f-score is 16.2. accuracy of simplewiki accuracy is 58.3. g-score of simplewiki g-score is 73.1. f-score of simplewiki f-score is 66.3. accuracy of simplewiki accuracy is 61.6. g-score of nearestcentroid g-score is 66.1. f-score of nearestcentroid f-score is 14.8. accuracy of nearestcentroid accuracy is 53.6. g-score of nearestcentroid g-score is 75.1. f-score of nearestcentroid f-score is 66.6. accuracy of nearestcentroid accuracy is 76.7. g-score of sv000gg g-score is 77.3. f-score of sv000gg f-score is 24.3. accuracy of sv000gg accuracy is 77.6. g-score of sv000gg g-score is 74.9. f-score of sv000gg f-score is 73.8. accuracy of sv000gg accuracy is 78.7. g-score of wc-only g-score is 68.5. f-score of wc-only f-score is 30.5. accuracy of wc-only accuracy is 87.7. g-score of wc-only g-score is 71.1. f-score of wc-only f-score is 67.5. accuracy of wc-only accuracy is 69.8. g-score of nearestcentroid+wc g-score is 70.2. f-score of nearestcentroid+wc f-score is 16.6. accuracy of nearestcentroid+wc accuracy is 61.8. g-score of nearestcentroid+wc g-score is 77.3. f-score of nearestcentroid+wc f-score is 68.8. accuracy of nearestcentroid+wc accuracy is 78.1. g-score of sv000gg+wc g-score is 78.1. f-score of sv000gg+wc f-score is 26.3. accuracy of sv000gg+wc accuracy is 80.0. g-score of sv000gg+wc g-score is 75.4. f-score of sv000gg+wc f-score is 74.8. accuracy of sv000gg+wc accuracy is 80.2.
table 2 shows propbank ssrl results, using gold predicates, on conll 2012 test. f1 of zhou and xu (2015) f1 is 81.3. prec.  of he et al. (2017) prec. is 81.7. rec. of he et al. (2017) rec. is 81.6. f1 of he et al. (2017) f1 is 81.7. prec.  of he et al. (2018a) prec. is 83.9. rec. of he et al. (2018a) rec. is 73.7. f1 of he et al. (2018a) f1 is 82.1. prec.  of tan et al. (2018) prec. is 81.9. rec. of tan et al. (2018) rec. is 83.6. f1 of tan et al. (2018) f1 is 82.7. prec.  of semi-crf baseline prec. is 84.8. rec. of semi-crf baseline rec. is 81.2. f1 of semi-crf baseline f1 is 83.0. prec.  of + common nonterminals prec. is 85.1. rec. of + common nonterminals rec. is 82.6. f1 of + common nonterminals f1 is 83.8.
table 9 shows f-scores of the baseline and the bothretrained models relative to role types on the two data sets. f-scores of baseline a0 is 67.95. f-scores of baseline a1 is 71.21. f-scores of baseline a2 is 51.43. f-scores of baseline am is 70.20. f-scores of both retrained a0 is 70.62. f-scores of both retrained a1 is 74.75. f-scores of both retrained a2 is 64.29. f-scores of both retrained am is 72.22. f-scores of baseline a0 is 69.49. f-scores of baseline a1 is 79.78. f-scores of baseline a2 is 61.84. f-scores of baseline am is 71.74. f-scores of both retrained a0 is 73.15. f-scores of both retrained a1 is 80.90. f-scores of both retrained a2 is 63.35. f-scores of both retrained am is 73.02.
table 1 shows performance of the original system when interacting with different user simulators. succ. of 0.00 succ. is 0.962. turn of 0.00 turn is 13.6. reward of 0.00 reward is 3.94. succ. of 0.00 succ. is 0.901. turn of 0.00 turn is 13.2. reward of 0.00 reward is 2.95. satis. of 0.00 satis. is 0.57. succ. of 0.05 succ. is 0.937. turn of 0.05 turn is 13.7. reward of 0.05 reward is 3.41. succ. of 0.05 succ. is 0.877. turn of 0.05 turn is 14.4. reward of 0.05 reward is 2.41. satis. of 0.05 satis. is 0.48. succ. of 0.10 succ. is 0.910. turn of 0.10 turn is 14.3. reward of 0.10 reward is 2.65. succ. of 0.10 succ. is 0.841. turn of 0.10 turn is 13.9. reward of 0.10 reward is 1.41. satis. of 0.10 satis. is 0.47. succ. of 0.20 succ. is 0.845. turn of 0.20 turn is 15.2. reward of 0.20 reward is 0.58. succ. of 0.20 succ. is 0.784. turn of 0.20 turn is 14.7. reward of 0.20 reward is 0.01. satis. of 0.20 satis. is 0.47.
table 1 shows results of independent training for slot filling in terms of f1-score. f1-score of crf (mesnil et al. 2013) f1-score is 92.94. f1-score of simple rnn (yao et al. 2013) f1-score is 94.11. f1-score of cnn-crf (xu and sarikaya, 2013) f1-score is 94.35. f1-score of lstm (yao et al. 2013) f1-score is 94.85. f1-score of rnn-sop (liu and lane 2015) f1-score is 84.89. f1-score of deep lstm (yao et al. 2013) f1-score is 95.08. f1-score of rnn-em (peng et al. 2015) f1-score is 95.25. f1-score of bi-rnn with ranking loss (vu et al. 2016) f1-score is 95.47. f1-score of encoder-labeler deep lstm (kurata et al. 2016) f1-score is 95.66. f1-score of attention birnn (liu and lane 2016a) f1-score is 95.75. f1-score of blstm-lstm (focus) (zhu and yu, 2017) f1-score is 95.79. f1-score of our model f1-score is 96.35.
table 4 shows feature ablation comparison of our proposed model on atis. f1-score of w/o char-embedding f1-score is 96.30. error(%) of w/o char-embedding error(%) is 1.23. f1-score of w/o self-attention f1-score is 96.26. error(%) of w/o self-attention error(%) is 1.34. f1-score of w/o attention-gating f1-score is 96.25. error(%) of w/o attention-gating error(%) is 1.46. f1-score of full model f1-score is 96.52. error(%) of full model error(%) is 1.23.
table 2 shows test results for various models on permuted-babi dialog task. accuracy of memn2n per-turn is 91.8. accuracy of memn2n per-dialog is 22. accuracy of memn2n per-turn is 93.3. accuracy of memn2n per-dialog is 30.3. accuracy of memn2n + all-answers per-turn is 88.5. accuracy of memn2n + all-answers per-dialog is 14.9. accuracy of memn2n + all-answers per-turn is 92.5. accuracy of memn2n + all-answers per-dialog is 26.4. accuracy of mask-memn2n per-turn is 93.4. accuracy of mask-memn2n per-dialog is 32. accuracy of mask-memn2n per-turn is 95.2. accuracy of mask-memn2n per-dialog is 47.3. accuracy of oov: memn2n per-turn is 63.4. accuracy of oov: memn2n per-dialog is 0.5. accuracy of oov: memn2n per-turn is 78.1. accuracy of oov: memn2n per-dialog is 0.6. accuracy of oov: memn2n + all-answers per-turn is 60.8. accuracy of oov: memn2n + all-answers per-dialog is 0.5. accuracy of oov: memn2n + all-answers per-turn is 74.9. accuracy of oov: memn2n + all-answers per-dialog is 0.6. accuracy of oov: mask-memn2n per-turn is 63.0. accuracy of oov: mask-memn2n per-dialog is 0.5. accuracy of oov: mask-memn2n per-turn is 80.1. accuracy of oov: mask-memn2n per-dialog is 1.
table 3 shows ablation study of our proposed model on permuted-babi dialog task. accuracy of mask-memn2n per-turn is 93.4. accuracy of mask-memn2n per-dialog is 32. accuracy of mask-memn2n (w/o entropy) per-turn is 92.1. accuracy of mask-memn2n (w/o entropy) per-dialog is 24.6. accuracy of mask-memn2n (w/o l2 mask pre-training) per-turn is 85.8. accuracy of mask-memn2n (w/o l2 mask pre-training) per-dialog is 2.2. accuracy of mask-memn2n (reinforcement learning phase only) per-turn is 16.0. accuracy of mask-memn2n (reinforcement learning phase only) per-dialog is 0.
table 2 shows performances on quora datasets. rouge-1 of seq2seq rouge-1 is 58.77. rouge-2 of seq2seq rouge-2 is 31.47. bleu of seq2seq bleu is 36.55. meteor of seq2seq meteor is 26.28. rouge-1 of seq2seq rouge-1 is 47.22. rouge-2 of seq2seq rouge-2 is 20.72. bleu of seq2seq bleu is 26.06. meteor of seq2seq meteor is 20.35. rouge-1 of residual lstm rouge-1 is 59.21. rouge-2 of residual lstm rouge-2 is 32.43. bleu of residual lstm bleu is 37.38. meteor of residual lstm meteor is 28.17. rouge-1 of residual lstm rouge-1 is 48.55. rouge-2 of residual lstm rouge-2 is 22.48. bleu of residual lstm bleu is 27.32. meteor of residual lstm meteor is 22.37. meteor of vae-svg-eq meteor is 25.50. meteor of vae-svg-eq meteor is 22.20. rouge-1 of pointer-generator rouge-1 is 61.96. rouge-2 of pointer-generator rouge-2 is 36.07. bleu of pointer-generator bleu is 40.55. meteor of pointer-generator meteor is 30.21. rouge-1 of pointer-generator rouge-1 is 51.98. rouge-2 of pointer-generator rouge-2 is 25.16. bleu of pointer-generator bleu is 30.01. meteor of pointer-generator meteor is 24.31. rouge-1 of rl-rouge rouge-1 is 63.35. rouge-2 of rl-rouge rouge-2 is 37.33. bleu of rl-rouge bleu is 41.83. meteor of rl-rouge meteor is 30.96. rouge-1 of rl-rouge rouge-1 is 54.50. rouge-2 of rl-rouge rouge-2 is 27.50. bleu of rl-rouge bleu is 32.54. meteor of rl-rouge meteor is 25.67. rouge-1 of rbm-sl (ours) rouge-1 is 64.39. rouge-2 of rbm-sl (ours) rouge-2 is 38.11. bleu of rbm-sl (ours) bleu is 43.54. meteor of rbm-sl (ours) meteor is 32.84. rouge-1 of rbm-sl (ours) rouge-1 is 57.34. rouge-2 of rbm-sl (ours) rouge-2 is 31.09. bleu of rbm-sl (ours) bleu is 35.81. meteor of rbm-sl (ours) meteor is 28.12. rouge-1 of rbm-irl (ours) rouge-1 is 64.02. rouge-2 of rbm-irl (ours) rouge-2 is 37.72. bleu of rbm-irl (ours) bleu is 43.09. meteor of rbm-irl (ours) meteor is 31.97. rouge-1 of rbm-irl (ours) rouge-1 is 56.86. rouge-2 of rbm-irl (ours) rouge-2 is 29.90. bleu of rbm-irl (ours) bleu is 34.79. meteor of rbm-irl (ours) meteor is 26.67.
table 4 shows human evaluation on quora datasets. relevance of pointer-generator relevance is 3.23. fluency of pointer-generator fluency is 4.55. relevance of pointer-generator relevance is 2.34. fluency of pointer-generator fluency is 2.96. relevance of rl-rouge relevance is 3.56. fluency of rl-rouge fluency is 4.61. relevance of rl-rouge relevance is 2.58. fluency of rl-rouge fluency is 3.14. relevance of rbm-sl (ours) relevance is 4.08. fluency of rbm-sl (ours) fluency is 4.67. relevance of rbm-sl (ours) relevance is 3.20. fluency of rbm-sl (ours) fluency is 3.48. relevance of rbm-irl (ours) relevance is 4.07. fluency of rbm-irl (ours) fluency is 4.69. relevance of rbm-irl (ours) relevance is 2.80. fluency of rbm-irl (ours) fluency is 3.53. relevance of reference relevance is 4.69. fluency of reference fluency is 4.95. relevance of reference relevance is 4.68. fluency of reference fluency is 4.90.
table 1 shows performance of our models on split1 with both sentence-level input and paragraph-level input. bleu 1 of s2s sen. is 30.41. bleu 1 of s2s par. is 28.49. bleu 2 of s2s sen. is 12.68. bleu 2 of s2s par. is 10.43. bleu 3 of s2s sen. is 6.33. bleu 3 of s2s par. is 4.70. bleu 4 of s2s sen. is 3.44. bleu 4 of s2s par. is 2.38. meteor of s2s sen. is 11.98. meteor of s2s par. is 10.69. rouge-l of s2s sen. is 29.93. rouge-l of s2s par. is 27.32. bleu 1 of s2s-a sen. is 34.46. bleu 1 of s2s-a par. is 31.26. bleu 2 of s2s-a sen. is 18.07. bleu 2 of s2s-a par. is 14.37. bleu 3 of s2s-a sen. is 11.20. bleu 3 of s2s-a par. is 8.02. bleu 4 of s2s-a sen. is 7.42. bleu 4 of s2s-a par. is 4.80. meteor of s2s-a sen. is 14.95. meteor of s2s-a par. is 12.52. rouge-l of s2s-a sen. is 34.69. rouge-l of s2s-a par. is 30.11. bleu 1 of s2s-a-at sen. is 40.57. bleu 1 of s2s-a-at par. is 40.56. bleu 2 of s2s-a-at sen. is 24.30. bleu 2 of s2s-a-at par. is 24.23. bleu 3 of s2s-a-at sen. is 16.40. bleu 3 of s2s-a-at par. is 16.33. bleu 4 of s2s-a-at sen. is 11.54. bleu 4 of s2s-a-at par. is 11.46. meteor of s2s-a-at sen. is 18.35. meteor of s2s-a-at par. is 18.42. rouge-l of s2s-a-at sen. is 40.76. rouge-l of s2s-a-at par. is 40.40. bleu 1 of s2s-a-at-cp sen. is 42.15. bleu 1 of s2s-a-at-cp par. is 41.66. bleu 2 of s2s-a-at-cp sen. is 26.28. bleu 2 of s2s-a-at-cp par. is 25.52. bleu 3 of s2s-a-at-cp sen. is 18.35. bleu 3 of s2s-a-at-cp par. is 17.48. bleu 4 of s2s-a-at-cp sen. is 13.37. bleu 4 of s2s-a-at-cp par. is 12.43. meteor of s2s-a-at-cp sen. is 18.02. meteor of s2s-a-at-cp par. is 17.76. rouge-l of s2s-a-at-cp sen. is 41.97. rouge-l of s2s-a-at-cp par. is 41.30. bleu 1 of s2s-a-at-mcp sen. is 43.65. bleu 1 of s2s-a-at-mcp par. is 44.22. bleu 2 of s2s-a-at-mcp sen. is 28.23. bleu 2 of s2s-a-at-mcp par. is 28.56. bleu 3 of s2s-a-at-mcp sen. is 20.33. bleu 3 of s2s-a-at-mcp par. is 20.57. bleu 4 of s2s-a-at-mcp sen. is 15.23. bleu 4 of s2s-a-at-mcp par. is 15.43. meteor of s2s-a-at-mcp sen. is 19.19. meteor of s2s-a-at-mcp par. is 19.55. rouge-l of s2s-a-at-mcp sen. is 43.60. rouge-l of s2s-a-at-mcp par. is 43.65. bleu 1 of s2s-a-at-mcp-gsa sen. is 43.47. bleu 1 of s2s-a-at-mcp-gsa par. is 45.07. bleu 2 of s2s-a-at-mcp-gsa sen. is 28.23. bleu 2 of s2s-a-at-mcp-gsa par. is 29.58. bleu 3 of s2s-a-at-mcp-gsa sen. is 20.40. bleu 3 of s2s-a-at-mcp-gsa par. is 21.60. bleu 4 of s2s-a-at-mcp-gsa sen. is 15.32. bleu 4 of s2s-a-at-mcp-gsa par. is 16.38. meteor of s2s-a-at-mcp-gsa sen. is 19.29. meteor of s2s-a-at-mcp-gsa par. is 20.25. rouge-l of s2s-a-at-mcp-gsa sen. is 43.91. rouge-l of s2s-a-at-mcp-gsa par. is 44.48.
table 8 shows performance obtained by training on different types of noisy questions (wikimovies). bleu of none bleu is 100. qbleu of none qbleu is 100. hit 1 of none hit 1 is 76.5. bleu of stop words bleu is 25.4. qbleu of stop words qbleu is 84.0. hit 1 of stop words hit 1 is 75.6. bleu of question type bleu is 74.0. qbleu of question type qbleu is 79.3. hit 1 of question type hit 1 is 73.5. bleu of content words bleu is 29.4. qbleu of content words qbleu is 64.3. hit 1 of content words hit 1 is 54.7. bleu of named entity bleu is 41.9. qbleu of named entity qbleu is 48.5. hit 1 of named entity hit 1 is 17.97.
table 3 shows state-of-the-art (sota) comparison on vqgcoco dataset. bleu1 of natural 2016 bleu1 is 19.2. meteor of natural 2016 meteor is 19.7. bleu1 of creative 2017 bleu1 is 35.6. meteor of creative 2017 meteor is 19.9. bleu1 of image only bleu1 is 20.8. meteor of image only meteor is 8.6. rouge of image only rouge is 22.6. cider of image only cider is 18.8. bleu1 of caption only bleu1 is 21.1. meteor of caption only meteor is 8.5. rouge of caption only rouge is 25.9. cider of caption only cider is 22.3. bleu1 of tag-hadamard bleu1 is 24.4. meteor of tag-hadamard meteor is 10.8. rouge of tag-hadamard rouge is 24.3. cider of tag-hadamard cider is 55.0. bleu1 of place cnn-joint bleu1 is 25.7. meteor of place cnn-joint meteor is 10.8. rouge of place cnn-joint rouge is 24.5. cider of place cnn-joint cider is 56.1. bleu1 of diff.image-joint bleu1 is 30.4. meteor of diff.image-joint meteor is 11.7. rouge of diff.image-joint rouge is 26.3. cider of diff.image-joint cider is 38.8. bleu1 of mdn-joint (ours) bleu1 is 36.0. meteor of mdn-joint (ours) meteor is 23.4. rouge of mdn-joint (ours) rouge is 41.8. cider of mdn-joint (ours) cider is 50.7. bleu1 of humans 2016 bleu1 is 86.0. meteor of humans 2016 meteor is 60.8.
table 2 shows comparison of template generator with coarse/fine-grained entity type. vocabulary size of raw-caption vocabulary size is 10979. bleu-1 of raw-caption bleu-1 is 15.1. bleu-2 of raw-caption bleu-2 is 11.7. bleu-3 of raw-caption bleu-3 is 9.9. bleu-4 of raw-caption bleu-4 is 8.8. meteor of raw-caption meteor is 8.8. rouge of raw-caption rouge is 24.2. cider of raw-caption cider is 34.7. vocabulary size of coarse template vocabulary size is 3533. bleu-1 of coarse template bleu-1 is 46.7. bleu-2 of coarse template bleu-2 is 36.1. bleu-3 of coarse template bleu-3 is 29.8. bleu-4 of coarse template bleu-4 is 25.7. meteor of coarse template meteor is 22.4. rouge of coarse template rouge is 43.5. cider of coarse template cider is 161.6. vocabulary size of fine template vocabulary size is 3642. bleu-1 of fine template bleu-1 is 43.0. bleu-2 of fine template bleu-2 is 33.4. bleu-3 of fine template bleu-3 is 27.8. bleu-4 of fine template bleu-4 is 24.3. meteor of fine template meteor is 20.3. rouge of fine template rouge is 39.8. cider of fine template cider is 165.3.
table 4 shows  comparison results using rouge recall at 75  bytes  without  oov  replacement. rouge-1 of hnnattti-3-oov rouge-1 is 24.03. rouge-2 of hnnattti-3-oov rouge-2 is 8.2. rouge-l of hnnattti-3-oov rouge-l is 16.52. rouge-1 of hnnatttc-3-oov rouge-1 is 18.18. rouge-2 of hnnatttc-3-oov rouge-2 is 6.53. rouge-l of hnnatttc-3-oov rouge-l is 12.87. rouge-1 of hnntatttic-3-oov rouge-1 is 20.50. rouge-2 of hnntatttic-3-oov rouge-2 is 7.67. rouge-l of hnntatttic-3-oov rouge-l is 14.36. rouge-1 of hnnattt-3-oov rouge-1 is 21.60. rouge-2 of hnnattt-3-oov rouge-2 is 7.82. rouge-l of hnnattt-3-oov rouge-l is 15.05.
table 1 shows comparison with other baselines on dailymail test dataset using rouge recall score with respect to the abstractive ground truth at 75 bytes and at 275 bytes. rouge-1 of lead-3 rouge-1 is 21.9. rouge-2 of lead-3 rouge-2 is 7.2. rouge-l of lead-3 rouge-l is 11.6. rouge-1 of lead-3 rouge-1 is 40.5. rouge-2 of lead-3 rouge-2 is 14.9. rouge-l of lead-3 rouge-l is 32.6. rouge-1 of lreg(500) rouge-1 is 18.5. rouge-2 of lreg(500) rouge-2 is 6.9. rouge-l of lreg(500) rouge-l is 10.2. rouge-1 of cheng et.al 16 rouge-1 is 22.7. rouge-2 of cheng et.al 16 rouge-2 is 8.5. rouge-l of cheng et.al 16 rouge-l is 12.5. rouge-1 of cheng et.al 16 rouge-1 is 42.2. rouge-2 of cheng et.al 16 rouge-2 is 17.3. rouge-l of cheng et.al 16 rouge-l is 34.8. rouge-1 of summarunner rouge-1 is 26.2. rouge-2 of summarunner rouge-2 is 10.8. rouge-l of summarunner rouge-l is 14.4. rouge-1 of summarunner rouge-1 is 42. rouge-2 of summarunner rouge-2 is 16.9. rouge-l of summarunner rouge-l is 34.1. rouge-1 of refresh rouge-1 is 24.1. rouge-2 of refresh rouge-2 is 11.5. rouge-l of refresh rouge-l is 12.5. rouge-1 of refresh rouge-1 is 40.3. rouge-2 of refresh rouge-2 is 15.1. rouge-l of refresh rouge-l is 32.9. rouge-1 of hybrid memnet rouge-1 is 26.3. rouge-2 of hybrid memnet rouge-2 is 11.2. rouge-l of hybrid memnet rouge-l is 15.5. rouge-1 of hybrid memnet rouge-1 is 41.4. rouge-2 of hybrid memnet rouge-2 is 16.7. rouge-l of hybrid memnet rouge-l is 33.2. rouge-1 of its rouge-1 is 27.4. rouge-2 of its rouge-2 is 11.9. rouge-l of its rouge-l is 16.1. rouge-1 of its rouge-1 is 42.4. rouge-2 of its rouge-2 is 17.4. rouge-l of its rouge-l is 34.1.
table 5 shows system ranking comparison with other baselines on dailymail corpus. percentage of lead-3 1st is 0.12. percentage of lead-3 2nd is 0.11. percentage of lead-3 3rd is 0.25. percentage of lead-3 4th is 0.52. percentage of hybrid memnet 1st is 0.24. percentage of hybrid memnet 2nd is 0.25. percentage of hybrid memnet 3rd is 0.28. percentage of hybrid memnet 4th is 0.23. percentage of its 1st is 0.31. percentage of its 2nd is 0.34. percentage of its 3rd is 0.23. percentage of its 4th is 0.12. percentage of gold 1st is 0.33. percentage of gold 2nd is 0.30. percentage of gold 3rd is 0.24. percentage of gold 4th is 0.13.
table 2 shows results on the nyt corpus, where we compare to rl trained models. r-1 of ml* r-1 is 44.26. r-2 of ml* r-2 is 27.43. r-l of ml* r-l is 40.41. r-1 of ml+rl* r-1 is 47.03. r-2 of ml+rl* r-2 is 30.72. r-l of ml+rl* r-l is 43.10. r-1 of dca† r-1 is 48.08. r-2 of dca† r-2 is 31.19. r-l of dca† r-l is 42.33. r-1 of point.gen. + coverage pen. r-1 is 45.13. r-2 of point.gen. + coverage pen. r-2 is 30.13. r-l of point.gen. + coverage pen. r-l is 39.67. r-1 of bottom-up summarization r-1 is 47.38. r-2 of bottom-up summarization r-2 is 31.23. r-l of bottom-up summarization r-l is 41.81.
table 3 shows upper-bound performance comparison. r1 of td(λ) r1 is .484. r2 of td(λ) r2 is .184. rl of td(λ) rl is .388. rsu4 of td(λ) rsu4 is .199. r1 of lstd(λ) r1 is .458. r2 of lstd(λ) r2 is .159. rl of lstd(λ) rl is .366. rsu4 of lstd(λ) rsu4 is .185. r1 of ilp r1 is .470. r2 of ilp r2 is .212. rl of ilp rl is n/a. rsu4 of ilp rsu4 is .185.
table 4 shows results of keyphrase generation for news from duc dataset with f1. f1 of seq2seq f1 is 0.056. f1 of syn.unsuper. f1 is 0.083. f1 of syn.self-learn. f1 is 0.065. f1 of multi-task f1 is 0.109. f1 of tf-idf f1 is 0.270. f1 of textrank f1 is 0.097. f1 of singlerank f1 is 0.256. f1 of expandrank f1 is 0.269.
table 2 shows results across different metrics on the test set of narrativeqa-summaries task. bleu-1 of seq2seq (kocisky et al. 2018) bleu-1 is 15.89. bleu-4 of seq2seq (kocisky et al. 2018) bleu-4 is 1.26. meteor of seq2seq (kocisky et al. 2018) meteor is 4.08. rouge-l of seq2seq (kocisky et al. 2018) rouge-l is 13.15. bleu-1 of asr (kocisky et al. 2018) bleu-1 is 23.20. bleu-4 of asr (kocisky et al. 2018) bleu-4 is 6.39. meteor of asr (kocisky et al. 2018) meteor is 7.77. rouge-l of asr (kocisky et al. 2018) rouge-l is 22.26. bleu-1 of bidaf (kocisky et al. 2018) bleu-1 is 33.72. bleu-4 of bidaf (kocisky et al. 2018) bleu-4 is 15.53. meteor of bidaf (kocisky et al. 2018) meteor is 15.38. rouge-l of bidaf (kocisky et al. 2018) rouge-l is 36.30. bleu-1 of biattn + mru-lstm (tay et al. 2018) bleu-1 is 36.55. bleu-4 of biattn + mru-lstm (tay et al. 2018) bleu-4 is 19.79. meteor of biattn + mru-lstm (tay et al. 2018) meteor is 17.87. rouge-l of biattn + mru-lstm (tay et al. 2018) rouge-l is 41.44. bleu-1 of mhpgm bleu-1 is 40.24. bleu-4 of mhpgm bleu-4 is 17.40. meteor of mhpgm meteor is 17.33. rouge-l of mhpgm rouge-l is 41.49. cider of mhpgm cider is 139.23. bleu-1 of mhpgm+ noic bleu-1 is 43.63. bleu-4 of mhpgm+ noic bleu-4 is 21.07. meteor of mhpgm+ noic meteor is 19.03. rouge-l of mhpgm+ noic rouge-l is 44.16. cider of mhpgm+ noic cider is 152.98.
table 2 shows automatic evaluations of the proposed model and the state-of-the-art models. bleu of ee-seq2seq bleu is 0.0029. bleu of de-seq2seq bleu is 0.0027. bleu of ge-seq2seq bleu is 0.0022. bleu of proposed model bleu is 0.0042 (+44.8%).
table 6 shows human evaluations of the key components. fluency of seq2seq fluency is 7.54. coherence of seq2seq coherence is 4.98. g-score of seq2seq g-score is 6.13. fluency of +skeleton extraction module fluency is 7.26. coherence of +skeleton extraction module coherence is 4.32. g-score of +skeleton extraction module g-score is 5.60. fluency of +reinforcement learning fluency is 8.69. coherence of +reinforcement learning coherence is 5.62. g-score of +reinforcement learning g-score is 6.99.
table 1 shows results of embedding-based metrics. average of greedy average is 0.443. greedy of greedy greedy is 0.376. extreme of greedy extreme is 0.328. average of greedy average is 0.510. greedy of greedy greedy is 0.341. extreme of greedy extreme is 0.356. average of beam average is 0.437. greedy of beam greedy is 0.350. extreme of beam extreme is 0.369. average of beam average is 0.505. greedy of beam greedy is 0.345. extreme of beam extreme is 0.352. average of mmi average is 0.457. greedy of mmi greedy is 0.371. extreme of mmi extreme is 0.371. average of mmi average is 0.518. greedy of mmi greedy is 0.353. extreme of mmi extreme is 0.365. average of rl average is 0.405. greedy of rl greedy is 0.329. extreme of rl extreme is 0.305. average of rl average is 0.460. greedy of rl greedy is 0.349. extreme of rl extreme is 0.323. average of vhred average is 0.491. greedy of vhred greedy is 0.375. extreme of vhred extreme is 0.313. average of vhred average is 0.525. greedy of vhred greedy is 0.389. extreme of vhred extreme is 0.372. average of nexus-h average is 0.479. greedy of nexus-h greedy is 0.381. extreme of nexus-h extreme is 0.385. average of nexus-h average is 0.558. greedy of nexus-h greedy is 0.392. extreme of nexus-h extreme is 0.373. average of nexus-f average is 0.476. greedy of nexus-f greedy is 0.383. extreme of nexus-f extreme is 0.373. average of nexus-f average is 0.549. greedy of nexus-f greedy is 0.393. extreme of nexus-f extreme is 0.386. average of nexus average is 0.488. greedy of nexus greedy is 0.392. extreme of nexus extreme is 0.384. average of nexus average is 0.556. greedy of nexus greedy is 0.397. extreme of nexus extreme is 0.391.
table 2 shows results of bleu score. bleu-1 of greedy bleu-1 is 0.394. bleu-2 of greedy bleu-2 is 0.245. bleu-3 of greedy bleu-3 is 0.157. bleu-1 of greedy bleu-1 is 0.340. bleu-2 of greedy bleu-2 is 0.203. bleu-3 of greedy bleu-3 is 0.116. bleu-1 of beam bleu-1 is 0.386. bleu-2 of beam bleu-2 is 0.251. bleu-3 of beam bleu-3 is 0.163. bleu-1 of beam bleu-1 is 0.338. bleu-2 of beam bleu-2 is 0.205. bleu-3 of beam bleu-3 is 0.112. bleu-1 of mmi bleu-1 is 0.407. bleu-2 of mmi bleu-2 is 0.269. bleu-3 of mmi bleu-3 is 0.172. bleu-1 of mmi bleu-1 is 0.347. bleu-2 of mmi bleu-2 is 0.208. bleu-3 of mmi bleu-3 is 0.118. bleu-1 of rl bleu-1 is 0.298. bleu-2 of rl bleu-2 is 0.186. bleu-3 of rl bleu-3 is 0.075. bleu-1 of rl bleu-1 is 0.314. bleu-2 of rl bleu-2 is 0.199. bleu-3 of rl bleu-3 is 0.103. bleu-1 of vhred bleu-1 is 0.395. bleu-2 of vhred bleu-2 is 0.281. bleu-3 of vhred bleu-3 is 0.190. bleu-1 of vhred bleu-1 is 0.355. bleu-2 of vhred bleu-2 is 0.211. bleu-3 of vhred bleu-3 is 0.124. bleu-1 of nexus-h bleu-1 is 0.418. bleu-2 of nexus-h bleu-2 is 0.279. bleu-3 of nexus-h bleu-3 is 0.199. bleu-1 of nexus-h bleu-1 is 0.366. bleu-2 of nexus-h bleu-2 is 0.212. bleu-3 of nexus-h bleu-3 is 0.126. bleu-1 of nexus-f bleu-1 is 0.399. bleu-2 of nexus-f bleu-2 is 0.260. bleu-3 of nexus-f bleu-3 is 0.167. bleu-1 of nexus-f bleu-1 is 0.359. bleu-2 of nexus-f bleu-2 is 0.213. bleu-3 of nexus-f bleu-3 is 0.123. bleu-1 of nexus bleu-1 is 0.424. bleu-2 of nexus bleu-2 is 0.276. bleu-3 of nexus bleu-3 is 0.198. bleu-1 of nexus bleu-1 is 0.363. bleu-2 of nexus bleu-2 is 0.220. bleu-3 of nexus bleu-3 is 0.131.
table 1 shows results on readability assessment. accuracy (%) of mesgar and strube (2016) accuracy (%) is 85.70. accuracy (%) of cohemb accuracy (%) is 92.17. accuracy (%) of cohlstm accuracy (%) is 97.77. accuracy (%) of de clercq and hoste (2016) accuracy (%) is 96.88.
table 4 shows the performance of correctly predicting the first and the last sentences on arxiv abstract and sind caption datasets. accuracy of random head is 23.06. accuracy of random tail is 23.16. accuracy of random head is 22.78. accuracy of random tail is 22.56. accuracy of pairwise ranking model head is 84.85. accuracy of pairwise ranking model tail is 62.37. accuracy of cnn+ptrnet head is 89.43. accuracy of cnn+ptrnet tail is 65.36. accuracy of cnn+ptrnet head is 73.53. accuracy of cnn+ptrnet tail is 53.26. accuracy of lstm+ptrnet head is 90.47. accuracy of lstm+ptrnet tail is 66.49. accuracy of lstm+ptrnet head is 74.66. accuracy of lstm+ptrnet tail is 53.30. accuracy of attordernet (att) head is 89.68. accuracy of attordernet (att) tail is 65.75. accuracy of attordernet (att) head is 75.88. accuracy of attordernet (att) tail is 54.30. accuracy of attordernet (cnn) head is 90.86. accuracy of attordernet (cnn) tail is 67.85. accuracy of attordernet (cnn) head is 75.95. accuracy of attordernet (cnn) tail is 54.37. accuracy of attordernet head is 91.00. accuracy of attordernet tail is 68.08. accuracy of attordernet head is 76.00. accuracy of attordernet tail is 54.42.
table 5 shows experimental results of pairwise accuracy for different approaches on two datasets in the order discrimination task. accuracy of random accident is 50.0. accuracy of random earthquake is 50.0. accuracy of graph accident is 84.6. accuracy of graph earthquake is 63.5. accuracy of hmm+entity accident is 84.2. accuracy of hmm+entity earthquake is 91.1. accuracy of hmm accident is 82.2. accuracy of hmm earthquake is 93.8. accuracy of entity grid accident is 90.4. accuracy of entity grid earthquake is 87.2. accuracy of recurrent accident is 84.0. accuracy of recurrent earthquake is 95.1. accuracy of recursive accident is 86.4. accuracy of recursive earthquake is 97.6. accuracy of discriminative model accident is 93.0. accuracy of discriminative model earthquake is 99.2. accuracy of varient-lstm+ptrnet accident is 94.4. accuracy of varient-lstm+ptrnet earthquake is 99.7. accuracy of cnn+ptrnet accident is 93.5. accuracy of cnn+ptrnet earthquake is 99.4. accuracy of lstm+ptrnet accident is 93.7. accuracy of lstm+ptrnet earthquake is 99.5. accuracy of attordernet (att) accident is 95.4. accuracy of attordernet (att) earthquake is 99.6. accuracy of attordernet (cnn) accident is 95.8. accuracy of attordernet (cnn) earthquake is 99.7. accuracy of attordernet accident is 96.2. accuracy of attordernet earthquake is 99.8.
table 5 shows crosslingual clustering results when considering two different approaches to compute distances across crosslingual clusters on the test set for spanish, german and english. f1 of τsearch (global) f1 is 72.7. p of τsearch (global) p is 89.8. r of τsearch (global) r is 61.0. f1 of τsearch (pivot) f1 is 84.0. p of τsearch (pivot) p is 83.0. r of τsearch (pivot) r is 85.0.
table 4 shows results of hot update, cold update and zero update in different cases accuracy of before update sst-1 is 48.6. accuracy of before update sst-2 is 87.6. accuracy of before update b is 83.7. accuracy of before update d is 84.5. accuracy of before update e is 85.9. accuracy of before update rn is 84.8. accuracy of before update qc is 93.4. accuracy of cold update sst-1 is 49.8. accuracy of cold update sst-2 is 88.5. accuracy of cold update imdb is 91.2. accuracy of cold update b is 84.4. accuracy of cold update d is 85.2. accuracy of cold update e is 87.2. accuracy of cold update k is 86.9. accuracy of cold update rn is 85.5. accuracy of cold update qc is 93.2. accuracy of cold update imdb is 91.0. accuracy of hot update sst-1 is 49.6. accuracy of hot update sst-2 is 88.1. accuracy of hot update imdb is 91.4. accuracy of hot update b is 84.2. accuracy of hot update d is 84.9. accuracy of hot update e is 87.0. accuracy of hot update k is 87.1. accuracy of hot update rn is 85.2. accuracy of hot update qc is 92.9. accuracy of hot update imdb is 91.1. accuracy of zero update imdb is 90.9. accuracy of zero update k is 86.7. accuracy of zero update imdb is 74.2.
table 5 shows comparisons of mtle against state-of-the-art models accuracy of nbow sst-1 is 42.4. accuracy of nbow sst-2 is 80.5. accuracy of nbow imdb is 83.6. accuracy of nbow qc is 88.2. accuracy of pv sst-1 is 44.6. accuracy of pv sst-2 is 82.7. accuracy of pv imdb is 91.7. accuracy of pv qc is 91.8. accuracy of cnn sst-1 is 48.0. accuracy of cnn sst-2 is 88.1. accuracy of cnn qc is 93.6. accuracy of mt-cnn books is 80.2. accuracy of mt-cnn dvds is 81.0. accuracy of mt-cnn electronics is 83.4. accuracy of mt-cnn kitchen is 83.0. accuracy of mt-dnn books is 79.7. accuracy of mt-dnn dvds is 80.5. accuracy of mt-dnn electronics is 82.5. accuracy of mt-dnn kitchen is 82.8. accuracy of mt-rnn sst-1 is 49.6. accuracy of mt-rnn sst-2 is 87.9. accuracy of mt-rnn imdb is 91.3. accuracy of dsm sst-1 is 49.5. accuracy of dsm sst-2 is 87.8. accuracy of dsm imdb is 91.2. accuracy of dsm books is 82.8. accuracy of dsm dvds is 83.0. accuracy of dsm electronics is 85.5. accuracy of dsm kitchen is 84.0. accuracy of grnn sst-1 is 47.5. accuracy of grnn sst-2 is 85.5. accuracy of grnn qc is 93.8. accuracy of tree-lstm sst-1 is 50.6. accuracy of tree-lstm sst-2 is 86.9. accuracy of mtle sst-1 is 49.8. accuracy of mtle sst-2 is 88.4. accuracy of mtle imdb is 91.3. accuracy of mtle books is 84.5. accuracy of mtle dvds is 85.2. accuracy of mtle electronics is 87.3. accuracy of mtle kitchen is 86.9. accuracy of mtle qc is 93.2.
table 5 shows performance of the hierarchical model and our model on the rcv1-v2 test set. hl(-) of hier-5 hl(-) is 0.0075. p(+) of hier-5 p(+) is 0.887. r(+) of hier-5 r(+) is 0.869. f1(+) of hier-5 f1(+) is 0.878. hl(-) of hier-10 hl(-) is 0.0077. p(+) of hier-10 p(+) is 0.883. r(+) of hier-10 r(+) is 0.873. f1(+) of hier-10 f1(+) is 0.878. hl(-) of hier-15 hl(-) is 0.0076. p(+) of hier-15 p(+) is 0.879. r(+) of hier-15 r(+) is 0.879. f1(+) of hier-15 f1(+) is 0.879. hl(-) of hier-20 hl(-) is 0.0076. p(+) of hier-20 p(+) is 0.876. r(+) of hier-20 r(+) is 0.881. f1(+) of hier-20 f1(+) is 0.878. hl(-) of our model hl(-) is 0.0072. p(+) of our model p(+) is 0.891. r(+) of our model r(+) is 0.873. f1(+) of our model f1(+) is 0.882.
table 2 shows single-task results. accuracy of lstm mr is 75.9. accuracy of lstm sst-1 is 45.9. accuracy of lstm sst-2 is 80.6. accuracy of lstm subj is 89.3. accuracy of lstm trec is 86.8. accuracy of lstm ag’s is 86.1. accuracy of bilstm mr is 79.3. accuracy of bilstm sst-1 is 46.2. accuracy of bilstm sst-2 is 83.2. accuracy of bilstm subj is 90.5. accuracy of bilstm trec is 89.6. accuracy of bilstm ag’s is 88.2. accuracy of lr-lstm mr is 81.5. accuracy of lr-lstm sst-1 is 48.2. accuracy of lr-lstm sst-2 is 87.5. accuracy of lr-lstm subj is 89.9. accuracy of vd-cnn ag’s is 91.3. accuracy of dcnn sst-1 is 48.5. accuracy of dcnn sst-2 is 86.8. accuracy of dcnn trec is 93.0. accuracy of cnn-mc mr is 81.1. accuracy of cnn-mc sst-1 is 47.4. accuracy of cnn-mc sst-2 is 88.1. accuracy of cnn-mc subj is 93.2. accuracy of cnn-mc trec is 92.2. accuracy of capsnet-1 mr is 81.5. accuracy of capsnet-1 sst-1 is 48.1. accuracy of capsnet-1 sst-2 is 86.4. accuracy of capsnet-1 subj is 93.3. accuracy of capsnet-1 trec is 91.8. accuracy of capsnet-1 ag’s is 91.1. accuracy of capsnet-2 mr is 82.4. accuracy of capsnet-2 sst-1 is 48.7. accuracy of capsnet-2 sst-2 is 87.8. accuracy of capsnet-2 subj is 93.6. accuracy of capsnet-2 trec is 92.9. accuracy of capsnet-2 ag’s is 92.3. accuracy of - orphan mr is 81.9. accuracy of - orphan sst-1 is 48.3. accuracy of - orphan sst-2 is 87.2. accuracy of - orphan subj is 93.4. accuracy of - orphan trec is 92.6. accuracy of - orphan ag’s is 91.7.
table 5 shows comparison of the acnn model to the stateof-the-art methods on the switchboard test set. p of yoshikawa et al.(2016) p is 67.9. r of yoshikawa et al.(2016) r is 57.9. f of yoshikawa et al.(2016) f is 62.5. p of georgila et al. (2010) p is 77.4. r of georgila et al. (2010) r is 64.6. f of georgila et al. (2010) f is 70.4. f of tran et al. (2018) f is 77.5. f of kahn et al. (2005) f is 78.2. p of johnson et al. (2004) p is 82.0. r of johnson et al. (2004) r is 77.8. f of johnson et al. (2004) f is 79.7. f of georgila (2009) f is 80.1. f of johnson et al. (2004) f is 81.0. p of rasooli et al. (2013) p is 85.1. r of rasooli et al. (2013) r is 77.9. f of rasooli et al. (2013) f is 81.4. f of zwarts et al. (2011) f is 83.8. f of qian et al. (2013) f is 84.1. f of honnibal et al. (2014) f is 84.1. p of acnn p is 89.5. r of acnn r is 80.0. f of acnn f is 84.5. p of ferguson et al. (2015) p is 90.0. r of ferguson et al. (2015) r is 81.2. f of ferguson et al. (2015) f is 85.4. p of zayats et al. (2016) p is 91.8. r of zayats et al. (2016) r is 80.6. f of zayats et al. (2016) f is 85.9. f of jamshid lou et al. (2017) f is 86.8.
table 7 shows word similarity results on isear. similarity of w2v men is 0.002. similarity of w2v simlex is -0.008. similarity of w2v rare is -0.119. similarity of siw2v men is 0.002. similarity of siw2v simlex is 0.017. similarity of siw2v rare is 0.062. similarity of sspmi men is 0.023. similarity of sspmi simlex is 0.028. similarity of sspmi rare is -0.004. similarity of sltm men is 0.169. similarity of sltm simlex is 0.037. similarity of sltm rare is 0.089.
table 3 shows results for different sampling numbers in different setting for the two datasets. score of 1 score is 0.24. score of 3 score is 0.28. score of 10 score is 0.25. score of 1 score is 0.21. score of 3 score is 0.26. score of 10 score is 0.25. score of 1 score is 0.27. score of 3 score is 0.22. score of 5 score is 0.20. score of 1 score is 0.26. score of 3 score is 0.20. score of 5 score is 0.17.
table 6 shows cross auc results for different representations on the beeradvocate data. auc of look look is 0.92. auc of look aroma is 0.89. auc of look palate is 0.88. auc of look taste is 0.87. auc of aroma look is 0.90. auc of aroma aroma is 0.93. auc of aroma palate is 0.91. auc of aroma taste is 0.92. auc of palate look is 0.89. auc of palate aroma is 0.92. auc of palate palate is 0.94. auc of palate taste is 0.95. auc of taste look is 0.90. auc of taste aroma is 0.94. auc of taste palate is 0.95. auc of taste taste is 0.96.
table 4 shows pos tagging results on sancl data. accuracy of answers best-ss is 88.16. accuracy of answers uni-ms is 88.89. accuracy of answers uni-ms† is 89.88. accuracy of answers moe is 90.26. accuracy of answers best-ss-a is 88.47. accuracy of answers uni-ms-a is 89.04. accuracy of answers uni-ms-a† is 89.99. accuracy of answers moe-a is 89.80. accuracy of reviews best-ss is 87.15. accuracy of reviews uni-ms is 87.45. accuracy of reviews uni-ms† is 88.91. accuracy of reviews moe is 89.37. accuracy of reviews best-ss-a is 87.26. accuracy of reviews uni-ms-a is 87.90. accuracy of reviews uni-ms-a† is 88.94. accuracy of reviews moe-a is 89.40. accuracy of newsgroup best-ss is 89.14. accuracy of newsgroup uni-ms is 89.95. accuracy of newsgroup uni-ms† is 90.70. accuracy of newsgroup moe is 91.03. accuracy of newsgroup best-ss-a is 89.54. accuracy of newsgroup uni-ms-a is 90.20. accuracy of newsgroup uni-ms-a† is 90.70. accuracy of newsgroup moe-a is 91.13. accuracy of average best-ss is 88.15. accuracy of average uni-ms is 88.76. accuracy of average uni-ms† is 89.83. accuracy of average moe is 90.22. accuracy of average best-ss-a is 88.42. accuracy of average uni-ms-a is 89.05. accuracy of average uni-ms-a† is 89.88. accuracy of average moe-a is 90.11.
table 1 shows experimental results of the two baselines, as well as single and label-wise attention modifications to the “vanilla” 2-bilstm model. f1 of fasttext f1 is 30.97. a@1 of fasttext a@1 is 42.57. a@5 of fasttext a@5 is 72.45. ce of fasttext ce is 4.56. f1 of 2-bilstm f1 is 33.52. a@1 of 2-bilstm a@1 is 45.76. a@5 of 2-bilstm a@5 is 75.54. ce of 2-bilstm ce is 3.88. f1 of 2-bilstma f1 is 34.11. a@1 of 2-bilstma a@1 is 46.11. a@5 of 2-bilstma a@5 is 75.68. ce of 2-bilstma ce is 3.86. f1 of 2-bilstml f1 is 33.51. a@1 of 2-bilstml a@1 is 45.94. a@5 of 2-bilstml a@5 is 76.02. ce of 2-bilstml ce is 3.82. f1 of fasttext f1 is 18.04. a@1 of fasttext a@1 is 22.33. a@5 of fasttext a@5 is 48.13. ce of fasttext ce is 14.27. f1 of 2-bilstm f1 is 19.07. a@1 of 2-bilstm a@1 is 25.35. a@5 of 2-bilstm a@5 is 53.38. ce of 2-bilstm ce is 9.37. f1 of 2-bilstma f1 is 19.83. a@1 of 2-bilstma a@1 is 25.52. a@5 of 2-bilstma a@5 is 53.51. ce of 2-bilstma ce is 9.35. f1 of 2-bilstml f1 is 20.08. a@1 of 2-bilstml a@1 is 25.64. a@5 of 2-bilstml a@5 is 53.77. ce of 2-bilstml ce is 9.26. f1 of fasttext f1 is 16.25. a@1 of fasttext a@1 is 20.29. a@5 of fasttext a@5 is 42.65. ce of fasttext ce is 26.04. f1 of 2-bilstm f1 is 17.44. a@1 of 2-bilstm a@1 is 23.01. a@5 of 2-bilstm a@5 is 47.46. ce of 2-bilstm ce is 15.24. f1 of 2-bilstma f1 is 17.56. a@1 of 2-bilstma a@1 is 22.77. a@5 of 2-bilstma a@5 is 46.93. ce of 2-bilstma ce is 15.51. f1 of 2-bilstml f1 is 17.92. a@1 of 2-bilstml a@1 is 22.80. a@5 of 2-bilstml a@5 is 47.41. ce of 2-bilstml ce is 15.17. f1 of fasttext f1 is 13.31. a@1 of fasttext a@1 is 18.80. a@5 of fasttext a@5 is 38.99. ce of fasttext ce is 51.06. f1 of 2-bilstm f1 is 16.16. a@1 of 2-bilstm a@1 is 21.05. a@5 of 2-bilstm a@5 is 42.64. ce of 2-bilstm ce is 24.68. f1 of 2-bilstma f1 is 16.30. a@1 of 2-bilstma a@1 is 21.13. a@5 of 2-bilstma a@5 is 42.50. ce of 2-bilstma ce is 24.60. f1 of 2-bilstml f1 is 16.91. a@1 of 2-bilstml a@1 is 21.39. a@5 of 2-bilstml a@5 is 43.35. ce of 2-bilstml ce is 23.73.
table 1 shows results on nist chinese-to-english translation task. bleu of basenmt dev(mt02) is 36.72. bleu of basenmt mt03 is 33.95. bleu of basenmt mt04 is 37.44. bleu of basenmt mt05 is 33.96. bleu of basenmt mt06 is 33.09. bleu of basenmt avg is 34.61. bleu of mrt dev(mt02) is 37.17. bleu of mrt mt03 is 34.89. bleu of mrt mt04 is 37.90. bleu of mrt mt05 is 34.62. bleu of mrt mt06 is 33.78. bleu of mrt avg is 35.30. bleu of rf dev(mt02) is 37.13. bleu of rf mt03 is 34.66. bleu of rf mt04 is 37.69. bleu of rf mt05 is 34.55. bleu of rf mt06 is 33.74. bleu of rf avg is 35.16. bleu of p-bleu dev(mt02) is 37.26. bleu of p-bleu mt03 is 34.54. bleu of p-bleu mt04 is 38.05. bleu of p-bleu mt05 is 34.30. bleu of p-bleu mt06 is 34.11. bleu of p-bleu avg is 35.25. bleu of p-gleu dev(mt02) is 37.44. bleu of p-gleu mt03 is 34.67. bleu of p-gleu mt04 is 38.11. bleu of p-gleu mt05 is 34.24. bleu of p-gleu mt06 is 34.58. bleu of p-gleu avg is 35.40. bleu of p-p2 dev(mt02) is 38.03. bleu of p-p2 mt03 is 35.45. bleu of p-p2 mt04 is 39.30. bleu of p-p2 mt05 is 35.10. bleu of p-p2 mt06 is 34.59. bleu of p-p2 avg is 36.11.
table 3 shows results (filtered setting) of the temporal knowledge graph completion experiments for the data sets yago15k and wikidata. mrr of ttranse mrr is 32.1. mr of ttranse mr is 578. hits@10 of ttranse hits@10 is 51.0. hits@1 of ttranse hits@1 is 23.0. mrr of ttranse mrr is 48.8. mr of ttranse mr is 80. hits@10 of ttranse hits@10 is 80.6. hits@1 of ttranse hits@1 is 33.9. mrr of transe mrr is 29.6. mr of transe mr is 614. hits@10 of transe hits@10 is 46.8. hits@1 of transe hits@1 is 22.8. mrr of transe mrr is 31.6. mr of transe mr is 50. hits@10 of transe hits@10 is 65.9. hits@1 of transe hits@1 is 18.1. mrr of distmult mrr is 27.5. mr of distmult mr is 578. hits@10 of distmult hits@10 is 43.8. hits@1 of distmult hits@1 is 21.5. mrr of distmult mrr is 31.6. mr of distmult mr is 77. hits@10 of distmult hits@10 is 66.1. hits@1 of distmult hits@1 is 18.1. mrr of ta-transe mrr is 32.1. mr of ta-transe mr is 564. hits@10 of ta-transe hits@10 is 51.2. hits@1 of ta-transe hits@1 is 23.1. mrr of ta-transe mrr is 48.4. mr of ta-transe mr is 79. hits@10 of ta-transe hits@10 is 80.7. hits@1 of ta-transe hits@1 is 32.9. mrr of ta-distmult mrr is 29.1. mr of ta-distmult mr is 551. hits@10 of ta-distmult hits@10 is 47.6. hits@1 of ta-distmult hits@1 is 21.6. mrr of ta-distmult mrr is 70.0. mr of ta-distmult mr is 198. hits@10 of ta-distmult hits@10 is 78.5. hits@1 of ta-distmult hits@1 is 65.2.
table 1 shows results of transfer learning tasks performance of the proposed autoencoder models. f1 of cross-entropy (vanilla ae) f1 is 79.0. acc of cross-entropy (vanilla ae) acc is 66.9. acc of cross-entropy (vanilla ae) acc is 44.8. acc of cross-entropy (vanilla ae) acc is 56.8. f1 of soft label n = 3 f1 is 77.6. acc of soft label n = 3 acc is 67.1. acc of soft label n = 3 acc is 57.8. acc of soft label n = 3 acc is 71.8. f1 of soft label n = 5 f1 is 79.1. acc of soft label n = 5 acc is 67.3. acc of soft label n = 5 acc is 57.2. acc of soft label n = 5 acc is 71.6. f1 of soft label n = 10 f1 is 77.9. acc of soft label n = 10 acc is 66.5. acc of soft label n = 10 acc is 57.9. acc of soft label n = 10 acc is 72.4. f1 of weighted similarity f1 is 77.5. acc of weighted similarity acc is 65.6. acc of weighted similarity acc is 69.1. acc of weighted similarity acc is 56.6. f1 of weighted cross-entropy f1 is 79.4. acc of weighted cross-entropy acc is 68.2. acc of weighted cross-entropy acc is 57.2. acc of weighted cross-entropy acc is 70.2.
table 3 shows performance of recent neural network based models without using pretrained embeddings. accuracy of liu et al. (2016) ctb6 is 94.6. accuracy of liu et al. (2016) msr is 94.8. accuracy of liu et al. (2016) pku is 94.9. accuracy of zhou et al. (2017) ctb6 is 94.9. accuracy of zhou et al. (2017) msr is 97.2. accuracy of zhou et al. (2017) pku is 95.0. accuracy of cai et al. (2017) as is 95.2. accuracy of cai et al. (2017) cityu is 95.4. accuracy of cai et al. (2017) msr is 97.0. accuracy of cai et al. (2017) pku is 95.4. accuracy of wang and xu (2017) msr is 96.7. accuracy of wang and xu (2017) pku is 94.7. accuracy of ours as is 95.5. accuracy of ours cityu is 95.7. accuracy of ours ctb6 is 95.5. accuracy of ours ctb7 is 95.6. accuracy of ours msr is 97.5. accuracy of ours pku is 95.4. accuracy of ours ud is 94.6.
table 6 shows ablation results on development data. accuracy of this work as is 98.03. accuracy of this work cityu is 98.22. accuracy of this work ctb6 is 97.06. accuracy of this work ctb7 is 97.07. accuracy of this work msr is 98.48. accuracy of this work pku is 97.95. accuracy of this work ud is 97.00. accuracy of this work average is 97.69. accuracy of -lstm dropout as is +0.03. accuracy of -lstm dropout cityu is -0.33. accuracy of -lstm dropout ctb6 is -0.31. accuracy of -lstm dropout ctb7 is -0.24. accuracy of -lstm dropout msr is +0.04. accuracy of -lstm dropout pku is -0.29. accuracy of -lstm dropout ud is -0.76. accuracy of -lstm dropout average is -0.35. accuracy of -stacked bi-lstm as is -0.13. accuracy of -stacked bi-lstm cityu is -0.20. accuracy of -stacked bi-lstm ctb6 is -0.15. accuracy of -stacked bi-lstm ctb7 is -0.14. accuracy of -stacked bi-lstm msr is -0.17. accuracy of -stacked bi-lstm pku is -0.17. accuracy of -stacked bi-lstm ud is -0.39. accuracy of -stacked bi-lstm average is -0.27. accuracy of -pretrain as is -0.13. accuracy of -pretrain cityu is -0.23. accuracy of -pretrain ctb6 is -0.94. accuracy of -pretrain ctb7 is -0.74. accuracy of -pretrain msr is -0.45. accuracy of -pretrain pku is -0.27. accuracy of -pretrain ud is -2.73. accuracy of -pretrain average is -0.78.
table 2 shows results of slm-4 incorporating ad hoc guidelines, where † represents using additional 1024 segmented setences for training data and * represents using a rule-based post-processing f1 score of slm-4 pku is 79.2. f1 score of slm-4 msr is 79.0. f1 score of slm-4 as is 79.8. f1 score of slm-4 cityu is 79.7. f1 score of slm-4* pku is 81.9. f1 score of slm-4* msr is 83.0. f1 score of slm-4* as is 81.0. f1 score of slm-4* cityu is 81.4. f1 score of slm-4† pku is 87.5. f1 score of slm-4† msr is 84.3. f1 score of slm-4† as is 84.2. f1 score of slm-4† cityu is 86.0. f1 score of slm-4†* pku is 87.3. f1 score of slm-4†* msr is 84.8. f1 score of slm-4†* as is 83.9. f1 score of slm-4†* cityu is 85.8.
table 1 shows comparison of baseline models (b) with the models trained with joint objective (j). test f1 of b100 test f1 is 84.40. average disagreement rate (%) of b100 average disagreement rate (%) is 14.69. test f1 of b10 test f1 is 78.56. average disagreement rate (%) of b10 average disagreement rate (%) is 17.01. test f1 of b1 test f1 is 67.28. average disagreement rate (%) of b1 average disagreement rate (%) is 21.17. test f1 of j100 test f1 is 84.75 (+0.35). average disagreement rate (%) of j100 average disagreement rate (%) is 14.48 (1.43%). test f1 of j10 test f1 is 79.09 (+0.53). average disagreement rate (%) of j10 average disagreement rate (%) is 16.25 (4.47%). test f1 of j1 test f1 is 68.02 (+0.74). average disagreement rate (%) of j1 average disagreement rate (%) is 20.49 (3.21%).
table 2 shows language modeling performance (perplexity) on the wsj test set, broken down by training data used and by whether early stopping is done using the parsing objective (up) or the language modeling objective (lm). ppl median of 10k ppl median is 61.4. ppl median of 10k ppl median is 81.6. ppl median of 10k ppl median is 92.8. ppl median of 15.8k ppl median is 112.1. ppl median of 15.8k ppl median is 112.8. ppl median of 76k ppl median is 797.5. ppl median of 76k ppl median is 848.9.
table 3 shows unlabeled parsing f1 on the multinli development set for models trained on allnli. f1 wrt. of nli lb is 19.3. f1 wrt. of nli rb is 36.9. f1 wrt. of nli sp is 70.2. f1 wrt. of nli depth is 6.2. f1 wrt. of nli lb is 21.2. f1 wrt. of nli rb is 39.0. f1 wrt. of nli sp is 63.5. f1 wrt. of nli depth is 6.4. f1 wrt. of nli lb is 19.2. f1 wrt. of nli rb is 36.2. f1 wrt. of nli sp is 70.5. f1 wrt. of nli depth is 6.1. f1 wrt. of nli lb is 20.6. f1 wrt. of nli rb is 38.9. f1 wrt. of nli sp is 64.1. f1 wrt. of nli depth is 6.3. f1 wrt. of nli lb is 32.6. f1 wrt. of nli rb is 37.5. f1 wrt. of nli sp is 23.7. f1 wrt. of nli depth is 4.1. f1 wrt. of nli lb is 30.8. f1 wrt. of nli rb is 35.6. f1 wrt. of nli sp is 27.5. f1 wrt. of nli depth is 4.6. f1 wrt. of nli lb is 95.0. f1 wrt. of nli rb is 13.5. f1 wrt. of nli sp is 18.8. f1 wrt. of nli depth is 8.6. f1 wrt. of nli lb is 99.1. f1 wrt. of nli rb is 10.7. f1 wrt. of nli sp is 18.1. f1 wrt. of nli depth is 8.6. f1 wrt. of lm lb is 25.6. f1 wrt. of lm rb is 26.9. f1 wrt. of lm sp is 45.7. f1 wrt. of lm depth is 4.9. f1 wrt. of up lb is 19.4. f1 wrt. of up rb is 41.0. f1 wrt. of up sp is 46.3. f1 wrt. of up depth is 4.9. f1 wrt. of lm lb is 19.9. f1 wrt. of lm rb is 37.4. f1 wrt. of lm sp is 48.6. f1 wrt. of lm depth is 4.9. f1 wrt. of - lb is 27.9. f1 wrt. of - rb is 28.0. f1 wrt. of - sp is 27.0. f1 wrt. of - depth is 4.4. f1 wrt. of - lb is 21.7. f1 wrt. of - rb is 36.8. f1 wrt. of - sp is 21.3. f1 wrt. of - depth is 3.9.
table 4 shows performance comparison of two different model architectures using a corpus-based evaluation. inform (%) of w/o attention inform (%) is 99.17. success (%) of w/o attention success (%) is 75.08. bleu of w/o attention bleu is 0.219. inform (%) of w/ attention inform (%) is 99.58. success (%) of w/ attention success (%) is 73.75. bleu of w/ attention bleu is 0.204. inform (%) of w/o attention inform (%) is 71.29. success (%) of w/o attention success (%) is 60.29. bleu of w/o attention bleu is 0.188. inform (%) of w/ attention inform (%) is 71.33. success (%) of w/ attention success (%) is 60.96. bleu of w/ attention bleu is 0.189.
table 1 shows results on the sharc test set, averaged over 3 independent runs for gpt2 and bison, reporting micro accuracy and macro accuracy in terms of the classification task and bleu-1 and bleu-4 on instances for which a clarification question was generated. micro acc. of e&d micro acc. is 31.9. macro acc. of e&d macro acc. is 38.9. b-1 of e&d b-1 is 17.1. b-4 of e&d b-4 is 1.9. micro acc. of e&d+b micro acc. is 54.7. macro acc. of e&d+b macro acc. is 60.4. b-1 of e&d+b b-1 is 24.3. b-4 of e&d+b b-4 is 4.3. micro acc. of gpt2 micro acc. is 60.4. macro acc. of gpt2 macro acc. is 65.1. b-1 of gpt2 b-1 is 53.7. b-4 of gpt2 b-4 is 33.9. micro acc. of bison micro acc. is 64.9. macro acc. of bison macro acc. is 68.8. b-1 of bison b-1 is 61.8. b-4 of bison b-4 is 46.2.
table 1 shows comparison of masked lm perplexity, wikidata probing mrr, and number of parameters (in millions) in the masked lm (word piece embeddings, transformer layers, and output layers), kar, and entity embeddings for bert and knowbert. ppl of bertbase - is 5.5. mrr of bertbase wikidata is 0.09. # params of bertbase masked lm is 110. # params of bertbase kar is 0. # params of bertbase entity embed. is 0. time of bertbase fwd. / bwd. is 0.25. ppl of bertlarge - is 4.5. mrr of bertlarge wikidata is 0.11. # params of bertlarge masked lm is 336. # params of bertlarge kar is 0. # params of bertlarge entity embed. is 0. time of bertlarge fwd. / bwd. is 0.75. ppl of knowbert-wiki - is 4.3. mrr of knowbert-wiki wikidata is 0.26. # params of knowbert-wiki masked lm is 110. # params of knowbert-wiki kar is 2.4. # params of knowbert-wiki entity embed. is 141. time of knowbert-wiki fwd. / bwd. is 0.27. ppl of knowbert-wordnet - is 4.1. mrr of knowbert-wordnet wikidata is 0.22. # params of knowbert-wordnet masked lm is 110. # params of knowbert-wordnet kar is 4.9. # params of knowbert-wordnet entity embed. is 265. time of knowbert-wordnet fwd. / bwd. is 0.31. ppl of knowbert-w+w - is 3.5. mrr of knowbert-w+w wikidata is 0.31. # params of knowbert-w+w masked lm is 110. # params of knowbert-w+w kar is 7.3. # params of knowbert-w+w entity embed. is 406. time of knowbert-w+w fwd. / bwd. is 0.33.
table 4 shows wsdgα results on the sick dataset. pearson of sense pearson is 46.5. spearman of sense spearman is 43.9. mse of sense mse is 7.9. pearson of word pearson is 39.8. spearman of word spearman is 39.9. mse of word mse is 8.6.
table 3 shows performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. f1 of gp-mbcm turns is 2.99. f1 of gp-mbcm inform is 19.04. f1 of gp-mbcm match is 44.29. f1 of gp-mbcm success is 28.9. f1 of acer turns is 10.49. f1 of acer inform is 77.98. f1 of acer match is 62.83. f1 of acer success is 50.8. f1 of ppo turns is 9.83. f1 of ppo inform is 83.34. f1 of ppo match is 69.09. f1 of ppo success is 59.1. f1 of aldm turns is 12.47. f1 of aldm inform is 81.20. f1 of aldm match is 62.60. f1 of aldm success is 61.2. f1 of gdpl-sess turns is 7.49. f1 of gdpl-sess inform is 88.39. f1 of gdpl-sess match is 77.56. f1 of gdpl-sess success is 76.4. f1 of gdpl-discr turns is 7.86. f1 of gdpl-discr inform is 93.21. f1 of gdpl-discr match is 80.43. f1 of gdpl-discr success is 80.5. f1 of gdpl turns is 7.64. f1 of gdpl inform is 94.97. f1 of gdpl match is 83.90. f1 of gdpl success is 86.5. f1 of human turns is 7.37. f1 of human inform is 66.89. f1 of human match is 95.29. f1 of human success is 75.0.
table 3 shows main results on pgr testest. f1 score of bo-lstm (lamurias et al., 2019)† f1 score is 52.3. f1 score of biobert (lee et al., 2019)† f1 score is 67.2. f1 score of textonly f1 score is 76.0. f1 score of deptree f1 score is 78.9. f1 score of kbesteisnerps f1 score is 83.6*. f1 score of edgewiseps f1 score is 85.7**.
table 1 shows precision, recall and f1 results (%) for different models. p of vae p is 17.9. r of vae r is 69.7. f1 of vae f1 is 28.5. p of vae p is 17.9. r of vae r is 69.7. f1 of vae f1 is 28.5. p of rw-hac p is 31.8. r of rw-hac r is 46. f1 of rw-hac f1 is 37.6. p of rw-hac p is 31.8. r of rw-hac r is 46.0. f1 of rw-hac f1 is 37.6. p of sn-hac p is 36.2. r of sn-hac r is 53.3. f1 of sn-hac f1 is 43.1. p of sn-hac p is 34.5. r of sn-hac r is 53.3. f1 of sn-hac f1 is 41.5. p of sn-l p is 36.5. r of sn-l r is 69.2. f1 of sn-l f1 is 47.8. p of sn-l p is 34.6. r of sn-l r is 59.8. f1 of sn-l f1 is 43.9. p of sn-l+v p is 46.1. r of sn-l+v r is 77.3. f1 of sn-l+v f1 is 57.8. p of sn-l+v p is 40.7. r of sn-l+v r is 52.4. f1 of sn-l+v f1 is 45.8. p of sn-l+c p is 47.1. r of sn-l+c r is 78.1. f1 of sn-l+c f1 is 58.8. p of sn-l+c p is 42.3. r of sn-l+c r is 66.0. f1 of sn-l+c f1 is 51.5. p of sn-l+cv1 p is 48.9. r of sn-l+cv1 r is 77.5. f1 of sn-l+cv1 f1 is 59.9. p of sn-l+cv1 p is 40.8. r of sn-l+cv1 r is 74.0. f1 of sn-l+cv1 f1 is 52.6.
table 1 shows micro-averaged precision (p), recall (r) and f1 score on tacred dataset. p of cnn p is 72.1. r of cnn r is 50.3. f1 of cnn f1 is 59.2. p of cnn-pe p is 68.2. r of cnn-pe r is 55.4. f1 of cnn-pe f1 is 61.1. p of gcn p is 69.8. r of gcn r is 59. f1 of gcn f1 is 64. p of lstm p is 61.4. r of lstm r is 61.7. f1 of lstm f1 is 61.5. p of pa-lstm p is 65.7. r of pa-lstm r is 64.5. f1 of pa-lstm f1 is 65.1. p of c-gcn p is 69.9. r of c-gcn r is 63.3. f1 of c-gcn f1 is 66.4. p of self-attn p is 64.6. r of self-attn r is 68.6. f1 of self-attn f1 is 66.5. p of knwl-attn p is 70. r of knwl-attn r is 63.1. f1 of knwl-attn f1 is 66.4. p of knwl+self (mca) p is 68.4. r of knwl+self (mca) r is 66.1. f1 of knwl+self (mca) f1 is 67.3*. p of knwl+self (si) p is 67.1. r of knwl+self (si) r is 68.4. f1 of knwl+self (si) f1 is 67.8*. p of  know+self (kisa) p is 69.4. r of  know+self (kisa) r is 66. f1 of  know+self (kisa) f1 is 67.7*.
table 2 shows performance (%) on low-resource languages. p of cnn-crfs p is 84.4. r of cnn-crfs r is 76.2. f1 of cnn-crfs f1 is 80.1. p of cnn-crfs p is 92. r of cnn-crfs r is 89.1. f1 of cnn-crfs f1 is 90.5. p of cnn-crfs p is 80.9. r of cnn-crfs r is 68.9. f1 of cnn-crfs f1 is 74.4. p of cnn-crfs p is 87.3. r of cnn-crfs r is 85.5. f1 of cnn-crfs f1 is 86.3. p of cnn-crfs p is 88.6. r of cnn-crfs r is 86.7. f1 of cnn-crfs f1 is 87.6. p of bilstm-crfs p is 86. r of bilstm-crfs r is 77.8. f1 of bilstm-crfs f1 is 81.6. p of bilstm-crfs p is 93.3. r of bilstm-crfs r is 91.5. f1 of bilstm-crfs f1 is 92.3. p of bilstm-crfs p is 74.1. r of bilstm-crfs r is 68.9. f1 of bilstm-crfs f1 is 71.3. p of bilstm-crfs p is 89. r of bilstm-crfs r is 85.5. f1 of bilstm-crfs f1 is 87.1. p of bilstm-crfs p is 89.5. r of bilstm-crfs r is 88.5. f1 of bilstm-crfs f1 is 89. p of trans-crfs p is 83.7. r of trans-crfs r is 73.2. f1 of trans-crfs f1 is 78.1. p of trans-crfs p is 93. r of trans-crfs r is 85.9. f1 of trans-crfs f1 is 89.3. p of trans-crfs p is 80.2. r of trans-crfs r is 60.5. f1 of trans-crfs f1 is 69. p of trans-crfs p is 88. r of trans-crfs r is 80. f1 of trans-crfs f1 is 83.8. p of trans-crfs p is 88.9. r of trans-crfs r is 83.2. f1 of trans-crfs f1 is 85.9. p of bilstm-pcrfs p is 85.2. r of bilstm-pcrfs r is 79.6. f1 of bilstm-pcrfs f1 is 82.3. p of bilstm-pcrfs p is 91.2. r of bilstm-pcrfs r is 92.7. f1 of bilstm-pcrfs f1 is 91.9. p of bilstm-pcrfs p is 68.1. r of bilstm-pcrfs r is 70.2. f1 of bilstm-pcrfs f1 is 69.1. p of bilstm-pcrfs p is 82.5. r of bilstm-pcrfs r is 91.2. f1 of bilstm-pcrfs f1 is 86.6. p of bilstm-pcrfs p is 84. r of bilstm-pcrfs r is 90.7. f1 of bilstm-pcrfs f1 is 87.1. p of ours p is 82.8. r of ours r is 82.5. f1 of ours f1 is 82.6. p of ours p is 93.4. r of ours r is 93.5. f1 of ours f1 is 93.4. p of ours p is 73.5. r of ours r is 76.8. f1 of ours f1 is 75.1. p of ours p is 86.9. r of ours r is 93.6. f1 of ours f1 is 90.1. p of ours p is 87.7. r of ours r is 91.5. f1 of ours f1 is 89.5.
table 3 shows performance comparison on cross-domain datasets using f1 score (%). f1 of aida (hoffart et al., 2011) msbnc is 79. f1 of aida (hoffart et al., 2011) aquaint is 56. f1 of aida (hoffart et al., 2011) ace2004 is 80. f1 of aida (hoffart et al., 2011) cweb is 58.6. f1 of aida (hoffart et al., 2011) wiki is 63. f1 of glow (ratinov et al., 2011) msbnc is 75. f1 of glow (ratinov et al., 2011) aquaint is 83. f1 of glow (ratinov et al., 2011) ace2004 is 82. f1 of glow (ratinov et al., 2011) cweb is 56.2. f1 of glow (ratinov et al., 2011) wiki is 67.2. f1 of ri (cheng and roth, 2013) msbnc is 90. f1 of ri (cheng and roth, 2013) aquaint is 90. f1 of ri (cheng and roth, 2013) ace2004 is 86. f1 of ri (cheng and roth, 2013) cweb is 67.5. f1 of ri (cheng and roth, 2013) wiki is 73.4. f1 of wned (guo and barbosa, 2016) msbnc is 92. f1 of wned (guo and barbosa, 2016) aquaint is 87. f1 of wned (guo and barbosa, 2016) ace2004 is 88. f1 of wned (guo and barbosa, 2016) cweb is 77. f1 of wned (guo and barbosa, 2016) wiki is 84.5. f1 of deep-ed (ganea and hofmann, 2017) msbnc is 93.7. f1 of deep-ed (ganea and hofmann, 2017) aquaint is 88.5. f1 of deep-ed (ganea and hofmann, 2017) ace2004 is 88.5. f1 of deep-ed (ganea and hofmann, 2017) cweb is 77.9. f1 of deep-ed (ganea and hofmann, 2017) wiki is 77.5. f1 of ment-norm (le and titov, 2018) msbnc is 93.9. f1 of ment-norm (le and titov, 2018) aquaint is 88.3. f1 of ment-norm (le and titov, 2018) ace2004 is 89.9. f1 of ment-norm (le and titov, 2018) cweb is 77.5. f1 of ment-norm (le and titov, 2018) wiki is 78.0. f1 of prior (p(ejm)) (ganea and hofmann, 2017) msbnc is 89.3. f1 of prior (p(ejm)) (ganea and hofmann, 2017) aquaint is 83.2. f1 of prior (p(ejm)) (ganea and hofmann, 2017) ace2004 is 84.4. f1 of prior (p(ejm)) (ganea and hofmann, 2017) cweb is 69.8. f1 of prior (p(ejm)) (ganea and hofmann, 2017) wiki is 64.2. f1 of berkeley-cnn (section 2.2) msbnc is 89.05. f1 of berkeley-cnn (section 2.2) aquaint is 80.55. f1 of berkeley-cnn (section 2.2) ace2004 is 87.32. f1 of berkeley-cnn (section 2.2) cweb is 67.97. f1 of berkeley-cnn (section 2.2) wiki is 60.27. f1 of berkeley-cnn + dca-sl msbnc is 93.38 ± 0.2. f1 of berkeley-cnn + dca-sl aquaint is 85.63 ± 0.3. f1 of berkeley-cnn + dca-sl ace2004 is 88.73 ± 0.3. f1 of berkeley-cnn + dca-sl cweb is 71.01 ± 0.1. f1 of berkeley-cnn + dca-sl wiki is 72.55 ± 0.2. f1 of berkeley-cnn + dca-rl msbnc is 93.65 ± 0.2. f1 of berkeley-cnn + dca-rl aquaint is 88.53 ± 0.3. f1 of berkeley-cnn + dca-rl ace2004 is 89.73 ± 0.4. f1 of berkeley-cnn + dca-rl cweb is 72.66 ± 0.4. f1 of berkeley-cnn + dca-rl wiki is 73.98 ± 0.2. f1 of ethz-attn (section 2.2) msbnc is 91.97. f1 of ethz-attn (section 2.2) aquaint is 84.06. f1 of ethz-attn (section 2.2) ace2004 is 86.92. f1 of ethz-attn (section 2.2) cweb is 70.07. f1 of ethz-attn (section 2.2) wiki is 74.37. f1 of ethz-attn + dca-sl msbnc is 94.57 ± 0.2. f1 of ethz-attn + dca-sl aquaint is 87.38 ± 0.5. f1 of ethz-attn + dca-sl ace2004 is 89.44 ± 0.4. f1 of ethz-attn + dca-sl cweb is 73.47 ± 0.1. f1 of ethz-attn + dca-sl wiki is 78.16 ± 0.1. f1 of ethz-attn + dca-rl msbnc is 93.80 ± 0.0. f1 of ethz-attn + dca-rl aquaint is 88.25 ± 0.4. f1 of ethz-attn + dca-rl ace2004 is 90.14 ± 0.0. f1 of ethz-attn + dca-rl cweb is 75.59 ± 0.3. f1 of ethz-attn + dca-rl wiki is 78.84 ± 0.2.
table 4 shows ablation study on neighbor entities. in-kb acc. (%) of ethz-attn (section 2.2) sl is  90.88. in-kb acc. (%) of ethz-attn (section 2.2)  rl is  -. in-kb acc. (%) of ethz-attn + 1-hop dca sl is  93.69. in-kb acc. (%) of ethz-attn + 1-hop dca  rl is  93.20. in-kb acc. (%) of ethz-attn + 2-hop dca sl is  94.47. in-kb acc. (%) of ethz-attn + 2-hop dca  rl is  93.76.
table 2 shows overall results for entity set expansion on google web 1t,where oursfull is the full version of our method, ours-mcts is our method with the mcts disabled, and ours-pmsn is our method but replacing the pmsn with fixed word embeddings. p@10 of pos p@10 is 0.84. p@20 of pos p@20 is 0.74. p@50 of pos p@50 is 0.55. p@100 of pos p@100 is 0.41. p@200 of pos p@200 is 0.34. map of pos map is 0.42. p@10 of meb p@10 is 0.83. p@20 of meb p@20 is 0.79. p@50 of meb p@50 is 0.68. p@100 of meb p@100 is 0.58. p@200 of meb p@200 is 0.51. p@10 of cob* p@10 is 0.97. p@20 of cob* p@20 is 0.96. p@50 of cob* p@50 is 0.9. p@100 of cob* p@100 is 0.79. p@200 of cob* p@200 is 0.66. map of cob* map is 0.85. p@10 of ours full p@10 is 0.97. p@20 of ours full p@20 is 0.96. p@50 of ours full p@50 is 0.92. p@100 of ours full p@100 is 0.82. p@200 of ours full p@200 is 0.69. map of ours full map is 0.87. p@10 of ours -mcts p@10 is 0.85. p@20 of ours -mcts p@20 is 0.81. p@50 of ours -mcts p@50 is 0.73. p@100 of ours -mcts p@100 is 0.63. p@200 of ours -mcts p@200 is 0.52. map of ours -mcts map is 0.75. p@10 of ours -pmsn p@10 is 0.63. p@20 of ours -pmsn p@20 is 0.6. p@50 of ours -pmsn p@50 is 0.56. p@100 of ours -pmsn p@100 is 0.48. p@200 of ours -pmsn p@200 is 0.42. map of ours -pmsn map is 0.61.
table 7 shows event argument role labeling results (f1 %) on chinese and arabic using english as training data (with system generated entity mentions) f1 score of chinese f1 score is 56.9. f1 score of arabic f1 score is 60.1.
table 4 shows our results on five categories compared to ju et al. p (%) of dna p (%) is 73.6. r (%) of dna r (%) is 67.8. f (%) of dna f (%) is 70.6. f (%) of dna f (%) is 70.1. f (%) of dna f (%) is 67.8. p (%) of rna p (%) is 82.2. r (%) of rna r (%) is 80.7. f (%) of rna f (%) is 81.5. f (%) of rna f (%) is 80.8. f (%) of rna f (%) is 75.9. p (%) of protein p (%) is 76.7. r (%) of protein r (%) is 76. f (%) of protein f (%) is 76.4. f (%) of protein f (%) is 72.7. f (%) of protein f (%) is 72.9. p (%) of cell line p (%) is 77.8. r (%) of cell line r (%) is 65.8. f (%) of cell line f (%) is 71.3. f (%) of cell line f (%) is 66.9. f (%) of cell line f (%) is 63.6. p (%) of cell type p (%) is 73.9. r (%) of cell type r (%) is 71.2. f (%) of cell type f (%) is 72.5. f (%) of cell type f (%) is 71.3. f (%) of cell type f (%) is 69.8. p (%) of overall p (%) is 75.8. r (%) of overall r (%) is 73.6. f (%) of overall f (%) is 74.7. f (%) of overall f (%) is 71.1. f (%) of overall f (%) is 70.7.
table 5 shows performance of boundary detection on genia test set. p (%) of sohrab and miwa (2018) p (%) is 76.6. r (%) of sohrab and miwa (2018) r (%) is 69.2. f (%) of sohrab and miwa (2018) f (%) is 72.7. p (%) of ju et al. (2018) p (%) is 79.9. r (%) of ju et al. (2018) r (%) is 67.08. f (%) of ju et al. (2018) f (%) is 73.4. p (%) of our model(softmax) p (%) is 79.7. r (%) of our model(softmax) r (%) is 76.9. f (%) of our model(softmax) f (%) is 78.3.
table 7 shows performance comparison of our pipeline model and multitask model on genia development set and test set. p (%) of pipeline p (%) is 74.5. r (%) of pipeline r (%) is 74.8. f (%) of pipeline f (%) is 74.6. p (%) of pipeline p (%) is 75.4. r (%) of pipeline r (%) is 72.2. f (%) of pipeline f (%) is 73.8. p (%) of multitask p (%) is 74.5. r (%) of multitask r (%) is 75.6. f (%) of multitask f (%) is 75. p (%) of multitask p (%) is 75.9. r (%) of multitask r (%) is 73.4. f (%) of multitask f (%) is 74.7.
table 3 shows p@n results for models with internal cnns self-attention and curriculum learning p@n (%) of 100 cnn+one is 67.3. p@n (%) of 100 rescnn-9 is 79. p@n (%) of 100 cnn+one+selfatt is 81.1. p@n (%) of 100 cnn+att is 76.2. p@n (%) of 100 cnn+att+selfatt is 81.1. p@n (%) of 100 pcnn+one is 72.3. p@n (%) of 100 pcnn+one+selatt is 84.1. p@n (%) of 100 [netmax+selfatt]+ccl-ct is 85.1. p@n (%) of 100 pcnn+att is 76.2. p@n (%) of 100 pcnn+att+selfatt is 81.1. p@n (%) of 100 [netatt+selfatt]+ccl-ct is 82.2. p@n (%) of 200 cnn+one is 64.7. p@n (%) of 200 rescnn-9 is 69. p@n (%) of 200 cnn+one+selfatt is 75.1. p@n (%) of 200 cnn+att is 68.6. p@n (%) of 200 cnn+att+selfatt is 74.1. p@n (%) of 200 pcnn+one is 69.7. p@n (%) of 200 pcnn+one+selatt is 75.1. p@n (%) of 200 [netmax+selfatt]+ccl-ct is 78.6. p@n (%) of 200 pcnn+att is 73.1. p@n (%) of 200 pcnn+att+selfatt is 71.6. p@n (%) of 200 [netatt+selfatt]+ccl-ct is 79.1. p@n (%) of 300 cnn+one is 58.1. p@n (%) of 300 rescnn-9 is 61. p@n (%) of 300 cnn+one+selfatt is 70.4. p@n (%) of 300 cnn+att is 59.8. p@n (%) of 300 cnn+att+selfatt is 72.4. p@n (%) of 300 pcnn+one is 64.1. p@n (%) of 300 pcnn+one+selatt is 69.1. p@n (%) of 300 [netmax+selfatt]+ccl-ct is 74.4. p@n (%) of 300 pcnn+att is 67.4. p@n (%) of 300 pcnn+att+selfatt is 70.4. p@n (%) of 300 [netatt+selfatt]+ccl-ct is 73.1. p@n (%) of mean cnn+one is 63.4. p@n (%) of mean rescnn-9 is 69.7. p@n (%) of mean cnn+one+selfatt is 75.5. p@n (%) of mean cnn+att is 68.2. p@n (%) of mean cnn+att+selfatt is 75.9. p@n (%) of mean pcnn+one is 68.7. p@n (%) of mean pcnn+one+selatt is 76.1. p@n (%) of mean [netmax+selfatt]+ccl-ct is 79.4. p@n (%) of mean pcnn+att is 72.2. p@n (%) of mean pcnn+att+selfatt is 74.4. p@n (%) of mean [netatt+selfatt]+ccl-ct is 78.1.
table 3 shows performances of entity representations on enteval tasks. accuracy of glove cap is 71.9. accuracy of glove cerp is 52.6. accuracy of glove efp is 67. accuracy of glove et is 10.3. accuracy of glove esr is 50.9. accuracy of glove ert is 40.8. accuracy of glove ned is 41.2. accuracy of glove average is 47.8. accuracy of bert base cap is 80.6. accuracy of bert base cerp is 65.6. accuracy of bert base efp is 74.8. accuracy of bert base et is 32. accuracy of bert base esr is 28.8. accuracy of bert base ert is 42.2. accuracy of bert base ned is 50.6. accuracy of bert base average is 53.5. accuracy of bert large cap is 79.1. accuracy of bert large cerp is 66.9. accuracy of bert large efp is 76.7. accuracy of bert large et is 32.3. accuracy of bert large esr is 32.6. accuracy of bert large ert is 48.8. accuracy of bert large ned is 54.3. accuracy of bert large average is 55.8. accuracy of elmo cap is 80.2. accuracy of elmo cerp is 61.2. accuracy of elmo efp is 75.8. accuracy of elmo et is 35.6. accuracy of elmo esr is 60.3. accuracy of elmo ert is 46.8. accuracy of elmo ned is 51.6. accuracy of elmo average is 58.8. accuracy of entelmo baseline cap is 78. accuracy of entelmo baseline cerp is 59.6. accuracy of entelmo baseline efp is 71.5. accuracy of entelmo baseline et is 31.3. accuracy of entelmo baseline esr is 61.6. accuracy of entelmo baseline ert is 46.5. accuracy of entelmo baseline ned is 48.5. accuracy of entelmo baseline average is 56.7. accuracy of entelmo cap is 76.9. accuracy of entelmo cerp is 59.9. accuracy of entelmo efp is 72.4. accuracy of entelmo et is 32.2. accuracy of entelmo esr is 59.7. accuracy of entelmo ert is 45.7. accuracy of entelmo ned is 49. accuracy of entelmo average is 56.5. accuracy of entelmo w/o lctx cap is 73.5. accuracy of entelmo w/o lctx cerp is 59.4. accuracy of entelmo w/o lctx efp is 71.1. accuracy of entelmo w/o lctx et is 33.2. accuracy of entelmo w/o lctx esr is 53.3. accuracy of entelmo w/o lctx ert is 44.6. accuracy of entelmo w/o lctx ned is 48.9. accuracy of entelmo w/o lctx average is 54.9. accuracy of entelmo w/ letn cap is 76.2. accuracy of entelmo w/ letn cerp is 60.4. accuracy of entelmo w/ letn efp is 70.9. accuracy of entelmo w/ letn et is 33.6. accuracy of entelmo w/ letn esr is 49. accuracy of entelmo w/ letn ert is 42.9. accuracy of entelmo w/ letn ned is 49.3. accuracy of entelmo w/ letn average is 54.6.
table 4 shows model performance breakdown for tb-dense. p of b p is 41.4. r of b r is 19.5. f1 of b f1 is 26.5. p of b p is 59. r of b r is 46.9. f1 of b f1 is 52.3. p of b p is 59.8. r of b r is 46.9. f1 of b f1 is 52.6. p of a p is 42.1. r of a r is 17.5. f1 of a f1 is 24.7. p of a p is 69.3. r of a r is 45.3. f1 of a f1 is 54.8. p of a p is 71.9. r of a r is 46.7. f1 of a f1 is 56.6. p of i p is 50. r of i r is 3.6. f1 of i f1 is 6.7. p of ii p is 38.5. r of ii r is 9.4. f1 of ii f1 is 15.2. p of s p is 14.3. r of s r is 4.5. f1 of s f1 is 6.9. p of v p is 44.9. r of v r is 59.4. f1 of v f1 is 51.1. p of v p is 45.1. r of v r is 55. f1 of v f1 is 49.5. p of v p is 45.9. r of v r is 55.8. f1 of v f1 is 50.4. p of avg p is 43.8. r of avg r is 35.7. f1 of avg f1 is 39.4. p of avg p is 51.5. r of avg r is 45.9. f1 of avg f1 is 48.5. p of avg p is 52.6. r of avg r is 46.5. f1 of avg f1 is 49.4.
table 1 shows experimental results of our model compared with other models. accuracy of svm [socher et al. 2013] sst-2 is 79.4. accuracy of svm [socher et al. 2013] sst-5 is 40.7. accuracy of nb [socher et al. 2013] sst-2 is 81.8. accuracy of nb [socher et al. 2013] sst-5 is 41. accuracy of nbsvm-bi [wang and manning 2012b] mr is 79.4. accuracy of nbsvm-bi [wang and manning 2012b] subj is 93.2. accuracy of standard-lstm sst-2 is 80.6. accuracy of standard-lstm sst-5 is 45.3. accuracy of standard-lstm mr is 75.9. accuracy of standard-lstm subj is 89.3. accuracy of standard-lstm trec is 86.8. accuracy of standard-lstm ag news is 86.1. accuracy of bi-lstm sst-2 is 83.2. accuracy of bi-lstm sst-5 is 46.7. accuracy of bi-lstm mr is 79.3. accuracy of bi-lstm subj is 90.5. accuracy of bi-lstm trec is 89.6. accuracy of bi-lstm ag news is 88.2. accuracy of rcnn [lai et al. 2015] sst-5 is 47.21. accuracy of rcnn [lai et al. 2015] ag news is . accuracy of snn [zhao et al. 2018] sst-5 is 50.4. accuracy of snn [zhao et al. 2018] mr is 82.1. accuracy of snn [zhao et al. 2018] subj is 93.9. accuracy of snn [zhao et al. 2018] trec is 96. accuracy of cnn-non-static [kim 2014] sst-2 is 87.2. accuracy of cnn-non-static [kim 2014] sst-5 is 48. accuracy of cnn-non-static [kim 2014] mr is 81.5. accuracy of cnn-non-static [kim 2014] subj is 93.4. accuracy of cnn-non-static [kim 2014] trec is 93.6. accuracy of cnn-non-static [kim 2014] ag news is 92.3. accuracy of vd-cnn [schwenk et al. 2017] subj is 88.2. accuracy of vd-cnn [schwenk et al. 2017] trec is 85.4. accuracy of vd-cnn [schwenk et al. 2017] ag news is 91.3. accuracy of cl-cnn [zhang et al. 2015a] subj is 88.4. accuracy of cl-cnn [zhang et al. 2015a] trec is 85.7. accuracy of cl-cnn [zhang et al. 2015a] ag news is 92.3. accuracy of capsule-b [yang et al. 2018] sst-2 is 86.8. accuracy of capsule-b [yang et al. 2018] mr is 82.3. accuracy of capsule-b [yang et al. 2018] subj is 93.8. accuracy of capsule-b [yang et al. 2018] trec is 93.2. accuracy of capsule-b [yang et al. 2018] ag news is 92.6. accuracy of hcapsnet sst-2 is 88.7. accuracy of hcapsnet sst-5 is 50.8. accuracy of hcapsnet mr is 83.5. accuracy of hcapsnet subj is 94.2. accuracy of hcapsnet trec is 94.2. accuracy of hcapsnet ag news is 93.5.
table 5 shows cross-lingual (xl) system results using bleu score on individual languages inside the dev set. bleu of xl-glove [all] f-seq is 18.86. bleu of xl-glove [all] word is 17.17. bleu of xl-glove [all] label is 25.52. bleu of xl-glove [all] f-seq is 28.99. bleu of xl-glove [all] word is 17.36. bleu of xl-glove [all] label is 32.76. bleu of xl-bert [all] f-seq is 27.22. bleu of xl-bert [all] word is 27.36. bleu of xl-bert [all] label is 29.59. bleu of xl-bert [all] f-seq is 33.59. bleu of xl-bert [all] word is 22.48. bleu of xl-bert [all] label is 37.17. bleu of xl-glove [greater than equal 10] f-seq is 30.58. bleu of xl-glove [greater than equal 10] word is 36.71. bleu of xl-glove [greater than equal 10] label is 51.68. bleu of xl-glove [greater than equal 10] f-seq is 38.99. bleu of xl-glove [greater than equal 10] word is 43.79. bleu of xl-glove [greater than equal 10] label is 61.73. bleu of xl-bert [greater than equal 10] f-seq is 36.95. bleu of xl-bert [greater than equal 10] word is 41.36. bleu of xl-bert [greater than equal 10] label is 55.73. bleu of xl-bert [greater than equal 10] f-seq is 42.66. bleu of xl-bert [greater than equal 10] word is 46.52. bleu of xl-bert [greater than equal 10] label is 65.32.
table 5 shows srl results with different incorporation methods of the syntactic information on the chinese dev set. p of dep p is 83.89. r of dep r is 83.61. f1 of dep f1 is 83.75. p of dep&rel p is 86.21. r of dep&rel r is 85. f1 of dep&rel f1 is 85.6. p of dep&relpath p is 86.01. r of dep&relpath r is 85.38. f1 of dep&relpath f1 is 85.69. p of deppath&relpath p is 85.84. r of deppath&relpath r is 85.54. f1 of deppath&relpath f1 is 85.69. p of dep p is 84.68. r of dep r is 85.38. f1 of dep f1 is 85.03. p of dep&rel p is 85.56. r of dep&rel r is 85.89. f1 of dep&rel f1 is 85.73. p of dep&relpath p is 85.84. r of dep&relpath r is 85.64. f1 of dep&relpath f1 is 85.74. p of dep p is 84.33. r of dep r is 84.47. f1 of dep f1 is 84.4. p of dep&rel p is 86.04. r of dep&rel r is 85.43. f1 of dep&rel f1 is 85.73. p of dep&relpath p is 86.21. r of dep&relpath r is 85.01. f1 of dep&relpath f1 is 85.6. p of deppath&relpath p is 86.4. r of deppath&relpath r is 85.52. f1 of deppath&relpath f1 is 85.96.
table 7 shows srl results on the chinese test set. p of metric p is 81.99. r of metric r is 80.65. f1 of metric f1 is 81.31. f1 of conll09 srl only f1 is 78.6. p of input(deppath&relpath) p is 84.19. r of input(deppath&relpath) r is 83.65. f1 of input(deppath&relpath) f1 is 83.92. p of lisa(dep&relpath) p is 83.84. r of lisa(dep&relpath) r is 83.54. f1 of lisa(dep&relpath) f1 is 83.69. p of relawe(deppath&relpath) p is 84.77. r of relawe(deppath&relpath) r is 83.68. f1 of relawe(deppath&relpath) f1 is 84.22. f1 of marcheggiani and titov (2017) f1 is 82.5. p of cai et al. (2018) p is 84.7. r of cai et al. (2018) r is 84. f1 of cai et al. (2018) f1 is 84.3. p of input(deppath&relpath) + bert p is 86.89. r of input(deppath&relpath) + bert r is 87.75. f1 of input(deppath&relpath) + bert f1 is 87.32. p of lisa(dep&relpath) + bert p is 86.45. r of lisa(dep&relpath) + bert r is 87.9. f1 of lisa(dep&relpath) + bert f1 is 87.17. p of relawe(deppath&relpath) + bert p is 86.73. r of relawe(deppath&relpath) + bert r is 87.98. f1 of relawe(deppath&relpath) + bert f1 is 87.35. p of metric p is 91.93. r of metric r is 92.36. f1 of metric f1 is 92.14.
table 8 shows results for predicting temporal anchors with the neural network (only text, and text and image). p of yes p is 0.74. r of yes r is 0.96. f1 of yes f1 is 0.83. p of yes p is 0.92. r of yes r is 0.98. f1 of yes f1 is 0.95. p of yes p is 0.82. r of yes r is 0.88. f1 of yes f1 is 0.85. p of no p is 0.35. r of no r is 0.07. f1 of no f1 is 0.11. p of no p is 0. r of no r is 0. f1 of no f1 is 0. p of no p is 0.29. r of no r is 0.21. f1 of no f1 is 0.24. p of macro avg. p is 0.55. r of macro avg. r is 0.52. f1 of macro avg. f1 is 0.47. p of macro avg. p is 0.46. r of macro avg. r is 0.49. f1 of macro avg. f1 is 0.48. p of macro avg. p is 0.56. r of macro avg. r is 0.55. f1 of macro avg. f1 is 0.55. p of yes p is 0.7. r of yes r is 0.78. f1 of yes f1 is 0.74. p of yes p is 0.88. r of yes r is 0.97. f1 of yes f1 is 0.92. p of yes p is 0.84. r of yes r is 0.89. f1 of yes f1 is 0.87. p of no p is 0.48. r of no r is 0.38. f1 of no f1 is 0.43. p of no p is 0.25. r of no r is 0.08. f1 of no f1 is 0.12. p of no p is 0.53. r of no r is 0.41. f1 of no f1 is 0.46. p of macro avg. p is 0.59. r of macro avg. r is 0.58. f1 of macro avg. f1 is 0.59. p of macro avg. p is 0.57. r of macro avg. r is 0.53. f1 of macro avg. f1 is 0.52. p of macro avg. p is 0.69. r of macro avg. r is 0.65. f1 of macro avg. f1 is 0.67.
table 6 shows results on test unseenall of our model, trained with and without curiosity-encouraging loss, and an lstm-based encoder-decoder model (both models have about 15m parameters). sr (%) of lstm-encdec sr (%) is 19.25. nav. mistake repeat (%) of lstm-encdec nav. mistake repeat (%) is 31.09. help-request repeat (%) of lstm-encdec help-request repeat (%) is 49.37. sr (%) of our model (alpha = 0) sr (%) is 43.12. nav. mistake repeat (%) of our model (alpha = 0) nav. mistake repeat (%) is 25. help-request repeat (%) of our model (alpha = 0) help-request repeat (%) is 40.17. sr (%) of our model (alpha = 1) sr (%) is 47.45. nav. mistake repeat (%) of our model (alpha = 1) nav. mistake repeat (%) is 17.85. help-request repeat (%) of our model (alpha = 1) help-request repeat (%) is 21.1.
table 2 shows experimental results in exploring different lexical mapping methods. pre. of embbeding_proj pre. is 26. rec. of embbeding_proj rec. is 20. f1 of embbeding_proj f1 is 22.6. pre. of cl trans (1 cand.) pre. is 31.2. rec. of cl trans (1 cand.) rec. is 21.4. f1 of cl trans (1 cand.) f1 is 25.4. pre. of cl trans (2 cand.) pre. is 31.7. rec. of cl trans (2 cand.) rec. is 22.3. f1 of cl trans (2 cand.) f1 is 26.2. pre. of cl trans (3 cand.) pre. is 32. rec. of cl trans (3 cand.) rec. is 23.4. f1 of cl trans (3 cand.) f1 is 27. pre. of cl trans (4 cand.) pre. is 30.7. rec. of cl trans (4 cand.) rec. is 23.6. f1 of cl trans (4 cand.) f1 is 26.7. pre. of cl trans (5 cand.) pre. is 30.2. rec. of cl trans (5 cand.) rec. is 23.6. f1 of cl trans (5 cand.) f1 is 26.5.
table 2 shows performance of the rule-based baselines and the post conditioned models on the ingredient detection task of the recipes dataset. acc of majority acc is 57.27. p of exact match p is 84.94. r of exact match r is 20.25. f1 of exact match f1 is 32.7. acc of exact match acc is 64.39. ur of exact match ur is 73.42. cr of exact match cr is 4.02. p of first occ p is 65.23. r of first occ r is 87.17. f1 of first occ f1 is 74.6. acc of first occ acc is 74.65. ur of first occ ur is 84.88. cr of first occ cr is 87.79. p of gptattn p is 63.94. r of gptattn r is 71.72. f1 of gptattn f1 is 67.6. acc of gptattn acc is 70.63. ur of gptattn ur is 54.3. cr of gptattn cr is 77.04. p of gptindep p is 67.05. r of gptindep r is 69.07. f1 of gptindep f1 is 68.04. acc of gptindep acc is 72.28. ur of gptindep ur is 47.09. cr of gptindep cr is 75.79. p of elmotoken p is 64.96. r of elmotoken r is 76.64. f1 of elmotoken f1 is 70.32. acc of elmotoken acc is 72.35. ur of elmotoken ur is 69.14. cr of elmotoken cr is 78.94. p of elmosent p is 69.09. r of elmosent r is 72.88. f1 of elmosent f1 is 70.9. acc of elmosent acc is 74.48. ur of elmosent ur is 57.05. cr of elmosent cr is 77.71.
table 9 shows model’s performance degradation with input ablations. accuracy of complete process accuracy is 84.59. accuracy of w/o other ingredients accuracy is 82.71. accuracy of w/o verbs accuracy is 79.08. accuracy of w/o verbs & other ingredients accuracy is 77.79.
table 1 shows the comparison between the proposed methods llmap and rgp, and the muse supervised method. p@1 of czech (cs) llmap is 28.29. p@1 of czech (cs) muse is 28.37. p@1 of czech (cs) rgp is 28.37. p@5 of czech (cs) llmap is 56.92. p@5 of czech (cs) muse is 55.65. p@5 of czech (cs) rgp is 55.88. p@10 of czech (cs) llmap is 65.99. p@10 of czech (cs) muse is 64.72. p@10 of czech (cs) rgp is 64.94. p@1 of norwegian (no) llmap is 32.9. p@1 of norwegian (no) muse is 31.62. p@1 of norwegian (no) rgp is 31.63. p@5 of norwegian (no) llmap is 58.74. p@5 of norwegian (no) muse is 56.13. p@5 of norwegian (no) rgp is 56.53. p@10 of norwegian (no) llmap is 66.23. p@10 of norwegian (no) muse is 63.57. p@10 of norwegian (no) rgp is 64.01. p@1 of dutch (nl) llmap is 42.3. p@1 of dutch (nl) muse is 41.06. p@1 of dutch (nl) rgp is 41.18. p@5 of dutch (nl) llmap is 67.13. p@5 of dutch (nl) muse is 65.43. p@5 of dutch (nl) rgp is 65.7. p@10 of dutch (nl) llmap is 73.73. p@10 of dutch (nl) muse is 72.03. p@10 of dutch (nl) rgp is 72.49. p@1 of chinese (zh) llmap is 17.4. p@1 of chinese (zh) muse is 14.19. p@1 of chinese (zh) rgp is 19.51. p@5 of chinese (zh) llmap is 43.19. p@5 of chinese (zh) muse is 35.6. p@5 of chinese (zh) rgp is 44.05. p@10 of chinese (zh) llmap is 52.11. p@10 of chinese (zh) muse is 44.62. p@10 of chinese (zh) rgp is 52.68. p@1 of korean (ko) llmap is 17.12. p@1 of korean (ko) muse is 17.02. p@1 of korean (ko) rgp is 16.81. p@5 of korean (ko) llmap is 35.8. p@5 of korean (ko) muse is 34.41. p@5 of korean (ko) rgp is 34.23. p@10 of korean (ko) llmap is 44.05. p@10 of korean (ko) muse is 42.69. p@10 of korean (ko) rgp is 42.12. p@1 of japanese (ja) llmap is 9.46. p@1 of japanese (ja) muse is 2.45. p@1 of japanese (ja) rgp is 9.97. p@5 of japanese (ja) llmap is 20.3. p@5 of japanese (ja) muse is 6.97. p@5 of japanese (ja) rgp is 20.72. p@10 of japanese (ja) llmap is 25.83. p@10 of japanese (ja) muse is 9.85. p@10 of japanese (ja) rgp is 26.66. p@1 of croatian (hr) llmap is 19.29. p@1 of croatian (hr) muse is 18.71. p@1 of croatian (hr) rgp is 18.87. p@5 of croatian (hr) llmap is 46.4. p@5 of croatian (hr) muse is 43.55. p@5 of croatian (hr) rgp is 44.25. p@10 of croatian (hr) llmap is 56.36. p@10 of croatian (hr) muse is 53.38. p@10 of croatian (hr) rgp is 54.12. p@1 of indonesian (id) llmap is 31.9. p@1 of indonesian (id) muse is 30.37. p@1 of indonesian (id) rgp is 30.45. p@5 of indonesian (id) llmap is 54.63. p@5 of indonesian (id) muse is 51.96. p@5 of indonesian (id) rgp is 52.4. p@10 of indonesian (id) llmap is 62.06. p@10 of indonesian (id) muse is 59.24. p@10 of indonesian (id) rgp is 59.67. p@1 of farsi (fa) llmap is 17.48. p@1 of farsi (fa) muse is 16.69. p@1 of farsi (fa) rgp is 17.12. p@5 of farsi (fa) llmap is 35.36. p@5 of farsi (fa) muse is 33.14. p@5 of farsi (fa) rgp is 33.7. p@10 of farsi (fa) llmap is 42.7. p@10 of farsi (fa) muse is 40.24. p@10 of farsi (fa) rgp is 40.75. p@1 of bulgarian (bg) llmap is 21.64. p@1 of bulgarian (bg) muse is 23.4. p@1 of bulgarian (bg) rgp is 23.02. p@5 of bulgarian (bg) llmap is 56.19. p@5 of bulgarian (bg) muse is 55.22. p@5 of bulgarian (bg) rgp is 55.35. p@10 of bulgarian (bg) llmap is 67.12. p@10 of bulgarian (bg) muse is 65.95. p@10 of bulgarian (bg) rgp is 66.11. p@1 of spanish (es) llmap is 42.36. p@1 of spanish (es) muse is 42.01. p@1 of spanish (es) rgp is 42.08. p@5 of spanish (es) llmap is 70.75. p@5 of spanish (es) muse is 69.94. p@5 of spanish (es) rgp is 70.12. p@10 of spanish (es) llmap is 77.32. p@10 of spanish (es) muse is 76.41. p@10 of spanish (es) rgp is 76.73. p@1 of tamil (ta) llmap is 9.79. p@1 of tamil (ta) muse is 9.76. p@1 of tamil (ta) rgp is 10.07. p@5 of tamil (ta) llmap is 24.3. p@5 of tamil (ta) muse is 22.34. p@5 of tamil (ta) rgp is 23.08. p@10 of tamil (ta) llmap is 31.38. p@10 of tamil (ta) muse is 28.78. p@10 of tamil (ta) rgp is 29.8. p@1 of hindi (hi) llmap is 16.83. p@1 of hindi (hi) muse is 16.26. p@1 of hindi (hi) rgp is 16.79. p@5 of hindi (hi) llmap is 36.51. p@5 of hindi (hi) muse is 32.88. p@5 of hindi (hi) rgp is 34.69. p@10 of hindi (hi) llmap is 43.91. p@10 of hindi (hi) muse is 39.92. p@10 of hindi (hi) rgp is 41.85. p@1 of bengali (bn) llmap is 15.51. p@1 of bengali (bn) muse is 15.3. p@1 of bengali (bn) rgp is 15.95. p@5 of bengali (bn) llmap is 39.67. p@5 of bengali (bn) muse is 35.55. p@5 of bengali (bn) rgp is 37.06. p@10 of bengali (bn) llmap is 48.43. p@10 of bengali (bn) muse is 43.94. p@10 of bengali (bn) rgp is 45.51. p@1 of average improvement (llmap-muse) llmap is 1.07. p@1 of average improvement (llmap-muse) muse is 1.07. p@1 of average improvement (llmap-muse) rgp is 1.07. p@5 of average improvement (llmap-muse) llmap is 3.36. p@5 of average improvement (llmap-muse) muse is 3.36. p@5 of average improvement (llmap-muse) rgp is 3.36. p@10 of average improvement (llmap-muse) llmap is 3.7. p@10 of average improvement (llmap-muse) muse is 3.7. p@10 of average improvement (llmap-muse) rgp is 3.7.
table 2 shows the comparison between the proposed method llmap and muse on pre-split train and test dictionaries for any and all senses recovery by the algorithms for precision@5. precision@5 of cs muse is 76.66. precision@5 of cs llmap is 78.86. precision@5 of cs muse is 61.32. precision@5 of cs llmap is 62.17. precision@5 of no muse is 80. precision@5 of no llmap is 81.85. precision@5 of no muse is 65.57. precision@5 of no llmap is 67.35. precision@5 of nl muse is 88.53. precision@5 of nl llmap is 89.33. precision@5 of nl muse is 69.28. precision@5 of nl llmap is 70.52. precision@5 of zh muse is 55. precision@5 of zh llmap is 64.77. precision@5 of zh muse is 41.88. precision@5 of zh llmap is 51.5. precision@5 of ko muse is 51.94. precision@5 of ko llmap is 54.23. precision@5 of ko muse is 42.81. precision@5 of ko llmap is 44.4. precision@5 of ja muse is 3.97. precision@5 of ja llmap is 17.62. precision@5 of ja muse is 3.22. precision@5 of ja llmap is 14.51. precision@5 of hr muse is 62.46. precision@5 of hr llmap is 64.7. precision@5 of hr muse is 52.19. precision@5 of hr llmap is 53.78. precision@5 of id muse is 81.2. precision@5 of id llmap is 85.19. precision@5 of id muse is 64.87. precision@5 of id llmap is 68.2. precision@5 of fa muse is 53.73. precision@5 of fa llmap is 56.03. precision@5 of fa muse is 42.64. precision@5 of fa llmap is 44.29. precision@5 of bg muse is 65.07. precision@5 of bg llmap is 68.13. precision@5 of bg muse is 59.45. precision@5 of bg llmap is 59.48. precision@5 of es muse is 92.1. precision@5 of es llmap is 92.23. precision@5 of es muse is 71.46. precision@5 of es llmap is 72.11. precision@5 of ta muse is 28.53. precision@5 of ta llmap is 29. precision@5 of ta muse is 25.6. precision@5 of ta llmap is 26.01. precision@5 of hi muse is 51.2. precision@5 of hi llmap is 53.93. precision@5 of hi muse is 38.22. precision@5 of hi llmap is 44.53. precision@5 of bn muse is 33.8. precision@5 of bn llmap is 36.42. precision@5 of bn muse is 42.42. precision@5 of bn llmap is 45.1. precision@5 of avg muse is 3.45. precision@5 of avg llmap is 3.45. precision@5 of avg muse is 3.07. precision@5 of avg llmap is 3.07.
table 6 shows results for docrepair trained on different amount of data. bleu of 2.5m bleu is 34.15. deixis of 2.5m deixis is 89.2. lex. c. of 2.5m lex. c. is 75.5. ellipsis of 2.5m ellipsis is 81.8/71.6. bleu of 5m bleu is 34.44. deixis of 5m deixis is 90.3. lex. c. of 5m lex. c. is 77.7. ellipsis of 5m ellipsis is 83.6/74.0. bleu of 30m bleu is 34.6. deixis of 30m deixis is 91.8. lex. c. of 30m lex. c. is 80.6. ellipsis of 30m ellipsis is 86.4/75.2.
table 4 shows tokenized case-sensitive bleu (bleu) and perplexity (ppl) on training (train) and development (newstest2013, dev) set. bleu of 1 train is 28.64. bleu of 1 dev is 26.16. ppl of 1 train is 5.23. ppl of 1 dev is 4.76. bleu of 11 train is 29.63. bleu of 11 dev is 26.44. ppl of 11 train is 4.48. ppl of 11 dev is 4.38. bleu of 12 train is 29.75. bleu of 12 dev is 26.16. ppl of 12 train is 4.6. ppl of 12 dev is 4.49. bleu of 13 train is 29.43. bleu of 13 dev is 26.51. ppl of 13 train is 5.09. ppl of 13 dev is 4.71. bleu of 14 train is 30.71. bleu of 14 dev is 26.52. ppl of 14 train is 3.96. ppl of 14 dev is 4.32. bleu of 15 train is 30.89. bleu of 15 dev is 26.53. ppl of 15 train is 4.09. ppl of 15 dev is 4.41. bleu of 16 train is 30.25. bleu of 16 dev is 26.56. ppl of 16 train is 4.62. ppl of 16 dev is 4.58.
table 4 shows f1 results on ontonotes test for systems trained on data projected via fastalign and discalign. p of 36k p is 75.46. r of 36k r is 80.55. f1 of 36k f1 is 77.81. p of 36k p is 38.99. r of 36k r is 36.61. f1 of 36k f1 is 37.55. p of 53k p is 39.46. r of 53k r is 36.65. f1 of 53k f1 is 37.77. p of 36k p is 51.94. r of 36k r is 52.37. f1 of 36k f1 is 51.76. p of 53k p is 51.92. r of 53k r is 51.93. f1 of 53k f1 is 51.57.
table 3 shows translation quality on japanese–english data. bleu of baseline bleu is 19.94. delta of baseline delta is –. bleu of external zp prediction bleu is 20.86. delta of external zp prediction delta is 0.92. bleu of joint model bleu is 21.39. delta of joint model delta is 1.45. bleu of + discourse-level context bleu is 22. delta of + discourse-level context delta is 2.06.
table 4 shows comparison with previous work (uas). uas of this de is 72.78. uas of this es is 81.44. uas of this fr is 83.77. uas of this it is 86.13. uas of this pt is 84.05. uas of guo15 de is 60.35. uas of guo15 es is 71.9. uas of guo15 fr is 72.93. uas of guo16 de is 65.01. uas of guo16 es is 79. uas of guo16 fr is 77.69. uas of guo16 it is 78.49. uas of guo16 pt is 81.86. uas of ta16 de is 75.27. uas of ta16 es is 76.85. uas of ta16 fr is 79.21. uas of mx14 de is 74.3. uas of mx14 es is 75.53. uas of mx14 fr is 70.14. uas of mx14 it is 77.74. uas of mx14 pt is 76.65. uas of rc15 de is 79.68. uas of rc15 es is 80.86. uas of rc15 fr is 82.72. uas of rc15 it is 83.67. uas of rc15 pt is 82.07. uas of la16 de is 75.99. uas of la16 es is 78.94. uas of la16 fr is 80.8. uas of la16 it is 79.39. uas of rc17 de is 82.1. uas of rc17 es is 82.6. uas of rc17 fr is 83.9. uas of rc17 it is 84.4. uas of rc17 pt is 84.6.
table 2 shows dependency parsing results on english penn treebank v3.0. uas of stackptr (paper) uas is  96.12±0.03. las of stackptr (paper) las is  95.06±0.05. uas of stackptr (code) uas is  95.94±0.03. las of stackptr (code) las is  94.91±0.05. uas of  h-ptrnet-pst (gate) uas is  96.03±0.02. las of  h-ptrnet-pst (gate) las is  94.99±0.02. uas of  h-ptrnet-pst (sgate) uas is  96.04±0.05 95.00±0.06. las of  h-ptrnet-pst (sgate) las is . uas of  h-ptrnet-ps (gate) uas is  96.09±0.05 95.03±0.03. las of  h-ptrnet-ps (gate) las is .
table 4 shows conll-2009 results on chinese, german, and spanish (test sets). p of björkelund et al. (2010) p is 82.4. r of björkelund et al. (2010) r is 75.1. f1 of björkelund et al. (2010) f 1 is 78.6. p of roth and lapata (2016) p is 83.2. r of roth and lapata (2016) r is 75.9. f1 of roth and lapata (2016) f 1 is 79.4. p of marcheggiani and titov (2017) p is 84.6. r of marcheggiani and titov (2017) r is 80.4. f1 of marcheggiani and titov (2017) f 1 is 82.5. p of he et al. (2018b) p is 84.2. r of he et al. (2018b) r is 81.5. f1 of he et al. (2018b) f 1 is 82.8. p of cai et al. (2018) p is 84.7. r of cai et al. (2018) r is 84. f1 of cai et al. (2018) f 1 is 84.3. p of li et al. (2018) p is 84.8. r of li et al. (2018) r is 81.2. f1 of li et al. (2018) f 1 is 83. p of ours (supervised training) p is 84.9. r of ours (supervised training) r is 84.3. f1 of ours (supervised training) f 1 is 84.6. p of ours (with cvt) p is 85.4. r of ours (with cvt) r is 84.6. f1 of ours (with cvt) f 1 is 85. p of björkelund et al. (2010) p is 81.2. r of björkelund et al. (2010) r is 78.3. f1 of björkelund et al. (2010) f 1 is 79.7. p of roth and lapata (2016) p is 81.8. r of roth and lapata (2016) r is 78.5. f1 of roth and lapata (2016) f 1 is 80.1. p of ours (supervised training) p is 84.5. r of ours (supervised training) r is 82.1. f1 of ours (supervised training) f 1 is 83.3. p of ours (with cvt) p is 84.9. r of ours (with cvt) r is 82.7. f1 of ours (with cvt) f 1 is 83.8. p of björkelund et al. (2010) p is 78.9. r of björkelund et al. (2010) r is 74.3. f1 of björkelund et al. (2010) f 1 is 76.5. p of roth and lapata (2016) p is 83.2. r of roth and lapata (2016) r is 77.4. f1 of roth and lapata (2016) f 1 is 80.2. p of marcheggiani et al. (2017) p is 81.4. r of marcheggiani et al. (2017) r is 79.3. f1 of marcheggiani et al. (2017) f 1 is 80.3. p of ours (supervised training) p is 83. r of ours (supervised training) r is 81.3. f1 of ours (supervised training) f 1 is 82.1. p of ours(with cvt) p is 83.6. r of ours(with cvt) r is 82.2. f1 of ours(with cvt) f 1 is 82.9.
table 3 shows las results on north sámi development data. las of t100 mono-base is 53.3. las of t100 morph is 56.0 (+3.3). las of t100 nonce is  56.3 (+3.0). las of t100 cross-base is 61.3 (+8.0). las of t100 morph is 60.9 (+7.6). las of t100 nonce is 61.7 (+8.4). las of t50 mono-base is 42.5. las of t50 morph is 46.6 (+4.1). las of t50 nonce is  46.5 (+4.0). las of t50 cross-base is 52.0 (+9.5). las of t50 morph is 51.7 (+9.2). las of t50 nonce is 52.0 (+9.5). las of t10 mono-base is 18.5. las of t10 morph is 27.1 (+8.6). las of t10 nonce is  27.8 (+9.3). las of t10 cross-base is 34.7 (+16.2). las of t10 morph is 37.3 (+18.8). las of t10 nonce is 35.4 (+16.9).
table 7 shows las results on development sets. las of galician zero-shot is 51.9. las of galician  +fasttext is 72.8. las of galician  +morph is 71. las of kazakh zero-shot is 12.5. las of kazakh  +fasttext is 27.7. las of kazakh  +morph is 28.4. las of kazakh (translit.) zero-shot is 21.2. las of kazakh (translit.)  +fasttext is 31.1. las of kazakh (translit.)  +morph is 36.7.
table 2 shows main results for task 1: commonsense knowledge base completion (test f1 score) and task 2: wikipedia mining (quality scores out of 4). f1 of concatenation  task 1 is  68.8. f1 of concatenation  task 2 is  2.95 ± 0.11. f1 of template  task 1 is  72.2. f1 of template  task 2 is  2.98 ± 0.11. f1 of templ.+grammar  task 1 is  74.4. f1 of templ.+grammar  task 2 is  2.56 ± 0.13. f1 of coherency rank  task 1 is  78.8. f1 of coherency rank  task 2 is  3.00 ± 0.12. f1 of dnn  task 1 is  89.2. f1 of dnn  task 2 is  2.50. f1 of factorized  task 1 is  89.0. f1 of factorized  task 2 is  2.61. f1 of prototypical  task 1 is  79.4. f1 of prototypical  task 2 is  2.55.
table 1 shows results on glue test sets. accuracy of bert cola is 52.1. accuracy of bert  mrpc is  88.9/84.8. accuracy of bert  sts-b is  87.1/85.8. accuracy of bert  rte is 66.4. accuracy of mt-dnn cola is 51.7. accuracy of mt-dnn  mrpc is  89.9/86.3. accuracy of mt-dnn  sts-b is  87.6/86.8. accuracy of mt-dnn  rte is 75.4. accuracy of maml cola is 53.4. accuracy of maml  mrpc is  89.5/85.8. accuracy of maml  sts-b is  88.0/87.3. accuracy of maml  rte is 76.4. accuracy of fomaml cola is 51.6. accuracy of fomaml  mrpc is  89.9/86.4. accuracy of fomaml  sts-b is  88.6/88.0. accuracy of fomaml  rte is 74.1. accuracy of reptile cola is 53.2. accuracy of reptile  mrpc is  90.2/86.7. accuracy of reptile  sts-b is  88.7/88.1. accuracy of reptile  rte is 77.
table 1 shows test results on different datasets. acc of infersent acc is 0.846. acc of infersent acc is 0.866. f1 of infersent f1 is 0.746. f1 of infersent f1 is 0.451. pearson’s r of infersent pearson’s r is 0.715. map of infersent map is 0.287. mrr of infersent mrr is 0.287. map of infersent map is 0.521. mrr of infersent mrr is 0.559. acc of sse acc is 0.855. acc of sse acc is 0.878. f1 of sse f1 is 0.65. f1 of sse f1 is 0.422. pearson’s r of sse pearson’s r is 0.378. map of sse map is 0.624. mrr of sse mrr is 0.638. map of sse map is 0.628. mrr of sse mrr is 0.670. acc of decatt acc is 0.856. acc of decatt acc is 0.845. f1 of decatt f1 is 0.652. f1 of decatt f1 is 0.43. pearson’s r of decatt pearson’s r is 0.317. map of decatt map is 0.603. mrr of decatt mrr is 0.619. map of decatt map is 0.660. mrr of decatt mrr is 0.712. acc of esimtree acc is 0.864. acc of esimtree acc is 0.755. f1 of esimtree f1 is 0.74. f1 of esimtree f1 is 0.447. pearson’s r of esimtree pearson’s r is 0.493. map of esimtree map is 0.618. mrr of esimtree mrr is 0.633. map of esimtree map is 0.698. mrr of esimtree mrr is 0.734. acc of esimseq acc is 0.87. acc of esimseq acc is 0.85. f1 of esimseq f1 is 0.748. f1 of esimseq f1 is 0.52. pearson’s r of esimseq pearson’s r is 0.602. map of esimseq map is 0.652. mrr of esimseq mrr is 0.664. map of esimseq map is 0.771. mrr of esimseq mrr is 0.795. acc of esimseq+tree acc is 0.871. acc of esimseq+tree acc is 0.854. f1 of esimseq+tree f1 is 0.759. f1 of esimseq+tree f1 is 0.538. pearson’s r of esimseq+tree pearson’s r is 0.589. map of esimseq+tree map is 0.647. mrr of esimseq+tree mrr is 0.658. map of esimseq+tree map is 0.749. mrr of esimseq+tree mrr is 0.768. acc of pwimour acc is 0.822. acc of pwimour acc is 0.853. f1 of pwimour f1 is 0.745. f1 of pwimour f1 is 0.602. pearson’s r of pwimour pearson’s r is 0.695. map of pwimour map is 0.709. mrr of pwimour mrr is 0.723. map of pwimour map is 0.759. mrr of pwimour mrr is 0.822. pearson’s r of pwimour pearson’s r is 0.871. ρ of pwimour pearson’s p is 0.809. acc of mpwimseq acc is 0.851. acc of mpwimseq acc is 0.862. f1 of mpwimseq f1 is 0.757. f1 of mpwimseq f1 is 0.612. pearson’s r of mpwimseq pearson’s r is 0.714. map of mpwimseq map is 0.717. mrr of mpwimseq mrr is 0.728. map of mpwimseq map is 0.774. mrr of mpwimseq mrr is 0.835. pearson’s r of mpwimseq pearson’s r is 0.878. ρ of mpwimseq pearson’s p is 0.821. acc of mpwimseq+tree acc is 0.855. acc of mpwimseq+tree acc is 0.87. f1 of mpwimseq+tree f1 is 0.743. f1 of mpwimseq+tree f1 is 0.623. pearson’s r of mpwimseq+tree pearson’s r is 0.718. map of mpwimseq+tree map is 0.735. mrr of mpwimseq+tree mrr is 0.751. map of mpwimseq+tree map is 0.781. mrr of mpwimseq+tree mrr is 0.821. pearson’s r of mpwimseq+tree pearson’s r is 0.887. ρ of mpwimseq+tree pearson’s p is 0.834.
table 7 shows accuracy of models trained and evaluated on different parts of the dataset. accuracy of  amt training data amt  simple is 0.99. accuracy of  amt training data amt  complex is 0.78. accuracy of  amt training data amt  all is 0.89. accuracy of  amt training data gcs  simple is 0.99. accuracy of  amt training data gcs  complex is 0.76. accuracy of  amt training data gcs  all is 0.79. accuracy of  amt training data amt + gcs  simple is 0.99. accuracy of  amt training data amt + gcs  complex is 0.77. accuracy of  amt training data amt + gcs  all is 0.88. accuracy of  gcs training data amt  simple is 0.96. accuracy of  gcs training data amt  complex is 0.76. accuracy of  gcs training data amt  all is 0.86. accuracy of  gcs training data gcs  simple is 0.97. accuracy of  gcs training data gcs  complex is 0.94. accuracy of  gcs training data gcs  all is 0.94. accuracy of  gcs training data amt + gcs  simple is 0.96. accuracy of  gcs training data amt + gcs  complex is 0.82. accuracy of  gcs training data amt + gcs  all is 0.87. accuracy of  amt & gcs training data amt  simple is 0.99. accuracy of  amt & gcs training data amt  complex is 0.9. accuracy of  amt & gcs training data amt  all is 0.95. accuracy of  amt & gcs training data gcs  simple is 0.98. accuracy of  amt & gcs training data gcs  complex is 0.94. accuracy of  amt & gcs training data gcs  all is 0.95. accuracy of  amt & gcs training data amt + gcs  simple is 0.99. accuracy of  amt & gcs training data amt + gcs  complex is 0.91. accuracy of  amt & gcs training data amt + gcs  all is 0.95.
table 2 shows performance (f1 score) on atis dataset f1 of attrnn (upper bound)  batch 0 is 92.12. f1 of attrnn (upper bound) batch 1 is 92.89. f1 of attrnn (upper bound) batch 2 is 93.04. f1 of attrnn (upper bound) batch 3 is 93.56. f1 of attrnn (upper bound) batch 4 is 95.13. f1 of ft-attrnn  batch 0 is . f1 of ft-attrnn batch 1 is 91.85. f1 of ft-attrnn batch 2 is 89.98. f1 of ft-attrnn batch 3 is 91.25. f1 of ft-attrnn batch 4 is 88.03. f1 of ft-lr-attrnn  batch 0 is . f1 of ft-lr-attrnn batch 1 is 91.96. f1 of ft-lr-attrnn batch 2 is 86.46. f1 of ft-lr-attrnn batch 3 is 88.03. f1 of ft-lr-attrnn batch 4 is 86.58. f1 of ft-cp-attrnn  batch 0 is 92.12. f1 of ft-cp-attrnn batch 1 is 92.1. f1 of ft-cp-attrnn batch 2 is 90.06. f1 of ft-cp-attrnn batch 3 is 91.98. f1 of ft-cp-attrnn batch 4 is 89.67. f1 of t-progmodel  batch 0 is . f1 of t-progmodel batch 1 is 92.33. f1 of t-progmodel batch 2 is 92.43. f1 of t-progmodel batch 3 is 92.57. f1 of t-progmodel batch 4 is 92.58. f1 of c-progmodel  batch 0 is . f1 of c-progmodel batch 1 is 92.4. f1 of c-progmodel batch 2 is 92.64. f1 of c-progmodel batch 3 is 92.71. f1 of c-progmodel batch 4 is 93.91.
table 2 shows benchmark classifier results under each data condition using the oos-train (top half) and oos-threshold (bottom half) prediction methods. in-scope accuracy of oos-train fasttext full is 89. in-scope accuracy of oos-train fasttext small is 84.5. in-scope accuracy of oos-train fasttext  imbal is 87.2. in-scope accuracy of oos-train fasttext  oos+ is 89.2. out-of-scope recall of oos-train fasttext full is 9.7. out-of-scope recall of oos-train fasttext small is 23.2. out-of-scope recall of oos-train fasttext  imbal is 12.2. out-of-scope recall of oos-train fasttext  oos+ is 32.2. in-scope accuracy of oos-train svm full is 91. in-scope accuracy of oos-train svm small is 89.6. in-scope accuracy of oos-train svm  imbal is 89.9. in-scope accuracy of oos-train svm  oos+ is 90.1. out-of-scope recall of oos-train svm full is 14.5. out-of-scope recall of oos-train svm small is 18.6. out-of-scope recall of oos-train svm  imbal is 16. out-of-scope recall of oos-train svm  oos+ is 29.8. in-scope accuracy of oos-train cnn full is 91.2. in-scope accuracy of oos-train cnn small is 88.9. in-scope accuracy of oos-train cnn  imbal is 89.1. in-scope accuracy of oos-train cnn  oos+ is 91. out-of-scope recall of oos-train cnn full is 18.9. out-of-scope recall of oos-train cnn small is 22.2. out-of-scope recall of oos-train cnn  imbal is 19. out-of-scope recall of oos-train cnn  oos+ is 34.2. in-scope accuracy of oos-train dialogflow full is 91.7. in-scope accuracy of oos-train dialogflow small is 89.4. in-scope accuracy of oos-train dialogflow  imbal is 90.7. in-scope accuracy of oos-train dialogflow  oos+ is 91.7. out-of-scope recall of oos-train dialogflow full is 14. out-of-scope recall of oos-train dialogflow small is 14.1. out-of-scope recall of oos-train dialogflow  imbal is 15.3. out-of-scope recall of oos-train dialogflow  oos+ is 28.5. in-scope accuracy of oos-train rasa full is 91.5. in-scope accuracy of oos-train rasa small is 88.9. in-scope accuracy of oos-train rasa  imbal is 89.2. in-scope accuracy of oos-train rasa  oos+ is 90.9. out-of-scope recall of oos-train rasa full is 45.3. out-of-scope recall of oos-train rasa small is 55. out-of-scope recall of oos-train rasa  imbal is 49.6. out-of-scope recall of oos-train rasa  oos+ is 66. in-scope accuracy of oos-train mlp full is 93.5. in-scope accuracy of oos-train mlp small is 91.5. in-scope accuracy of oos-train mlp  imbal is 92.5. in-scope accuracy of oos-train mlp  oos+ is 94.1. out-of-scope recall of oos-train mlp full is 47.4. out-of-scope recall of oos-train mlp small is 52.2. out-of-scope recall of oos-train mlp  imbal is 35.6. out-of-scope recall of oos-train mlp  oos+ is 53.9. in-scope accuracy of oos-train bert full is 96.9. in-scope accuracy of oos-train bert small is 96.4. in-scope accuracy of oos-train bert  imbal is 96.3. in-scope accuracy of oos-train bert  oos+ is 96.7. out-of-scope recall of oos-train bert full is 40.3. out-of-scope recall of oos-train bert small is 40.9. out-of-scope recall of oos-train bert  imbal is 43.8. out-of-scope recall of oos-train bert  oos+ is 59.2. in-scope accuracy of oos-threshold svm full is 88.2. in-scope accuracy of oos-threshold svm small is 85.6. in-scope accuracy of oos-threshold svm  imbal is 86. in-scope accuracy of oos-threshold svm  oos+ is  —. out-of-scope recall of oos-threshold svm full is 18. out-of-scope recall of oos-threshold svm small is 13. out-of-scope recall of oos-threshold svm  imbal is 0. out-of-scope recall of oos-threshold svm  oos+ is  —. in-scope accuracy of oos-thresholdfasttext full is 88.6. in-scope accuracy of oos-thresholdfasttext small is 84.8. in-scope accuracy of oos-thresholdfasttext  imbal is 86.6. in-scope accuracy of oos-thresholdfasttext  oos+ is  —. out-of-scope recall of oos-thresholdfasttext full is 28.3. out-of-scope recall of oos-thresholdfasttext small is 6. out-of-scope recall of oos-thresholdfasttext  imbal is 33.2. out-of-scope recall of oos-thresholdfasttext  oos+ is  —. in-scope accuracy of oos-threshold dialogflow full is 90.8. in-scope accuracy of oos-threshold dialogflow small is 89.2. in-scope accuracy of oos-threshold dialogflow  imbal is 89.2. in-scope accuracy of oos-threshold dialogflow  oos+ is  —. out-of-scope recall of oos-threshold dialogflow full is 26.7. out-of-scope recall of oos-threshold dialogflow small is 20.5. out-of-scope recall of oos-threshold dialogflow  imbal is 38.1. out-of-scope recall of oos-threshold dialogflow  oos+ is  —. in-scope accuracy of oos-threshold rasa full is 90.9. in-scope accuracy of oos-threshold rasa small is 89.6. in-scope accuracy of oos-threshold rasa  imbal is 89.4. in-scope accuracy of oos-threshold rasa  oos+ is  —. out-of-scope recall of oos-threshold rasa full is 31.2. out-of-scope recall of oos-threshold rasa small is 1. out-of-scope recall of oos-threshold rasa  imbal is 0. out-of-scope recall of oos-threshold rasa  oos+ is  —. in-scope accuracy of oos-threshold cnn full is 90.9. in-scope accuracy of oos-threshold cnn small is 88.9. in-scope accuracy of oos-threshold cnn  imbal is 90. in-scope accuracy of oos-threshold cnn  oos+ is  —. out-of-scope recall of oos-threshold cnn full is 30.9. out-of-scope recall of oos-threshold cnn small is 25.5. out-of-scope recall of oos-threshold cnn  imbal is 26.9. out-of-scope recall of oos-threshold cnn  oos+ is  —. in-scope accuracy of oos-threshold mlp full is 93.4. in-scope accuracy of oos-threshold mlp small is 91.3. in-scope accuracy of oos-threshold mlp  imbal is 92.5. in-scope accuracy of oos-threshold mlp  oos+ is  —. out-of-scope recall of oos-threshold mlp full is 49.1. out-of-scope recall of oos-threshold mlp small is 32.4. out-of-scope recall of oos-threshold mlp  imbal is 13.3. out-of-scope recall of oos-threshold mlp  oos+ is  —. in-scope accuracy of oos-threshold bert full is 96.2. in-scope accuracy of oos-threshold bert small is 96.2. in-scope accuracy of oos-threshold bert  imbal is 95.9. in-scope accuracy of oos-threshold bert  oos+ is  —. out-of-scope recall of oos-threshold bert full is 52.3. out-of-scope recall of oos-threshold bert small is 58.9. out-of-scope recall of oos-threshold bert  imbal is 52.8. out-of-scope recall of oos-threshold bert  oos+ is  —.
table 3 shows results of oos-binary experiments on oos+, where we compare performance of undersampling (under) and augmentation using sentences from wikipedia (wiki aug). in-scope accuracy of dialogflow under is 84.7. in-scope accuracy of dialogflow wiki aug is  —. out-of-scope recall of dialogflow under is 37.3. out-of-scope recall of dialogflow wiki aug is  —. in-scope accuracy of rasa under is 87.5. in-scope accuracy of rasa wiki aug is  —. out-of-scope recall of rasa under is 37.7. out-of-scope recall of rasa wiki aug is  —. in-scope accuracy of fasttext under is 88.1. in-scope accuracy of fasttext wiki aug is 87. out-of-scope recall of fasttext under is 22.7. out-of-scope recall of fasttext wiki aug is 31.4. in-scope accuracy of svm under is 88.4. in-scope accuracy of svm wiki aug is 89.3. out-of-scope recall of svm under is 32.2. out-of-scope recall of svm wiki aug is 37.7. in-scope accuracy of cnn under is 89.8. in-scope accuracy of cnn wiki aug is 90.1. out-of-scope recall of cnn under is 25.6. out-of-scope recall of cnn wiki aug is 39.7. in-scope accuracy of mlp under is 90.1. in-scope accuracy of mlp wiki aug is 92.9. out-of-scope recall of mlp under is 52.8. out-of-scope recall of mlp wiki aug is 32.4. in-scope accuracy of bert under is 94.4. in-scope accuracy of bert wiki aug is 96. out-of-scope recall of bert under is 46.5. out-of-scope recall of bert wiki aug is 40.4.
table 1 shows activity, entity f1 results reported by previous work (rows 1-4 from serban et al. activity f1 of lstm activity f1 is 1.18. entity f1 of lstm entity f1 is 0.87. activity f1 of hred activity f1 is 4.34. entity f1 of hred entity f1 is 2.22. activity f1 of vhred activity f1 is 4.63. entity f1 of vhred entity f1 is 2.53. activity f1 of vhred (w/ attn.) activity f1 is 5.94. entity f1 of vhred (w/ attn.) entity f1 is 3.52. activity f1 of all-operations activity f1 is 6.53. entity f1 of all-operations entity f1 is 3.79. activity f1 of input-aware activity f1 is 7.04. entity f1 of input-aware entity f1 is 3.9. activity f1 of input-agnostic activity f1 is 7.02. entity f1 of input-agnostic entity f1 is 4.
table 2 shows classification results (f1) obtained with: i) automatic english translations by three models (generic, reinforce, mo-reinforce), and ii) gold-standard english (english) and untranslated german/italian (original) tweets. f1 of generic 5% is 79.7. f1 of generic 100% is 83.2. f1 of generic 5% is 78.2. f1 of generic 100% is 81.6. f1 of reinforce 5% is 80.4. f1 of reinforce 100% is 83.7. f1 of reinforce 5% is 77.8. f1 of reinforce 100% is 82.8. f1 of mo-reinforce 5% is 80.9. f1 of mo-reinforce 100% is 84.4. f1 of mo-reinforce 5% is 80.3. f1 of mo-reinforce 100% is 84.5. f1 of english 5% is 85.1. f1 of english 100% is 85.1. f1 of english 5% is 85.1. f1 of english 100% is 85.1. f1 of original 5% is 74.4. f1 of original 100% is 74.4. f1 of original 5% is 75.6. f1 of original 100% is 75.6.
table 2 shows models’ performance on all words and oov words per language. accuracy of pasha et al. (2014) wer is 12.3%. accuracy of pasha et al. (2014) oov is 29.8%. accuracy of zalmout and habash (2017) wer is 8.3%. accuracy of zalmout and habash (2017) oov is 20.2%. accuracy of lstm der is 19.2%. accuracy of lstm wer is 51.9%. accuracy of lstm oov is 86.6%. accuracy of tcn der is 17.5%. accuracy of tcn wer is 47.6%. accuracy of tcn oov is 87.2%. accuracy of bilstm der is 2.8%. accuracy of bilstm wer is 8.2%. accuracy of bilstm oov is 33.6%. accuracy of a-tcn der is 3.0%. accuracy of a-tcn wer is 10.2%. accuracy of a-tcn oov is 36.3%. accuracy of naplava et al. (2018) der is 11.2%. accuracy of naplava et al. (2018) wer is 44.5%. accuracy of lstm der is 13.3%. accuracy of lstm wer is 39.5%. accuracy of lstm oov is 33.1%. accuracy of tcn der is 11.1%. accuracy of tcn wer is 32.9%. accuracy of tcn oov is 32.4%. accuracy of bilstm der is 2.6%. accuracy of bilstm wer is 7.8%. accuracy of bilstm oov is 15.3%. accuracy of a-tcn der is 2.5%. accuracy of a-tcn wer is 7.7%. accuracy of a-tcn oov is 15.3%. accuracy of orife (2018) wer is 4.6%. accuracy of lstm der is 13.4%. accuracy of lstm wer is 37.2%. accuracy of lstm oov is 84.9%. accuracy of tcn der is 12.7%. accuracy of tcn wer is 35.5%. accuracy of tcn oov is 83.8%. accuracy of bilstm der is 3.6%. accuracy of bilstm wer is 12.1%. accuracy of bilstm oov is 69.3%. accuracy of a-tcn der is 3.8%. accuracy of a-tcn wer is 12.6%. accuracy of a-tcn oov is 70.2%.
table 3 shows comparison with the transformer model on german-english document-level translation. bleu of baseline tst13 is 27.89. bleu of baseline tst14 is 23.75. bleu of baseline avg is 25.82. bleu of ours tst13 is 28.58. bleu of ours tst14 is 24.85. bleu of ours avg is 26.72.
table 1 shows the performance of mtmsn and other competing approaches on drop dev and test set. em of heuristic baseline (dua et al., 2019) em is 4.28. f1 of heuristic baseline (dua et al., 2019) f1 is 8.07. em of heuristic baseline (dua et al., 2019) em is 4.18. f1 of heuristic baseline (dua et al., 2019) f1 is 8.59. em of semantic role labeling (carreras and marquez, 2004) em is 11.03. f1 of semantic role labeling (carreras and marquez, 2004) f1 is 13.67. em of semantic role labeling (carreras and marquez, 2004) em is 10.87. f1 of semantic role labeling (carreras and marquez, 2004) f1 is 13.35. em of bidaf (seo et al., 2017) em is 26.06. f1 of bidaf (seo et al., 2017) f1 is 28.85. em of bidaf (seo et al., 2017) em is 24.75. f1 of bidaf (seo et al., 2017) f1 is 27.49. em of qanet+elmo (yu et al., 2018) em is 27.71. f1 of qanet+elmo (yu et al., 2018) f1 is 30.33. em of qanet+elmo (yu et al., 2018) em is 27.08. f1 of qanet+elmo (yu et al., 2018) f1 is 29.67. em of bertbase (devlin et al., 2019) em is 30.10. f1 of bertbase (devlin et al., 2019) f1 is 33.36. em of bertbase (devlin et al., 2019) em is 29.45. f1 of bertbase (devlin et al., 2019) f1 is 32.70. em of naqanet (dua et al., 2019) em is 46.20. f1 of naqanet (dua et al., 2019) f1 is 49.24. em of naqanet (dua et al., 2019) em is 44.07. f1 of naqanet (dua et al., 2019) f1 is 47.01. em of nabertbase em is 55.82. f1 of nabertbase f1 is 58.75. em of nabertlarge em is 64.61. f1 of nabertlarge f1 is 67.35. em of mtmsnbase em is 68.17. f1 of mtmsnbase f1 is 72.81. em of mtmsnlarge em is 76.68. f1 of mtmsnlarge f1 is 80.54. em of mtmsnlarge em is 75.85. f1 of mtmsnlarge f1 is 79.88. em of human performance (dua et al., 2019) em is 92.38. f1 of human performance (dua et al., 2019) f1 is 95.98.
table 4 shows performance breakdown of nabertlarge and mtmsnlarge by gold answer types. (%) of date (%) is 1.6. em of date em is 55.7. f1 of date f1 is 60.8. em of date em is 55.7. f1 of date f1 is 69. (%) of number (%) is 61.9. em of number em is 63.8. f1 of number f1 is 64. em of number em is 80.9. f1 of number f1 is 81.1. (%) of single span (%) is 31.7. em of single span em is 75.9. f1 of single span f1 is 80.6. em of single span em is 77.5. f1 of single span f1 is 82.8. (%) of multi span (%) is 4.8. em of multi span em is 0. f1 of multi span f1 is 22.7. em of multi span em is 25.1. f1 of multi span f1 is 62.8.
table 5 shows answer selection performances (averaged over five datasets) when trained with question-answer pairs vs. accuracy of bilstm supervised is 35.3. accuracy of bilstm ws-tb is 37.5. accuracy of bilstm ws-tb (all) is 42.5. accuracy of coala supervised is 44.7. accuracy of coala ws-tb is 45.2. accuracy of coala ws-tb (all) is 44.5.
table 8 shows results of clarification question generation models. bleu of seq2seq single-turn is 18.84. bleu of seq2seq multi-turn is 31.62. bleu of transformer single-turn is 20.69. bleu of transformer multi-turn is 44.42. bleu of the proposed model single-turn is 24.04. bleu of the proposed model multi-turn is 45.02.
table 2 shows performance on multiwoz. mrr of dual encoder mrr is 79.55. hits@1 of dual encoder hits@1 is 66.13%. mrr of ensemble (5) mrr is 81.53. hits@1 of ensemble (5) hits@1 is 69.47%. mrr of multi-granularity (5) mrr is 82.74. hits@1 of multi-granularity (5) hits@1 is 72.18%.
table 5 shows experimental results demonstrating performance on two downstream tasks, without any finetuning of the latent representations. f-1 of dual encoder f-1 is 60.13. f-1 of dual encoder f-1 is 19.09. f-1 of ensemble (5) f-1 is 64.11. f-1 of ensemble (5) f-1 is 22.39. f-1 of multi-granularity (5) f-1 is 67.51. f-1 of multi-granularity (5) f-1 is 22.85. f-1 of fine-tuned f-1 is 90.33. f-1 of fine-tuned f-1 is 28.75.
table 6 shows experimental results demonstrating performance on the downstream task of dialog act prediction, when the model is fine-tuned on all available data. da (f-1) of random init da (f-1) is 28.75. da (f-1) of dual encoder da (f-1) is 32.63. da (f-1) of ensemble (5) da (f-1) is 31.71. da (f-1) of multi-granularity (5) da (f-1) is 33.46.
table 1 shows quantitative evaluation results (%). bleu of seq2seq bleu is 0.845. average of seq2seq average is 69.60. greedy of seq2seq greedy is 64.94. extrema of seq2seq extrema is 45.29. distinct-1 of seq2seq distinct-1 is 0.2822. distinct-2 of seq2seq distinct-2 is 0.5922. distinct-3 of seq2seq distinct-3 is 0.7873. bleu of cvae bleu is 1.546. average of cvae average is 71.23. greedy of cvae greedy is 66.67. extrema of cvae extrema is 47.14. distinct-1 of cvae distinct-1 is 0.5465. distinct-2 of cvae distinct-2 is 1.716. distinct-3 of cvae distinct-3 is 2.731. bleu of laed bleu is 0.7545. average of laed average is 69.91. greedy of laed greedy is 63.55. extrema of laed extrema is 43.12. distinct-1 of laed distinct-1 is 0.3890. distinct-2 of laed distinct-2 is 0.9165. distinct-3 of laed distinct-3 is 1.243. bleu of ta-seq2seq bleu is 1.465. average of ta-seq2seq average is 72.47. greedy of ta-seq2seq greedy is 65.9. extrema of ta-seq2seq extrema is 45.19. distinct-1 of ta-seq2seq distinct-1 is 0.3593. distinct-2 of ta-seq2seq distinct-2 is 0.7994. distinct-3 of ta-seq2seq distinct-3 is 1.016. bleu of dom-seq2seq bleu is 1.189. average of dom-seq2seq average is 74.42. greedy of dom-seq2seq greedy is 66.6. extrema of dom-seq2seq extrema is 48.47. distinct-1 of dom-seq2seq distinct-1 is 0.4977. distinct-2 of dom-seq2seq distinct-2 is 1.294. distinct-3 of dom-seq2seq distinct-3 is 1.814. bleu of adand (with context para.) bleu is 1.94. average of adand (with context para.) average is 74.03. greedy of adand (with context para.) greedy is 66.76. extrema of adand (with context para.) extrema is 49.23. distinct-1 of adand (with context para.) distinct-1 is 0.6493. distinct-2 of adand (with context para.) distinct-2 is 1.889. distinct-3 of adand (with context para.) distinct-3 is 2.745. bleu of adand (with topic para.) bleu is 2.051. average of adand (with topic para.) average is 74.17. greedy of adand (with topic para.) greedy is 66.65. extrema of adand (with topic para.) extrema is 49.04. distinct-1 of adand (with topic para.) distinct-1 is 0.5919. distinct-2 of adand (with topic para.) distinct-2 is 1.699. distinct-3 of adand (with topic para.) distinct-3 is 2.438. bleu of adand (with both) bleu is 1.90. average of adand (with both) average is 75.59. greedy of adand (with both) greedy is 67.25. extrema of adand (with both) extrema is 51.17. distinct-1 of adand (with both) distinct-1 is 0.7092. distinct-2 of adand (with both) distinct-2 is 2.10. distinct-3 of adand (with both) distinct-3 is 3.108.
table 1 shows the result of rewriting quality. bleu-4 of last utterance bleu-4 is 34.2. bleu-4 of last utterance + context bleu-4 is 37.1. bleu-4 of last utterance + keyword bleu-4 is 49.8. bleu-4 of crn bleu-4 is 50.9. bleu-4 of crn + rl bleu-4 is 54.2.
table 2 shows the imn model and previous methods on persona-chat dataset without using personas. hits@1 of ir baseline hits@1 is 21.4. hits@1 of starspace hits@1 is 31.8. hits@1 of profile hits@1 is 31.8. hits@1 of kv profile hits@1 is 34.9. hits@1 of imn hits@1 is 63.8. mrr of imn mrr is 75.8.
table 3 shows performance of the proposed and previous methods on the persona-chat under various persona configurations. hits@1 of ir baseline hits@1 is 41.0 (+19.6). hits@1 of ir baseline hits@1 is 20.7 (-0.7). hits@1 of ir baseline hits@1 is 18.1 (-3.3). hits@1 of ir baseline hits@1 is 18.1 (-3.3). hits@1 of starspace hits@1 is 48.1 (+16.3). hits@1 of starspace hits@1 is 32.2 (+0.4). hits@1 of starspace hits@1 is 24.5 (-7.3). hits@1 of starspace hits@1 is 26.1 (-5.7). hits@1 of profile hits@1 is 47.3 (+15.5). hits@1 of profile hits@1 is 35.4 (+3.6). hits@1 of profile hits@1 is 28.3 (-3.5). hits@1 of profile hits@1 is 29.4 (-2.4). hits@1 of kv profile hits@1 is 51.1 (+16.2). hits@1 of kv profile hits@1 is 35.1 (+0.2). hits@1 of kv profile hits@1 is 29.1 (-5.8). hits@1 of kv profile hits@1 is 28.9 (-6.0). hits@1 of ft-pc hits@1 is 60.7 (-). hits@1 of imnctx hits@1 is 64.3 (+0.5). mrr of imnctx mrr is 76.2 (+0.4). hits@1 of imnctx hits@1 is 63.8 (+0.0). mrr of imnctx mrr is 75.8 (+0.0). hits@1 of imnctx hits@1 is 63.7 (-0.1). mrr of imnctx mrr is 75.8 (+0.0). hits@1 of imnctx hits@1 is 63.5 (-0.3). mrr of imnctx mrr is 75.7 (-0.1). hits@1 of imnutr hits@1 is 66.7 (+2.9). mrr of imnutr mrr is 78.1 (+2.3). hits@1 of imnutr hits@1 is 64.0 (+0.2). mrr of imnutr mrr is 76.0 (+0.2). hits@1 of imnutr hits@1 is 63.9 (+0.1). mrr of imnutr mrr is 75.9 (+0.1). hits@1 of imnutr hits@1 is 63.7 (-0.1). mrr of imnutr mrr is 75.7 (-0.1). hits@1 of dim hits@1 is 78.8 (+15.0). mrr of dim mrr is 6.7 (+10.9). hits@1 of dim hits@1 is 70.7 (+6.9). mrr of dim mrr is 1.2 (+5.4). hits@1 of dim hits@1 is 64.0 (+0.2). mrr of dim mrr is 76.1 (+0.3). hits@1 of dim hits@1 is 63.9 (+0.1). mrr of dim mrr is 76.0 (+0.2).
table 6 shows the results of responses generation with bleu, perplexity (ppl), distinct scores (1-gram to 4-gram). bleu of seq2seq bleu is 14.20. ppl of seq2seq ppl is 94.48. dist-1 of seq2seq dist-1 is 0.008. dist-2 of seq2seq dist-2 is 0.039. dist-3 of seq2seq dist-3 is 0.092. dist-4 of seq2seq dist-4 is 0.150. bleu of seq2seq bleu is 15.46. ppl of seq2seq ppl is 73.23. dist-1 of seq2seq dist-1 is 0.004. dist-2 of seq2seq dist-2 is 0.016. dist-3 of seq2seq dist-3 is 0.026. dist-4 of seq2seq dist-4 is 0.032. bleu of memnet bleu is 15.73. ppl of memnet ppl is 88.29. dist-1 of memnet dist-1 is 0.012. dist-2 of memnet dist-2 is 0.062. dist-3 of memnet dist-3 is 0.150. dist-4 of memnet dist-4 is 0.240. bleu of memnet bleu is 14.61. ppl of memnet ppl is 67.58. dist-1 of memnet dist-1 is 0.005. dist-2 of memnet dist-2 is 0.023. dist-3 of memnet dist-3 is 0.040. dist-4 of memnet dist-4 is 0.049. bleu of memnet + multi bleu is 15.88. ppl of memnet + multi ppl is 86.76. dist-1 of memnet + multi dist-1 is 0.010. dist-2 of memnet + multi dist-2 is 0.058. dist-3 of memnet + multi dist-3 is 0.138. dist-4 of memnet + multi dist-4 is 0.224. bleu of memnet + multi bleu is 12.97. ppl of memnet + multi ppl is 54.67. dist-1 of memnet + multi dist-1 is 0.006. dist-2 of memnet + multi dist-2 is 0.022. dist-3 of memnet + multi dist-3 is 0.032. dist-4 of memnet + multi dist-4 is 0.036. bleu of taware bleu is 15.97. ppl of taware ppl is 81.54. dist-1 of taware dist-1 is 0.013. dist-2 of taware dist-2 is 0.068. dist-3 of taware dist-3 is 0.153. dist-4 of taware dist-4 is 0.223. bleu of taware bleu is 14.78. ppl of taware ppl is 60.61. dist-1 of taware dist-1 is 0.002. dist-2 of taware dist-2 is 0.007. dist-3 of taware dist-3 is 0.013. dist-4 of taware dist-4 is 0.016. bleu of taware + multi bleu is 13.34. ppl of taware + multi ppl is 80.48. dist-1 of taware + multi dist-1 is 0.022. dist-2 of taware + multi dist-2 is 0.122. dist-3 of taware + multi dist-3 is 0.239. dist-4 of taware + multi dist-4 is 0.304. bleu of taware + multi bleu is 15.74. ppl of taware + multi ppl is 56.67. dist-1 of taware + multi dist-1 is 0.003. dist-2 of taware + multi dist-2 is 0.011. dist-3 of taware + multi dist-3 is 0.019. dist-4 of taware + multi dist-4 is 0.023. bleu of kaware bleu is 14.14. ppl of kaware ppl is 90.11. dist-1 of kaware dist-1 is 0.011. dist-2 of kaware dist-2 is 0.061. dist-3 of kaware dist-3 is 0.135. dist-4 of kaware dist-4 is 0.198. bleu of kaware bleu is 15.70. ppl of kaware ppl is 64.70. dist-1 of kaware dist-1 is 0.002. dist-2 of kaware dist-2 is 0.009. dist-3 of kaware dist-3 is 0.017. dist-4 of kaware dist-4 is 0.021. bleu of qadpt bleu is 14.52. ppl of qadpt ppl is 88.24. dist-1 of qadpt dist-1 is 0.013. dist-2 of qadpt dist-2 is 0.081. dist-3 of qadpt dist-3 is 0.169. dist-4 of qadpt dist-4 is 0.242. bleu of qadpt bleu is 17.01. ppl of qadpt ppl is 68.27. dist-1 of qadpt dist-1 is 0.002. dist-2 of qadpt dist-2 is 0.008. dist-3 of qadpt dist-3 is 0.013. dist-4 of qadpt dist-4 is 0.016. bleu of qadpt + multi bleu is 15.47. ppl of qadpt + multi ppl is 86.65. dist-1 of qadpt + multi dist-1 is 0.021. dist-2 of qadpt + multi dist-2 is 0.129. dist-3 of qadpt + multi dist-3 is 0.259. dist-4 of qadpt + multi dist-4 is 0.342. bleu of qadpt + multi bleu is 14.79. ppl of qadpt + multi ppl is 66.70. dist-1 of qadpt + multi dist-1 is 0.005. dist-2 of qadpt + multi dist-2 is 0.023. dist-3 of qadpt + multi dist-3 is 0.041. dist-4 of qadpt + multi dist-4 is 0.051. bleu of qadpt + taware bleu is 15.05. ppl of qadpt + taware ppl is 81.75. dist-1 of qadpt + taware dist-1 is 0.022. dist-2 of qadpt + taware dist-2 is 0.123. dist-3 of qadpt + taware dist-3 is 0.246. dist-4 of qadpt + taware dist-4 is 0.332. bleu of qadpt + taware bleu is 16.85. ppl of qadpt + taware ppl is 55.46. dist-1 of qadpt + taware dist-1 is 0.003. dist-2 of qadpt + taware dist-2 is 0.012. dist-3 of qadpt + taware dist-3 is 0.020. dist-4 of qadpt + taware dist-4 is 0.024.
table 2 shows automatic evaluation results for the task of sentiment response generation. bleu-1 of seq2seq bleu-1 is 0.065. rouge-l of seq2seq rouge-l is 0.118. average of seq2seq average is 0.726. extreme of seq2seq extreme is 0.474. greedy of seq2seq greedy is 0.582. bleu-1 of cvae bleu-1 is 0.088. rouge-l of cvae rouge-l is 0.081. average of cvae average is 0.727. extreme of cvae extreme is 0.408. greedy of cvae greedy is 0.563. bleu-1 of ecm bleu-1 is 0.051. rouge-l of ecm rouge-l is 0.102. average of ecm average is 0.708. extreme of ecm extreme is 0.462. greedy of ecm greedy is 0.559. bleu-1 of s2s-temp-mle bleu-1 is 0.103. rouge-l of s2s-temp-mle rouge-l is 0.124. average of s2s-temp-mle average is 0.732. extreme of s2s-temp-mle extreme is 0.458. greedy of s2s-temp-mle greedy is 0.593. bleu-1 of s2s-temp-none bleu-1 is 0.078. rouge-l of s2s-temp-none rouge-l is 0.089. average of s2s-temp-none average is 0.687. extreme of s2s-temp-none extreme is 0.479. greedy of s2s-temp-none greedy is 0.501. bleu-1 of s2s-temp-50% bleu-1 is 0.102. rouge-l of s2s-temp-50% rouge-l is 0.121. average of s2s-temp-50% average is 0.691. extreme of s2s-temp-50% extreme is 0.491. greedy of s2s-temp-50% greedy is 0.586. bleu-1 of s2s-temp bleu-1 is 0.106. rouge-l of s2s-temp rouge-l is 0.130. average of s2s-temp average is 0.738. extreme of s2s-temp extreme is 0.492. greedy of s2s-temp greedy is 0.603.
table 4 shows consistency comparison between human inference and model predictions on overlapping rate (%). p@1 of preceding p@1 is 63.50. p@2 of preceding p@2 is 90.05. p@3 of preceding p@3 is 98.83. acc. of preceding acc. is 40.46. p@1 of preceding p@1 is 56.84. p@2 of preceding p@2 is 80.15. p@3 of preceding p@3 is 91.86. acc. of preceding acc. is 21.06. p@1 of preceding p@1 is 54.97. p@2 of preceding p@2 is 77.19. p@3 of preceding p@3 is 88.75. acc. of preceding acc. is 13.08. p@1 of subsequent p@1 is 61.03. p@2 of subsequent p@2 is 88.86. p@3 of subsequent p@3 is 98.54. acc. of subsequent acc. is 40.25. p@1 of subsequent p@1 is 54.57. p@2 of subsequent p@2 is 73.60. p@3 of subsequent p@3 is 87.26. acc. of subsequent acc. is 20.26. p@1 of subsequent p@1 is 53.07. p@2 of subsequent p@2 is 69.85. p@3 of subsequent p@3 is 81.93. acc. of subsequent acc. is 12.79. p@1 of drnn p@1 is 72.75. p@2 of drnn p@2 is 93.21. p@3 of drnn p@3 is 99.24. acc. of drnn acc. is 58.18. p@1 of drnn p@1 is 65.58. p@2 of drnn p@2 is 85.85. p@3 of drnn p@3 is 94.92. acc. of drnn acc. is 34.47. p@1 of drnn p@1 is 62.60. p@2 of drnn p@2 is 82.68. p@3 of drnn p@3 is 92.14. acc. of drnn acc. is 22.58. p@1 of sirnn p@1 is 75.98. p@2 of sirnn p@2 is 94.49. p@3 of sirnn p@3 is 99.39. acc. of sirnn acc. is 62.06. p@1 of sirnn p@1 is 70.88. p@2 of sirnn p@2 is 89.14. p@3 of sirnn p@3 is 96.10. acc. of sirnn acc. is 40.66. p@1 of sirnn p@1 is 68.13. p@2 of sirnn p@2 is 85.82. p@3 of sirnn p@3 is 93.52. acc. of sirnn acc. is 28.05. p@1 of w2w p@1 is 77.55*. p@2 of w2w p@2 is 95.11*. p@3 of w2w p@3 is 99.57. acc. of w2w acc. is 63.81*. p@1 of w2w p@1 is 73.52*. p@2 of w2w p@2 is 90.33*. p@3 of w2w p@3 is 96.64. acc. of w2w acc. is 44.14*. p@1 of w2w p@1 is 73.42*. p@2 of w2w p@2 is 89.44*. p@3 of w2w p@3 is 95.51*. acc. of w2w acc. is 34.23*.
table 2 shows the results of both automatic evaluations and human evaluation. extrema of seq2seq extrema is 0.1640. greedy of seq2seq greedy is 0.4098. average of seq2seq average is 0.4911. recall of seq2seq recall is 0.1646. precision of seq2seq precision is 0.1646. f1 of seq2seq f1 is 0.1646. read. of seq2seq read. is 2.30. info. of seq2seq info. is 2.16. p-score of seq2seq p-score is 0.49. extrema of persona extrema is 0.1631. greedy of persona greedy is 0.3982. average of persona average is 0.4871. recall of persona recall is 0.1646. precision of persona precision is 0.1646. f1 of persona f1 is 0.1646. read. of persona read. is 2.31. info. of persona info. is 2.15. p-score of persona p-score is 0.50. extrema of adaptation extrema is 0.1722. greedy of adaptation greedy is 0.4038. average of adaptation average is 0.5113. recall of adaptation recall is 0.1689. precision of adaptation precision is 0.1689. f1 of adaptation f1 is 0.1689. read. of adaptation read. is 2.29. info. of adaptation info. is 1.93. p-score of adaptation p-score is 0.47. extrema of cvae extrema is 0.2643. greedy of cvae greedy is 0.2911. average of cvae average is 0.5759. recall of cvae recall is 0.1931. precision of cvae precision is 0.1636. f1 of cvae f1 is 0.1771. read. of cvae read. is 2.02. info. of cvae info. is 2.33. p-score of cvae p-score is 0.45. extrema of rl-persona extrema is 0.1694. greedy of rl-persona greedy is 0.4536. average of rl-persona average is 0.4906. recall of rl-persona recall is 0.1723. precision of rl-persona precision is 0.1723. f1 of rl-persona f1 is 0.1723. read. of rl-persona read. is 2.21. info. of rl-persona info. is 2.22. p-score of rl-persona p-score is 0.63. extrema of diawae-gmd extrema is 0.4387. greedy of diawae-gmd greedy is 0.4752. average of diawae-gmd average is 0.7573. recall of diawae-gmd recall is 0.3409. precision of diawae-gmd precision is 0.1710. f1 of diawae-gmd f1 is 0.2277. read. of diawae-gmd read. is 2.31. info. of diawae-gmd info. is 2.35. p-score of diawae-gmd p-score is 0.50. extrema of personawae extrema is 0.4542. greedy of personawae greedy is 0.5914. average of personawae average is 0.7585. recall of personawae recall is 0.3365. precision of personawae precision is 0.1806. f1 of personawae f1 is 0.2350. read. of personawae read. is 2.33. info. of personawae info. is 2.37. p-score of personawae p-score is 0.66. extrema of ground-truth extrema is . greedy of ground-truth greedy is . average of ground-truth average is . recall of ground-truth recall is . precision of ground-truth precision is . f1 of ground-truth f1 is . read. of ground-truth read. is 2.73. info. of ground-truth info. is 2.66. p-score of ground-truth p-score is 0.86.
table 2 shows comparison of mrr when using dialogue acts of context-only, response-only and crossway fashion mrr of siamese-pda-st context-da (zhao et al., 2017) is 0.912. mrr of siamese-pda-st response-da is 0.900. mrr of siamese-pda-st crossway is 0.900. mrr of siamese-pda-st context-da (zhao et al., 2017) is 0.639. mrr of siamese-pda-st response-da is 0.649. mrr of siamese-pda-st crossway is 0.669. mrr of siamese-pda-mt (zhao et al., 2017) context-da (zhao et al., 2017) is 0.921. mrr of siamese-pda-mt (zhao et al., 2017) response-da is 0.919. mrr of siamese-pda-mt (zhao et al., 2017) crossway is 0.946. mrr of siamese-pda-mt (zhao et al., 2017) context-da (zhao et al., 2017) is 0.698. mrr of siamese-pda-mt (zhao et al., 2017) response-da is 0.685. mrr of siamese-pda-mt (zhao et al., 2017) crossway is 0.703. mrr of siamese-ada (kumar et al., 2018) context-da (zhao et al., 2017) is 0.934. mrr of siamese-ada (kumar et al., 2018) response-da is 0.927. mrr of siamese-ada (kumar et al., 2018) crossway is 0.956. mrr of siamese-ada (kumar et al., 2018) context-da (zhao et al., 2017) is 0.656. mrr of siamese-ada (kumar et al., 2018) response-da is 0.682. mrr of siamese-ada (kumar et al., 2018) crossway is 0.719.
table 2 shows comparison with the semi-supervised image captioning method, “self-retrieval” [liu et al., 2018]. bleu1 of self-retrieval [liu et al., 2018] (w/o unlabeled) bleu1 is 79.8. bleu2 of self-retrieval [liu et al., 2018] (w/o unlabeled) bleu2 is 62.3. bleu3 of self-retrieval [liu et al., 2018] (w/o unlabeled) bleu3 is 47.1. bleu4 of self-retrieval [liu et al., 2018] (w/o unlabeled) bleu4 is 34.9. rouge-l of self-retrieval [liu et al., 2018] (w/o unlabeled) rouge-l is 56.6. spice of self-retrieval [liu et al., 2018] (w/o unlabeled) spice is 20.5. meteor of self-retrieval [liu et al., 2018] (w/o unlabeled) meteor is 27.5. cider of self-retrieval [liu et al., 2018] (w/o unlabeled) cider is 114.6. bleu1 of self-retrieval [liu et al., 2018] (with unlabeled) bleu1 is 80.1. bleu2 of self-retrieval [liu et al., 2018] (with unlabeled) bleu2 is 63.1. bleu3 of self-retrieval [liu et al., 2018] (with unlabeled) bleu3 is 48. bleu4 of self-retrieval [liu et al., 2018] (with unlabeled) bleu4 is 35.8. rouge-l of self-retrieval [liu et al., 2018] (with unlabeled) rouge-l is 57. spice of self-retrieval [liu et al., 2018] (with unlabeled) spice is 21. meteor of self-retrieval [liu et al., 2018] (with unlabeled) meteor is 27.4. cider of self-retrieval [liu et al., 2018] (with unlabeled) cider is 117.1. bleu1 of baseline (w/o unlabeled) bleu1 is 77.7. bleu2 of baseline (w/o unlabeled) bleu2 is 61.6. bleu3 of baseline (w/o unlabeled) bleu3 is 46.9. bleu4 of baseline (w/o unlabeled) bleu4 is 36.2. rouge-l of baseline (w/o unlabeled) rouge-l is 56.8. spice of baseline (w/o unlabeled) spice is 20. meteor of baseline (w/o unlabeled) meteor is 26.7. cider of baseline (w/o unlabeled) cider is 114.2. bleu1 of ours (w/o unlabeled) bleu1 is 80.8. bleu2 of ours (w/o unlabeled) bleu2 is 65.3. bleu3 of ours (w/o unlabeled) bleu3 is 49.9. bleu4 of ours (w/o unlabeled) bleu4 is 37.6. rouge-l of ours (w/o unlabeled) rouge-l is 58.7. spice of ours (w/o unlabeled) spice is 22.7. meteor of ours (w/o unlabeled) meteor is 28.4. cider of ours (w/o unlabeled) cider is 122.6. bleu1 of ours (with unlabeled) bleu1 is 81.2. bleu2 of ours (with unlabeled) bleu2 is 66. bleu3 of ours (with unlabeled) bleu3 is 50.9. bleu4 of ours (with unlabeled) bleu4 is 39.1. rouge-l of ours (with unlabeled) rouge-l is 59.4. spice of ours (with unlabeled) spice is 23.8. meteor of ours (with unlabeled) meteor is 29.4. cider of ours (with unlabeled) cider is 125.5.
table 4 shows binary accuracy for different variants of cmfn and training scenarios outlined in section 5. accuracy of c-mfn (p) t is 62.85. accuracy of c-mfn (p) a+v is 53.3. accuracy of c-mfn (p) t+a is 63.28. accuracy of c-mfn (p) t+v is 63.22. accuracy of c-mfn (p) t+a+v is 64.47. accuracy of c-mfn (c) t is 57.96. accuracy of c-mfn (c) a+v is 50.23. accuracy of c-mfn (c) t+a is 57.78. accuracy of c-mfn (c) t+v is 57.99. accuracy of c-mfn (c) t+a+v is 58.45. accuracy of c-mfn t is 64.44. accuracy of c-mfn a+v is 57.99. accuracy of c-mfn t+a is 64.47. accuracy of c-mfn t+v is 64.22. accuracy of c-mfn t+a+v is 65.23.
table 2 shows performance compared with state-of-the-art methods on youtube2text dataset. b-4 of lstm-i b-4 is 0.446. m of lstm-i m is 0.297. b-4 of hrne b-4 is 0.438. m of hrne m is 0.331. b-4 of ma b-4 is 0.504. m of ma m is 0.318. c of ma c is 0.699. b-4 of scn b-4 is 0.511. m of scn m is 0.335. c of scn c is 0.777. b-4 of tsa b-4 is 0.517. m of tsa m is 0.34. c of tsa c is 0.749. b-4 of sa-lstm b-4 is 0.523. m of sa-lstm m is 0.341. r of sa-lstm r is 0.698. c of sa-lstm c is 0.803. b-4 of picknet b-4 is 0.523. m of picknet m is 0.333. r of picknet r is 0.696. c of picknet c is 0.765. b-4 of asgn+lna b-4 is 0.547. m of asgn+lna m is 0.342. r of asgn+lna r is 0.717. c of asgn+lna c is 0.813.
table 3 shows the slu performance on baseline models compared with our stack-propagation model on two datasets. slot (f1) of gate-mechanism slot (f1) is 92.2. intent (acc) of gate-mechanism intent (acc) is 97.6. overall (acc) of gate-mechanism overall (acc) is 82.4. slot (f1) of gate-mechanism slot (f1) is 95.3. intent (acc) of gate-mechanism intent (acc) is 96.2. overall (acc) of gate-mechanism overall (acc) is 83.4. slot (f1) of pipelined model slot (f1) is 90.8. intent (acc) of pipelined model intent (acc) is 97.6. overall (acc) of pipelined model overall (acc) is 81.8. slot (f1) of pipelined model slot (f1) is 95.1. intent (acc) of pipelined model intent (acc) is 96.1. overall (acc) of pipelined model overall (acc) is 82.3. slot (f1) of sentence intent augmented slot (f1) is 93.7. intent (acc) of sentence intent augmented intent (acc) is 97.5. overall (acc) of sentence intent augmented overall (acc) is 86.1. slot (f1) of sentence intent augmented slot (f1) is 95.5. intent (acc) of sentence intent augmented intent (acc) is 96.7. overall (acc) of sentence intent augmented overall (acc) is 85.8. intent (acc) of lstm+last-hidden intent (acc) is 97.1. intent (acc) of lstm+last-hidden intent (acc) is 95.2. intent (acc) of lstm+token-level intent (acc) is 97.5. intent (acc) of lstm+token-level intent (acc) is 96. slot (f1) of without self-attention slot (f1) is 94.1. intent (acc) of without self-attention intent (acc) is 97.8. overall (acc) of without self-attention overall (acc) is 86.6. slot (f1) of without self-attention slot (f1) is 95.6. intent (acc) of without self-attention intent (acc) is 96.6. overall (acc) of without self-attention overall (acc) is 86.2. slot (f1) of our model slot (f1) is 94.2. intent (acc) of our model intent (acc) is 98. overall (acc) of our model overall (acc) is 86.9. slot (f1) of our model slot (f1) is 95.9. intent (acc) of our model intent (acc) is 96.9. overall (acc) of our model overall (acc) is 86.5.
table 1 shows experimental results of answer generation on tacos-multilevel and youtubeclip datasets. bleu-1 of esa+ bleu-1 is 0.356. bleu-2 of esa+ bleu-2 is 0.244. rouge of esa+ rouge is 0.422. meteor of esa+ meteor is 0.109. bleu-1 of esa+ bleu-1 is 0.268. bleu-2 of esa+ bleu-2 is 0.151. rouge of esa+ rouge is 0.276. meteor of esa+ meteor is 0.082. bleu-1 of stan+ bleu-1 is 0.408. bleu-2 of stan+ bleu-2 is 0.312. rouge of stan+ rouge is 0.449. meteor of stan+ meteor is 0.133. bleu-1 of stan+ bleu-1 is 0.315. bleu-2 of stan+ bleu-2 is 0.185. rouge of stan+ rouge is 0.306. meteor of stan+ meteor is 0.09. bleu-1 of cdmn+ bleu-1 is 0.429. bleu-2 of cdmn+ bleu-2 is 0.341. rouge of cdmn+ rouge is 0.46. meteor of cdmn+ meteor is 0.142. bleu-1 of cdmn+ bleu-1 is 0.293. bleu-2 of cdmn+ bleu-2 is 0.161. rouge of cdmn+ rouge is 0.311. meteor of cdmn+ meteor is 0.094. bleu-1 of lf+ bleu-1 is 0.404. bleu-2 of lf+ bleu-2 is 0.29. rouge of lf+ rouge is 0.465. meteor of lf+ meteor is 0.135. bleu-1 of lf+ bleu-1 is 0.284. bleu-2 of lf+ bleu-2 is 0.183. rouge of lf+ rouge is 0.307. meteor of lf+ meteor is 0.083. bleu-1 of hre+ bleu-1 is 0.438. bleu-2 of hre+ bleu-2 is 0.32. rouge of hre+ rouge is 0.502. meteor of hre+ meteor is 0.153. bleu-1 of hre+ bleu-1 is 0.293. bleu-2 of hre+ bleu-2 is 0.172. rouge of hre+ rouge is 0.308. meteor of hre+ meteor is 0.094. bleu-1 of mn+ bleu-1 is 0.43. bleu-2 of mn+ bleu-2 is 0.326. rouge of mn+ rouge is 0.472. meteor of mn+ meteor is 0.149. bleu-1 of mn+ bleu-1 is 0.306. bleu-2 of mn+ bleu-2 is 0.185. rouge of mn+ rouge is 0.29. meteor of mn+ meteor is 0.086. bleu-1 of sfqih+ bleu-1 is 0.438. bleu-2 of sfqih+ bleu-2 is 0.334. rouge of sfqih+ rouge is 0.481. meteor of sfqih+ meteor is 0.153. bleu-1 of sfqih+ bleu-1 is 0.326. bleu-2 of sfqih+ bleu-2 is 0.202. rouge of sfqih+ rouge is 0.319. meteor of sfqih+ meteor is 0.085. bleu-1 of hacrn bleu-1 is 0.451. bleu-2 of hacrn bleu-2 is 0.346. rouge of hacrn rouge is 0.499. meteor of hacrn meteor is 0.161. bleu-1 of hacrn bleu-1 is 0.307. bleu-2 of hacrn bleu-2 is 0.174. rouge of hacrn rouge is 0.331. meteor of hacrn meteor is 0.104. bleu-1 of rict (ours) bleu-1 is 0.464. bleu-2 of rict (ours) bleu-2 is 0.361. rouge of rict (ours) rouge is 0.527. meteor of rict (ours) meteor is 0.178. bleu-1 of rict (ours) bleu-1 is 0.333. bleu-2 of rict (ours) bleu-2 is 0.194. rouge of rict (ours) rouge is 0.332. meteor of rict (ours) meteor is 0.104.
table 2 shows experimental results of question generation on tacos-multilevel and youtubeclip datasets. bleu-1 of esa+ bleu-1 is 0.693. bleu-2 of esa+ bleu-2 is 0.582. rouge of esa+ rouge is 0.718. meteor of esa+ meteor is 0.341. bleu-1 of esa+ bleu-1 is 0.497. bleu-2 of esa+ bleu-2 is 0.333. rouge of esa+ rouge is 0.565. meteor of esa+ meteor is 0.212. bleu-1 of stan+ bleu-1 is 0.706. bleu-2 of stan+ bleu-2 is 0.599. rouge of stan+ rouge is 0.73. meteor of stan+ meteor is 0.354. bleu-1 of stan+ bleu-1 is 0.483. bleu-2 of stan+ bleu-2 is 0.322. rouge of stan+ rouge is 0.559. meteor of stan+ meteor is 0.208. bleu-1 of cdmn+ bleu-1 is 0.707. bleu-2 of cdmn+ bleu-2 is 0.603. rouge of cdmn+ rouge is 0.74. meteor of cdmn+ meteor is 0.357. bleu-1 of cdmn+ bleu-1 is 0.507. bleu-2 of cdmn+ bleu-2 is 0.341. rouge of cdmn+ rouge is 0.567. meteor of cdmn+ meteor is 0.219. bleu-1 of lf+ bleu-1 is 0.704. bleu-2 of lf+ bleu-2 is 0.598. rouge of lf+ rouge is 0.728. meteor of lf+ meteor is 0.349. bleu-1 of lf+ bleu-1 is 0.512. bleu-2 of lf+ bleu-2 is 0.346. rouge of lf+ rouge is 0.574. meteor of lf+ meteor is 0.218. bleu-1 of hre+ bleu-1 is 0.694. bleu-2 of hre+ bleu-2 is 0.592. rouge of hre+ rouge is 0.729. meteor of hre+ meteor is 0.348. bleu-1 of hre+ bleu-1 is 0.515. bleu-2 of hre+ bleu-2 is 0.35. rouge of hre+ rouge is 0.571. meteor of hre+ meteor is 0.223. bleu-1 of mn+ bleu-1 is 0.698. bleu-2 of mn+ bleu-2 is 0.589. rouge of mn+ rouge is 0.718. meteor of mn+ meteor is 0.345. bleu-1 of mn+ bleu-1 is 0.488. bleu-2 of mn+ bleu-2 is 0.324. rouge of mn+ rouge is 0.556. meteor of mn+ meteor is 0.204. bleu-1 of sfqih+ bleu-1 is 0.694. bleu-2 of sfqih+ bleu-2 is 0.592. rouge of sfqih+ rouge is 0.729. meteor of sfqih+ meteor is 0.349. bleu-1 of sfqih+ bleu-1 is 0.503. bleu-2 of sfqih+ bleu-2 is 0.339. rouge of sfqih+ rouge is 0.563. meteor of sfqih+ meteor is 0.217. bleu-1 of hacrn bleu-1 is 0.715. bleu-2 of hacrn bleu-2 is 0.616. rouge of hacrn rouge is 0.741. meteor of hacrn meteor is 0.358. bleu-1 of hacrn bleu-1 is 0.524. bleu-2 of hacrn bleu-2 is 0.352. rouge of hacrn rouge is 0.577. meteor of hacrn meteor is 0.229. bleu-1 of rict (ours) bleu-1 is 0.733. bleu-2 of rict (ours) bleu-2 is 0.625. rouge of rict (ours) rouge is 0.748. meteor of rict (ours) meteor is 0.367. bleu-1 of rict (ours) bleu-1 is 0.536. bleu-2 of rict (ours) bleu-2 is 0.375. rouge of rict (ours) rouge is 0.593. meteor of rict (ours) meteor is 0.234.
table 2 shows performances of negative focus detection systems on the sem’12 corpus. acc of clac (rosenberg (2012)) acc is 60. acc of foc-det (blanco (2013)) acc is 65.5. acc of wtgm (zou (2015)) acc is 68.4. acc of lstm acc is 58.71. acc of bilstm acc is 60.81. acc of bilstm-crf acc is 67.28. acc of w-att bilstm-crf acc is 70.22. acc of t-att bilstm-crf acc is 70.51. acc of wt-att bilstm-crf acc is 69.8.
table 7 shows performance comparison for different pretrained word embeddings on the t-att bilstm-crf model. dimension of senna dimension is 50. acc of senna acc is 70.08. dimension of glove dimension is 100. acc of glove acc is 69.66. dimension of word2vec dimension is 300. acc of word2vec acc is 69.1. dimension of bert dimension is 768. acc of bert acc is 70.22. dimension of elmo dimension is 1024. acc of elmo acc is 70.51.
table 3 shows results in accuracy on the local discrimination task. accuracy of word2vec dw=1,2,3 is 60.27. accuracy of word2vec dw=1 is 56.11. accuracy of word2vec dw=2 is 60.23. accuracy of word2vec dw=3 is 62.23. accuracy of word2vec dw=1,2,3 is 55.01. accuracy of word2vec dw=1 is 53.81. accuracy of word2vec dw=2 is 55.37. accuracy of word2vec dw=3 is 56.16. accuracy of word2vec dw=1,2,3 is 6.76. accuracy of word2vec dw=1 is 4.28. accuracy of word2vec dw=2 is 6.82. accuracy of word2vec dw=3 is 9.25. accuracy of word2vec dw=1,2,3 is 57.24. accuracy of word2vec dw=1 is 53.35. accuracy of word2vec dw=2 is 56.58. accuracy of word2vec dw=3 is 59.67. accuracy of word2vec dw=1,2,3 is 73.23. accuracy of word2vec dw=1 is 66.21. accuracy of word2vec dw=2 is 73.16. accuracy of word2vec dw=3 is 77.93. accuracy of elmo dw=1,2,3 is 74.12. accuracy of elmo dw=1 is 65.82. accuracy of elmo dw=2 is 73.54. accuracy of elmo dw=3 is 78.16. accuracy of word2vec dw=1,2,3 is 75.37. accuracy of word2vec dw=1 is 67.29. accuracy of word2vec dw=2 is 75.58. accuracy of word2vec dw=3 is 80.21. accuracy of elmo dw=1,2,3 is 77.07. accuracy of elmo dw=1 is 64.38. accuracy of elmo dw=2 is 76.12. accuracy of elmo dw=3 is 81.23.
table 4 shows test set micro-averaged f1 scores on labelled attachment decisions. f1 of hayashi et al. (2016) s is 65.1. f1 of hayashi et al. (2016) n is 54.6. f1 of hayashi et al. (2016) r is 44.7. f1 of hayashi et al. (2016) f is 44.1. f1 of surdeanu et al. (2015) s is 65.3. f1 of surdeanu et al. (2015) n is 54.2. f1 of surdeanu et al. (2015) r is 45.1. f1 of surdeanu et al. (2015) f is 44.2. f1 of joty et al. (2015) s is 65.1. f1 of joty et al. (2015) n is 55.5. f1 of joty et al. (2015) r is 45.1. f1 of joty et al. (2015) f is 44.3. f1 of feng and hirst (2014a) s is 68.6. f1 of feng and hirst (2014a) n is 55.9. f1 of feng and hirst (2014a) r is 45.8. f1 of feng and hirst (2014a) f is 44.6. f1 of braud et al. (2016) s is 59.5. f1 of braud et al. (2016) n is 47.2. f1 of braud et al. (2016) r is 34.7. f1 of braud et al. (2016) f is 34.3. f1 of li et al. (2016) s is 64.5. f1 of li et al. (2016) n is 54. f1 of li et al. (2016) r is 38.1. f1 of li et al. (2016) f is 36.6. f1 of braud et al. (2017) (mono) s is 61.9. f1 of braud et al. (2017) (mono) n is 53.4. f1 of braud et al. (2017) (mono) r is 44.5. f1 of braud et al. (2017) (mono) f is 44. f1 of discriminative baseline s is 65.2. f1 of discriminative baseline n is 54.9. f1 of discriminative baseline r is 42.8. f1 of discriminative baseline f is 42.4. f1 of generative model s is 67.1. f1 of generative model n is 57.4. f1 of generative model r is 45.5. f1 of generative model f is 45. f1 of ji and eisenstein (2014) (updated) s is 64.1. f1 of ji and eisenstein (2014) (updated) n is 54.2. f1 of ji and eisenstein (2014) (updated) r is 46.8. f1 of ji and eisenstein (2014) (updated) f is 46.3. f1 of braud et al. (2017) (cross + dev) s is 62.7. f1 of braud et al. (2017) (cross + dev) n is 54.5. f1 of braud et al. (2017) (cross + dev) r is 45.5. f1 of braud et al. (2017) (cross + dev) f is 45.1.
table 2 shows evaluations of weakly supervised (snorkel and stand alone gen) and supervised approaches on stac data. precision of last precision is 0.54. recall of last recall is 0.55. f1 score of last f1 score is 0.55. accuracy of last accuracy is 0.84. precision of bilstm on gold labels precision is 0.33. recall of bilstm on gold labels recall is 0.8. f1 score of bilstm on gold labels f1 score is 0.47. accuracy of bilstm on gold labels accuracy is 0.75. precision of bert on gold labels precision is 0.56. recall of bert on gold labels recall is 0.48. f1 score of bert on gold labels f1 score is 0.52. accuracy of bert on gold labels accuracy is 0.88. precision of logreg* on gold labels precision is 0.73. recall of logreg* on gold labels recall is 0.52. f1 score of logreg* on gold labels f1 score is 0.61. accuracy of logreg* on gold labels accuracy is 0.91. precision of bert+logreg* on gold labels precision is 0.59. recall of bert+logreg* on gold labels recall is 0.49. f1 score of bert+logreg* on gold labels f1 score is 0.53. accuracy of bert+logreg* on gold labels accuracy is 0.89. precision of gen + disc (bilstm) precision is 0.28. recall of gen + disc (bilstm) recall is 0.59. f1 score of gen + disc (bilstm) f1 score is 0.38. accuracy of gen + disc (bilstm) accuracy is 0.74. precision of gen + disc (bert) precision is 0.49. recall of gen + disc (bert) recall is 0.4. f1 score of gen + disc (bert) f1 score is 0.44. accuracy of gen + disc (bert) accuracy is 0.86. precision of gen + disc (logreg*) precision is 0.68. recall of gen + disc (logreg*) recall is 0.65. f1 score of gen + disc (logreg*) f1 score is 0.67. accuracy of gen + disc (logreg*) accuracy is 0.91. precision of gen precision is 0.69. recall of gen recall is 0.66. f1 score of gen f1 score is 0.68. accuracy of gen accuracy is 0.92. precision of gen + mst-short precision is 0.73. recall of gen + mst-short recall is 0.71. f1 score of gen + mst-short f1 score is 0.72. accuracy of gen + mst-short accuracy is 0.93.
table 4 shows performance of the different adr detection techniques on the twitter and reddit test sets. precision of quickumls precision is 0.47. recall of quickumls recall is 0.34. fscore of quickumls fscore is 0.39. precision of crf precision is 0.67. recall of crf recall is 0.42. fscore of crf fscore is 0.51. precision of blstm-rnn precision is 0.61. recall of blstm-rnn recall is 0.87. fscore of blstm-rnn fscore is 0.72. precision of crf+vae precision is 0.68. recall of crf+vae recall is 0.49. fscore of crf+vae fscore is 0.57. precision of blstm-rnn+vae precision is 0.71. recall of blstm-rnn+vae recall is 0.85. fscore of blstm-rnn+vae fscore is 0.77. precision of quickumls.1 precision is 0.14. recall of quickumls.1 recall is 0.21. fscore of quickumls.1 fscore is 17. precision of crf precision is 0.72. recall of crf recall is 0.47. fscore of crf fscore is 0.57. precision of blstm-rnn precision is 0.67. recall of blstm-rnn recall is 0.28. fscore of blstm-rnn fscore is 0.39. precision of crf+vae precision is 0.69. recall of crf+vae recall is 0.52. fscore of crf+vae fscore is 0.6. precision of blstm-rnn+vae precision is 0.63. recall of blstm-rnn+vae recall is 0.29. fscore of blstm-rnn+vae fscore is 0.4.
table 6 shows performances of whether using the pre-trained kb embedding by transe. bleu4 of true bleu4 is 36.56. rouge-l of true rouge-l is 58.09. meteor of true meteor is 34.41. bleu4 of false bleu4 is 33.67. rouge-l of false rouge-l is 55.57. meteor of false meteor is 33.2. bleu4 of true bleu4 is 41.72. rouge-l of true rouge-l is 69.31. meteor of true meteor is 48.13. bleu4 of false bleu4 is 41.55. rouge-l of false rouge-l is 68.59. meteor of false meteor is 47.52.
table 3 shows results on the xqa. accuracy of xlm (lample and conneau, 2019) en is 80.2. accuracy of xlm (lample and conneau, 2019) fr is 65.1. accuracy of xlm (lample and conneau, 2019) de is 63.3. accuracy of xlm (lample and conneau, 2019) average is 64.2. accuracy of unicoder en is 81.1. accuracy of unicoder fr is 66.2. accuracy of unicoder de is 66.5. accuracy of unicoder average is 66.4. accuracy of xlm (lample and conneau, 2019) en is 80.2. accuracy of xlm (lample and conneau, 2019) fr is 62.3. accuracy of xlm (lample and conneau, 2019) de is 61.7. accuracy of xlm (lample and conneau, 2019) average is 62. accuracy of unicoder en is 81.1. accuracy of unicoder fr is 64.1. accuracy of unicoder de is 63.7. accuracy of unicoder average is 63.9. accuracy of bert (devlin et al., 2018) en is 76.4. accuracy of bert (devlin et al., 2018) fr is 61.6. accuracy of bert (devlin et al., 2018) de is 64.6. accuracy of bert (devlin et al., 2018) average is 63.1. accuracy of xlm (lample and conneau, 2019) en is 80.7. accuracy of xlm (lample and conneau, 2019) fr is 67.1. accuracy of xlm (lample and conneau, 2019) de is 68.2. accuracy of xlm (lample and conneau, 2019) average is 67.7. accuracy of unicoder en is 81.4. accuracy of unicoder fr is 69.3. accuracy of unicoder de is 70.1. accuracy of unicoder average is 69.7.
table 2 shows performance of adamrc compared with baseline models on three datasets, using san as the mrc model. em of san em is 36.68. f1 of san f1 is 52.79. em of synnet + san em is 35.19. f1 of synnet + san f1 is 49.61. em of adamrc em is 38.46. f1 of adamrc f1 is 54.20. em of adamrc with gt questions em is 39.37. f1 of adamrc with gt questions f1 is 54.63. em of san em is 56.83. f1 of san f1 is 68.62. em of synnet + san em is 50.34. f1 of synnet + san f1 is 62.42. em of adamrc em is 58.20. f1 of adamrc f1 is 69.75. em of adamrc with gt questions em is 58.82. f1 of adamrc with gt questions f1 is 70.14. em of san em is 13.06. f1 of san f1 is 25.80. em of synnet + san em is 12.52. f1 of synnet + san f1 is 25.47. em of adamrc em is 14.09. f1 of adamrc f1 is 26.09. em of adamrc with gt questions em is 15.59. f1 of adamrc with gt questions f1 is 26.40. em of san em is 27.06. f1 of san f1 is 40.07. em of synnet + san em is 23.67. f1 of synnet + san f1 is 36.79. em of adamrc em is 27.92. f1 of adamrc f1 is 40.69. em of adamrc with gt questions em is 27.79. f1 of adamrc with gt questions f1 is 41.47.
table 3 shows human evaluation of keag and state-of-theart answer generation models. syntactic of gqa syntactic is 3.78. correct of gqa correct is 3.54. syntactic of gqa w/ kblstm syntactic is 3.98. correct of gqa w/ kblstm correct is 3.62. syntactic of gqa w/ crwe syntactic is 3.91. correct of gqa w/ crwe correct is 3.69. syntactic of mhpgm syntactic is 4.1. correct of mhpgm correct is 3.81. syntactic of keag syntactic is 4.18. correct of keag correct is 4.03.
table 3 shows results on arc easy test   accuracy of random guess accuracy is 25.00%. accuracy of ir solver accuracy is 62.55%. accuracy of reading strategies (previous sota) accuracy is 68.90%. accuracy of attentive ranker (ours) accuracy is 72.30%.
table 6 shows downstream model performance on   accuracy (d) of tf-idf accuracy (d) is 35.59%. accuracy (d) of ours accuracy (d) is 38.3%(+2.71). accuracy (d) of tf-idf accuracy (d) is 35.93%. accuracy (d) of ours accuracy (d) is 43.72%(+7.79). accuracy (d) of tf-idf accuracy (d) is 34.93%. accuracy (d) of ours accuracy (d) is 37.51%(+3.58). accuracy (d) of tf-idf accuracy (d) is 37.08%. accuracy (d) of ours accuracy (d) is 40%(+2.92).
table 4 shows human performance (single-annotator). accuracy (%) of reasoning-free accuracy (%) is 90.4. macro-f1 (%) of reasoning-free macro-f1 (%) is 84.18. accuracy (%) of reasoning-required accuracy (%) is 78. macro-f1 (%) of reasoning-required macro-f1 (%) is 72.19.
table 2 shows evaluation results on link prediction mean rank of se raw is 1011. mean rank of se filtered is 985. hits@10(%) of se raw is 68.5. hits@10(%) of se filtered is 80.5. mean rank of se raw is 273. mean rank of se filtered is 162. hits@10(%) of se raw is 28.8. hits@10(%) of se filtered is 39.8. mean rank of sme raw is 545. mean rank of sme filtered is 533. hits@10(%) of sme raw is 65.1. hits@10(%) of sme filtered is 74.1. mean rank of sme raw is 274. mean rank of sme filtered is 154. hits@10(%) of sme raw is 30.7. hits@10(%) of sme filtered is 40.8. mean rank of transe raw is 263. mean rank of transe filtered is 251. hits@10(%) of transe raw is 75.4. hits@10(%) of transe filtered is 89.2. mean rank of transe raw is 243. mean rank of transe filtered is 125. hits@10(%) of transe raw is 34.9. hits@10(%) of transe filtered is 47.1. mean rank of transh raw is 318. mean rank of transh filtered is 303. hits@10(%) of transh raw is 75.4. hits@10(%) of transh filtered is 86.7. mean rank of transh raw is 212. mean rank of transh filtered is 87. hits@10(%) of transh raw is 45.7. hits@10(%) of transh filtered is 64.4. mean rank of transr raw is 238. mean rank of transr filtered is 225. hits@10(%) of transr raw is 79.8. hits@10(%) of transr filtered is 92. mean rank of transr raw is 198. mean rank of transr filtered is 77. hits@10(%) of transr raw is 48.2. hits@10(%) of transr filtered is 68.7. mean rank of transparse raw is 223. mean rank of transparse filtered is 211. hits@10(%) of transparse raw is 80.1. hits@10(%) of transparse filtered is 93.2. mean rank of transparse raw is 187. mean rank of transparse filtered is 82. hits@10(%) of transparse raw is 53.5. hits@10(%) of transparse filtered is 79.5. mean rank of stranse raw is 217. mean rank of stranse filtered is 206. hits@10(%) of stranse raw is 80.9. hits@10(%) of stranse filtered is 93.4. mean rank of stranse raw is 219. mean rank of stranse filtered is 69. hits@10(%) of stranse raw is 51.6. hits@10(%) of stranse filtered is 79.7. mean rank of itransf filtered is 205. hits@10(%) of itransf filtered is 94.2. mean rank of itransf filtered is 65. hits@10(%) of itransf filtered is 81. hits@10(%) of hole filtered is 94.9. hits@10(%) of hole filtered is 73.9. hits@10(%) of complex filtered is 94.7. hits@10(%) of complex filtered is 84. hits@10(%) of analogy filtered is 94.7. hits@10(%) of analogy filtered is 85.4. mean rank of proje raw is 277. mean rank of proje filtered is 260. hits@10(%) of proje raw is 79.4. hits@10(%) of proje filtered is 94.9. mean rank of proje raw is 124. mean rank of proje filtered is 34. hits@10(%) of proje raw is 54.7. hits@10(%) of proje filtered is 88.4. mean rank of rtranse filtered is 50. hits@10(%) of rtranse filtered is 76.2. mean rank of ptranse (add, 2-step) raw is 235. mean rank of ptranse (add, 2-step) filtered is 221. hits@10(%) of ptranse (add, 2-step) raw is 81.3. hits@10(%) of ptranse (add, 2-step) filtered is 92.7. mean rank of ptranse (add, 2-step) raw is 200. mean rank of ptranse (add, 2-step) filtered is 54. hits@10(%) of ptranse (add, 2-step) raw is 51.8. hits@10(%) of ptranse (add, 2-step) filtered is 83.4. mean rank of ptranse (mul, 2-step) raw is 243. mean rank of ptranse (mul, 2-step) filtered is 230. hits@10(%) of ptranse (mul, 2-step) raw is 79.5. hits@10(%) of ptranse (mul, 2-step) filtered is 90.9. mean rank of ptranse (mul, 2-step) raw is 216. mean rank of ptranse (mul, 2-step) filtered is 67. hits@10(%) of ptranse (mul, 2-step) raw is 47.4. hits@10(%) of ptranse (mul, 2-step) filtered is 77.7. mean rank of ptranse (add, 3-step) raw is 238. mean rank of ptranse (add, 3-step) filtered is 219. hits@10(%) of ptranse (add, 3-step) raw is 81.1. hits@10(%) of ptranse (add, 3-step) filtered is 94.2. mean rank of ptranse (add, 3-step) raw is 207. mean rank of ptranse (add, 3-step) filtered is 58. hits@10(%) of ptranse (add, 3-step) raw is 51.4. hits@10(%) of ptranse (add, 3-step) filtered is 84.6. hits@10(%) of paskoge raw is 81.3. hits@10(%) of paskoge filtered is 95. hits@10(%) of paskoge raw is 53.1. hits@10(%) of paskoge filtered is 88. mean rank of rpe (acom) raw is 171. mean rank of rpe (acom) filtered is 41. hits@10(%) of rpe (acom) raw is 52. hits@10(%) of rpe (acom) filtered is 85.5. mean rank of rpe (mcom) raw is 183. mean rank of rpe (mcom) filtered is 43. hits@10(%) of rpe (mcom) raw is 52.2. hits@10(%) of rpe (mcom) filtered is 81.7. mean rank of rotate filtered is 309. hits@10(%) of rotate filtered is 95.9. mean rank of rotate filtered is 40. hits@10(%) of rotate filtered is 88.4. mean rank of optranse raw is 211. mean rank of optranse filtered is 199. hits@10(%) of optranse raw is 83.2. hits@10(%) of optranse filtered is 95.7. mean rank of optranse raw is 136. mean rank of optranse filtered is 33. hits@10(%) of optranse raw is 58. hits@10(%) of optranse filtered is 89.9.
table 2 shows overall average results by model (with % changes from the input) slor of input slor is 0.5962. css of input css is 0.1166. spa of smerti-transformer spa is 0.6606. slor of smerti-transformer slor is 0.5255 (-11.86%). css of smerti-transformer css is 0.2857 (+145.03%). stes of smerti-transformer stes is 0.4337. spa of smerti-rnn spa is 0.6574. slor of smerti-rnn slor is 0.5122 (-14.09%). css of smerti-rnn css is 0.2927 (+151.03%). stes of smerti-rnn stes is 0.4354. spa of w2v-stem spa is 0.6667. slor of w2v-stem slor is 0.4672 (-21.64%). css of w2v-stem css is 0.2851 (+144.51%). stes of w2v-stem stes is 0.4197. spa of gwn-stem spa is 0.8903. slor of gwn-stem slor is 0.4864 (-18.42%). css of gwn-stem css is 0.1419 (+21.70%). stes of gwn-stem stes is 0.2934. spa of nwn-stem spa is 0.9116. slor of nwn-stem slor is 0.4832 (-18.95%). css of nwn-stem css is 0.1335 (+14.49%). stes of nwn-stem stes is 0.2814.
table 2 shows model performance (precision, recall, f1) on pmb data (v.2.1.0, test set); models were trained on gold standard data. f1 of van noord et al. (2018) f1 is 72.8. illformed of van noord et al. (2018) illformed is 20%. p of seq2seq+copy p is 75.57. r of seq2seq+copy r is 67.27. f1 of seq2seq+copy f1 is 71.18. illformed of seq2seq+copy illformed is 4.12%. p of seq2graph p is 75.51. r of seq2graph r is 71.69. f1 of seq2graph f1 is 73.55. illformed of seq2graph illformed is 0.40%.
table 1 shows comparisons with large pre-trained language model fine-tuning with different amount of training data. ihdev-acc. (%) of random guess ihdev-acc. (%) is 20. ihtest-acc. (%) of random guess ihtest-acc. (%) is 20. ihdev-acc. (%) of random guess ihdev-acc. (%) is 20. ihtest-acc. (%) of random guess ihtest-acc. (%) is 20. ihdev-acc. (%) of random guess ihdev-acc. (%) is 20. ihtest-acc. (%) of random guess ihtest-acc. (%) is 20. ihdev-acc. (%) of gpt-finetuning ihdev-acc. (%) is 27.55. ihtest-acc. (%) of gpt-finetuning ihtest-acc. (%) is 26.51. ihdev-acc. (%) of gpt-finetuning ihdev-acc. (%) is 32.46. ihtest-acc. (%) of gpt-finetuning ihtest-acc. (%) is 31.28. ihdev-acc. (%) of gpt-finetuning ihdev-acc. (%) is 47.35. ihtest-acc. (%) of gpt-finetuning ihtest-acc. (%) is 45.58. ihdev-acc. (%) of gpt-kagnet ihdev-acc. (%) is 28.13. ihtest-acc. (%) of gpt-kagnet ihtest-acc. (%) is 26.98. ihdev-acc. (%) of gpt-kagnet ihdev-acc. (%) is 33.72. ihtest-acc. (%) of gpt-kagnet ihtest-acc. (%) is 32.33. ihdev-acc. (%) of gpt-kagnet ihdev-acc. (%) is 48.95. ihtest-acc. (%) of gpt-kagnet ihtest-acc. (%) is 46.79. ihdev-acc. (%) of bert-base-finetuning ihdev-acc. (%) is 30.11. ihtest-acc. (%) of bert-base-finetuning ihtest-acc. (%) is 29.78. ihdev-acc. (%) of bert-base-finetuning ihdev-acc. (%) is 38.66. ihtest-acc. (%) of bert-base-finetuning ihtest-acc. (%) is 36.83. ihdev-acc. (%) of bert-base-finetuning ihdev-acc. (%) is 53.48. ihtest-acc. (%) of bert-base-finetuning ihtest-acc. (%) is 53.26. ihdev-acc. (%) of bert-base-kagnet ihdev-acc. (%) is 31.05. ihtest-acc. (%) of bert-base-kagnet ihtest-acc. (%) is 30.94. ihdev-acc. (%) of bert-base-kagnet ihdev-acc. (%) is 40.32. ihtest-acc. (%) of bert-base-kagnet ihtest-acc. (%) is 39.01. ihdev-acc. (%) of bert-base-kagnet ihdev-acc. (%) is 55.57. ihtest-acc. (%) of bert-base-kagnet ihtest-acc. (%) is 56.19. ihdev-acc. (%) of bert-large-finetuning ihdev-acc. (%) is 35.71. ihtest-acc. (%) of bert-large-finetuning ihtest-acc. (%) is 32.88. ihdev-acc. (%) of bert-large-finetuning ihdev-acc. (%) is 55.45. ihtest-acc. (%) of bert-large-finetuning ihtest-acc. (%) is 49.88. ihdev-acc. (%) of bert-large-finetuning ihdev-acc. (%) is 60.61. ihtest-acc. (%) of bert-large-finetuning ihtest-acc. (%) is 55.84. ihdev-acc. (%) of bert-large-kagnet ihdev-acc. (%) is 36.82. ihtest-acc. (%) of bert-large-kagnet ihtest-acc. (%) is 33.91. ihdev-acc. (%) of bert-large-kagnet ihdev-acc. (%) is 58.73. ihtest-acc. (%) of bert-large-kagnet ihtest-acc. (%) is 51.13. ihdev-acc. (%) of bert-large-kagnet ihdev-acc. (%) is 62.35. ihtest-acc. (%) of bert-large-kagnet ihtest-acc. (%) is 57.16. ihtest-acc. (%) of human performance ihtest-acc. (%) is 88.9. ihtest-acc. (%) of human performance ihtest-acc. (%) is 88.9. ihtest-acc. (%) of human performance ihtest-acc. (%) is 88.9.
table 4 shows results on wikisql. accuracy of reinforce (williams, 1992) dev is < 10. accuracy of iterative ml (liang et al., 2017) dev is 70.1. accuracy of hard em (liang et al., 2018) dev is 70.2. accuracy of beam-based mml (liang et al., 2018) dev is 70.7. accuracy of mapo (liang et al., 2018) dev is 71.8. accuracy of mapo (liang et al., 2018) test is 72.4. accuracy of mapox (agarwal et al., 2019) dev is 74.5. accuracy of mapox (agarwal et al., 2019) test is 74.2. accuracy of mapox+merl (agarwal et al., 2019) dev is 74.9. accuracy of mapox+merl (agarwal et al., 2019) test is 74.8. accuracy of mml dev is 70.6. accuracy of mml test is 70.5. accuracy of ours dev is 84.4. accuracy of ours test is 83.9. accuracy of sqlnet (xu et al., 2018) dev is 69.8. accuracy of sqlnet (xu et al., 2018) test is 68. accuracy of typesql (yu et al., 2018b) dev is 74.5. accuracy of typesql (yu et al., 2018b) test is 73.5. accuracy of coarse2fine (dong and lapata, 2018) dev is 79. accuracy of coarse2fine (dong and lapata, 2018) test is 78.5. accuracy of sqlova (hwang et al., 2019) dev is 87.2. accuracy of sqlova (hwang et al., 2019) test is 86.2. accuracy of x-sql (he et al., 2019) dev is 89.5. accuracy of x-sql (he et al., 2019) test is 88.7.
table 3 shows results for intra-turn relation prediction with gold and predicted premises precision of all relations gold is 5. recall of all relations gold is 100. f-score of all relations gold is 9. precision of menini et al. (2018) gold is 7. precision of menini et al. (2018) pred is 5.9. recall of menini et al. (2018) gold is 82. recall of menini et al. (2018) pred is 80. f-score of menini et al. (2018) gold is 13. f-score of menini et al. (2018) pred is 11. precision of menini et al. (2018) + rst features gold is 7.4. precision of menini et al. (2018) + rst features pred is 6.1. recall of menini et al. (2018) + rst features gold is 83. recall of menini et al. (2018) + rst features pred is 81. f-score of menini et al. (2018) + rst features gold is 13.7. f-score of menini et al. (2018) + rst features pred is 11.4. precision of rst features gold is 6.3. precision of rst features pred is 5.7. recall of rst features gold is 79.5. recall of rst features pred is 77. f-score of rst features gold is 11.8. f-score of rst features pred is 10.6. precision of morio and fujita (2018) gold is 10. recall of morio and fujita (2018) gold is 48.8. f-score of morio and fujita (2018) gold is 16.6. precision of bert devlin et al. (2019) gold is 12. precision of bert devlin et al. (2019) pred is 11. recall of bert devlin et al. (2019) gold is 67. recall of bert devlin et al. (2019) pred is 60. f-score of bert devlin et al. (2019) gold is 20.3. f-score of bert devlin et al. (2019) pred is 18.5. precision of imho context fine-tuned bert gold is 14.3. precision of imho context fine-tuned bert pred is 13.2. recall of imho context fine-tuned bert gold is 69. recall of imho context fine-tuned bert pred is 65. f-score of imho context fine-tuned bert gold is 23.7. f-score of imho context fine-tuned bert pred is 21.8. precision of  + rst ensemble gold is 16.7. precision of  + rst ensemble pred is 15.5. recall of  + rst ensemble gold is 73. recall of  + rst ensemble pred is 70.2. f-score of  + rst ensemble gold is 27.2. f-score of  + rst ensemble pred is 25.4.
table 2 shows performance for classifying review segments as good or bad for recommendation justification. f1 of bow-xgboost f1 is 0.559. recall of bow-xgboost recall is 0.679. precision of bow-xgboost precision is 0.475. f1 of cnn f1 is 0.644. recall of cnn recall is 0.596. precision of cnn precision is 0.7. f1 of lstm-maxpool f1 is 0.675. recall of lstm-maxpool recall is 0.703. precision of lstm-maxpool precision is 0.65. f1 of bert f1 is 0.747. recall of bert recall is 0.7. precision of bert precision is 0.8. f1 of bert-sa (one epoch) f1 is 0.481. recall of bert-sa (one epoch) recall is 0.975. precision of bert-sa (one epoch) precision is 0.32. f1 of bert-sa (three epoch) f1 is 0.491. recall of bert-sa (three epoch) recall is 1. precision of bert-sa (three epoch) precision is 0.325.
table 1 shows performance results of the compared methods. acc of section acc is 50.56. prec of section prec is 100. rec of section rec is 1.12. f1 of section f1 is 2.21. acc of infobox acc is 53.71. prec of infobox prec is 100. rec of infobox rec is 7.41. f1 of infobox f1 is 13.81. acc of related acc is 68.86. prec of related prec is 66.23. rec of related rec is 76.97. f1 of related f1 is 71.2. acc of o racle re acc is 75.89. prec of o racle re prec is 100. rec of o racle re rec is 51.77. f1 of o racle re f1 is 68.22. acc of prop acc is 81.45. prec of prop prec is 98.28. rec of prop rec is 64.02. f1 of prop f1 is 77.53.
table 2 shows results on the arxiv dataset. rouge-1 of sumbasic* rouge-1 is 29.47. rouge-2 of sumbasic* rouge-2 is 6.95. rouge-l of sumbasic* rouge-l is 26.3. rouge-1 of lsa* rouge-1 is 29.91. rouge-2 of lsa* rouge-2 is 7.42. rouge-l of lsa* rouge-l is 25.67. rouge-1 of lexrank* rouge-1 is 33.85. rouge-2 of lexrank* rouge-2 is 10.73. rouge-l of lexrank* rouge-l is 28.99. rouge-1 of attn-seq2seq* rouge-1 is 29.3. rouge-2 of attn-seq2seq* rouge-2 is 6. rouge-l of attn-seq2seq* rouge-l is 25.56. rouge-1 of pntr-gen-seq2seq* rouge-1 is 32.06. rouge-2 of pntr-gen-seq2seq* rouge-2 is 9.04. rouge-l of pntr-gen-seq2seq* rouge-l is 25.16. rouge-1 of discourse-aware* rouge-1 is 35.8. rouge-2 of discourse-aware* rouge-2 is 11.05. rouge-l of discourse-aware* rouge-l is 31.8. rouge-1 of baseline rouge-1 is 42.91. rouge-2 of baseline rouge-2 is 16.65. rouge-l of baseline rouge-l is 28.53. meteor of baseline meteor is 21.35. rouge-1 of cheng & lapata rouge-1 is 42.24. rouge-2 of cheng & lapata rouge-2 is 15.97. rouge-l of cheng & lapata rouge-l is 27.88. meteor of cheng & lapata meteor is 20.97. rouge-1 of summarunner rouge-1 is 42.81. rouge-2 of summarunner rouge-2 is 16.52. rouge-l of summarunner rouge-l is 28.23. meteor of summarunner meteor is 21.35. rouge-1 of ours-attentive context rouge-1 is 43.58. rouge-2 of ours-attentive context rouge-2 is 17.37. rouge-l of ours-attentive context rouge-l is 29.3. meteor of ours-attentive context meteor is 21.71. rouge-1 of ours-concat rouge-1 is 43.62. rouge-2 of ours-concat rouge-2 is 17.36. rouge-l of ours-concat rouge-l is 29.14. meteor of ours-concat meteor is 21.78. rouge-1 of lead rouge-1 is 33.66. rouge-2 of lead rouge-2 is 8.94. rouge-l of lead rouge-l is 22.19. meteor of lead meteor is 16.45. rouge-1 of oracle rouge-1 is 53.88. rouge-2 of oracle rouge-2 is 23.05. rouge-l of oracle rouge-l is 34.9. meteor of oracle meteor is 24.11.
table 1 shows results on the combined cnn/dailymail test set. r1 of lead-3 r1 is 40. r2 of lead-3  r2 is 17.5. rl of lead-3  rl is 36.2. r1 of summarunner r1 is 39.6. r2 of summarunner  r2 is 16.2. rl of summarunner  rl is 35.3. r1 of dqn r1 is 39.4. r2 of dqn  r2 is 16.1. rl of dqn  rl is 35.6. r1 of refresh r1 is 40. r2 of refresh  r2 is 18.2. rl of refresh  rl is 36.6. r1 of rnes r1 is 41.3. r2 of rnes  r2 is 18.9. rl of rnes  rl is 37.6. r1 of banditsum r1 is 41.5. r2 of banditsum  r2 is 18.7. rl of banditsum  rl is 37.6. r1 of her r1 is 42.3. r2 of her  r2 is 18.9. rl of her  rl is 37.9.
table 3 shows the results of ablation test on the test split of the combined cnn/dailymail dataset. r1 of her r1 is 42.3. r2 of her  r2 is 18.9. rl of her  rl is 37.9. r1 of her-3 r1 is 42. r2 of her-3  r2 is 18.5. rl of her-3  rl is 37.6. r1 of her-3 w/o policy r1 is 41.7. r2 of her-3 w/o policy  r2 is 18.3. rl of her-3 w/o policy  rl is 37.1. r1 of her-3 w/o policy&l r1 is 41.2. r2 of her-3 w/o policy&l  r2 is 18.4. rl of her-3 w/o policy&l  rl is 37. r1 of her-3 w/o policy&f r1 is 40.6. r2 of her-3 w/o policy&f  r2 is 18.2. rl of her-3 w/o policy&f  rl is 36.9.
table 1 shows rouge scores on the english evaluation sets of both gigaword and duc2004. r-1 of abs (rush et al. 2015) r-1 is 29.55. r-2 of abs (rush et al. 2015) r-2 is 11.32. r-l of abs (rush et al. 2015) r-l is 26.42. r-1 of abs (rush et al. 2015) r-1 is 26.55. r-2 of abs (rush et al. 2015) r-2 is 7.06. r-l of abs (rush et al. 2015) r-l is 22.05. r-1 of abs+ (rush et al. 2015) r-1 is 29.76. r-2 of abs+ (rush et al. 2015) r-2 is 11.88. r-l of abs+ (rush et al. 2015) r-l is 26.96. r-1 of abs+ (rush et al. 2015) r-1 is 28.18. r-2 of abs+ (rush et al. 2015) r-2 is 8.49. r-l of abs+ (rush et al. 2015) r-l is 23.81. r-1 of ras-elman (chopra et al. 2016) r-1 is 33.78. r-2 of ras-elman (chopra et al. 2016) r-2 is 15.97. r-l of ras-elman (chopra et al. 2016) r-l is 31.15. r-1 of ras-elman (chopra et al. 2016) r-1 is 28.97. r-2 of ras-elman (chopra et al. 2016) r-2 is 8.26. r-l of ras-elman (chopra et al. 2016) r-l is 24.06. r-1 of words-lvt5k-1sent (nallapati et al. 2016) r-1 is 35.3. r-2 of words-lvt5k-1sent (nallapati et al. 2016) r-2 is 16.64. r-l of words-lvt5k-1sent (nallapati et al. 2016) r-l is 32.62. r-1 of words-lvt5k-1sent (nallapati et al. 2016) r-1 is 28.61. r-2 of words-lvt5k-1sent (nallapati et al. 2016) r-2 is 9.42. r-l of words-lvt5k-1sent (nallapati et al. 2016) r-l is 25.24. r-1 of seassbeam (zhou et al. 2017) r-1 is 36.15. r-2 of seassbeam (zhou et al. 2017) r-2 is 17.54. r-l of seassbeam (zhou et al. 2017) r-l is 33.63. r-1 of seassbeam (zhou et al. 2017) r-1 is 29.21. r-2 of seassbeam (zhou et al. 2017) r-2 is 9.56. r-l of seassbeam (zhou et al. 2017) r-l is 25.51. r-1 of rnnmrt (ayana et al. 2016) r-1 is 36.54. r-2 of rnnmrt (ayana et al. 2016) r-2 is 16.59. r-l of rnnmrt (ayana et al. 2016) r-l is 33.44. r-1 of rnnmrt (ayana et al. 2016) r-1 is 30.41. r-2 of rnnmrt (ayana et al. 2016) r-2 is 10.87. r-l of rnnmrt (ayana et al. 2016) r-l is 26.79. r-1 of actor-critic (li et al. 2018) r-1 is 36.05. r-2 of actor-critic (li et al. 2018) r-2 is 17.35. r-l of actor-critic (li et al. 2018) r-l is 33.49. r-1 of actor-critic (li et al. 2018) r-1 is 29.41. r-2 of actor-critic (li et al. 2018) r-2 is 9.84. r-l of actor-critic (li et al. 2018) r-l is 25.85. r-1 of structuredloss (edunov et al. 2018) r-1 is 36.7. r-2 of structuredloss (edunov et al. 2018) r-2 is 17.88. r-l of structuredloss (edunov et al. 2018) r-l is 34.29. r-1 of drgd (li et al. 2017) r-1 is 36.27. r-2 of drgd (li et al. 2017) r-2 is 17.57. r-l of drgd (li et al. 2017) r-l is 33.62. r-1 of drgd (li et al. 2017) r-1 is 31.79. r-2 of drgd (li et al. 2017) r-2 is 10.75. r-l of drgd (li et al. 2017) r-l is 27.48. r-1 of convs2s (gehring et al. 2017) r-1 is 35.88. r-2 of convs2s (gehring et al. 2017) r-2 is 17.48. r-l of convs2s (gehring et al. 2017) r-l is 33.29. r-1 of convs2s (gehring et al. 2017) r-1 is 30.44. r-2 of convs2s (gehring et al. 2017) r-2 is 10.84. r-l of convs2s (gehring et al. 2017) r-l is 26.9. r-1 of convs2sreinforcetopic (wang et al. 2018) r-1 is 36.92. r-2 of convs2sreinforcetopic (wang et al. 2018) r-2 is 18.29. r-l of convs2sreinforcetopic (wang et al. 2018) r-l is 34.58. r-1 of convs2sreinforcetopic (wang et al. 2018) r-1 is 31.15. r-2 of convs2sreinforcetopic (wang et al. 2018) r-2 is 10.85. r-l of convs2sreinforcetopic (wang et al. 2018) r-l is 27.68. r-1 of factaware (cao et al. 2018) r-1 is 37.27. r-2 of factaware (cao et al. 2018) r-2 is 17.65. r-l of factaware (cao et al. 2018) r-l is 34.24. r-1 of transformer r-1 is 37.87. r-2 of transformer r-2 is 18.69. r-l of transformer r-l is 35.22. r-1 of transformer r-1 is 31.38. r-2 of transformer r-2 is 10.89. r-l of transformer r-l is 27.18. r-1 of transformer+contrastiveattention r-1 is 38.72. r-2 of transformer+contrastiveattention r-2 is 19.09. r-l of transformer+contrastiveattention r-l is 35.82. r-1 of transformer+contrastiveattention r-1 is 32.22. r-2 of transformer+contrastiveattention r-2 is 11.04. r-l of transformer+contrastiveattention r-l is 27.59.
table 2 shows the full-length f-1 based rouge scores on the chinese evaluation set of lcsts. r-1 of rnn context (hu et al., 2015)  r-1 is 29.9. r-2 of rnn context (hu et al., 2015)  r-2 is 17.4. r-l of rnn context (hu et al., 2015)  r-l is 27.2. r-1 of copynet (gu et al., 2016)  r-1 is 34.4. r-2 of copynet (gu et al., 2016)  r-2 is 21.6. r-l of copynet (gu et al., 2016)  r-l is 31.3. r-1 of rnnmrt (ayana et al., 2016)  r-1 is 38.2. r-2 of rnnmrt (ayana et al., 2016)  r-2 is 25.2. r-l of rnnmrt (ayana et al., 2016)  r-l is 35.4. r-1 of rnndistraction (chen et al., 2016)  r-1 is 35.2. r-2 of rnndistraction (chen et al., 2016)  r-2 is 22.6. r-l of rnndistraction (chen et al., 2016)  r-l is 32.5. r-1 of drgd (li et al., 2017)  r-1 is 36.99. r-2 of drgd (li et al., 2017)  r-2 is 24.15. r-l of drgd (li et al., 2017)  r-l is 34.21. r-1 of actor-critic (li et al., 2018)  r-1 is 37.51. r-2 of actor-critic (li et al., 2018)  r-2 is 24.68. r-l of actor-critic (li et al., 2018)  r-l is 35.02. r-1 of global (lin et al., 2018)  r-1 is 39.4. r-2 of global (lin et al., 2018)  r-2 is 26.9. r-l of global (lin et al., 2018)  r-l is 36.5. r-1 of transformer  r-1 is 41.93. r-2 of transformer  r-2 is 28.28. r-l of transformer  r-l is 38.32. r-1 of transformer+contrastiveattention  r-1 is 44.35. r-2 of transformer+contrastiveattention  r-2 is 30.65. r-l of transformer+contrastiveattention  r-l is 40.58.
table 4 shows human evaluation on extractive summaries. avg. human rating of ours avg. human rating is 2.52. best% of ours best% is 70. avg. human rating of refresh avg. human rating is 2.27. best% of refresh best% is 33.3. avg. human rating of extabsrl avg. human rating is 1.66. best% of extabsrl best% is 6.7.
table 5 shows performance of extabsrl with different reward functions, measured in terms of rouge (center) and human judgements (right). r-1 of r-l (original) r-1 is 40.9. r-2 of r-l (original)  r-2 is 17.8. r-l of r-l (original)  r-l is 38.5. human of r-l (original)  human is 1.75. pref% of r-l (original)  pref% is 15. r-1 of learned (ours) r-1 is 39.2. r-2 of learned (ours)  r-2 is 17.4. r-l of learned (ours)  r-l is 37.5. human of learned (ours)  human is 2.2. pref% of learned (ours)  pref% is 75.
table 4 shows comparison of single-expert selector with state-of-the-art abstractive summarization methods on cnn-dm. r-1 of pg (see et al., 2017)  r-1 is 39.53. r-2 of pg (see et al., 2017)  r-2 is 17.28. r-l of pg (see et al., 2017)  r-l is 36.38. r-1 of bottom-up (gehrmann et al., 2018)  r-1 is 41.22. r-2 of bottom-up (gehrmann et al., 2018)  r-2 is 18.68. r-l of bottom-up (gehrmann et al., 2018)  r-l is 38.34. r-1 of dca (celikyilmaz et al., 2018)  r-1 is 41.69. r-2 of dca (celikyilmaz et al., 2018)  r-2 is 19.47. r-l of dca (celikyilmaz et al., 2018)  r-l is 37.92. r-1 of selector & 10-beam pg (ours)  r-1 is 41.72. r-2 of selector & 10-beam pg (ours)  r-2 is 18.74. r-l of selector & 10-beam pg (ours)  r-l is 38.79.
table 5 shows training time: comparison of training time on cnn-dm. training time (ms. / step) of pg  training time (ms. / step) is 641.2. training time (ms. / step) of 3-m. decoder  training time (ms. / step) is  1804.1 ( 2.81). training time (ms. / step) of 5-m. decoder  training time (ms. / step) is  2367.6 ( 4.37). training time (ms. / step) of selector (ours)  training time (ms. / step) is  692.1 ( 1.08). training time (ms. / step) of 3-m. selector (ours)  training time (ms. / step) is  740.8 ( 1.16). training time (ms. / step) of 5-m. selector (ours)  training time (ms. / step) is  747.6 ( 1.17).
table 4 shows evaluation for seen and unseen entities. acc. of demonstrative acc. is 0.00%. support of demonstrative  support is 22. acc. of description acc. is 48.72%. support of description  support is 862. acc. of name acc. is 79.11%. support of name  support is 2547. acc. of pronoun acc. is 90.00%. support of pronoun  support is 160. acc. of total acc. is 71.82%. support of total  support is 3591. acc. of demonstrative acc. is 0.00%. support of demonstrative  support is 3. acc. of description acc. is 20.54%. support of description  support is 409. acc. of name acc. is 74.74%. support of name  support is 2423. acc. of pronoun acc. is 88.33%. support of pronoun  support is 120. acc. of total acc. is 67.72%. support of total  support is 2955.
table 3 shows performances on twitter dataset. bleu2 of seq2seq-bs bleu2 is 32.69. bleu3 of seq2seq-bs bleu3 is 28.25. bleu4 of seq2seq-bs bleu4 is 24.99. meteor of seq2seq-bs meteor is 22.51. self-bleu2 of seq2seq-bs self-bleu2 is 82.79. self-bleu3 of seq2seq-bs self-bleu3 is 80.29. self-bleu4 of seq2seq-bs self-bleu4 is 77.98. bleu2 of vae-svg-eq bleu2 is 26.43. bleu3 of vae-svg-eq bleu3 is 23.04. bleu4 of vae-svg-eq bleu4 is 20.57. meteor of vae-svg-eq meteor is 18.34. self-bleu2 of vae-svg-eq self-bleu2 is 91.46. self-bleu3 of vae-svg-eq self-bleu3 is 90.51. self-bleu4 of vae-svg-eq self-bleu4 is 89.67. bleu2 of gan bleu2 is 23.1. bleu3 of gan bleu3 is 20.45. bleu4 of gan bleu4 is 18.47. meteor of gan meteor is 15.33. self-bleu2 of gan self-bleu2 is 94.35. self-bleu3 of gan self-bleu3 is 93.75. self-bleu4 of gan self-bleu4 is 93.22. bleu2 of dp-gan bleu2 is 33.07. bleu3 of dp-gan bleu3 is 28.68. bleu4 of dp-gan bleu4 is 25.49. meteor of dp-gan meteor is 22.84. self-bleu2 of dp-gan self-bleu2 is 82.52. self-bleu3 of dp-gan self-bleu3 is 80.05. self-bleu4 of dp-gan self-bleu4 is 77.63. bleu2 of irl bleu2 is 32.96. bleu3 of irl bleu3 is 28.6. bleu4 of irl bleu4 is 25.39. meteor of irl meteor is 22.72. self-bleu2 of irl self-bleu2 is 83.53. self-bleu3 of irl self-bleu3 is 81.22. self-bleu4 of irl self-bleu4 is 78.95. bleu2 of d-page bleu2 is 32.95. bleu3 of d-page bleu3 is 28.82. bleu4 of d-page bleu4 is 25.88. meteor of d-page meteor is 22.59. self-bleu2 of d-page self-bleu2 is 88.35. self-bleu3 of d-page self-bleu3 is 86.45. self-bleu4 of d-page self-bleu4 is 84.76. bleu2 of pg-bs bleu2 is 33.86. bleu3 of pg-bs bleu3 is 29.42. bleu4 of pg-bs bleu4 is 26.15. meteor of pg-bs meteor is 23.52. self-bleu2 of pg-bs self-bleu2 is 82.57. self-bleu3 of pg-bs self-bleu3 is 79.99. self-bleu4 of pg-bs self-bleu4 is 77.48. bleu2 of ours bleu2 is 34.23. bleu3 of ours bleu3 is 29.66. bleu4 of ours bleu4 is 26.38. meteor of ours meteor is 24.29. self-bleu2 of ours self-bleu2 is 65.83. self-bleu3 of ours self-bleu3 is 61.17. self-bleu4 of ours self-bleu4 is 57.45.
table 4 shows human evaluation results. fluency of d-page fluency is 4.21. consistency of d-page consistency is 3.44. fluency of d-page fluency is 3.66. consistency of d-page  consistency is 3.08. fluency of pg-bs fluency is 4.2. consistency of pg-bs consistency is 3.34. fluency of pg-bs fluency is 3.85. consistency of pg-bs  consistency is 3.17. fluency of dp-gan fluency is 4.27. consistency of dp-gan consistency is 3.49. fluency of dp-gan fluency is 4.09. consistency of dp-gan  consistency is 3.3. fluency of ours fluency is 4.57. consistency of ours consistency is 3.82. fluency of ours fluency is 4.24. consistency of ours  consistency is 3.59.
table 1 shows comparison of human evaluation results with their consistency. accuracy of baseline hg-kg is 56.80%. accuracy of baseline  hg-cc is 37.60%. accuracy of baseline kg-cc is 37.60%. accuracy of baseline 3 outputs is 30.00%. accuracy of proposed hg-kg is 58.80%. accuracy of proposed  hg-cc is 39.60%. accuracy of proposed kg-cc is 39.20%. accuracy of proposed 3 outputs is 32.40%. accuracy of gold hg-kg is 65.20%. accuracy of gold  hg-cc is 44.40%. accuracy of gold kg-cc is 48.80%. accuracy of gold 3 outputs is 35.20%.
table 2 shows comparison of human evaluation results for headlines. adequacy of baseline adequacy is 3.34. fluency of baseline  fluency is 3.69. occupation adequacy of baseline  occupation adequacy is 3.45. adequacy of proposed adequacy is 3.76. fluency of proposed  fluency is 3.86. occupation adequacy of proposed  occupation adequacy is 3.89. adequacy of gold adequacy is 4.09. fluency of gold  fluency is 4.12. occupation adequacy of gold  occupation adequacy is 4.13.
table 3 shows automatic evaluation results based on the rouge metrics and accuracy (%) of classification of job advertisement dataset. r-1 of baseline (pointer-generator network) r-1 is 25.1. r-2 of baseline (pointer-generator network) r-2 is 5.3. r-l of baseline (pointer-generator network) r-l is 21.1. r-1 of baseline (pointer-generator network) r-1 is 30.9. r-2 of baseline (pointer-generator network) r-2 is 10.6. r-l of baseline (pointer-generator network) r-l is 28.7. accuracy of baseline (pointer-generator network) accuracy is 62.8. r-1 of multi-task learning (mtl) r-1 is 26.2. r-2 of multi-task learning (mtl) r-2 is 5.8. r-l of multi-task learning (mtl) r-l is 21.6. r-1 of multi-task learning (mtl) r-1 is 32.3. r-2 of multi-task learning (mtl) r-2 is 10.9. r-l of multi-task learning (mtl) r-l is 30. accuracy of multi-task learning (mtl) accuracy is 64.1. r-1 of mtl + scheduling (sd) r-1 is 26.3. r-2 of mtl + scheduling (sd) r-2 is 6. r-l of mtl + scheduling (sd) r-l is 21.8. r-1 of mtl + scheduling (sd) r-1 is 32.3. r-2 of mtl + scheduling (sd) r-2 is 10.4. r-l of mtl + scheduling (sd) r-l is 29.9. accuracy of mtl + scheduling (sd) accuracy is 63.9. r-1 of proposed (mtl + sd + hierarchical consistency loss (hcl)) r-1 is  *26.9. r-2 of proposed (mtl + sd + hierarchical consistency loss (hcl)) r-2 is  *6.1. r-l of proposed (mtl + sd + hierarchical consistency loss (hcl)) r-l is  *22.4. r-1 of proposed (mtl + sd + hierarchical consistency loss (hcl)) r-1 is  *32.8. r-2 of proposed (mtl + sd + hierarchical consistency loss (hcl)) r-2 is  *11.2. r-l of proposed (mtl + sd + hierarchical consistency loss (hcl)) r-l is  *30.5. accuracy of proposed (mtl + sd + hierarchical consistency loss (hcl)) accuracy is  *64.4. r-1 of lead-1 r-1 is 19. r-2 of lead-1 r-2 is 4.3. r-l of lead-1 r-l is 13.5. r-1 of lead-1 r-1 is  -.
table 4 shows automatic evaluation results based on the rouge metrics and accuracy (%) of classification of the cnn and dailymail datasets. r-1 of baseline r-1 is 30.7. r-2 of baseline r-2 is 10.6. r-l of baseline r-l is 27.3. r-1 of baseline r-1 is 19.5. r-2 of baseline r-2 is 5. r-l of baseline r-l is 17. accuracy of baseline accuracy is 43.8. r-1 of proposed (mtl + sd + hcl) r-1 is  *31.0. r-2 of proposed (mtl + sd + hcl) r-2 is  *10.9. r-l of proposed (mtl + sd + hcl) r-l is  *27.8. r-1 of proposed (mtl + sd + hcl) r-1 is 19.6. r-2 of proposed (mtl + sd + hcl) r-2 is 5. r-l of proposed (mtl + sd + hcl) r-l is 17.1. accuracy of proposed (mtl + sd + hcl) accuracy is 43.9. r-1 of lead r-1 is 33.4. r-2 of lead r-2 is 12.2. r-l of lead r-l is 26.1. r-1 of lead r-1 is 17.2. r-2 of lead r-2 is 5. r-l of lead r-l is 11.1. accuracy of lead accuracy is  -. r-1 of baseline r-1 is 38.4. r-2 of baseline r-2 is 15.8. r-l of baseline r-l is 35. r-1 of baseline r-1 is 43.1. r-2 of baseline r-2 is 25.3. r-l of baseline r-l is 39.6. accuracy of baseline accuracy is 89. r-1 of proposed (mtl + sd + hcl) r-1 is  *38.9. r-2 of proposed (mtl + sd + hcl) r-2 is  *16.3. r-l of proposed (mtl + sd + hcl) r-l is  *35.4. r-1 of proposed (mtl + sd + hcl) r-1 is  *43.7. r-2 of proposed (mtl + sd + hcl) r-2 is 25.5. r-l of proposed (mtl + sd + hcl) r-l is  *40.1. accuracy of proposed (mtl + sd + hcl) accuracy is 89.8. r-1 of lead r-1 is 43.8. r-2 of lead r-2 is 19.2. r-l of lead r-l is 37.3. r-1 of lead r-1 is 27.7. r-2 of lead r-2 is 10.9. r-l of lead r-l is 21.7. accuracy of lead accuracy is  -.
table 2 shows generation performance (numbers in brackets correspond to the relaxed measures). recall of retrieval-based recall is  0.23 (0.25). precision of retrieval-based precision is  0.61 (0.67). f-measure of retrieval-based f-measure is  0.34 (0.37). recall of retrieval-based recall is  0.28 (0.30). precision of retrieval-based precision is  0.72 (0.78). f-measure of retrieval-based f-measure is  0.41 (0.44). recall of seq2seq recall is  0.06 (0.07). precision of seq2seq precision is  0.07 (0.08). f-measure of seq2seq f-measure is  0.07 (0.08). recall of seq2seq recall is  0.10 (0.13). precision of seq2seq precision is  0.11 (0.13). f-measure of seq2seq f-measure is  0.10 (0.13). recall of case frame-based recall is  0.10 (0.10). precision of case frame-based precision is  0.62 (0.62). f-measure of case frame-based f-measure is  0.16 (0.16). recall of case frame-based recall is  0.05 (0.05). precision of case frame-based precision is  0.75 (0.75). f-measure of case frame-based f-measure is  0.09 (0.09).
table 4 shows the main experimental results for our model and several baselines. b1 of s2s (du et al., 2017) b1 is 43.09. b2 of s2s (du et al., 2017) b2 is 25.96. b3 of s2s (du et al., 2017) b3 is 17.5. b4 of s2s (du et al., 2017) b4 is 12.28. met of s2s (du et al., 2017) met is 16.62. r-l of s2s (du et al., 2017) r-l is 39.75. b2 of s2s (du et al., 2017) b2 is  -. b3 of s2s (du et al., 2017) b3 is  -. b4 of s2s (du et al., 2017) b4 is  -. met of s2s (du et al., 2017) met is  -. r-l of s2s (du et al., 2017) r-l is  -. b3 of nqg++ (zhou et al., 2017) b3 is  -. b4 of nqg++ (zhou et al., 2017) b4 is  -. met of nqg++ (zhou et al., 2017) met is  -. b3 of nqg++ (zhou et al., 2017) b3 is  -. b4 of nqg++ (zhou et al., 2017) b4 is 13.29. met of nqg++ (zhou et al., 2017) met is  -. r-l of nqg++ (zhou et al., 2017) r-l is  -. b3 of m2s+cp (song et al., 2018) b3 is  -. b4 of m2s+cp (song et al., 2018) b4 is 13.98. met of m2s+cp (song et al., 2018) met is 18.77. r-l of m2s+cp (song et al., 2018) r-l is 42.72. b3 of m2s+cp (song et al., 2018) b3 is  -. b4 of m2s+cp (song et al., 2018) b4 is 13.91. met of m2s+cp (song et al., 2018) met is  -. r-l of m2s+cp (song et al., 2018) r-l is  -. b1 of s2s+mp+gsa (zhao et al., 2018) b1 is 43.47. b2 of s2s+mp+gsa (zhao et al., 2018) b2 is 28.23. b3 of s2s+mp+gsa (zhao et al., 2018) b3 is 20.4. b4 of s2s+mp+gsa (zhao et al., 2018) b4 is 15.32. met of s2s+mp+gsa (zhao et al., 2018) met is 19.29. r-l of s2s+mp+gsa (zhao et al., 2018) r-l is 43.91. b1 of s2s+mp+gsa (zhao et al., 2018) b1 is 44.51. b2 of s2s+mp+gsa (zhao et al., 2018) b2 is 29.07. b3 of s2s+mp+gsa (zhao et al., 2018) b3 is 21.06. b4 of s2s+mp+gsa (zhao et al., 2018) b4 is 15.82. met of s2s+mp+gsa (zhao et al., 2018) met is 19.67. r-l of s2s+mp+gsa (zhao et al., 2018) r-l is 44.24. b3 of hybrid model (sun et al., 2018) b3 is  -. b4 of hybrid model (sun et al., 2018) b4 is  -. met of hybrid model (sun et al., 2018) met is  -. b1 of hybrid model (sun et al., 2018) b1 is 43.02. b2 of hybrid model (sun et al., 2018) b2 is 28.14. b3 of hybrid model (sun et al., 2018) b3 is 20.51. b4 of hybrid model (sun et al., 2018) b4 is 15.64. met of hybrid model (sun et al., 2018) met is  -. r-l of hybrid model (sun et al., 2018) r-l is  -. b3 of ass2s (kim et al., 2019) b3 is  -. b4 of ass2s (kim et al., 2019) b4 is 16.2. met of ass2s (kim et al., 2019) met is 19.92. r-l of ass2s (kim et al., 2019) r-l is 43.96. b3 of ass2s (kim et al., 2019) b3 is  -. b4 of ass2s (kim et al., 2019) b4 is 16.17. met of ass2s (kim et al., 2019) met is  -. r-l of ass2s (kim et al., 2019) r-l is  -. b1 of our model b1 is 45.66. b2 of our model b2 is 30.21. b3 of our model b3 is 21.82. b4 of our model b4 is 16.27. met of our model met is 20.36. r-l of our model r-l is 44.35. b1 of our model b1 is 44.4. b2 of our model b2 is 29.48. b3 of our model b3 is 21.54. b4 of our model b4 is 16.37. met of our model met is 20.68. r-l of our model r-l is 44.73.
table 5 shows automatic evaluation for recipe text generation. bleu (%) of checklist § bleu (%) is 3. coverage (%) of checklist § coverage (%) is 67.9. bleu (%) of checklist bleu (%) is 2.6**. coverage (%) of checklist coverage (%) is 66.9*. length of checklist length is 67.59. distinct-4 (%) of checklist distinct-4 (%) is 30.67**. repetition-4 (%) of checklist repetition-4 (%) is 39.1**. bleu (%) of cvae bleu (%) is 4.6. coverage (%) of cvae coverage (%) is 63.0**. length of cvae length is 57.49**. distinct-4 (%) of cvae distinct-4 (%) is 52.53**. repetition-4 (%) of cvae repetition-4 (%) is 38.7**. bleu (%) of pointer-s2s bleu (%) is 4.3. coverage (%) of pointer-s2s coverage (%) is 70.4**. length of pointer-s2s length is 59.18**. distinct-4 (%) of pointer-s2s distinct-4 (%) is 30.72**. repetition-4 (%) of pointer-s2s repetition-4 (%) is 36.4**. bleu (%) of link-s2s bleu (%) is 1.9**. coverage (%) of link-s2s coverage (%) is 53.8**. length of link-s2s length is 40.34**. distinct-4 (%) of link-s2s distinct-4 (%) is 24.93**. repetition-4 (%) of link-s2s repetition-4 (%) is 31.6**. bleu (%) of phvm (ours) bleu (%) is 4.6. coverage (%) of phvm (ours) coverage (%) is 73.2. length of phvm (ours) length is 70.92. distinct-4 (%) of phvm (ours) distinct-4 (%) is 67.86. repetition-4 (%) of phvm (ours) repetition-4 (%) is 17.3.
table 5 shows experimental results on the nyt50 dataset. r-1 of lead r-1 is 41.8. r-2 of lead r-2 is 22.6. r-l of lead r-l is 35. r-1 of leaddedup r-1 is 42. r-2 of leaddedup r-2 is 22.8. r-l of leaddedup r-l is 35. r-1 of leadcomp r-1 is 42.4. r-2 of leadcomp r-2 is 22.7. r-l of leadcomp r-l is 35.4. r-1 of extraction r-1 is 44.3. r-2 of extraction r-2 is 25.5. r-l of extraction r-l is 37.1. r-1 of jecs r-1 is 45.5. r-2 of jecs r-2 is 25.3. r-l of jecs r-l is 38.2.
table 2 shows translation performance on iwslt datasets. bleu of indiv en-zh is 15.68. bleu of indiv en-ja is 16.56. bleu of indiv en-de is 27.11. bleu of indiv en-fr is 40.62. bleu of indiv + pseudo en-zh is 16.72. bleu of indiv + pseudo en-ja is 18.02. bleu of indiv + pseudo en-de is 28.47. bleu of indiv + pseudo en-fr is 40.39. bleu of multi en-zh is 17.06. bleu of multi en-ja is 18.31. bleu of multi en-de is 27.79. bleu of multi en-fr is 40.97. bleu of multi + pseudo en-zh is 17.10. bleu of multi + pseudo en-ja is 18.40. bleu of multi + pseudo en-de is 28.56. bleu of multi + pseudo en-fr is 40.62. bleu of synctrans en-zh is 17.97. bleu of synctrans en-ja is 19.31. bleu of synctrans en-de is 29.16. bleu of synctrans en-fr is 41.53.
table 3 shows experimental results for robustness analysis. mrr of k = 1 mrr is 20.8. hits@1 of k = 1 hits@1 is 16.9. mrr of k = 1 mrr is 22.3. hits@1 of k = 1 hits@1 is 19.3. mrr of k = 5 mrr is 25.7. hits@1 of k = 5 hits@1 is 20.8. mrr of k = 5 mrr is 29.6. hits@1 of k = 5 hits@1 is 26.6. mrr of k = 10 mrr is 29.1. hits@1 of k = 10 hits@1 is 25.0. mrr of k = 10 mrr is 31.3. hits@1 of k = 10 hits@1 is 27.2. mrr of k = max mrr is 42.7. hits@1 of k = max hits@1 is 36.7. mrr of k = max mrr is 46.9. hits@1 of k = max hits@1 is 41.2.
table 2 shows human evaluation results. ambiguity of lm (mikolov et al., 2010) ambiguity is 1.6. fluency of lm (mikolov et al., 2010) fluency is 3.1. overall of lm (mikolov et al., 2010) overall is 2.5. ambiguity of clm (mou et al., 2015) ambiguity is 2.0. fluency of clm (mou et al., 2015) fluency is 2.1. overall of clm (mou et al., 2015) overall is 2.0. ambiguity of clm+jd (yu et al., 2018) ambiguity is 3.4. fluency of clm+jd (yu et al., 2018) fluency is 3.6. overall of clm+jd (yu et al., 2018) overall is 3.5. ambiguity of pun-gan ambiguity is 3.9. fluency of pun-gan fluency is 3.7. overall of pun-gan overall is 3.8. ambiguity of human ambiguity is 4.3. fluency of human fluency is 4.6. overall of human overall is 4.5.
table 2 shows experiment results on semeval 2014 dataset. accuracy of at-lstm overall is 77.13%. accuracy of at-lstm conflict is 11.54%. accuracy of atae-lstm overall is 78.00%. accuracy of atae-lstm conflict is 23.08%. accuracy of gcae overall is 78.30%. accuracy of gcae conflict is 25.00%. accuracy of at-gru overall is 77.22%. accuracy of at-gru conflict is 19.23%. accuracy of at-gru 2-label overall is 77.02%. accuracy of at-gru 2-label conflict is 26.92%. accuracy of d-at-gru w/o orthogonal overall is 77.22%. accuracy of d-at-gru w/o orthogonal conflict is 26.92%. accuracy of d-at-gru overall is 78.50%. accuracy of d-at-gru conflict is 40.38%.
table 2 shows text classification datasets. accuracy of cnn r8 is 94.0±0.5. accuracy of cnn r52 is 85.3±0.5. accuracy of cnn ohsumed is 43.9±1.0. accuracy of lstm r8 is 93.7±0.8. accuracy of lstm r52 is 85.6±1.0. accuracy of lstm ohsumed is 41.1±1.0. accuracy of graph-cnn r8 is 97.0±0.2. accuracy of graph-cnn r52 is 92.8±0.2. accuracy of graph-cnn ohsumed is 63.9±0.5. accuracy of text-gcn r8 is 97.1±0.1. accuracy of text-gcn r52 is 93.6±0.2. accuracy of text-gcn ohsumed is 68.4±0.6. accuracy of cnn* r8 is 95.7±0.5. accuracy of cnn* r52 is 87.6±0.5. accuracy of cnn* ohsumed is 58.4±1.0. accuracy of lstm* r8 is 96.1±0.2. accuracy of lstm* r52 is 90.5±0.8. accuracy of lstm* ohsumed is 51.1±1.5. accuracy of bi-lstm* r8 is 96.3±0.3. accuracy of bi-lstm* r52 is 90.5±0.9. accuracy of bi-lstm* ohsumed is 49.3±1.0. accuracy of fasttext* r8 is 96.1±0.2. accuracy of fasttext* r52 is 92.8±0.1. accuracy of fasttext* ohsumed is 57.7±0.5. accuracy of text-gcn* r8 is 97.0±0.1. accuracy of text-gcn* r52 is 93.7±0.1. accuracy of text-gcn* ohsumed is 67.7±0.3. accuracy of our model* r8 is 97.8±0.2. accuracy of our model* r52 is 94.6±0.3. accuracy of our model* ohsumed is 69.4±0.6.
table 1 shows perplexity and topic coherence results of difference models. ppl of lda ppl is 1,213.1. cv of lda  cv is 0.503. ppl of lda ppl is 1,042.7. cv of lda cv is 0.507. ppl of nvdm ppl is 980.8. cv of nvdm  cv is 0.497. ppl of nvdm ppl is 931.6. cv of nvdm cv is 0.492. ppl of ngtm ppl is 929.3. cv of ngtm  cv is 0.479. ppl of ngtm ppl is 938.9. cv of ngtm cv is 0.503. ppl of scholar ppl is 1,345.9. cv of scholar  cv is 0.537. ppl of scholar ppl is 1,350.9. cv of scholar cv is 0.512. ppl of lda ppl is 1,451.7. cv of lda  cv is 0.522. ppl of lda ppl is 1,093.1. cv of lda cv is 0.534. ppl of nvdm ppl is 845.8. cv of nvdm  cv is 0.510. ppl of nvdm ppl is 768.7. cv of nvdm cv is 0.509. ppl of ngtm ppl is 791.5. cv of ngtm  cv is 0.517. ppl of ngtm ppl is 757.2. cv of ngtm cv is 0.527. ppl of scholar ppl is 1,158.4. cv of scholar  cv is 0.560. ppl of scholar ppl is 1,273.6. cv of scholar cv is 0.548. ppl of vtmrl ppl is 803.7. cv of vtmrl  cv is 0.577. ppl of vtmrl ppl is 730.6. cv of vtmrl cv is 0.568. ppl of lda ppl is 1,015.9. cv of lda  cv is 0.501. ppl of lda ppl is 995.5. cv of lda cv is 0.503. ppl of nvdm ppl is 1,014.0. cv of nvdm  cv is 0.471. ppl of nvdm ppl is 927.6. cv of nvdm cv is 0.506. ppl of ngtm ppl is 903.5. cv of ngtm  cv is 0.491. ppl of ngtm ppl is 908.8. cv of ngtm cv is 0.498. ppl of scholar ppl is 1,514.5. cv of scholar  cv is 0.521. ppl of scholar ppl is 1,373.2. cv of scholar cv is 0.508. ppl of lda ppl is 1,251.6. cv of lda  cv is 0.518. ppl of lda ppl is 921.4. cv of lda cv is 0.527. ppl of nvdm ppl is 837.9. cv of nvdm  cv is 0.502. ppl of nvdm ppl is 767.0. cv of nvdm cv is 0.514. ppl of ngtm ppl is 772.2. cv of ngtm  cv is 0.514. ppl of ngtm ppl is 749.7. cv of ngtm cv is 0.511. ppl of scholar ppl is 1,335.9. cv of scholar  cv is 0.526. ppl of scholar ppl is 1,299.8. cv of scholar cv is 0.530. ppl of vtmrl ppl is 725.2. cv of vtmrl  cv is 0.559. ppl of vtmrl ppl is 712.2. cv of vtmrl cv is 0.566.
table 3 shows f1 scores (%) of ukb+syntagnet against the best supervised systems for english all-words wsd. f1 of lstmlp sens2 is 73.8. f1 of lstmlp sens3 is 71.8. f1 of lstmlp sem07 is 63.5. f1 of lstmlp sem13 is 69.5. f1 of lstmlp sem15 is 72.6. f1 of lstmlp all is 71.5. f1 of imsc2v+pr sens2 is 73.8. f1 of imsc2v+pr sens3 is 71.9. f1 of imsc2v+pr sem07 is 63.3. f1 of imsc2v+pr sem13 is 68.2. f1 of imsc2v+pr sem15 is 72.8. f1 of imsc2v+pr all is 71.2. f1 of fastsense sens2 is 73.5. f1 of fastsense sens3 is 73.5. f1 of fastsense sem07 is 62.4. f1 of fastsense sem13 is 66.2. f1 of fastsense sem15 is 73.2. f1 of fastsense all is 71.1. f1 of ukb+syntagnet sens2 is 71.2. f1 of ukb+syntagnet sens3 is 71.6. f1 of ukb+syntagnet sem07 is 59.6. f1 of ukb+syntagnet sem13 is 72.4. f1 of ukb+syntagnet sem15 is 75.6. f1 of ukb+syntagnet all is 71.5.
table 2 shows performance on different datasets against baselines, where h@k denotes hits at k. h@1 of simple h@1 is 0.160. h@3 of simple h@3 is 0.268. h@10 of simple h@10 is 0.43. mrr of simple mrr is 0.248. h@1 of distmult h@1 is 0.158. h@3 of distmult h@3 is 0.271. h@10 of distmult h@10 is 0.432. mrr of distmult mrr is 0.247. h@1 of complex h@1 is 0.159. h@3 of complex h@3 is 0.275. h@10 of complex h@10 is 0.441. mrr of complex mrr is 0.25. h@1 of jobi simple h@1 is 0.188. h@3 of jobi simple h@3 is 0.301. h@10 of jobi simple h@10 is 0.461. mrr of jobi simple mrr is 0.277. h@1 of jobi distmult h@1 is 0.205. h@3 of jobi distmult h@3 is 0.316. h@10 of jobi distmult h@10 is 0.466. mrr of jobi distmult mrr is 0.29. h@1 of jobi complex h@1 is 0.199. h@3 of jobi complex h@3 is 0.319. h@10 of jobi complex h@10 is 0.479. mrr of jobi complex mrr is 0.29. h@1 of distmult h@1 is 0.587. h@3 of distmult h@3 is 0.785. h@10 of distmult h@10 is 0.867. mrr of distmult mrr is 0.697. h@1 of complex h@1 is 0.617. h@3 of complex h@3 is 0.803. h@10 of complex h@10 is 0.874. mrr of complex mrr is 0.72. h@1 of jobi complex h@1 is 0.681. h@3 of jobi complex h@3 is 0.824. h@10 of jobi complex h@10 is 0.883. mrr of jobi complex mrr is 0.761. h@1 of distmult h@1 is 0.252. h@3 of distmult h@3 is 0.407. h@10 of distmult h@10 is 0.568. mrr of distmult mrr is 0.357. h@1 of complex h@1 is 0.277. h@3 of complex h@3 is 0.44. h@10 of complex h@10 is 0.589. mrr of complex mrr is 0.383. h@1 of jobi complex h@1 is 0.333. h@3 of jobi complex h@3 is 0.477. h@10 of jobi complex h@10 is 0.617. mrr of jobi complex mrr is 0.428.
table 6 shows results of ablation study on complex model. h@1 of baseline h@1 is 0.277. h@3 of baseline h@3 is 0.44. h@10 of baseline h@10 is 0.589. mrr of baseline mrr is 0.383. h@1 of biasedneg h@1 is 0.276. h@3 of biasedneg h@3 is 0.427. h@10 of biasedneg h@10 is 0.568. mrr of biasedneg mrr is 0.375. h@1 of joint h@1 is 0.287. h@3 of joint h@3 is 0.447. h@10 of joint h@10 is 0.601. mrr of joint mrr is 0.392. h@1 of jobi h@1 is 0.333. h@3 of jobi h@3 is 0.477. h@10 of jobi h@10 is 0.617. mrr of jobi mrr is 0.428.
table 2 shows results on ptb test with encoder pretraining. recon# of ae recon# is 70.36. au of ae au is 32. ppl# of vae ppl# is 101.39. recon# of vae recon# is 101.27. au of vae au is 0. kl of vae kl is 0.00. -elbo of vae -elbo is 101.27. ppl# of + pretrain ppl# is 102.26. recon# of + pretrain recon# is 101.46. au of + pretrain au is 0. kl of + pretrain kl is 0.00. -elbo of + pretrain -elbo is 101.46. ppl# of + pretrain + anneal ppl# is 97.74. recon# of + pretrain + anneal recon# is 99.67. au of + pretrain + anneal au is 2. kl of + pretrain + anneal kl is 1.01. -elbo of + pretrain + anneal -elbo is 100.68.
table 3 shows comparison of methods on pun of the day dataset. accuracy of word2vec+hcf accuracy is 0.797. precision of word2vec+hcf precision is 0.776. recall of word2vec+hcf recall is 0.836. f1 of word2vec+hcf f1 is 0.705. accuracy of cnn accuracy is 0.867. precision of cnn precision is 0.88. recall of cnn recall is 0.859. f1 of cnn f1 is 0.869. accuracy of cnn+f accuracy is 0.892. precision of cnn+f precision is 0.886. recall of cnn+f recall is 0.907. f1 of cnn+f f1 is 0.896. accuracy of cnn+hn accuracy is 0.892. precision of cnn+hn precision is 0.889. recall of cnn+hn recall is 0.903. f1 of cnn+hn f1 is 0.896. accuracy of cnn+f+hn accuracy is 0.894. precision of cnn+f+hn precision is 0.866. recall of cnn+f+hn recall is 0.94. f1 of cnn+f+hn f1 is 0.901. accuracy of transformer accuracy is 0.93. precision of transformer precision is 0.93. recall of transformer recall is 0.931. f1 of transformer f1 is 0.931.
table 1 shows macro-averaged f1 comparison of per-language models and multilingual models over 48 languages. part-of-speech f1 of no part-of-speech f1 is 94.5. morphology f1 of no  morphology f1 is 92.5. part-of-speech f1 of no part-of-speech f1 is 95.1. morphology f1 of no  morphology f1 is 93. part-of-speech f1 of yes part-of-speech f1 is 91.1. morphology f1 of yes  morphology f1 is 82.9. part-of-speech f1 of yes part-of-speech f1 is 94.5. morphology f1 of yes  morphology f1 is 91.
table 3 shows unlabeled unsupervised parsing f1 on wsj40. unlabeled f1 of right branching unlabeled f1 is 40.7. unlabeled f1 of ydiora unlabeled f1 is 60.6. unlabeled f1 of zprpn unlabeled f1 is 52.4. unlabeled f1 of zpalm-u unlabeled f1 is 42.0.
table 3 shows performance comparison between lm finetuning on target domain unlabeled data of the same size as each test set, “controlled unlabeled data (cu),” and transductive lm fine-tuning on each test set (t). f1 of bc cu is 90.4. f1 of bc t is 90.8. f1 of bc cu is 78.6. f1 of bc t is 79.3. f1 of bn cu is 91.1. f1 of bn t is 91.6. f1 of bn cu is 79.8. f1 of bn t is 80.4. f1 of mz cu is 90. f1 of mz t is 90.4. f1 of mz cu is 77.9. f1 of mz t is 78.5. f1 of nw cu is 92.1. f1 of nw t is 92.3. f1 of nw cu is 81.1. f1 of nw t is 81.7. f1 of pt cu is 87.1. f1 of pt t is 87.3. f1 of pt cu is 73.5. f1 of pt t is 74. f1 of tc cu is 87.1. f1 of tc t is 87.6. f1 of tc cu is 71.3. f1 of tc t is 71.6. f1 of wb cu is 91.8. f1 of wb t is 92. f1 of wb cu is 76.6. f1 of wb t is 77.1.
table 4 shows performance comparison between lm finetuning on target domain unlabeled data (u) and on the combination of the unlabeled data and test sets (u + t). f1 of bc u is 90.5. f1 of bc u + t is 91.0. f1 of bc u is 79.0. f1 of bc u + t is 79.4. f1 of bn u is 91.3. f1 of bn u + t is 91.6. f1 of bn u is 80.1. f1 of bn u + t is 80.6. f1 of mz u is 90.2. f1 of mz u + t is 90.6. f1 of mz u is 78.3. f1 of mz u + t is 78.7. f1 of nw u is 92.1. f1 of nw u + t is 92.5. f1 of nw u is 81.5. f1 of nw u + t is 81.9. f1 of pt u is 87.3. f1 of pt u + t is 87.7. f1 of pt u is 73.6. f1 of pt u + t is 74.3. f1 of tc u is 87.2. f1 of tc u + t is 87.6. f1 of tc u is 71.4. f1 of tc u + t is 72.0. f1 of wb u is 91.8. f1 of wb u + t is 92.2. f1 of wb u is 76.8. f1 of wb u + t is 77.2.
table 5 shows standard benchmark results. f1 of base 2000 is  96.6. f1 of base 2005 wsj is  87.7. f1 of base 2005 brown is  78.3. f1 of base 2012 is  86.2. f1 of trans 2000 is  96.7. f1 of trans 2005 wsj is  87.9*. f1 of trans 2005 brown is  79.5*. f1 of trans 2012 is  86.6*. f1 of clark et al. (2018) 2000 is  97.0. f1 of peters et al. (2017) 2000 is  96.4. f1 of hashimoto et al. (2017) 2000 is  95.8. f1 of wang et al. (2019) 2005 wsj is  88.2. f1 of wang et al. (2019) 2005 brown is  79.3. f1 of wang et al. (2019) 2012 is  86.4. f1 of li et al. (2019) 2005 wsj is  87.7. f1 of li et al. (2019) 2005 brown is  80.5. f1 of li et al. (2019) 2012 is  86.0. f1 of ouchi et al. (2018) 2005 wsj is  87.6. f1 of ouchi et al. (2018) 2005 brown is  78.7. f1 of ouchi et al. (2018) 2012 is  86.2. f1 of he et al. (2018) 2005 wsj is  87.4. f1 of he et al. (2018) 2005 brown is  80.4. f1 of he et al. (2018) 2012 is  85.5.
table 4 shows performance in text classification (20-ng, r-8) and sentiment (sst-5) tasks of various models as reported in (kayal and tsatsaronis, 2019), where dct* refers to the implementation in (kayal and tsatsaronis, 2019). p of pca p is 55.43. r of pca r is 54.67. f1 of pca f1 is 54.77. p of pca p is 83.83. r of pca r is 83.42. f1 of pca f1 is 83.41. p of pca p is 26.47. r of pca r is 25.08. f1 of pca f1 is 25.23. p of dct* p is 61.07. r of dct* r is 59.16. f1 of dct* f1 is 59.78. p of dct* p is 90.41. r of dct* r is 90.78. f1 of dct* f1 is 90.38. p of dct* p is 30.11. r of dct* r is 30.09. f1 of dct* f1 is 29.53. p of avg. vec. p is 68.72. r of avg. vec. r is 68.19. f1 of avg. vec. f1 is 68.25. p of avg. vec. p is 96.34. r of avg. vec. r is 96.3. f1 of avg. vec. f1 is 96.27. p of avg. vec. p is 27.88. r of avg. vec. r is 26.44. f1 of avg. vec. f1 is 24.81. p of p-means p is 72.2. r of p-means r is 71.65. f1 of p-means f1 is 71.79. p of p-means p is 96.69. r of p-means r is 96.67. f1 of p-means f1 is 96.65. p of p-means p is 33.77. r of p-means r is 33.41. f1 of p-means f1 is 33.26. p of elmo p is 71.2. r of elmo r is 71.79. f1 of elmo f1 is 71.36. p of elmo p is 94.54. r of elmo r is 91.32. f1 of elmo f1 is 91.32. p of elmo p is 42.35. r of elmo r is 41.51. f1 of elmo f1 is 41.54. p of bert p is 70.89. r of bert r is 70.79. f1 of bert f1 is 70.88. p of bert p is 95.52. r of bert r is 95.39. f1 of bert f1 is 95.39. p of bert p is 39.92. r of bert r is 39.38. f1 of bert f1 is 39.35. p of eigensent p is 66.98. r of eigensent r is 66.4. f1 of eigensent f1 is 66.54. p of eigensent p is 95.91. r of eigensent r is 95.8. f1 of eigensent f1 is 95.76. p of eigensent p is 35.32. r of eigensent r is 33.69. f1 of eigensent f1 is 33.91. p of eigensentâš•avg p is 72.24. r of eigensentâš•avg r is 71.62. f1 of eigensentâš•avg f1 is 71.78. p of eigensentâš•avg p is 97.18. r of eigensentâš•avg r is 97.13. f1 of eigensentâš•avg f1 is 97.14. p of eigensentâš•avg p is 42.77. r of eigensentâš•avg r is 41.67. f1 of eigensentâš•avg f1 is 41.81. p of ck p is 72.2. r of ck r is 71.58. f1 of ck f1 is 71.73. p of ck p is 96.98. r of ck r is 96.98. f1 of ck f1 is 96.94. p of ck p is 37.67. r of ck r is 34.47. f1 of ck f1 is 34.54.
table 1 shows event detection performance on the cg task 2013 test dataset. p of tees p is 61.42. r of tees r is 52.93. f (%) of tees f (%) is 56.86. p of sbnn p is 63.67. r of sbnn r is 51.43. f (%) of sbnn f (%) is 56.9.
table 3 shows comparison on computational efficiency on the cg task 2013 development dataset. number of classification of tees number of classification is 6141. running time (s) of tees running time (s) is 155. number of classification of sbnn k = 8 number of classification is 4093. running time (s) of sbnn k = 8 running time (s) is 131.
table 4 shows results on cspubsum rouge-l of saf + f ens (collins et al., 2017) rouge-l is 0.313. rouge-l of bert +transformer rouge-l is 0.287. rouge-l of our model rouge-l is 0.306. rouge-l of our model + abstractrouge rouge-l is 0.314.
table 3 shows rouge recall results on nyt test set. r1 of oracle r1 is 49.18. r2 of oracle r2 is 33.24. rl of oracle rl is 46.02. r1 of lead-3 r1 is 39.58. r2 of lead-3 r2 is 20.11. rl of lead-3 rl is 35.78. r1 of compress (durrett et al. 2016) r1 is 42.2. r2 of compress (durrett et al. 2016) r2 is 24.9. rl of compress (durrett et al. 2016) rl is  -. r1 of sumo (liu et al. 2019) r1 is 42.3. r2 of sumo (liu et al. 2019) r2 is 22.7. rl of sumo (liu et al. 2019) rl is 38.6. r1 of transformerext r1 is 41.95. r2 of transformerext r2 is 22.68. rl of transformerext rl is 38.51. r1 of ptgen (see et al. 2017) r1 is 42.47. r2 of ptgen (see et al. 2017) r2 is 25.61. rl of ptgen (see et al. 2017) rl is  -. r1 of ptgen + cov (see et al. 2017) r1 is 43.71. r2 of ptgen + cov (see et al. 2017) r2 is 26.4. rl of ptgen + cov (see et al. 2017) rl is  -. r1 of drm (paulus et al. 2018) r1 is 42.94. r2 of drm (paulus et al. 2018) r2 is 26.02. rl of drm (paulus et al. 2018) rl is  -. r1 of transformerabs r1 is 35.75. r2 of transformerabs r2 is 17.23. rl of transformerabs rl is 31.41. r1 of bertsumext r1 is 46.66. r2 of bertsumext r2 is 26.35. rl of bertsumext rl is 42.62. r1 of bertsumabs r1 is 48.92. r2 of bertsumabs r2 is 30.84. rl of bertsumabs rl is 45.41. r1 of bertsumextabs r1 is 49.02. r2 of bertsumextabs r2 is 31.02. rl of bertsumextabs rl is 45.55.
table 4 shows fluency and consistency comparison by human evaluation. fluency of uni-model fluency is 1.61. consistency of uni-model consistency is 1.53. fluency of re 3 sum fluency is 1.53. consistency of re 3 sum consistency is 1.14. fluency of pesg fluency is 1.86*. consistency of pesg consistency is 1.73*.
table 6 shows performance on the conll-2003 english dataset. f1 of peters et al. (2018a) elmo f1 is 92.2. prec. of bilstm-crf + elmo (l = 2) prec. is 92.1. rec. of bilstm-crf + elmo (l = 2) rec. is 92.3. f1 of bilstm-crf + elmo (l = 2) f1 is 92.2. prec. of dglstm-crf + elmo (l = 2) prec. is 92.2. rec. of dglstm-crf + elmo (l = 2) rec. is 92.5. f1 of dglstm-crf + elmo (l = 2) f1 is 92.4.
table 4 shows auc performance for various representation methods. auc of amazon motors mt avg is 95.26. auc of amazon motors mt idf is 95.47. auc of amazon motors mt lang is 95.61. auc of amazon motors nn avg is 76.93. auc of amazon motors nn idf is 86.46. auc of amazon motors nn lang is 91.76*. auc of amazon fashion mt avg is 90.31. auc of amazon fashion mt idf is 91.19. auc of amazon fashion mt lang is 91.67. auc of amazon fashion nn avg is 61.3. auc of amazon fashion nn idf is 75.83. auc of amazon fashion nn lang is 84.59*. auc of new york times mt avg is 82.45. auc of new york times mt idf is 82.27. auc of new york times mt lang is 84.3. auc of new york times nn avg is 70.64. auc of new york times nn idf is 75.14. auc of new york times nn lang is 81.02*. auc of answers mt avg is 87.88. auc of answers mt idf is 87.82. auc of answers mt lang is 91.16*. auc of answers nn avg is 80.1. auc of answers nn idf is 80.6. auc of answers nn lang is 88.60*. auc of blog mt avg is 78.63. auc of blog mt idf is 77.89. auc of blog mt lang is 79.29. auc of blog nn avg is 65.56. auc of blog nn idf is 68.8. auc of blog nn lang is 78.01*. auc of email mt avg is 87.22. auc of email mt idf is 88.21. auc of email mt lang is 88.91. auc of email nn avg is 73.8. auc of email nn idf is 77.7. auc of email nn lang is 88.30*. auc of news mt avg is 77.71. auc of news mt idf is 77.84. auc of news mt lang is 78.04. auc of news nn avg is 65.3. auc of news nn idf is 69.4. auc of news nn lang is 77.70*. auc of sarcasm gen mt avg is 76.68. auc of sarcasm gen mt idf is 76.82. auc of sarcasm gen mt lang is 76.99. auc of sarcasm gen nn avg is 61.4. auc of sarcasm gen nn idf is 65.00*. auc of sarcasm gen nn lang is 62.1. auc of sarcasm rq mt avg is 76.81. auc of sarcasm rq mt idf is 77.34. auc of sarcasm rq mt lang is 77.16. auc of sarcasm rq nn avg is 61.13. auc of sarcasm rq nn idf is 63.70*. auc of sarcasm rq nn lang is 60.85. auc of sarcasm hyp mt avg is 64.31. auc of sarcasm hyp mt idf is 65.12. auc of sarcasm hyp mt lang is 65.32. auc of sarcasm hyp nn avg is 50.94. auc of sarcasm hyp nn idf is 53.32. auc of sarcasm hyp nn lang is 54.13.
table 2 shows short text classification on-device results & comparisons to prior work accuracy of proseqo (our on-device model) swda is 88.3. accuracy of proseqo (our on-device model) mrda is 90.1. accuracy of proseqo (our on-device model) atis is 97.8. accuracy of proseqo (our on-device model) snips is 97.9. accuracy of sgnn(ravi and kozareva, 2018)(on-device) swda is 83.1. accuracy of sgnn(ravi and kozareva, 2018)(on-device) mrda is 86.7. accuracy of sgnn(ravi and kozareva, 2018)(on-device) atis is 88.9. accuracy of sgnn(ravi and kozareva, 2018)(on-device) snips is 93.4. accuracy of rnn(khanpour et al., 2016) swda is 80.1. accuracy of rnn(khanpour et al., 2016) mrda is 86.8. accuracy of rnn+attention(ortega and vu, 2017) swda is 73.8. accuracy of rnn+attention(ortega and vu, 2017) mrda is 84.3. accuracy of cnn(lee and dernoncourt, 2016) swda is 73.1. accuracy of cnn(lee and dernoncourt, 2016) mrda is 84.6. accuracy of gatedintentatten.(goo et al., 2018) atis is 94.1. accuracy of gatedintentatten.(goo et al., 2018) snips is 96.8. accuracy of gatedfullatten.(goo et al., 2018) atis is 93.6. accuracy of gatedfullatten.(goo et al., 2018) snips is 97. accuracy of jointbilstm(hakkani-tur et al., 2016) atis is 92.6. accuracy of jointbilstm(hakkani-tur et al., 2016) snips is 96.9. accuracy of atten.rnn(liu and lane, 2016) atis is 91.1. accuracy of atten.rnn(liu and lane, 2016) snips is 96.7.
table 3 shows long text classification on-device results & comparisons to prior work accuracy of proseqo (our on-device model) ag is 91.5. accuracy of proseqo (our on-device model) y!a is 72.4. accuracy of proseqo (our on-device model) amzn is 62.3. accuracy of sgnn (ravi and kozareva 2018)(on-device) ag is 57.6. accuracy of sgnn (ravi and kozareva 2018)(on-device) y!a is 36.5. accuracy of sgnn (ravi and kozareva 2018)(on-device) amzn is 39.3. accuracy of fasttext-full (joulin et al. 2016) ag is 92.5. accuracy of fasttext-full (joulin et al. 2016) y!a is 72.3. accuracy of fasttext-full (joulin et al. 2016) amzn is 60.2. accuracy of charcnnlargewiththesau. (zhang et al. 2015) ag is 90.6. accuracy of charcnnlargewiththesau. (zhang et al. 2015) y!a is 71.2. accuracy of charcnnlargewiththesau. (zhang et al. 2015) amzn is 59.6. accuracy of cnn+ngm (bui et al. 2018) ag is 86.9. accuracy of lstm-full (zhang et al. 2015) ag is 86.1. accuracy of lstm-full (zhang et al. 2015) y!a is 70.8. accuracy of lstm-full (zhang et al. 2015) amzn is 59.4. accuracy of hier.-attention (yang et al. 2016) amzn is 63.6. accuracy of hier.-ave (yang et al. 2016) amzn is 62.9.
table 3 shows results on the vqa-cp v2.0 test set. acc. of none acc. is 39.18. acc. of reweight acc. is 40.06. acc. of bias product acc. is 39.93. acc. of learned-mixin acc. is 48.69. acc. of learned-mixin +h acc. is 52.05. acc. of ramakrishnan et al. (2018) acc. is 41.17. acc. of grand and belinkov (2019) acc. is 42.33.
table 5 shows subjective evaluations on the task of controlling the unselected rationale words. %unk of lei2016 %unk is 43.5. acc of lei2016 acc is 63.5. acc w/o unk of lei2016 acc w/o unk is 69. %unk of intros+minimax %unk is 54.0*. acc of intros+minimax acc is 58. acc w/o unk of intros+minimax acc w/o unk is 66.3.
table 6 shows main results on wsj. accuracy of plank et al. (2016) accuracy is 97.22. accuracy of huang et al. (2015) accuracy is 97.55. accuracy of ma and hovy (2016) accuracy is 97.55. accuracy of liu et al. (2017) accuracy is 97.53. accuracy of yang et al. (2018) accuracy is 97.51. accuracy of zhang et al. (2018c) accuracy is 97.55. accuracy of yasunaga et al. (2018) accuracy is 97.58. accuracy of xin et al. (2018) accuracy is 97.58. accuracy of transformer-softmax (guo et al., 2019) accuracy is 97.04. accuracy of bilstm-softmax (yang et al., 2018) accuracy is 97.51. accuracy of bilstm-crf (yang et al., 2018) accuracy is 97.51. accuracy of bilstm-lan accuracy is 97.65.
table 1 shows human teacher evaluation for learned and random question asking policy. avg. reward of lid avg. reward is 0.524. natural of lid natural is 3.2. avg. rew (simulated) of lid avg. rew (simulated) is 0.607. avg. reward of random avg. reward is 0.493. natural of random natural is 2.9. avg. rew (simulated) of random avg. rew (simulated) is 0.551.
table 4 shows results of baselines and fine-grained knowledge fusion methods on cws. f of source only f is 83.86. roov of source only roov is 62.4. f of source only f is 83.75. roov of source only roov is 70.74. f of target only f is 92.8. roov of target only roov is 65.81. f of target only f is 84.01. roov of target only roov is 64.12. f of basickd f is 94.23. roov of basickd roov is 74.08. f of basickd f is 89.21. roov of basickd roov is 76.26. f of sampdomain-q a samp f is 94.55. roov of sampdomain-q a samp roov is 74.02. f of sampdomain-q a samp f is 89.63. roov of sampdomain-q a samp roov is 75.93. f of elemdomain-q a elem f is 94.81. roov of elemdomain-q a elem roov is 74.75. f of elemdomain-q a elem f is 89.99. roov of elemdomain-q a elem roov is 77.59. f of multidomain-q a multi f is 94.75. roov of multidomain-q a multi roov is 74.96. f of multidomain-q a multi f is 90.06. roov of multidomain-q a multi roov is 77.25. f of sample-q a samp f is 94.57. roov of sample-q a samp roov is 74.47. f of sample-q a samp f is 89.77. roov of sample-q a samp roov is 76.81. f of elemsample-q a elem f is 94.78. roov of elemsample-q a elem roov is 74.52. f of elemsample-q a elem f is 90.07. roov of elemsample-q a elem roov is . f of multisample-q a multi f is 94.91. roov of multisample-q a multi roov is 75.56. f of multisample-q a multi f is 90.2. roov of multisample-q a multi roov is 77.46. f of fgkf f is 95.01. roov of fgkf roov is 77.26. f of fgkf f is 90.45. roov of fgkf roov is 77.27.
table 5 shows results of ablation study on hits@10 of 1-shot link prediction in nell-one. hits@10 of standard bg:pre-train is 0.331. hits@10 of standard bg:in-train is 0.401. hits@10 of  -g bg:pre-train is 0.234. hits@10 of  -g bg:in-train is 0.341. hits@10 of  -g -r bg:pre-train is 0.052. hits@10 of  -g -r bg:in-train is 0.052.
table 1 shows bleu scores on three mt benchmark datasets for flowseq with argmax decoding and baselines with purely non-autoregressive decoding method. bleu of cmlm-base en-de is 10.88. bleu of cmlm-base en-ro is 20.24. bleu of lv nar en-de is 11.8. bleu of flowseq-base en-de is 18.55. bleu of flowseq-base de-en is 23.36. bleu of flowseq-base en-ro is 29.26. bleu of flowseq-base ro-en is 30.16. bleu of flowseq-base de-en is 24.75. bleu of flowseq-large en-de is 20.85. bleu of flowseq-large de-en is 25.4. bleu of flowseq-large en-ro is 29.86. bleu of flowseq-large ro-en is 30.69. bleu of nat-ir en-de is 13.91. bleu of nat-ir de-en is 16.77. bleu of nat-ir en-ro is 24.45. bleu of nat-ir ro-en is 25.73. bleu of nat-ir de-en is 21.86. bleu of ctc loss en-de is 17.68. bleu of ctc loss de-en is 19.8. bleu of ctc loss en-ro is 19.93. bleu of ctc loss ro-en is 24.71. bleu of nat w/ ft en-de is 17.69. bleu of nat w/ ft de-en is 21.47. bleu of nat w/ ft en-ro is 27.29. bleu of nat w/ ft ro-en is 29.06. bleu of nat w/ ft de-en is 20.32. bleu of nat-reg en-de is 20.65. bleu of nat-reg de-en is 24.77. bleu of nat-reg de-en is 23.89. bleu of cmlm-small en-de is 15.06. bleu of cmlm-small de-en is 19.26. bleu of cmlm-small en-ro is 20.12. bleu of cmlm-small ro-en is 20.36. bleu of cmlm-base en-de is 18.12. bleu of cmlm-base de-en is 22.26. bleu of cmlm-base en-ro is 23.65. bleu of cmlm-base ro-en is 22.78. bleu of flowseq-base en-de is 21.45. bleu of flowseq-base de-en is 26.16. bleu of flowseq-base en-ro is 29.34. bleu of flowseq-base ro-en is 30.44. bleu of flowseq-base de-en is 27.55. bleu of flowseq-large en-de is 23.72. bleu of flowseq-large de-en is 28.39. bleu of flowseq-large en-ro is 29.73. bleu of flowseq-large ro-en is 30.72.
table 1 shows results on the dependency task (test set). p of prolocal p is 24.7. r of prolocal r is 18. f1 of prolocal f1 is 20.8. p of qrn p is 32.6. r of qrn r is 30.3. f1 of qrn f1 is 31.4. p of entnet p is 32.8. r of entnet r is 38.6. f1 of entnet f1 is 35.5. p of prostruct p is 76.3. r of prostruct r is 21.3. f1 of prostruct f1 is 33.4. p of proglobal p is 43.4. r of proglobal r is 37. f1 of proglobal f1 is 39.9. p of xpad p is 62. r of xpad r is 32.9. f1 of xpad f1 is 43.
table 2 shows comparison between our models based on fasttext and bert with the bilstm used by (khatri et al., 2018) on wikipedia toxic comments. offensive f1 of fasttext offensive f1 is 71.40%. weighted f1 of fasttext weighted f1 is 94.80%. offensive f1 of bert-based offensive f1 is 83.40%. weighted f1 of bert-based weighted f1 is 96.70%. weighted f1 of (khatri et al. 2018) weighted f1 is 95.40%.
table 3 shows results of per-response accuracy and per-dialog accuracy (in brackets) on babi dialogues. per-response accuracy of t3 per-response accuracy is 74.8. per-dialog accuracy of t3 per-dialog accuracy is 0. per-response accuracy of t3 per-response accuracy is 74.8. per-dialog accuracy of t3 per-dialog accuracy is 0. per-response accuracy of t3 per-response accuracy is 83.9. per-dialog accuracy of t3 per-dialog accuracy is 15.6. per-response accuracy of t3 per-response accuracy is 93.7. per-dialog accuracy of t3 per-dialog accuracy is 55.9. per-response accuracy of t3 per-response accuracy is 93.6. per-dialog accuracy of t3 per-dialog accuracy is 56.1. per-response accuracy of t4 per-response accuracy is 56.5. per-dialog accuracy of t4 per-dialog accuracy is 0. per-response accuracy of t4 per-response accuracy is 56.5. per-dialog accuracy of t4 per-dialog accuracy is 0. per-response accuracy of t4 per-response accuracy is 97. per-dialog accuracy of t4 per-dialog accuracy is 90.5. per-response accuracy of t4 per-response accuracy is 96.8. per-dialog accuracy of t4 per-dialog accuracy is 89.3. per-response accuracy of t4 per-response accuracy is 100. per-dialog accuracy of t4 per-dialog accuracy is 100. per-response accuracy of t5 per-response accuracy is 98.9. per-dialog accuracy of t5 per-dialog accuracy is 82.9. per-response accuracy of t5 per-response accuracy is 98.6. per-dialog accuracy of t5 per-dialog accuracy is 83. per-response accuracy of t5 per-response accuracy is 96.2. per-dialog accuracy of t5 per-dialog accuracy is 46.4. per-response accuracy of t5 per-response accuracy is 97.1. per-dialog accuracy of t5 per-dialog accuracy is 58.2. per-response accuracy of t5 per-response accuracy is 98. per-dialog accuracy of t5 per-dialog accuracy is 69. per-response accuracy of t3-oov per-response accuracy is 74.9. per-dialog accuracy of t3-oov per-dialog accuracy is 0. per-response accuracy of t3-oov per-response accuracy is 74. per-dialog accuracy of t3-oov per-dialog accuracy is 0. per-response accuracy of t3-oov per-response accuracy is 83.6. per-dialog accuracy of t3-oov per-dialog accuracy is 18.1. per-response accuracy of t3-oov per-response accuracy is 92.3. per-dialog accuracy of t3-oov per-dialog accuracy is 45.2. per-response accuracy of t3-oov per-response accuracy is 92.5. per-dialog accuracy of t3-oov per-dialog accuracy is 48.2. per-response accuracy of t4-oov per-response accuracy is 56.5. per-dialog accuracy of t4-oov per-dialog accuracy is 0. per-response accuracy of t4-oov per-response accuracy is 57. per-dialog accuracy of t4-oov per-dialog accuracy is 0. per-response accuracy of t4-oov per-response accuracy is 97. per-dialog accuracy of t4-oov per-dialog accuracy is 89.4. per-response accuracy of t4-oov per-response accuracy is 96.1. per-dialog accuracy of t4-oov per-dialog accuracy is 90.3. per-response accuracy of t4-oov per-response accuracy is 100. per-dialog accuracy of t4-oov per-dialog accuracy is 100. per-response accuracy of t5-oov per-response accuracy is 67.2. per-dialog accuracy of t5-oov per-dialog accuracy is 0. per-response accuracy of t5-oov per-response accuracy is 67.6. per-dialog accuracy of t5-oov per-dialog accuracy is 0. per-response accuracy of t5-oov per-response accuracy is 71.4. per-dialog accuracy of t5-oov per-dialog accuracy is 0. per-response accuracy of t5-oov per-response accuracy is 78.3. per-dialog accuracy of t5-oov per-dialog accuracy is 0. per-response accuracy of t5-oov per-response accuracy is 84.1. per-dialog accuracy of t5-oov per-dialog accuracy is 2.6.
table 4 shows the results on the dstc 2 f1 of seq2seq f1 is 69.7. bleu of seq2seq bleu is 55. f1 of seq2seq+attn. f1 is 67.1. bleu of seq2seq+attn. bleu is 56.6. f1 of seq2seq+copy f1 is 71.6. bleu of seq2seq+copy bleu is 55.4. f1 of mem2seq f1 is 75.3. bleu of mem2seq bleu is 55.3. f1 of our model f1 is 77.7. bleu of our model bleu is 56.4.
table 2 shows results of the alsc task in single-task settings in terms of accuracy (%) and macro-f1 (%). acc of lstm acc is 80.92. f1 of lstm f1 is 68.3. acc of lstm acc is 85.83. f1 of lstm f1 is 80.88. acc of lstm acc is 71.24. f1 of lstm f1 is 49.4. acc of lstm acc is 71.97. f1 of lstm f1 is 69.97. acc of at-lstm acc is 81.24. f1 of at-lstm f1 is 69.19. acc of at-lstm acc is 87.25. f1 of at-lstm f1 is 82.2. acc of at-lstm acc is 73.37. f1 of at-lstm f1 is 51.74. acc of at-lstm acc is 76.79. f1 of at-lstm f1 is 74.61. acc of atae-lstm acc is 82.18. f1 of atae-lstm f1 is 69.18. acc of atae-lstm acc is 88.08. f1 of atae-lstm f1 is 83.03. acc of atae-lstm acc is 74.56. f1 of atae-lstm f1 is 51.4. acc of atae-lstm acc is 79.79. f1 of atae-lstm f1 is 78.69. acc of gcae acc is 82.08. f1 of gcae f1 is 70.2. acc of gcae acc is 87.72. f1 of gcae f1 is 83.84. acc of gcae acc is 76.69. f1 of gcae f1 is 53. acc of gcae acc is 79.66. f1 of gcae f1 is 77.96. acc of at-can-rs acc is 82.28. f1 of at-can-rs f1 is 70.94. acc of at-can-rs acc is 88.43. f1 of at-can-rs f1 is 84.07. acc of at-can-rs acc is 75.62. f1 of at-can-rs f1 is 53.56. acc of at-can-rs acc is 78.36. f1 of at-can-rs f1 is 76.69. acc of at-can-ro acc is 82.81. f1 of at-can-ro f1 is 71.32. acc of at-can-ro acc is 89.37. f1 of at-can-ro f1 is 85.66. acc of at-can-ro acc is 76.92. f1 of at-can-ro f1 is 55.67. acc of at-can-ro acc is 79.92. f1 of at-can-ro f1 is 78.77. acc of atae-can-rs acc is 81.97. f1 of atae-can-rs f1 is 72.19. acc of atae-can-rs acc is 88.9. f1 of atae-can-rs f1 is 84.29. acc of atae-can-rs acc is 77.28. f1 of atae-can-rs f1 is 52.45. acc of atae-can-rs acc is 81.49. f1 of atae-can-rs f1 is 80.61. acc of atae-can-ro acc is 83.33. f1 of atae-can-ro f1 is 73.23. acc of atae-can-ro acc is 89.02. f1 of atae-can-ro f1 is 84.76. acc of atae-can-ro acc is 78.58. f1 of atae-can-ro f1 is 54.72. acc of atae-can-ro acc is 81.75. f1 of atae-can-ro f1 is 80.91.
table 3 shows results of the alsc task in multi-task settings in terms of accuracy (%) and macro-f1 (%). acc of m-at-lstm acc is 82.6. f1 of m-at-lstm f1 is 71.44. acc of m-at-lstm acc is 88.55. f1 of m-at-lstm f1 is 83.76. acc of m-at-lstm acc is 76.33. f1 of m-at-lstm f1 is 51.64. acc of m-at-lstm acc is 79.53. f1 of m-at-lstm f1 is 78.31. acc of m-can-rs acc is 83.65. f1 of m-can-rs f1 is 73.97. acc of m-can-rs acc is 89.26. f1 of m-can-rs f1 is 85.43. acc of m-can-rs acc is 75.74. f1 of m-can-rs f1 is 52.43. acc of m-can-rs acc is 79.66. f1 of m-can-rs f1 is 78.46. acc of m-can-ro acc is 83.12. f1 of m-can-ro f1 is 72.29. acc of m-can-ro acc is 89.61. f1 of m-can-ro f1 is 85.18. acc of m-can-ro acc is 77.04. f1 of m-can-ro f1 is 52.69. acc of m-can-ro acc is 79.4. f1 of m-can-ro f1 is 77.88. acc of m-can-2rs acc is 83.23. f1 of m-can-2rs f1 is 72.81. acc of m-can-2rs acc is 89.37. f1 of m-can-2rs f1 is 85.42. acc of m-can-2rs acc is 78.22. f1 of m-can-2rs f1 is 55.8. acc of m-can-2rs acc is 80.44. f1 of m-can-2rs f1 is 80.01. acc of m-can-2ro acc is 84.28. f1 of m-can-2ro f1 is 74.45. acc of m-can-2ro acc is 89.96. f1 of m-can-2ro f1 is 86.16. acc of m-can-2ro acc is 77.51. f1 of m-can-2ro f1 is 52.78. acc of m-can-2ro acc is 82.14. f1 of m-can-2ro f1 is 81.58.
table 4 shows results of the acd task. precision of m-at-lstm precision is 0.8626. recall of m-at-lstm recall is 0.8553. f1 of m-at-lstm f1 is 0.8589. precision of m-at-lstm precision is 0.6691. recall of m-at-lstm recall is 0.4748. f1 of m-at-lstm f1 is 0.5555. precision of m-can-2rs precision is 0.8698. recall of m-can-2rs recall is 0.8595. f1 of m-can-2rs f1 is 0.8645. precision of m-can-2rs precision is 0.6244. recall of m-can-2rs recall is 0.5019. f1 of m-can-2rs f1 is 0.5565. precision of m-can-2ro precision is 0.8907. recall of m-can-2ro recall is 0.8627. f1 of m-can-2ro f1 is 0.8765. precision of m-can-2ro precision is 0.7127. recall of m-can-2ro recall is 0.4865. f1 of m-can-2ro f1 is 0.5782.
table 3 shows results of our model variants on development set. train time of bilstm - is 0.94. map of bilstm twitter is 0.617. map of bilstm reddit is 0.498. train time of glstm - is 1.25. map of glstm twitter is 0.617. map of glstm reddit is 0.528. train time of gcn (w/o bilstm) - is 1.03. map of gcn (w/o bilstm) twitter is 0.619. map of gcn (w/o bilstm) reddit is 0.53. train time of gcn (with bilstm) - is 1. map of gcn (with bilstm) twitter is 0.62. map of gcn (with bilstm) reddit is 0.533.
table 4 shows main results on conversation recommendation. map of random map is 0.006. p@1 of random p@1 is 0.001. ndcg of random ndcg is 0.002. map of random map is 0.04. p@1 of random p@1 is 0.01. ndcg of random ndcg is 0.022. map of popularity map is 0.023. p@1 of popularity p@1 is 0.005. ndcg of popularity ndcg is 0.01. map of popularity map is 0.082. p@1 of popularity p@1 is 0.033. ndcg of popularity ndcg is 0.063. map of rsvm map is 0.554. p@1 of rsvm p@1 is 0.575. ndcg of rsvm ndcg is 0.559. map of rsvm map is 0.453. p@1 of rsvm p@1 is 0.457. ndcg of rsvm ndcg is 0.466. map of ncf map is 0.573. p@1 of ncf p@1 is 0.593. ndcg of ncf ndcg is 0.576. map of ncf map is 0.412. p@1 of ncf p@1 is 0.544. ndcg of ncf ndcg is 0.461. map of convmf map is 0.579. p@1 of convmf p@1 is 0.596. ndcg of convmf ndcg is 0.583. map of convmf map is 0.485. p@1 of convmf p@1 is 0.532. ndcg of convmf ndcg is 0.52. map of cr_jtd map is 0.591. p@1 of cr_jtd p@1 is 0.591. ndcg of cr_jtd ndcg is 0.6. map of cr_jtd map is 0.453. p@1 of cr_jtd p@1 is 0.559. ndcg of cr_jtd ndcg is 0.485. map of ours map is 0.625. p@1 of ours p@1 is 0.632. ndcg of ours ndcg is 0.626. map of ours map is 0.538. p@1 of ours p@1 is 0.674. ndcg of ours ndcg is 0.59.
table 2 shows results of rumor stance classification. macro-f1 of affective feature + svm (pamungkas et al., 2018) macro-f1 is 0.47. fs of affective feature + svm (pamungkas et al., 2018) fs is 0.41. fd of affective feature + svm (pamungkas et al., 2018) fd is 0. fq of affective feature + svm (pamungkas et al., 2018) fq is 0.58. fc of affective feature + svm (pamungkas et al., 2018) fc is 0.88. acc. of affective feature + svm (pamungkas et al., 2018) acc. is 0.795. macro-f1 of branchlstm (kochkina et al., 2017) macro-f1 is 0.434. fs of branchlstm (kochkina et al., 2017) fs is 0.403. fd of branchlstm (kochkina et al., 2017) fd is 0. fq of branchlstm (kochkina et al., 2017) fq is 0.462. fc of branchlstm (kochkina et al., 2017) fc is 0.873. acc. of branchlstm (kochkina et al., 2017) acc. is 0.784. macro-f1 of temporalattention (veyseh et al., 2017) macro-f1 is 0.482. acc. of temporalattention (veyseh et al., 2017) acc. is 0.82. macro-f1 of conversational-gcn (ours, l = 2) macro-f1 is 0.499. fs of conversational-gcn (ours, l = 2) fs is 0.311. fd of conversational-gcn (ours, l = 2) fd is 0.194. fq of conversational-gcn (ours, l = 2) fq is 0.646. fc of conversational-gcn (ours, l = 2) fc is 0.847. acc. of conversational-gcn (ours, l = 2) acc. is 0.751.
table 3 shows results of veracity prediction. macro-f1 of td-rvnn (ma et al., 2018b) macro-f1 is 0.509. acc. of td-rvnn (ma et al., 2018b) acc. is 0.536. macro-f1 of td-rvnn (ma et al., 2018b) macro-f1 is 0.264. acc. of td-rvnn (ma et al., 2018b) acc. is 0.341. macro-f1 of hierarchical gcn-rnn (ours) macro-f1 is 0.54. acc. of hierarchical gcn-rnn (ours) acc. is 0.536. macro-f1 of hierarchical gcn-rnn (ours) macro-f1 is 0.317. acc. of hierarchical gcn-rnn (ours) acc. is 0.356. macro-f1 of branchlstm+niletmrg (kochkina et al., 2018) macro-f1 is 0.539. acc. of branchlstm+niletmrg (kochkina et al., 2018) acc. is 0.57. macro-f1 of branchlstm+niletmrg (kochkina et al., 2018) macro-f1 is 0.297. acc. of branchlstm+niletmrg (kochkina et al., 2018) acc. is 0.36. macro-f1 of mtl2 (veracity+stance) (kochkina et al., 2018) macro-f1 is 0.558. acc. of mtl2 (veracity+stance) (kochkina et al., 2018) acc. is 0.571. macro-f1 of mtl2 (veracity+stance) (kochkina et al., 2018) macro-f1 is 0.318. acc. of mtl2 (veracity+stance) (kochkina et al., 2018) acc. is 0.357. macro-f1 of hierarchical-psv (ours, î» = 1) macro-f1 is 0.588. acc. of hierarchical-psv (ours, î» = 1) acc. is 0.643. macro-f1 of hierarchical-psv (ours, î» = 1) macro-f1 is 0.333. acc. of hierarchical-psv (ours, î» = 1) acc. is 0.361.
table 2 shows test accuracy (%) of different models on six standard datasets. accuracy of agnews svm +tfidf is 57.73. accuracy of agnews svm +ldacnn is 65.16. accuracy of agnews cnn -rand is 32.65. accuracy of agnews cnn -pretrain is 67.24. accuracy of agnews lstm -rand is 31.24. accuracy of agnews lstm -pretrain is 66.28. accuracy of agnews pte is 36. accuracy of agnews textgcn is 67.61. accuracy of agnews han is 62.64. accuracy of agnews hgat is 72.10*. accuracy of snippets svm +tfidf is 63.85. accuracy of snippets svm +ldacnn is 63.91. accuracy of snippets cnn -rand is 48.34. accuracy of snippets cnn -pretrain is 77.09. accuracy of snippets lstm -rand is 26.38. accuracy of snippets lstm -pretrain is 75.89. accuracy of snippets pte is 63.1. accuracy of snippets textgcn is 77.82. accuracy of snippets han is 58.38. accuracy of snippets hgat is 82.36*. accuracy of ohsumed svm +tfidf is 41.47. accuracy of ohsumed svm +ldacnn is 31.26. accuracy of ohsumed cnn -rand is 35.25. accuracy of ohsumed cnn -pretrain is 32.92. accuracy of ohsumed lstm -rand is 19.87. accuracy of ohsumed lstm -pretrain is 28.7. accuracy of ohsumed pte is 36.63. accuracy of ohsumed textgcn is 41.56. accuracy of ohsumed han is 36.97. accuracy of ohsumed hgat is 42.68*. accuracy of tagmynews svm +tfidf is 42.9. accuracy of tagmynews svm +ldacnn is 21.88. accuracy of tagmynews cnn -rand is 28.76. accuracy of tagmynews cnn -pretrain is 57.12. accuracy of tagmynews lstm -rand is 25.52. accuracy of tagmynews lstm -pretrain is 57.32. accuracy of tagmynews pte is 40.32. accuracy of tagmynews textgcn is 54.28. accuracy of tagmynews han is 42.18. accuracy of tagmynews hgat is 61.72*. accuracy of mr svm +tfidf is 56.67. accuracy of mr svm +ldacnn is 54.69. accuracy of mr cnn -rand is 54.85. accuracy of mr cnn -pretrain is 58.32. accuracy of mr lstm -rand is 52.62. accuracy of mr lstm -pretrain is 60.89. accuracy of mr pte is 54.74. accuracy of mr textgcn is 59.12. accuracy of mr han is 57.11. accuracy of mr hgat is 62.75*. accuracy of twitter svm +tfidf is 54.39. accuracy of twitter svm +ldacnn is 50.42. accuracy of twitter cnn -rand is 52.58. accuracy of twitter cnn -pretrain is 56.34. accuracy of twitter lstm -rand is 54.8. accuracy of twitter lstm -pretrain is 60.28. accuracy of twitter pte is 54.24. accuracy of twitter textgcn is 60.15. accuracy of twitter han is 53.75. accuracy of twitter hgat is 63.21*.
table 7 shows ranking results on benchls dataset n=1 of s n=1 is 0.4974. n=2 of s n=2 is 0.7381. n=3 of s n=3 is 0.8899. mrr of s mrr is 0.6648. n=1 of c n=1 is 0.3509. n=2 of c n=2 is 0.5885. n=3 of c n=3 is 0.7877. mrr of c mrr is 0.5998. n=1 of s+c n=1 is 0.5602. n=2 of s+c n=2 is 0.8064. n=3 of s+c n=3 is 0.9428. mrr of s+c mrr is 0.7219. n=1 of s n=1 is 0.5839. n=2 of s n=2 is 0.7546. n=3 of s n=3 is 0.9302. mrr of s mrr is 0.7083. n=1 of c n=1 is 0.4086. n=2 of c n=2 is 0.7142. n=3 of c n=3 is 0.895. mrr of c mrr is 0.6563. n=1 of s+c n=1 is 0.6774. n=2 of s+c n=2 is 0.7857. n=3 of s+c n=3 is 0.9308. mrr of s+c mrr is 0.8218. n=1 of p&s n=1 is 0.4841. n=2 of p&s n=2 is 0.5596. n=3 of p&s n=3 is 0.7004. mrr of p&s mrr is 0.6615.
table 3 shows performance of sc and disp on identifying perpetuated tokens. precision of insertion precision is 0.5087. recall of insertion recall is 0.9369. f1 of insertion f1 is 0.6594. precision of insertion precision is 0.9725. recall of insertion recall is 0.8865. f1 of insertion f1 is 0.9275. precision of insertion precision is 0.0429. recall of insertion recall is 0.9367. f1 of insertion f1 is 0.082. precision of insertion precision is 0.915. recall of insertion recall is 0.5068. f1 of insertion f1 is 0.6523. precision of deletion precision is 0.4703. recall of deletion recall is 0.8085. f1 of deletion f1 is 0.5947. precision of deletion precision is 0.9065. recall of deletion recall is 0.876. f1 of deletion f1 is 0.891. precision of deletion precision is 0.0369. recall of deletion recall is 0.8052. f1 of deletion f1 is 0.0706. precision of deletion precision is 0.8181. recall of deletion recall is 0.4886. f1 of deletion f1 is 0.6118. precision of swap precision is 0.5044. recall of swap recall is 0.9151. f1 of swap f1 is 0.6504. precision of swap precision is 0.9552. recall of swap recall is 0.868. f1 of swap f1 is 0.9095. precision of swap precision is 0.0406. recall of swap recall is 0.8895. f1 of swap f1 is 0.0777. precision of swap precision is 0.886. recall of swap recall is 0.5. f1 of swap f1 is 0.6392. precision of random precision is 0.1612. recall of random recall is 0.1732. f1 of random f1 is 0.1669. precision of random precision is 0.8407. recall of random recall is 0.6504. f1 of random f1 is 0.7334. precision of random precision is 0.0084. recall of random recall is 0.179. f1 of random f1 is 0.0161. precision of random precision is 0.5233. recall of random recall is 0.3876. f1 of random f1 is 0.4454. precision of embed precision is 0.1484. recall of embed recall is 0.1617. f1 of embed f1 is 0.1548. precision of embed precision is 0.4828. recall of embed recall is 0.5515. f1 of embed f1 is 0.5149. precision of embed precision is 0.0064. recall of embed recall is 0.1352. f1 of embed f1 is 0.0122. precision of embed precision is 0.2024. recall of embed recall is 0.2063. f1 of embed f1 is 0.2044. precision of overall attacks precision is 0.3586. recall of overall attacks recall is 0.5991. f1 of overall attacks f1 is 0.4452. precision of overall attacks precision is 0.8315. recall of overall attacks recall is 0.7665. f1 of overall attacks f1 is 0.7952. precision of overall attacks precision is 0.027. recall of overall attacks recall is 0.5891. f1 of overall attacks f1 is 0.0517. precision of overall attacks precision is 0.669. recall of overall attacks recall is 0.4179. f1 of overall attacks f1 is 0.5106.
table 1 shows overall, intraand inter-sentence pairs performance comparison with the state-of-the-art on the cdr test set. p of gu et al. (2017) p is 55.7. r of gu et al. (2017) r is 68.1. f1 of gu et al. (2017) f1 is 61.3. p of gu et al. (2017) p is 59.7. r of gu et al. (2017) r is 55.0. f1 of gu et al. (2017) f1 is 57.2. p of gu et al. (2017) p is 51.9. r of gu et al. (2017) r is 7.0. f1 of gu et al. (2017) f1 is 11.7. p of verga et al. (2018) p is 55.6. r of verga et al. (2018) r is 70.8. f1 of verga et al. (2018) f1 is 62.1. p of nguyen and verspoor (2018) p is 57.0. r of nguyen and verspoor (2018) r is 68.6. f1 of nguyen and verspoor (2018) f1 is 62.3. p of eog p is 62.1. r of eog r is 65.2. f1 of eog f1 is 63.6. p of eog p is 64.0. r of eog r is 73.0. f1 of eog f1 is 68.2. p of eog p is 56.0. r of eog r is 46.7. f1 of eog f1 is 50.9. p of eog (full) p is 59.1. r of eog (full) r is 56.2. f1 of eog (full) f1 is 57.6. p of eog (full) p is 71.2. r of eog (full) r is 62.3. f1 of eog (full) f1 is 66.5. p of eog (full) p is 37.1. r of eog (full) r is 42.0. f1 of eog (full) f1 is 39.4. p of eog (noinf) p is 48.2. r of eog (noinf) r is 50.2. f1 of eog (noinf) f1 is 49.2. p of eog (noinf) p is 65.8. r of eog (noinf) r is 55.2. f1 of eog (noinf) f1 is 60.2. p of eog (noinf) p is 25.4. r of eog (noinf) r is 38.5. f1 of eog (noinf) f1 is 30.6. p of eog (sent) p is 56.9. r of eog (sent) r is 53.5. f1 of eog (sent) f1 is 55.2. p of eog (sent) p is 56.9. r of eog (sent) r is 76.4. f1 of eog (sent) f1 is 65.2. p of zhou et al. (2016) p is 55.6. r of zhou et al. (2016) r is 68.4. f1 of zhou et al. (2016) f1 is 61.3. p of peng et al. (2016) p is 62.1. r of peng et al. (2016) r is 64.2. f1 of peng et al. (2016) f1 is 63.1. p of li et al. (2016b) p is 60.8. r of li et al. (2016b) r is 76.4. f1 of li et al. (2016b) f1 is 67.7. p of li et al. (2016b) p is 67.3. r of li et al. (2016b) r is 52.4. f1 of li et al. (2016b) f1 is 58.9. p of panyam et al. (2018) p is 53.2. r of panyam et al. (2018) r is 69.7. f1 of panyam et al. (2018) f1 is 60.3. p of panyam et al. (2018) p is 54.7. r of panyam et al. (2018) r is 80.6. f1 of panyam et al. (2018) f1 is 65.1. p of panyam et al. (2018) p is 47.8. r of panyam et al. (2018) r is 43.8. f1 of panyam et al. (2018) f1 is 45.7. p of zheng et al. (2018) p is 56.2. r of zheng et al. (2018) r is 67.9. f1 of zheng et al. (2018) f1 is 61.5.
table 3 shows automatic evaluation results on four style transfer tasks. acc of s2s acc is 87.2%. bleu of s2s bleu is 4.2. gleu of s2s gleu is 3.24. acc of s2s acc is 74.8%. bleu of s2s bleu is 3.66. gleu of s2s gleu is 3.43. acc of s2s acc is 88.9%. bleu of s2s bleu is 33.9. gleu of s2s gleu is 14.06. acc of s2s acc is 71.8%. bleu of s2s bleu is 18.34. gleu of s2s gleu is 2.99. acc of sls acc is 82.0%. bleu of sls bleu is 5.89. gleu of sls gleu is 4.49. acc of sls acc is 81.9%. bleu of sls bleu is 3.05. gleu of sls gleu is 1.88. acc of sls acc is 89.5%. bleu of sls bleu is 41.41. gleu of sls gleu is 16.77. acc of sls acc is 63.5%. bleu of sls bleu is 19.21. gleu of sls gleu is 2.55. acc of dar acc is 82.5%. bleu of dar bleu is 6.33. gleu of dar gleu is 5.21. acc of dar acc is 80.4%. bleu of dar bleu is 4.72. gleu of dar gleu is 4.26. acc of dar acc is 89.2%. bleu of dar bleu is 44.72. gleu of dar gleu is 18.52. acc of dar acc is 63.5%. bleu of dar bleu is 23.32. gleu of dar gleu is 3.26. acc of cpls acc is 85.4%. bleu of cpls bleu is 7.11. gleu of cpls gleu is 5.52. acc of cpls acc is 84.4%. bleu of cpls bleu is 4.95. gleu of cpls gleu is 4.12. acc of cpls acc is 91.3%. bleu of cpls bleu is 48.6. gleu of cpls gleu is 19.04. acc of cpls acc is 64.3%. bleu of cpls bleu is 27.25. gleu of cpls gleu is 3.61.
table 4 shows the human annotation results of the s2s model and cpls model from three aspects. content of to m.zh content is 0.1875. style of to m.zh style is 0.5675. fluency of to m.zh fluency is 0.3575. content of to anc.p content is 0.2275. style of to anc.p style is 0.54. fluency of to anc.p fluency is 0.4425. content of to inf.en content is 0.3175. style of to inf.en style is 0.46. fluency of to inf.en fluency is 0.58. content of to f.en content is 0.3625. style of to f.en style is 0.5125. fluency of to f.en fluency is 0.6325. content of to m.zh content is 0.31. style of to m.zh style is 0.5825. fluency of to m.zh fluency is 0.305. content of to anc.p content is 0.4375. style of to anc.p style is 0.6875. fluency of to anc.p fluency is 0.5475. content of to inf.en content is 0.4675. style of to inf.en style is 0.4625. fluency of to inf.en fluency is 0.5725. content of to f.en content is 0.46. style of to f.en style is 0.5675. fluency of to f.en fluency is 0.62.
table 4 shows performance on edit anchoring acc of passive-aggr acc is 0.581. f1 of passive-aggr f1 is 0.533. acc of passive-aggr acc is 0.716. f1 of passive-aggr f1 is 0.262. acc of randforest acc is 0.639. f1 of randforest f1 is 0.290. acc of randforest acc is 0.743. f1 of randforest f1 is 0.112. acc of adaboost acc is 0.657. f1 of adaboost f1 is 0.398. acc of adaboost acc is 0.751. f1 of adaboost f1 is 0.207. acc of gated rnn acc is 0.696. f1 of gated rnn f1 is 0.651. acc of gated rnn acc is 0.665. f1 of gated rnn f1 is 0.539. acc of cmntedit-mt acc is 0.635. f1 of cmntedit-mt f1 is 0.587. acc of cmntedit-mt acc is 0.619. f1 of cmntedit-mt f1 is 0.468. acc of cmntedit-ea acc is 0.744. f1 of cmntedit-ea f1 is 0.687. acc of cmntedit-ea acc is 0.726. f1 of cmntedit-ea f1 is 0.583.
table 3 shows evaluation results accuracy of prado yelp is 64.7. accuracy of prado amazon is 61.2. accuracy of prado yahoo is 72.3. accuracy of prado 8-bit quantized yelp is 65.9. accuracy of prado 8-bit quantized amazon is 61.9. accuracy of prado 8-bit quantized yahoo is 72.5. accuracy of sgnn (ravi and kozareva, 2018) yelp is 35.4. accuracy of sgnn (ravi and kozareva, 2018) amazon is 39.1. accuracy of sgnn (ravi and kozareva, 2018) yahoo is 36.6. accuracy of hn-att* (yang et al., 2016) amazon is 63.6. accuracy of hn-max* (yang et al., 2016) amazon is 62.9. accuracy of hn-ave* (yang et al., 2016) amazon is 62.9. accuracy of lstm-grnn (tang et al., 2015) yelp is 67.6. accuracy of conv-grnn (tang et al., 2015) yelp is 66. accuracy of cnn-char (zhang et al., 2015) yelp is 62. accuracy of cnn-char (zhang et al., 2015) amazon is 59.6. accuracy of cnn-char (zhang et al., 2015) yahoo is 71.2. accuracy of cnn-word (tang et al., 2015) yelp is 61.5. accuracy of cnn-word (zhang et al., 2015) yelp is 60.5. accuracy of cnn-word (zhang et al., 2015) amazon is 57.6. accuracy of cnn-word (zhang et al., 2015) yahoo is 71.2. accuracy of paragraph vector (tang et al., 2015) yelp is 60.5. accuracy of lstm (zhang et al., 2015) yelp is 58.2. accuracy of lstm (zhang et al., 2015) amazon is 59.4. accuracy of lstm (zhang et al., 2015) yahoo is 70.8. accuracy of svm + bigrams (tang et al., 2015) yelp is 62.4. accuracy of svm + unigrams (tang et al., 2015) yelp is 61.1. accuracy of svm + averagesg (tang et al., 2015) yelp is 56.8. accuracy of svm + sswe (tang et al., 2015) yelp is 55.4. accuracy of bow tfidf (zhang et al., 2015) yelp is 59.9. accuracy of bow tfidf (zhang et al., 2015) amazon is 55.3. accuracy of bow tfidf (zhang et al., 2015) yahoo is 71. accuracy of ngrams tfidf (zhang et al., 2015) yelp is 54.8. accuracy of ngrams tfidf (zhang et al., 2015) amazon is 52.4. accuracy of ngrams tfidf (zhang et al., 2015) yahoo is 68.5.
table 2 shows sentence fusion results on dfwiki. exact of transformer (geva et al., 2019) exact is  51.1. sari of transformer (geva et al., 2019) sari is  84.5. exact of seq2seqbert exact is 53.6. sari of seq2seqbert sari is 85.3. exact of lasertaggerar (no swap) exact is  46.4. sari of lasertaggerar (no swap) sari is  80.4. exact of lasertaggerff exact is  52.2. sari of lasertaggerff sari is  84.1. exact of lasertaggerar exact is  53.8. sari of lasertaggerar sari is  85.5.
table 5 shows results on grammatical-error correction. p of grundkiewicz et al. (2019) p is 70.19. r of grundkiewicz et al. (2019) r is 47.99. f 0.5 of grundkiewicz et al. (2019) f 0.5 is 64.24. p of seq2seqbert p is 6.13. r of seq2seqbert r is 14.14. f 0.5 of seq2seqbert f 0.5 is 6.91. p of lasertagger ff p is 44.17. r of lasertagger ff r is 24. f 0.5 of lasertagger ff f 0.5 is 37.82. p of lasertagger ar p is 47.46. r of lasertagger ar r is 25.58. f 0.5 of lasertagger ar f 0.5 is 40.52.
table 5 shows model ablation results meteor of no reading meteor is 0.096. w-meteor of no reading w-meteor is 0.072. rouge_l of no reading rouge_l is 0.282. w-rouge_l of no reading w-rouge_l is 0.219. cider of no reading cider is 0.012. w-cider of no reading w-cider is 0.009. bleu-1 of no reading bleu-1 is 0.426. w-bleu-1 of no reading w-bleu-1 is 0.388. meteor of no reading meteor is 0.081. rouge_l of no reading rouge_l is 0.232. cider of no reading cider is 0.017. bleu-1 of no reading bleu-1 is 0.490. meteor of no prediction meteor is 0.171. w-meteor of no prediction w-meteor is 0.129. rouge_l of no prediction rouge_l is 0.307. w-rouge_l of no prediction w-rouge_l is 0.241. cider of no prediction cider is 0.024. w-cider of no prediction w-cider is 0.019. bleu-1 of no prediction bleu-1 is 0.674. w-bleu-1 of no prediction w-bleu-1 is 0.614. meteor of no prediction meteor is 0.092. rouge_l of no prediction rouge_l is 0.245. cider of no prediction cider is 0.023. bleu-1 of no prediction bleu-1 is 0.531. meteor of no sampling meteor is 0.171. w-meteor of no sampling w-meteor is 0.131. rouge_l of no sampling rouge_l is 0.303. w-rouge_l of no sampling w-rouge_l is 0.239. cider of no sampling cider is 0.026. w-cider of no sampling w-cider is 0.021. bleu-1 of no sampling bleu-1 is 0.667. w-bleu-1 of no sampling w-bleu-1 is 0.607. meteor of no sampling meteor is 0.102. rouge_l of no sampling rouge_l is 0.244. cider of no sampling cider is 0.020. bleu-1 of no sampling bleu-1 is 0.609. meteor of full model meteor is 0.181. w-meteor of full model w-meteor is 0.138. rouge_l of full model rouge_l is 0.317. w-rouge_l of full model w-rouge_l is 0.250. cider of full model cider is 0.029. w-cider of full model w-cider is 0.023. bleu-1 of full model bleu-1 is 0.721. w-bleu-1 of full model w-bleu-1 is 0.656. meteor of full model meteor is 0.107. rouge_l of full model rouge_l is 0.263. cider of full model cider is 0.024. bleu-1 of full model bleu-1 is 0.665.
table 1 shows performance of different phrase grounding methods on flickr30k entities (test set). grounding accuracy (%) of fast r-cnn (girshick 2015) grounding accuracy (%) is 42.08. grounding accuracy (%) of fast r-cnn (girshick 2015) grounding accuracy (%) is 55.85. grounding accuracy (%) of fast r-cnn (girshick 2015) grounding accuracy (%) is 65.14. grounding accuracy (%) of bottom-up attention (anderson et al. 2018) grounding accuracy (%) is 69.69. grounding accuracy (%) of bottom-up attention (anderson et al. 2018) grounding accuracy (%) is 73.3. grounding accuracy (%) of bottom-up attention (anderson et al. 2018) grounding accuracy (%) is 71.88. grounding accuracy (%) of bottom-up attention (anderson et al. 2018) grounding accuracy (%) is 72.21. grounding accuracy (%) of bottom-up attention (anderson et al. 2018) grounding accuracy (%) is 74.29. grounding accuracy (%) of bottom-up attention (anderson et al. 2018) grounding accuracy (%) is 72.26. grounding accuracy (%) of bottom-up attention (anderson et al. 2018) grounding accuracy (%) is 74.69.
table 7 shows performance of bling-kpe ablations. p@1 of no elmo p@1 is 0.27. r@1 of no elmo r@1 is 0.145. p@3 of no elmo p@3 is 0.172. r@3 of no elmo r@3 is 0.271. p@5 of no elmo p@5 is 0.132. r@5 of no elmo r@5 is 0.347. p@1 of no elmo p@1 is 0.323. r@1 of no elmo r@1 is 0.274. p@3 of no elmo p@3 is 0.189. r@3 of no elmo r@3 is 0.45. p@5 of no elmo p@5 is 0.136. r@5 of no elmo r@5 is 0.527. p@1 of no transformer p@1 is 0.389. r@1 of no transformer r@1 is 0.211. p@3 of no transformer p@3 is 0.247. r@3 of no transformer r@3 is 0.385. p@5 of no transformer p@5 is 0.189. r@5 of no transformer r@5 is 0.481. p@1 of no transformer p@1 is 0.489. r@1 of no transformer r@1 is 0.407. p@3 of no transformer p@3 is 0.258. r@3 of no transformer r@3 is 0.618. p@5 of no transformer p@5 is 0.178. r@5 of no transformer r@5 is 0.698. p@1 of no position p@1 is 0.394. r@1 of no position r@1 is 0.213. p@3 of no position p@3 is 0.247. r@3 of no position r@3 is 0.386. p@5 of no position p@5 is 0.187. r@5 of no position r@5 is 0.475. p@1 of no position p@1 is 0.543. r@1 of no position r@1 is 0.452. p@3 of no position p@3 is 0.281. r@3 of no position r@3 is 0.666. p@5 of no position p@5 is 0.191. r@5 of no position r@5 is 0.742. p@1 of no visual p@1 is 0.37. r@1 of no visual r@1 is 0.201. p@3 of no visual p@3 is 0.23. r@3 of no visual r@3 is 0.362. p@5 of no visual p@5 is 0.176. r@5 of no visual r@5 is 0.45. p@1 of no visual p@1 is 0.492. r@1 of no visual r@1 is 0.409. p@3 of no visual p@3 is 0.258. r@3 of no visual r@3 is 0.615. p@5 of no visual p@5 is 0.178. r@5 of no visual r@5 is 0.695. p@1 of no pretraining p@1 is 0.369. r@1 of no pretraining r@1 is 0.198. p@3 of no pretraining p@3 is 0.236. r@3 of no pretraining r@3 is 0.367. p@5 of no pretraining p@5 is 0.181. r@5 of no pretraining r@5 is 0.46. p@1 of no pretraining p@1 is –. r@1 of no pretraining r@1 is –. p@3 of no pretraining p@3 is –. r@3 of no pretraining r@3 is –. p@5 of no pretraining p@5 is –. r@5 of no pretraining r@5 is –. p@1 of full model p@1 is 0.404. r@1 of full model r@1 is 0.22. p@3 of full model p@3 is 0.248. r@3 of full model r@3 is 0.39. p@5 of full model p@5 is 0.188. r@5 of full model r@5 is 0.481. p@1 of full model p@1 is 0.54. r@1 of full model r@1 is 0.449. p@3 of full model p@3 is 0.275. r@3 of full model r@3 is 0.654. p@5 of full model p@5 is 0.188. r@5 of full model r@5 is 0.729.
table 6 shows the precision@top3 and the map results for the ranking list predicted by sciresrec. precision@top3 of rf (bow+tfidf) precision@top3 is 0.438. map of rf (bow+tfidf) map is 0.275. precision@top3 of rf (n-grams+tfidf) precision@top3 is 0.449. map of rf (n-grams+tfidf) map is 0.306. precision@top3 of sciresrec precision@top3 is 0.4890. map of sciresrec map is .597. precision@top3 of -function feature precision@top3 is 0.471. map of -function feature map is 0.569. precision@top3 of -role 2nd feature precision@top3 is 0.420. map of -role 2nd feature map is 0.539. precision@top3 of -role 1st feature precision@top3 is 0.399. map of -role 1st feature map is 0.497.
table 2 shows the performances of different supervised hashing models on three datasets under different lengths of hashing codes. accuracy of ksh 16bit is 0.6842. accuracy of ksh 32bit is 0.7047. accuracy of ksh 64bit is 0.7175. accuracy of ksh 128bit is 0.7243. accuracy of ksh 16bit is 0.5559. accuracy of ksh 32bit is 0.6103. accuracy of ksh 64bit is 0.6488. accuracy of ksh 128bit is 0.6638. accuracy of ksh 16bit is 0.8376. accuracy of ksh 32bit is 0.848. accuracy of ksh 64bit is 0.8537. accuracy of ksh 128bit is 0.862. accuracy of shttm 16bit is 0.6571. accuracy of shttm 32bit is 0.6485. accuracy of shttm 64bit is 0.6893. accuracy of shttm 128bit is 0.6474. accuracy of shttm 16bit is 0.3235. accuracy of shttm 32bit is 0.2357. accuracy of shttm 64bit is 0.1411. accuracy of shttm 128bit is 0.1299. accuracy of shttm 16bit is 0.852. accuracy of shttm 32bit is 0.8323. accuracy of shttm 64bit is 0.8271. accuracy of shttm 128bit is 0.815. accuracy of vdsh-s 16bit is 0.7887. accuracy of vdsh-s 32bit is 0.7883. accuracy of vdsh-s 64bit is 0.7967. accuracy of vdsh-s 128bit is 0.8018. accuracy of vdsh-s 16bit is 0.6791. accuracy of vdsh-s 32bit is 0.7564. accuracy of vdsh-s 64bit is 0.685. accuracy of vdsh-s 128bit is 0.6916. accuracy of vdsh-s 16bit is 0.9121. accuracy of vdsh-s 32bit is 0.9337. accuracy of vdsh-s 64bit is 0.9407. accuracy of vdsh-s 128bit is 0.9299. accuracy of nash-dn-s 16bit is 0.7946. accuracy of nash-dn-s 32bit is 0.7987. accuracy of nash-dn-s 64bit is 0.8014. accuracy of nash-dn-s 128bit is 0.8139. accuracy of nash-dn-s 16bit is 0.6973. accuracy of nash-dn-s 32bit is 0.8069. accuracy of nash-dn-s 64bit is 0.8213. accuracy of nash-dn-s 128bit is 0.784. accuracy of nash-dn-s 16bit is 0.9327. accuracy of nash-dn-s 32bit is 0.938. accuracy of nash-dn-s 64bit is 0.9427. accuracy of nash-dn-s 128bit is 0.9336. accuracy of gmsh-s 16bit is 0.7806. accuracy of gmsh-s 32bit is 0.7929. accuracy of gmsh-s 64bit is 0.8103. accuracy of gmsh-s 128bit is 0.8144. accuracy of gmsh-s 16bit is 0.6972. accuracy of gmsh-s 32bit is 0.7426. accuracy of gmsh-s 64bit is 0.7574. accuracy of gmsh-s 128bit is 0.769. accuracy of gmsh-s 16bit is 0.9144. accuracy of gmsh-s 32bit is 0.9175. accuracy of gmsh-s 64bit is 0.9414. accuracy of gmsh-s 128bit is 0.9522. accuracy of bmsh-s 16bit is 0.8051. accuracy of bmsh-s 32bit is 0.8247. accuracy of bmsh-s 64bit is 0.834. accuracy of bmsh-s 128bit is 0.831. accuracy of bmsh-s 16bit is 0.7316. accuracy of bmsh-s 32bit is 0.8144. accuracy of bmsh-s 64bit is 0.8216. accuracy of bmsh-s 128bit is 0.8183. accuracy of bmsh-s 16bit is 0.935. accuracy of bmsh-s 32bit is 0.964. accuracy of bmsh-s 64bit is 0.9633. accuracy of bmsh-s 128bit is 0.959.
table 2 shows word similarity results rs of none gigaword is 0.385. rs of none wikipedia is 0.368. rs of cda gigaword is 0.381. rs of cda wikipedia is 0.363. rs of gcda gigaword is 0.381. rs of gcda wikipedia is 0.363. rs of ncda gigaword is 0.380. rs of ncda wikipedia is 0.365. rs of gcds gigaword is 0.382. rs of gcds wikipedia is 0.366. rs of ncds gigaword is 0.380. rs of ncds wikipedia is 0.362. rs of wed40 gigaword is 0.386. rs of wed40 wikipedia is 0.371. rs of wed70 gigaword is 0.395. rs of wed70 wikipedia is 0.375. rs of nwed70 gigaword is 0.384. rs of nwed70 wikipedia is 0.367.
table 2 shows english all-words task results in f1 measure (%), averaged over three runs. f1 of mfs baseline se07 is 54.5. f1 of mfs baseline se2 is 65.6. f1 of mfs baseline se3 is 66.0. f1 of mfs baseline se13 is 63.8. f1 of mfs baseline se15 is 67.1. f1 of mfs baseline avg is 65.6. f1 of ims (zhong and ng, 2010) se07 is 61.3. f1 of ims (zhong and ng, 2010) se2 is 70.9. f1 of ims (zhong and ng, 2010) se3 is 69.3. f1 of ims (zhong and ng, 2010) se13 is 65.3. f1 of ims (zhong and ng, 2010) se15 is 69.5. f1 of ims (zhong and ng, 2010) avg is 68.8. f1 of ims+emb (iacobacci et al., 2016) se07 is 60.9. f1 of ims+emb (iacobacci et al., 2016) se2 is 71.0. f1 of ims+emb (iacobacci et al., 2016) se3 is 69.3. f1 of ims+emb (iacobacci et al., 2016) se13 is 67.3. f1 of ims+emb (iacobacci et al., 2016) se15 is 71.3. f1 of ims+emb (iacobacci et al., 2016) avg is 69.7. f1 of supwsd (papandrea et al., 2017) se07 is 60.2. f1 of supwsd (papandrea et al., 2017) se2 is 71.3. f1 of supwsd (papandrea et al., 2017) se3 is 68.8. f1 of supwsd (papandrea et al., 2017) se13 is 65.8. f1 of supwsd (papandrea et al., 2017) se15 is 70.0. f1 of supwsd (papandrea et al., 2017) avg is 69.0. f1 of supwsd+emb (papandrea et al., 2017) se07 is 63.1. f1 of supwsd+emb (papandrea et al., 2017) se2 is 72.7. f1 of supwsd+emb (papandrea et al., 2017) se3 is 70.6. f1 of supwsd+emb (papandrea et al., 2017) se13 is 66.8. f1 of supwsd+emb (papandrea et al., 2017) se15 is 71.8. f1 of supwsd+emb (papandrea et al., 2017) avg is 70.5. f1 of bilstmatt+lex (raganato et al., 2017b) se07 is 63.7. f1 of bilstmatt+lex (raganato et al., 2017b) se2 is 72.0. f1 of bilstmatt+lex (raganato et al., 2017b) se3 is 69.4. f1 of bilstmatt+lex (raganato et al., 2017b) se13 is 66.4. f1 of bilstmatt+lex (raganato et al., 2017b) se15 is 72.4. f1 of bilstmatt+lex (raganato et al., 2017b) avg is 70.1. f1 of gasext concat (luo et al., 2018) se07 is â€“. f1 of gasext concat (luo et al., 2018) se2 is 72.2. f1 of gasext concat (luo et al., 2018) se3 is 70.5. f1 of gasext concat (luo et al., 2018) se13 is 67.2. f1 of gasext concat (luo et al., 2018) se15 is 72.6. f1 of gasext concat (luo et al., 2018) avg is 70.6. f1 of context2vec (melamud et al., 2016) se07 is 61.3. f1 of context2vec (melamud et al., 2016) se2 is 71.8. f1 of context2vec (melamud et al., 2016) se3 is 69.1. f1 of context2vec (melamud et al., 2016) se13 is 65.6. f1 of context2vec (melamud et al., 2016) se15 is 71.9. f1 of context2vec (melamud et al., 2016) avg is 69.6. f1 of elmo (peters et al., 2018) se07 is 62.2. f1 of elmo (peters et al., 2018) se2 is 71.6. f1 of elmo (peters et al., 2018) se3 is 69.6. f1 of elmo (peters et al., 2018) se13 is 66.2. f1 of elmo (peters et al., 2018) se15 is 71.3. f1 of elmo (peters et al., 2018) avg is 69.7. f1 of 1nn (1sent) se07 is 64.0. f1 of 1nn (1sent) se2 is 73.0. f1 of 1nn (1sent) se3 is 69.7. f1 of 1nn (1sent) se13 is 67.8. f1 of 1nn (1sent) se15 is 73.3. f1 of 1nn (1sent) avg is 71.0. f1 of 1nn (1sent+1sur) se07 is 63.3. f1 of 1nn (1sent+1sur) se2 is 73.8. f1 of 1nn (1sent+1sur) se3 is 71.6. f1 of 1nn (1sent+1sur) se13 is 69.2. f1 of 1nn (1sent+1sur) se15 is 74.4. f1 of 1nn (1sent+1sur) avg is 72.3. f1 of simple (1sent) se07 is 67.0. f1 of simple (1sent) se2 is 75.0. f1 of simple (1sent) se3 is 71.6. f1 of simple (1sent) se13 is 69.7. f1 of simple (1sent) se15 is 74.4. f1 of simple (1sent) avg is 72.7. f1 of simple (1sent+1sur) se07 is 69.3*. f1 of simple (1sent+1sur) se2 is 75.9*. f1 of simple (1sent+1sur) se3 is 73.4. f1 of simple (1sent+1sur) se13 is 70.4*. f1 of simple (1sent+1sur) se15 is 75.1. f1 of simple (1sent+1sur) avg is 73.7*. f1 of lw (1sent) se07 is 66.7. f1 of lw (1sent) se2 is 75.0. f1 of lw (1sent) se3 is 71.6. f1 of lw (1sent) se13 is 69.9. f1 of lw (1sent) se15 is 74.2. f1 of lw (1sent) avg is 72.7. f1 of lw (1sent+1sur) se07 is 69.0*. f1 of lw (1sent+1sur) se2 is 76.4*. f1 of lw (1sent+1sur) se3 is 74.0*. f1 of lw (1sent+1sur) se13 is 70.1*. f1 of lw (1sent+1sur) se15 is 75.0. f1 of lw (1sent+1sur) avg is 73.9*. f1 of glu (1sent) se07 is 64.9. f1 of glu (1sent) se2 is 74.1. f1 of glu (1sent) se3 is 71.6. f1 of glu (1sent) se13 is 69.8. f1 of glu (1sent) se15 is 74.3. f1 of glu (1sent) avg is 72.5. f1 of glu (1sent+1sur) se07 is 68.1*. f1 of glu (1sent+1sur) se2 is 75.5*. f1 of glu (1sent+1sur) se3 is 73.6*. f1 of glu (1sent+1sur) se13 is 71.1*. f1 of glu (1sent+1sur) se15 is 76.2*. f1 of glu (1sent+1sur) avg is 74.1*. f1 of glu+lw (1sent) se07 is 65.7. f1 of glu+lw (1sent) se2 is 74.0. f1 of glu+lw (1sent) se3 is 70.9. f1 of glu+lw (1sent) se13 is 68.8. f1 of glu+lw (1sent) se15 is 73.6. f1 of glu+lw (1sent) avg is 71.8. f1 of glu+lw (1sent+1sur) se07 is 68.5*. f1 of glu+lw (1sent+1sur) se2 is 75.5*. f1 of glu+lw (1sent+1sur) se3 is 73.4*. f1 of glu+lw (1sent+1sur) se13 is 71.0*. f1 of glu+lw (1sent+1sur) se15 is 76.2*. f1 of glu+lw (1sent+1sur) avg is 74.0*.
table 1 shows symacc, bleu and ansacc on the followup dataset. symacc (%) of seq2seqê(bahdanau et al., 2015) symacc (%) is 0.63±0.00. bleu (%) of seq2seqê(bahdanau et al., 2015) bleu (%) is 21.34±1.14. symacc (%) of seq2seqê(bahdanau et al., 2015) symacc (%) is 0.50±0.22. bleu (%) of seq2seqê(bahdanau et al., 2015) bleu (%) is 20.72±1.31. symacc (%) of copynetê(gu et al., 2016) symacc (%) is 17.50±0.87. bleu (%) of copynetê(gu et al., 2016) bleu (%) is 43.36±0.54. symacc (%) of copynetê(gu et al., 2016) symacc (%) is 19.30±0.93. bleu (%) of copynetê(gu et al., 2016) bleu (%) is 43.34±0.45. symacc (%) of copy+bert (devlin et al., 2019) symacc (%) is 18.63±0.61. bleu (%) of copy+bert (devlin et al., 2019) bleu (%) is 45.14±0.68. symacc (%) of copy+bert (devlin et al., 2019) symacc (%) is 22.00±0.45. bleu (%) of copy+bert (devlin et al., 2019) bleu (%) is 44.87±0.52. symacc (%) of concat symacc (%) is 22.00±-. bleu (%) of concat bleu (%) is 52.02±-. ansacc (%) of concat ansacc (%) is 25.24. symacc (%) of e2ecrê(lee et al., 2017) symacc (%) is 27.00±-. bleu (%) of e2ecrê(lee et al., 2017) bleu (%) is 52.47±-. ansacc (%) of e2ecrê(lee et al., 2017) ansacc (%) is 27.18. symacc (%) of fanda (liu et al., 2019) symacc (%) is 49.00±1.28. bleu (%) of fanda (liu et al., 2019) bleu (%) is 60.14±0.98. symacc (%) of fanda (liu et al., 2019) symacc (%) is 47.80±1.14. bleu (%) of fanda (liu et al., 2019) bleu (%) is 59.02±0.54. ansacc (%) of fanda (liu et al., 2019) ansacc (%) is 60.19. symacc (%) of star symacc (%) is 55.38±1.21. bleu (%) of star bleu (%) is 67.62±0.65. symacc (%) of star symacc (%) is 54.00±1.09. bleu (%) of star bleu (%) is 67.05±1.05. ansacc (%) of star ansacc (%) is 65.05.
table 3 shows semantic f1-score on conll-2009 in-domain test set. f1-score of conll-2009 st best system catalan is 80.3. f1-score of conll-2009 st best system chinese is 78.6. f1-score of conll-2009 st best system czech is 85.4. f1-score of conll-2009 st best system english is 85.6. f1-score of conll-2009 st best system german is 79.7. f1-score of conll-2009 st best system japanese is 78.2. f1-score of conll-2009 st best system spanish is 80.5. f1-score of zhao et al. (2009a) catalan is 80.3. f1-score of zhao et al. (2009a) chinese is 77.7. f1-score of zhao et al. (2009a) czech is 85.2. f1-score of zhao et al. (2009a) english is 86.2. f1-score of zhao et al. (2009a) german is 76.0. f1-score of zhao et al. (2009a) japanese is 78.2. f1-score of zhao et al. (2009a) spanish is 80.5. f1-score of roth and lapata (2016) catalan is −. f1-score of roth and lapata (2016) chinese is 79.4. f1-score of roth and lapata (2016) czech is −. f1-score of roth and lapata (2016) english is 87.7. f1-score of roth and lapata (2016) german is 80.1. f1-score of roth and lapata (2016) japanese is −. f1-score of roth and lapata (2016) spanish is 80.2. f1-score of marcheggiani et al. (2017) catalan is −. f1-score of marcheggiani et al. (2017) chinese is 81.2. f1-score of marcheggiani et al. (2017) czech is 86.0. f1-score of marcheggiani et al. (2017) english is 87.7. f1-score of marcheggiani et al. (2017) german is −. f1-score of marcheggiani et al. (2017) japanese is −. f1-score of marcheggiani et al. (2017) spanish is 80.3. f1-score of li et al. (2019) catalan is −. f1-score of li et al. (2019) chinese is −. f1-score of li et al. (2019) czech is −. f1-score of li et al. (2019) english is 90.4. f1-score of li et al. (2019) german is −. f1-score of li et al. (2019) japanese is −. f1-score of li et al. (2019) spanish is −. f1-score of the best previously published catalan is 80.3. f1-score of the best previously published chinese is 84.3. f1-score of the best previously published czech is 86.0. f1-score of the best previously published english is 90.4. f1-score of the best previously published german is 80.1. f1-score of the best previously published japanese is 78.2. f1-score of the best previously published spanish is 80.5. f1-score of our baseline catalan is 84.07. f1-score of our baseline chinese is 84.05. f1-score of our baseline czech is 88.35. f1-score of our baseline english is 89.61. f1-score of our baseline german is 78.36. f1-score of our baseline japanese is 83.08. f1-score of our baseline spanish is 83.47.
table 3 shows conll-2003 named entity recognition results. dev f1 of elmobase dev f1 is 95.7. test f1 of elmobase test f1 is 92.2. dev f1 of cnn large + elmo dev f1 is 96.4. test f1 of cnn large + elmo test f1 is 93.2. dev f1 of cnn large + fine-tune dev f1 is 96.9. test f1 of cnn large + fine-tune test f1 is 93.5. dev f1 of bertbase dev f1 is 96.4. test f1 of bertbase test f1 is 92.4. dev f1 of bertlarge dev f1 is 96.6. test f1 of bertlarge test f1 is 92.8.
table 4 shows penn treebank constituency parsing results. dev f1 of elmobase dev f1 is 95.2. test f1 of elmobase test f1 is 95.1. dev f1 of cnn large + elmo dev f1 is 95.1. test f1 of cnn large + elmo test f1 is 95.2. dev f1 of cnn large + fine-tune dev f1 is 95.5. test f1 of cnn large + fine-tune test f1 is 95.6.
table 5 shows different loss functions on the development sets of glue (cf. loss of cloze cola (mcc) is 55.1. loss of cloze sst-2 (acc) is 92.9. loss of cloze mrpc (f1) is 88.3. loss of cloze sts-b (scc) is 88.3. loss of cloze qqp (f1) is 87.2. loss of cloze mnli-m (acc) is 82.3. loss of cloze qnli (acc) is 86.5. loss of cloze rte (acc) is 66.4. loss of cloze avg is 80.9. loss of bilm cola (mcc) is 50. loss of bilm sst-2 (acc) is 92.4. loss of bilm mrpc (f1) is 86.6. loss of bilm sts-b (scc) is 87.1. loss of bilm qqp (f1) is 86.1. loss of bilm mnli-m (acc) is 81.7. loss of bilm qnli (acc) is 84. loss of bilm rte (acc) is 66.4. loss of bilm avg is 79.3. loss of cloze + bilm cola (mcc) is 52.6. loss of cloze + bilm sst-2 (acc) is 93.2. loss of cloze + bilm mrpc (f1) is 88.9. loss of cloze + bilm sts-b (scc) is 87.9. loss of cloze + bilm qqp (f1) is 87.2. loss of cloze + bilm mnli-m (acc) is 82.1. loss of cloze + bilm qnli (acc) is 86.1. loss of cloze + bilm rte (acc) is 65.5. loss of cloze + bilm avg is 80.4.
table 2 shows results on trecqa, twitterurl, and quora. map of infersent map is 0.521. mrr of infersent mrr is 0.559. macro-f1 of infersent macro-f1 is 0.797. acc of infersent acc is 0.866. map of decatt map is 0.660. mrr of decatt mrr is 0.712. macro-f1 of decatt macro-f1 is 0.785. acc of decatt acc is 0.845. map of esimseq map is 0.771. mrr of esimseq mrr is 0.795. macro-f1 of esimseq macro-f1 is 0.822. acc of esimseq acc is 0.850. map of esimtree map is 0.698. mrr of esimtree mrr is 0.734. acc of esimtree acc is 0.755. map of esimseq+tree map is 0.749. mrr of esimseq+tree mrr is 0.768. acc of esimseq+tree acc is 0.854. map of pwim map is 0.739. mrr of pwim mrr is 0.795. macro-f1 of pwim macro-f1 is 0.809. acc of pwim acc is 0.834. map of rao et al. (2016) map is 0.780. mrr of rao et al. (2016) mrr is 0.834. acc of gong et al. (2018) acc is 0.891. map of bert map is 0.838. mrr of bert mrr is 0.887. macro-f1 of bert macro-f1 is 0.852. acc of bert acc is 0.892. map of rm map is 0.756. mrr of rm mrr is 0.812. macro-f1 of rm macro-f1 is 0.790. acc of rm acc is 0.842. map of sm map is 0.663. mrr of sm mrr is 0.725. macro-f1 of sm macro-f1 is 0.708. acc of sm acc is 0.817. map of hcan map is 0.774. mrr of hcan mrr is 0.843. macro-f1 of hcan macro-f1 is 0.817. acc of hcan acc is 0.853.
table 1 shows experimental results of syntax-aware methods we compare on cpb1.0 dataset. p of baseline p is 81.52. r of baseline r is 82.17. f1 of baseline f1 is 81.85. p of baseline p is 80.95. r of baseline r is 80.01. f1 of baseline f1 is 80.48. p of baseline + dep (tree-gru) p is 82.35. r of baseline + dep (tree-gru) r is 80.24. f1 of baseline + dep (tree-gru) f1 is 81.28. p of baseline + dep (tree-gru) p is 82.1. r of baseline + dep (tree-gru) r is 78.11. f1 of baseline + dep (tree-gru) f1 is 80.06. p of baseline + dep (fir) p is 83.56. r of baseline + dep (fir) r is 83.05. f1 of baseline + dep (fir) f1 is 83.3. p of baseline + dep (fir) p is 83.38. r of baseline + dep (fir) r is 81.93. f1 of baseline + dep (fir) f1 is 82.65. p of baseline + dep (hps) p is 82.58. r of baseline + dep (hps) r is 84.15. f1 of baseline + dep (hps) f1 is 83.36. p of baseline + dep (hps) p is 83.22. r of baseline + dep (hps) r is 83.81. f1 of baseline + dep (hps) f1 is 83.51. p of baseline + dep (iir) p is 83.12. r of baseline + dep (iir) r is 83.66. f1 of baseline + dep (iir) f1 is 83.39. p of baseline + dep (iir) p is 84.49. r of baseline + dep (iir) r is 83.34. f1 of baseline + dep (iir) f1 is 83.91.
table 2 shows results and comparison with previous works on cpb1.0 test set. f1 of sun et al. (2009) f1 is 74.12. f1 of wang et al. (2015b) f1 is 77.59. f1 of sha et al. (2016) f1 is 77.69. f1 of xia et al. (2017) f1 is 79.67. f1 of baseline f1 is 80.48. f1 of baseline + dep (hps) f1 is 83.51. f1 of baseline + dep (iir) f1 is 83.91. f1 of baseline + bert f1 is 86.62. f1 of baseline + bert + dep (hps) f1 is 87.03. f1 of baseline + bert + dep (iir) f1 is 87.54.
table 4 shows results and comparison with previous works on conll-2009 chinese test set. p of roth and lapata (2016) p is 83.2. r of roth and lapata (2016) r is 75.9. f1 of roth and lapata (2016) f1 is 79.4. p of marcheggiani et al. (2017) p is 84.6. r of marcheggiani et al. (2017) r is 80.4. f1 of marcheggiani et al. (2017) f1 is 82.5. p of he et al. (2018b) p is 84.2. r of he et al. (2018b) r is 81.5. f1 of he et al. (2018b) f1 is 82.8. p of cai et al. (2018) p is 84.7. r of cai et al. (2018) r is 84.0. f1 of cai et al. (2018) f1 is 84.3. p of baseline p is 83.7. r of baseline r is 84.8. f1 of baseline f1 is 84.2. p of baseline + dep (iir) p is 84.6. r of baseline + dep (iir) r is 85.7. f1 of baseline + dep (iir) f1 is 85.1. p of baseline + bert p is 87.8. r of baseline + bert r is 89.2. f1 of baseline + bert f1 is 88.5. p of baseline + bert + dep (iir) p is 88.0. r of baseline + bert + dep (iir) r is 89.1. f1 of baseline + bert + dep (iir) f1 is 88.5.
table 3 shows glue test results scored by the glue evaluation server. accuracy of bert-base mrpc is 88.3. accuracy of bert-base sts-b is 84.7. accuracy of bert-base qqp is 71.2. accuracy of bert-base mnli (m/mm) is 84.3/83.0. accuracy of bert-base rte is 59.8. accuracy of bert-base qnli is 89.1. accuracy of bert-base sst is 93.3. accuracy of bert-base cola is 52.7. accuracy of bert-large mrpc is 88.6. accuracy of bert-large sts-b is 86.0. accuracy of bert-large qqp is 72.1. accuracy of bert-large mnli (m/mm) is 86.2/85.5. accuracy of bert-large rte is 65.5. accuracy of bert-large qnli is 92.7. accuracy of bert-large sst is 94.1. accuracy of bert-large cola is 55.7. accuracy of transfer fine-tuning mrpc is 89.2. accuracy of transfer fine-tuning sts-b is 87.4. accuracy of transfer fine-tuning qqp is 71.2. accuracy of transfer fine-tuning mnli (m/mm) is 83.9/83.1. accuracy of transfer fine-tuning rte is 64.8. accuracy of transfer fine-tuning qnli is 89.3. accuracy of transfer fine-tuning sst is 93.1. accuracy of transfer fine-tuning cola is 47.2.
table 2 shows performance improvement of the neural semantic parser on spider with different hardness levels. easy (%) of syntaxsqlnet (yu et al., 2018b) easy (%) is 38.4. medium (%) of syntaxsqlnet (yu et al., 2018b) medium (%) is 15.0. hard (%) of syntaxsqlnet (yu et al., 2018b) hard (%) is 16.1. extra hard (%) of syntaxsqlnet (yu et al., 2018b) extra hard (%) is 3.5. all (%) of syntaxsqlnet (yu et al., 2018b) all (%) is 18.9. easy (%) of syntaxsqlnet + dae easy (%) is 39.6(+1.2). medium (%) of syntaxsqlnet + dae medium (%) is 18.2(+3.2). hard (%) of syntaxsqlnet + dae hard (%) is 20.7(+4.6). extra hard (%) of syntaxsqlnet + dae extra hard (%) is 7.6(+4.1). all (%) of syntaxsqlnet + dae all (%) is 22.1(+3.2). easy (%) of syntaxsqlnetaug (yu et al., 2018b) easy (%) is 44.4. medium (%) of syntaxsqlnetaug (yu et al., 2018b) medium (%) is 23.0. hard (%) of syntaxsqlnetaug (yu et al., 2018b) hard (%) is 23.0. extra hard (%) of syntaxsqlnetaug (yu et al., 2018b) extra hard (%) is 2.9. all (%) of syntaxsqlnetaug (yu et al., 2018b) all (%) is 24.9. easy (%) of syntaxsqlnetaug + dae easy (%) is 44.8(+0.4). medium (%) of syntaxsqlnetaug + dae medium (%) is 27.0(+4.0). hard (%) of syntaxsqlnetaug + dae hard (%) is 24.1(+1.1). extra hard (%) of syntaxsqlnetaug + dae extra hard (%) is 5.9(+3.0). all (%) of syntaxsqlnetaug + dae all (%) is 27.4(+2.5).
table 5 shows performances of different anonymization models on wikisql. accsc of typesql (yu et al., 2018a) accsc is 75.9. accoc of typesql (yu et al., 2018a) accoc is 92.9. accsc of typesql (yu et al., 2018a) accsc is 76.0. accoc of typesql (yu et al., 2018a) accoc is 92.9. accsc of annotatedseq2seq (wang et al., 2018b) accsc is 88.8. accoc of annotatedseq2seq (wang et al., 2018b) accoc is 64.6. accsc of annotatedseq2seq (wang et al., 2018b) accsc is 88.8. accoc of annotatedseq2seq (wang et al., 2018b) accoc is 63.6. accsc of dae accsc is 92.6. accoc of dae accoc is 93.6. accce of dae accce is 86.7. accsc of dae accsc is 92.0. accoc of dae accoc is 93.7. accce of dae accce is 86.2.
table 5 shows labeled f1 score (including senses) for all languages on the conll-2009 in-domain test sets. f1 of previous best single model ja is 78.69. f1 of previous best single model es is 80.50. f1 of previous best single model ca is 80.32. f1 of previous best single model de is 80.10. f1 of previous best single model cz is 86.02. f1 of previous best single model zh is 84.30. f1 of previous best single model en is 90.40. f1 of previous best single model avg. is 82.90. f1 of baseline model ja is 80.12. f1 of baseline model es is 81.0. f1 of baseline model ca is 81.39. f1 of baseline model de is 76.01. f1 of baseline model cz is 87.79. f1 of baseline model zh is 81.05. f1 of baseline model en is 90.49. f1 of baseline model avg. is 82.55. f1 of capsulenet srl (this work) ja is 81.26. f1 of capsulenet srl (this work) es is 81.32. f1 of capsulenet srl (this work) ca is 81.65. f1 of capsulenet srl (this work) de is 76.44. f1 of capsulenet srl (this work) cz is 88.08. f1 of capsulenet srl (this work) zh is 81.65. f1 of capsulenet srl (this work) en is 91.06. f1 of capsulenet srl (this work) avg. is 83.07.
table 1 shows exact match and bleu scores for our simplified model (iyer-simp) with and without idioms, compared with results from iyer et al. exact of seq2seq exact is 3.2 (2.9). bleu of seq2seq bleu is 23.5 (21.0). exact of seq2prod exact is 6.7 (5.6). bleu of seq2prod bleu is 21.3 (20.6). exact of iyer et al. (2018)â€  exact is 8.6 (7.1). bleu of iyer et al. (2018)â€  bleu is 22.1 (21.3). exact of iyer-simp exact is 12.5 (9.8). bleu of iyer-simp bleu is 24.4 (23.2). exact of iyer-simp + 200 idioms exact is 12.2 (9.8). bleu of iyer-simp + 200 idioms bleu is 26.6 (24.0).
table 5 shows human evaluation on 100 random examples for misp-sql agents based on sqlnet, sqlova and syntaxsqlnet, respectively. accqm/em of no interaction accqm/em is 0.580. accex of no interaction accex is 0.660. avg. #q of no interaction avg. #q is n/a. accqm/em of misp-sql (simulation) accqm/em is 0.770. accex of misp-sql (simulation) accex is 0.810. avg. #q of misp-sql (simulation) avg. #q is 1.800. accqm/em of misp-sql (real user) accqm/em is 0.633. accex of misp-sql (real user) accex is 0.717. avg. #q of misp-sql (real user) avg. #q is 1.510. accqm/em of no interaction accqm/em is 0.830. accex of no interaction accex is 0.890. avg. #q of no interaction avg. #q is n/a. accqm/em of misp-sql (simulation) accqm/em is 0.920. accex of misp-sql (simulation) accex is 0.950. avg. #q of misp-sql (simulation) avg. #q is 0.550. accqm/em of misp-sql (real user) accqm/em is 0.837. accex of misp-sql (real user) accex is 0.880. avg. #q of misp-sql (real user) avg. #q is 0.533. accqm/em of + w/ full info. accqm/em is 0.907. accex of + w/ full info. accex is 0.937. avg. #q of + w/ full info. avg. #q is 0.547. accqm/em of no interaction accqm/em is 0.180. accex of no interaction accex is n/a. avg. #q of no interaction avg. #q is n/a. accqm/em of misp-sql (simulation) accqm/em is 0.290. accex of misp-sql (simulation) accex is n/a. avg. #q of misp-sql (simulation) avg. #q is 2.730. accqm/em of misp-sql (real user) accqm/em is 0.230. accex of misp-sql (real user) accex is n/a. avg. #q of misp-sql (real user) avg. #q is 2.647.
table 2 shows ablation results of our baseline system on the ldc2015e86 development set. bleu of baseline bleu is 24.93. meteor of baseline meteor is 33.2. chrf++ of baseline chrf++ is 60.3. bleu of -bpe bleu is 23.02. meteor of -bpe meteor is 31.6. chrf++ of -bpe chrf++ is 58.09. bleu of -share vocab. bleu is 23.24. meteor of -share vocab. meteor is 31.78. chrf++ of -share vocab. chrf++ is 58.43. bleu of -both bleu is 18.77. meteor of -both meteor is 28.04. chrf++ of -both chrf++ is 51.88.
table 3 shows comparison results of our approaches and related studies on the test sets of ldc2015e86 and ldc2017t10. bleu of baseline bleu is 25.50. meteor of baseline meteor is 33.16. chrf++ of baseline chrf++ is 59.88. #p (m) of baseline #p (m) is 49.1. bleu of baseline bleu is 27.43. meteor of baseline meteor is 34.62. chrf++ of baseline chrf++ is 61.85. bleu of feature-based bleu is 27.23. meteor of feature-based meteor is 34.53. chrf++ of feature-based chrf++ is 61.55. #p (m) of feature-based #p (m) is 49.4. bleu of feature-based bleu is 30.18. meteor of feature-based meteor is 35.83. chrf++ of feature-based chrf++ is 63.20. bleu of avg-based bleu is 28.37. meteor of avg-based meteor is 35.10. chrf++ of avg-based chrf++ is 62.29. #p (m) of avg-based #p (m) is 49.1. bleu of avg-based bleu is 29.56. meteor of avg-based meteor is 35.24. chrf++ of avg-based chrf++ is 62.86. bleu of sum-based bleu is 28.69. meteor of sum-based meteor is 34.97. chrf++ of sum-based chrf++ is 62.05. #p (m) of sum-based #p (m) is 49.1. bleu of sum-based bleu is 29.92. meteor of sum-based meteor is 35.68. chrf++ of sum-based chrf++ is 63.04. bleu of sa-based bleu is 29.66. meteor of sa-based meteor is 35.45. chrf++ of sa-based chrf++ is 63.00. #p (m) of sa-based #p (m) is 49.3. bleu of sa-based bleu is 31.54. meteor of sa-based meteor is 36.02. chrf++ of sa-based chrf++ is 63.84. bleu of cnn-based bleu is 29.10. meteor of cnn-based meteor is 35.00. chrf++ of cnn-based chrf++ is 62.10. #p (m) of cnn-based #p (m) is 49.2. bleu of cnn-based bleu is 31.82. meteor of cnn-based meteor is 36.38. chrf++ of cnn-based chrf++ is 64.05. bleu of konstas et al. (2017) bleu is 22.00. bleu of cao and clark (2019) bleu is 23.5. bleu of cao and clark (2019) bleu is 26.8. bleu of song et al. (2018) bleu is 23.30. bleu of beck et al. (2018) bleu is 23.3. chrf++ of beck et al. (2018) chrf++ is 50.4. bleu of damonte and cohen (2019) bleu is 24.40. meteor of damonte and cohen (2019) meteor is 23.60. bleu of damonte and cohen (2019) bleu is 24.54. meteor of damonte and cohen (2019) meteor is 24.07. bleu of guo et al. (2019) bleu is 25.7. bleu of guo et al. (2019) bleu is 27.6. chrf++ of guo et al. (2019) chrf++ is 57.3. bleu of song et al. (2016) bleu is 22.44. bleu of konstas et al. (2017) bleu is 33.8. bleu of song et al. (2018) bleu is 33.0. bleu of beck et al. (2018) bleu is 27.5. chrf++ of beck et al. (2018) chrf++ is 53.5. bleu of guo et al. (2019) bleu is 35.3.
table 4 shows performance on the test set of our approach with or without modeling structural information of indirectly connected concept pairs. bleu of baseline bleu is 27.43. bleu of our approach bleu is 31.82. bleu of no indirectly connected concept pairs bleu is 29.92.
table 7 shows effects of different replacement actions. accuracy of all words (rnn) sst-2 is 81.60. accuracy of all words (rnn) sst-5 is 41.14. accuracy of all words (rnn) rt is 75.76. accuracy of all words (rnn) average is 66.17. accuracy of -super words sst-2 is 80.72. accuracy of -super words sst-5 is 41.17. accuracy of -super words rt is 75.02. accuracy of -super words average is 65.64. accuracy of -subordinate words sst-2 is 80.56. accuracy of -subordinate words sst-5 is 41.67. accuracy of -subordinate words rt is 75.48. accuracy of -subordinate words average is 65.90. accuracy of -synonymy words sst-2 is 80.45. accuracy of -synonymy words sst-5 is 41.99. accuracy of -synonymy words rt is 76.12. accuracy of -synonymy words average is 66.19. accuracy of -neighbor words sst-2 is 80.56. accuracy of -neighbor words sst-5 is 40.91. accuracy of -neighbor words rt is 74.66. accuracy of -neighbor words average is 65.38. accuracy of all words (cnn) sst-2 is 80.18. accuracy of all words (cnn) sst-5 is 41.86. accuracy of all words (cnn) rt is 74.84. accuracy of all words (cnn) average is 65.63. accuracy of -super words sst-2 is 79.96. accuracy of -super words sst-5 is 41.67. accuracy of -super words rt is 76.22. accuracy of -super words average is 65.95. accuracy of -subordinate words sst-2 is 81.58. accuracy of -subordinate words sst-5 is 41.49. accuracy of -subordinate words rt is 75.39. accuracy of -subordinate words average is 66.15. accuracy of -synonymy words sst-2 is 79.24. accuracy of -synonymy words sst-5 is 41.67. accuracy of -synonymy words rt is 74.74. accuracy of -synonymy words average is 65.22. accuracy of -neighbor words sst-2 is 81.33. accuracy of -neighbor words sst-5 is 40.68. accuracy of -neighbor words rt is 74.47. accuracy of -neighbor words average is 65.49.
table 1 shows summary of the evaluation datasets. # of reviews of hoteluser # of reviews is 28165. # of sentences of hoteluser # of sentences is 362153. sentence/review of hoteluser sentence/review is 13. words/sentence of hoteluser words/sentence is 8. # of reviews of resuser # of reviews is 23873. # of sentences of resuser # of sentences is 276008. sentence/review of resuser sentence/review is 12. words/sentence of resuser words/sentence is 7. # of reviews of hoteltype # of reviews is 22984. # of sentences of hoteltype # of sentences is 302920. sentence/review of hoteltype sentence/review is 13. words/sentence of hoteltype words/sentence is 7. # of reviews of hotelloc # of reviews is 136446. # of sentences of hotelloc # of sentences is 1428722. sentence/review of hotelloc sentence/review is 10. words/sentence of hotelloc words/sentence is 7.
table 3 shows this table reports performance (accuracy) on the mr and sst data sets. accuracy of bow (generic) mr is 75.7*. accuracy of bow (generic) sst is 48.9*. accuracy of bow (da embeddings) mr is 77.0*. accuracy of bow (da embeddings) sst is 49.2*. accuracy of vanilla cnn mr is 72.5*. accuracy of vanilla cnn sst is 49.06*. accuracy of vanilla bilstm mr is 81.8*. accuracy of vanilla bilstm sst is 50.3*. accuracy of lr-bi-lstm mr is 82.1. accuracy of lr-bi-lstm sst is 50.6. accuracy of self-attention mr is 81.7. accuracy of self-attention sst is 48.9. accuracy of adapted cnn mr is 80.8*. accuracy of adapted cnn sst is 50.0*. accuracy of adapted bilstm mr is 83.1*. accuracy of adapted bilstm sst is 51.2. accuracy of bert mr is 74.4*. accuracy of bert sst is 51.5.
table 2 shows experimental results on the chinese dataset. p of rb* p is 0.6747. r of rb* r is 0.4287. f of rb* f is 0.5243. p of cb* p is 0.2672. r of cb* r is 0.713. f of cb* f is 0.3887. p of svm* p is 0.42. r of svm* r is 0.4375. f of svm* f is 0.4285. p of word2vec* p is 0.4301. r of word2vec* r is 0.4233. f of word2vec* f is 0.4136. p of multi-kernel* p is 0.6588. r of multi-kernel* r is 0.6972. f of multi-kernel* f is 0.6752. p of lambdamart* p is 0.772. r of lambdamart* r is 0.7499. f of lambdamart* f is 0.7608. p of cnn* p is 0.6472. r of cnn* r is 0.5493. f of cnn* f is 0.5915. p of convms-memnet* p is 0.7076. r of convms-memnet* r is 0.6838. f of convms-memnet* f is 0.6955. p of cann p is 0.7721. r of cann r is 0.6891. f of cann f is 0.7266. p of hcs p is 0.7388. r of hcs r is 0.7154. f of hcs f is 0.7269. p of mann p is 0.7843. r of mann r is 0.7587. f of mann f is 0.7706. p of rhnn p is 0.8112. r of rhnn r is 0.7725. f of rhnn f is 0.7914.
table 6 shows fragment-level experiments (flc task). p of bert p is 39.57. r of bert r is 36.42. f1 of bert f1 is 37.9. p of bert p is 21.48. r of bert r is 21.39. f1 of bert f1 is 21.39. p of joint p is 39.26. r of joint r is 35.48. f1 of joint f1 is 37.25. p of joint p is 20.11. r of joint r is 19.74. f1 of joint f1 is 19.92. p of granu p is 43.08. r of granu r is 33.98. f1 of granu f1 is 37.93. p of granu p is 23.85. r of granu r is 20.14. f1 of granu f1 is 21.8. p of relu p is 43.29. r of relu r is 34.74. f1 of relu f1 is 38.28. p of relu p is 23.98. r of relu r is 20.33. f1 of relu f1 is 21.82. p of sigmoid p is 44.12. r of sigmoid r is 35.01. f1 of sigmoid f1 is 38.98. p of sigmoid p is 24.42. r of sigmoid r is 21.05. f1 of sigmoid f1 is 22.58.
table 2 shows performance comparison on different models on the benchmark datasets. acc of sota acc is 81.6. f1 of sota f1 is 71.91. acc of sota acc is 76.54. f1 of sota f1 is 71.75. acc of sota acc is 74.97. f1 of sota f1 is 73.6. acc of sota acc is 85.58. f1 of sota f1 is 69.76. acc of cnn+position acc is 79.37. f1 of cnn+position f1 is 68.64. acc of cnn+position acc is 72.73. f1 of cnn+position f1 is 68.28. acc of cnn+position acc is 72.69. f1 of cnn+position f1 is 70.92. acc of cnn+position acc is 84.63. f1 of cnn+position f1 is 64.75. acc of lstm+position acc is 77.59. f1 of lstm+position f1 is 67.05. acc of lstm+position acc is 70.06. f1 of lstm+position f1 is 64.46. acc of lstm+position acc is 71.39. f1 of lstm+position f1 is 69.45. acc of lstm+position acc is 83.47. f1 of lstm+position f1 is 62.69. acc of cnn+att acc is 79.46. f1 of cnn+att f1 is 69.44. acc of cnn+att acc is 70.53. f1 of cnn+att f1 is 64.27. acc of cnn+att acc is 73.12. f1 of cnn+att f1 is 71.01. acc of cnn+att acc is 84.28. f1 of cnn+att f1 is 60.86. acc of tnet (li et al., 2018a) acc is 80.79. f1 of tnet (li et al., 2018a) f1 is 70.84. acc of tnet (li et al., 2018a) acc is 76.54. f1 of tnet (li et al., 2018a) f1 is 71.75. acc of tnet (li et al., 2018a) acc is 74.97. f1 of tnet (li et al., 2018a) f1 is 73.6. acc of pret+mult (he et al., 2018b) acc is 79.11. f1 of pret+mult (he et al., 2018b) f1 is 69.73. acc of pret+mult (he et al., 2018b) acc is 71.15. f1 of pret+mult (he et al., 2018b) f1 is 67.46. acc of pret+mult (he et al., 2018b) acc is 85.58. f1 of pret+mult (he et al., 2018b) f1 is 69.76. acc of sa-lstm-p (wang and lu, 2018) acc is 81.6. acc of sa-lstm-p (wang and lu, 2018) acc is 75.1. acc of sa-lstm-p (wang and lu, 2018) acc is 69. acc of lstm+synatt+tarrep (he et al., 2018a) acc is 80.63. f1 of lstm+synatt+tarrep (he et al., 2018a) f1 is 71.32. acc of lstm+synatt+tarrep (he et al., 2018a) acc is 71.94. f1 of lstm+synatt+tarrep (he et al., 2018a) f1 is 69.23. acc of lstm+synatt+tarrep (he et al., 2018a) acc is 84.61. f1 of lstm+synatt+tarrep (he et al., 2018a) f1 is 67.45. acc of mgan (fan et al., 2018b) acc is 81.25. f1 of mgan (fan et al., 2018b) f1 is 71.94. acc of mgan (fan et al., 2018b) acc is 75.39. f1 of mgan (fan et al., 2018b) f1 is 72.47. acc of mgan (fan et al., 2018b) acc is 72.54. f1 of mgan (fan et al., 2018b) f1 is 70.81. acc of mgan (li et al., 2018b) acc is 81.49. f1 of mgan (li et al., 2018b) f1 is 71.48. acc of mgan (li et al., 2018b) acc is 76.21. f1 of mgan (li et al., 2018b) f1 is 71.42. acc of mgan (li et al., 2018b) acc is 74.62. f1 of mgan (li et al., 2018b) f1 is 73.53. acc of hscn (li et al., 2018b) acc is 77.8. f1 of hscn (li et al., 2018b) f1 is 70.2. acc of hscn (li et al., 2018b) acc is 76.1. f1 of hscn (li et al., 2018b) f1 is 72.5. acc of hscn (li et al., 2018b) acc is 69.6. f1 of hscn (li et al., 2018b) f1 is 66.1. acc of asp-bilstm acc is 80.95. f1 of asp-bilstm f1 is 72.38. acc of asp-bilstm acc is 74.22. f1 of asp-bilstm f1 is 69.35. acc of asp-bilstm acc is 73.66. f1 of asp-bilstm f1 is 72.32. acc of asp-bilstm acc is 85.12. f1 of asp-bilstm f1 is 66.92. acc of asp-gcn acc is 81.3. f1 of asp-gcn f1 is 73.18. acc of asp-gcn acc is 74.53. f1 of asp-gcn f1 is 69.78. acc of asp-gcn acc is 70.91. f1 of asp-gcn f1 is 69.07. acc of asp-gcn acc is 81.85. f1 of asp-gcn f1 is 61.2. acc of cdt acc is 82.3. f1 of cdt f1 is 74.02. acc of cdt acc is 77.19. f1 of cdt f1 is 72.99. acc of cdt acc is 74.66. f1 of cdt f1 is 73.66. acc of cdt acc is 85.58. f1 of cdt f1 is 69.93.
table 1 shows main bleu results (ctc=0.62). bleu of baseline fr®en is 38.38 (5). bleu of baseline en®fr is 38.88 (6). bleu of baseline zh®en is 17.25 (6). bleu of baseline en®de is 26.19 (4). bleu of raml fr®en is +0.22 (3). bleu of raml en®fr is +0.67 (3). bleu of raml zh®en is +0.23 (4). bleu of raml en®de is -0.16 (6). bleu of so fr®en is +0.01 (4). bleu of so en®fr is +0.62 (4). bleu of so zh®en is +0.02 (5). bleu of so en®de is -0.15 (5). bleu of st fr®en is -0.13 (6). bleu of st en®fr is +0.46 (5). bleu of st zh®en is +1.51 (2). bleu of st en®de is +0.83 (2). bleu of ta fr®en is +0.62 (2). bleu of ta en®fr is +1.13 (1). bleu of ta zh®en is +2.41 (1). bleu of ta en®de is +1.01 (1). bleu of bt fr®en is +0.82 (1). bleu of bt en®fr is +0.99 (2). bleu of bt zh®en is +1.06 (3). bleu of bt en®de is +0.39 (3).
table 1 shows online decoding accuracy for a direct model (dir), ensembling two direct models (dir ens) and the channel approach (ch+dir+lm). accuracy of dir news2016 is 39. accuracy of dir news2017 is 34.3. accuracy of dir ens news2016 is 40. accuracy of dir ens news2017 is 35.3. accuracy of dir+lm news2016 is 39.8. accuracy of dir+lm news2017 is 35.2. accuracy of ch+dir+lm news2016 is 41. accuracy of ch+dir+lm news2017 is 36.2. accuracy of  - per word scores news2016 is 40. accuracy of  - per word scores news2017 is 35.1.
table 3 shows comparison of the recent state-of-the-art approaches and g/g+i. dda of et convex mst is 49.4. dda of et lc-dmv is 31.8. dda of et d-j is 44. dda of et g is 56. dda of et g+i is 56.4. dda of fi convex mst is 44.7. dda of fi lc-dmv is 26.9. dda of fi d-j is 43.5. dda of fi g is 50.7. dda of fi g+i is 49.3. dda of nl convex mst is 45.3. dda of nl lc-dmv is 34.1. dda of nl d-j is 43.5. dda of nl g is 50.4. dda of nl g+i is 50.6. dda of en convex mst is 54. dda of en lc-dmv is 56. dda of en d-j is 60.1. dda of en g is 51.7. dda of en g+i is 52.7. dda of de convex mst is 51.4. dda of de lc-dmv is 50.5. dda of de d-j is 55.7. dda of de g is 59.6. dda of de g+i is 61.4. dda of no convex mst is 55.3. dda of no lc-dmv is 45.5. dda of no d-j is 60.8. dda of no g is 61. dda of no g+i is 61.3. dda of grc convex mst is 43.4. dda of grc lc-dmv is 33.1. dda of grc d-j is 44.9. dda of grc g is 46.8. dda of grc g+i is 46.2. dda of hi convex mst is 56.8. dda of hi lc-dmv is 54.2. dda of hi d-j is 60. dda of hi g is 47.4. dda of hi g+i is 46.8. dda of ja convex mst is 44.8. dda of ja lc-dmv is 43.8. dda of ja d-j is 45.8. dda of ja g is 43.4. dda of ja g+i is 44.2. dda of fr convex mst is 62. dda of fr lc-dmv is 48.6. dda of fr d-j is 57. dda of fr g is 58.4. dda of fr g+i is 60.1. dda of it convex mst is 69.1. dda of it lc-dmv is 71.1. dda of it d-j is 70.3. dda of it g is 64.4. dda of it g+i is 65.9. dda of la convex mst is 38.8. dda of la lc-dmv is 38.6. dda of la d-j is 42.2. dda of la g is 45.1. dda of la g+i is 45. dda of bg convex mst is 61.6. dda of bg lc-dmv is 62.4. dda of bg d-j is 73.8. dda of bg g is 71.3. dda of bg g+i is 71.3. dda of sl convex mst is 54. dda of sl lc-dmv is 49.5. dda of sl d-j is 69.6. dda of sl g is 68.3. dda of sl g+i is 68.6. dda of eu convex mst is 50. dda of eu lc-dmv is 45.4. dda of eu d-j is 55.7. dda of eu g is 54.2. dda of eu g+i is 53.6. dda of avg convex mst is 52. dda of avg lc-dmv is 46.1. dda of avg d-j is 55.1. dda of avg g is 55.3. dda of avg g+i is 55.6.
table 3 shows performance of various models on the acp test set. acc of bigru acc is 0.843. acc of bert acc is 0.863. acc of bigru acc is 0.866. acc of bert acc is 0.835. acc of bigru acc is 0.919. acc of bert acc is 0.933. acc of bigru acc is 0.917. acc of bert acc is 0.913. acc of metrics acc is 0.5. acc of metrics acc is 0.503.
table 1 shows overall performance of different methods on the test set with gold-standard entities. p of cross event p is 68.7. r of cross event r is 68.9. f1 of cross event f1 is 68.8. p of dmcnn p is 75.6. r of dmcnn r is 63.6. f1 of dmcnn f1 is 69.1. p of jrnn p is 66. r of jrnn r is 73.9. f1 of jrnn f1 is 69.3. p of deeb-rnn p is 72.3. r of deeb-rnn r is 75.8. f1 of deeb-rnn f1 is 74. p of dbrnn p is 74.1. r of dbrnn r is 69.8. f1 of dbrnn f1 is 71.9. p of gcn-ed p is 77.9. r of gcn-ed r is 68.8. f1 of gcn-ed f1 is 73.1. p of jmee p is 76.3. r of jmee r is 71.3. f1 of jmee f1 is 73.7. p of moganed p is 79.5. r of moganed r is 72.3. f1 of moganed f1 is 75.7.
table 1 shows dygie++ achieves state-of-the-art results. f1 of entity sota is 88.4. f1 of entity ours is 88.6. f1 of entity d% is 1.7. f1 of relation sota is 63.2. f1 of relation ours is 63.4. f1 of relation d% is 0.5. f1 of entity sota is 87.1. f1 of entity ours is 90.7. f1 of entity d% is 27.9. f1 of trig-id sota is 73.9. f1 of trig-id ours is 76.5. f1 of trig-id d% is 9.6. f1 of trig-c sota is 72. f1 of trig-c ours is 73.6. f1 of trig-c d% is 5.7. f1 of arg-id sota is 57.2. f1 of arg-id ours is 55.4. f1 of arg-id d% is -4.2. f1 of arg-c sota is 52.4. f1 of arg-c ours is 52.5. f1 of arg-c d% is 0.2. f1 of entity sota is 65.2. f1 of entity ours is 67.5. f1 of entity d% is 6.6. f1 of relation sota is 41.6. f1 of relation ours is 48.4. f1 of relation d% is 11.6. f1 of entity sota is 76.2. f1 of entity ours is 77.9. f1 of entity d% is 7.1. f1 of entity sota is 79.5. f1 of entity ours is 79.7. f1 of entity d% is 1. f1 of relation sota is 64.1. f1 of relation ours is 65.9. f1 of relation d% is 5.
table 1 shows ontonotes: bert improves the c2f-coref model on english by 0.9% and 3.9% respectively for base and large variants. p of martschat and strube (2015) p is 76.7. r of martschat and strube (2015) r is 68.1. f1 of martschat and strube (2015) f1 is 72.2. p of martschat and strube (2015) p is 66.1. r of martschat and strube (2015) r is 54.2. f1 of martschat and strube (2015) f1 is 59.6. p of martschat and strube (2015) p is 59.5. r of martschat and strube (2015) r is 52.3. f1 of martschat and strube (2015) f1 is 55.7. avg. f1 of martschat and strube (2015) avg. f1 is 62.5. p of (clark and manning, 2015) p is 76.1. r of (clark and manning, 2015) r is 69.4. f1 of (clark and manning, 2015) f1 is 72.6. p of (clark and manning, 2015) p is 65.6. r of (clark and manning, 2015) r is 56. f1 of (clark and manning, 2015) f1 is 60.4. p of (clark and manning, 2015) p is 59.4. r of (clark and manning, 2015) r is 53. f1 of (clark and manning, 2015) f1 is 56. avg. f1 of (clark and manning, 2015) avg. f1 is 63. p of (wiseman et al., 2015) p is 76.2. r of (wiseman et al., 2015) r is 69.3. f1 of (wiseman et al., 2015) f1 is 72.6. p of (wiseman et al., 2015) p is 66.2. r of (wiseman et al., 2015) r is 55.8. f1 of (wiseman et al., 2015) f1 is 60.5. p of (wiseman et al., 2015) p is 59.4. r of (wiseman et al., 2015) r is 54.9. f1 of (wiseman et al., 2015) f1 is 57.1. avg. f1 of (wiseman et al., 2015) avg. f1 is 63.4. p of wiseman et al. (2016) p is 77.5. r of wiseman et al. (2016) r is 69.8. f1 of wiseman et al. (2016) f1 is 73.4. p of wiseman et al. (2016) p is 66.8. r of wiseman et al. (2016) r is 57. f1 of wiseman et al. (2016) f1 is 61.5. p of wiseman et al. (2016) p is 62.1. r of wiseman et al. (2016) r is 53.9. f1 of wiseman et al. (2016) f1 is 57.7. avg. f1 of wiseman et al. (2016) avg. f1 is 64.2. p of clark and manning (2016) p is 79.2. r of clark and manning (2016) r is 70.4. f1 of clark and manning (2016) f1 is 74.6. p of clark and manning (2016) p is 69.9. r of clark and manning (2016) r is 58. f1 of clark and manning (2016) f1 is 63.4. p of clark and manning (2016) p is 63.5. r of clark and manning (2016) r is 55.5. f1 of clark and manning (2016) f1 is 59.2. avg. f1 of clark and manning (2016) avg. f1 is 65.7. p of e2e-coref (lee et al., 2017) p is 78.4. r of e2e-coref (lee et al., 2017) r is 73.4. f1 of e2e-coref (lee et al., 2017) f1 is 75.8. p of e2e-coref (lee et al., 2017) p is 68.6. r of e2e-coref (lee et al., 2017) r is 61.8. f1 of e2e-coref (lee et al., 2017) f1 is 65. p of e2e-coref (lee et al., 2017) p is 62.7. r of e2e-coref (lee et al., 2017) r is 59. f1 of e2e-coref (lee et al., 2017) f1 is 60.8. avg. f1 of e2e-coref (lee et al., 2017) avg. f1 is 67.2. p of c2f-coref (lee et al., 2018) p is 81.4. r of c2f-coref (lee et al., 2018) r is 79.5. f1 of c2f-coref (lee et al., 2018) f1 is 80.4. p of c2f-coref (lee et al., 2018) p is 72.2. r of c2f-coref (lee et al., 2018) r is 69.5. f1 of c2f-coref (lee et al., 2018) f1 is 70.8. p of c2f-coref (lee et al., 2018) p is 68.2. r of c2f-coref (lee et al., 2018) r is 67.1. f1 of c2f-coref (lee et al., 2018) f1 is 67.6. avg. f1 of c2f-coref (lee et al., 2018) avg. f1 is 73. p of fei et al. (2019) p is 85.4. r of fei et al. (2019) r is 77.9. f1 of fei et al. (2019) f1 is 81.4. p of fei et al. (2019) p is 77.9. r of fei et al. (2019) r is 66.4. f1 of fei et al. (2019) f1 is 71.7. p of fei et al. (2019) p is 70.6. r of fei et al. (2019) r is 66.3. f1 of fei et al. (2019) f1 is 68.4. avg. f1 of fei et al. (2019) avg. f1 is 73.8. p of ee (kantor and globerson, 2019) p is 82.6. r of ee (kantor and globerson, 2019) r is 84.1. f1 of ee (kantor and globerson, 2019) f1 is 83.4. p of ee (kantor and globerson, 2019) p is 73.3. r of ee (kantor and globerson, 2019) r is 76.2. f1 of ee (kantor and globerson, 2019) f1 is 74.7. p of ee (kantor and globerson, 2019) p is 72.4. r of ee (kantor and globerson, 2019) r is 71.1. f1 of ee (kantor and globerson, 2019) f1 is 71.8. avg. f1 of ee (kantor and globerson, 2019) avg. f1 is 76.6. p of bert-base + c2f-coref (independent) p is 80.2. r of bert-base + c2f-coref (independent) r is 82.4. f1 of bert-base + c2f-coref (independent) f1 is 81.3. p of bert-base + c2f-coref (independent) p is 69.6. r of bert-base + c2f-coref (independent) r is 73.8. f1 of bert-base + c2f-coref (independent) f1 is 71.6. p of bert-base + c2f-coref (independent) p is 69. r of bert-base + c2f-coref (independent) r is 68.6. f1 of bert-base + c2f-coref (independent) f1 is 68.8. avg. f1 of bert-base + c2f-coref (independent) avg. f1 is 73.9. p of bert-base + c2f-coref (overlap) p is 80.4. r of bert-base + c2f-coref (overlap) r is 82.3. f1 of bert-base + c2f-coref (overlap) f1 is 81.4. p of bert-base + c2f-coref (overlap) p is 69.6. r of bert-base + c2f-coref (overlap) r is 73.8. f1 of bert-base + c2f-coref (overlap) f1 is 71.7. p of bert-base + c2f-coref (overlap) p is 69. r of bert-base + c2f-coref (overlap) r is 68.5. f1 of bert-base + c2f-coref (overlap) f1 is 68.8. avg. f1 of bert-base + c2f-coref (overlap) avg. f1 is 73.9. p of bert-large + c2f-coref (independent) p is 84.7. r of bert-large + c2f-coref (independent) r is 82.4. f1 of bert-large + c2f-coref (independent) f1 is 83.5. p of bert-large + c2f-coref (independent) p is 76.5. r of bert-large + c2f-coref (independent) r is 74. f1 of bert-large + c2f-coref (independent) f1 is 75.3. p of bert-large + c2f-coref (independent) p is 74.1. r of bert-large + c2f-coref (independent) r is 69.8. f1 of bert-large + c2f-coref (independent) f1 is 71.9. avg. f1 of bert-large + c2f-coref (independent) avg. f1 is 76.9. p of bert-large + c2f-coref (overlap) p is 85.1. r of bert-large + c2f-coref (overlap) r is 80.5. f1 of bert-large + c2f-coref (overlap) f1 is 82.8. p of bert-large + c2f-coref (overlap) p is 77.5. r of bert-large + c2f-coref (overlap) r is 70.9. f1 of bert-large + c2f-coref (overlap) f1 is 74.1. p of bert-large + c2f-coref (overlap) p is 73.8. r of bert-large + c2f-coref (overlap) r is 69.3. f1 of bert-large + c2f-coref (overlap) f1 is 71.5. avg. f1 of bert-large + c2f-coref (overlap) avg. f1 is 76.1.
table 3 shows comparison of different delta functions. macro-averaged of subtract m is 3.35. macro-averaged of subtract ve is 67.2. macro-averaged of add m is 3.45. macro-averaged of add ve is 65.35. macro-averaged of mlp m is 3.32. macro-averaged of mlp ve is 62.97.
table 5 shows results of proposedru and its variants r of proposedruwiki r is 57.4. p of proposedruwiki p is 49.6. f of proposedruwiki f is 53.2?. avg.p of proposedruwiki avg.p is 53.3. r of proposedruweb r is 59. p of proposedruweb p is 50.9. f of proposedruweb f is 54.6?. avg.p of proposedruweb avg.p is 54.5. r of proposedru r is 64. p of proposedru p is 52. f of proposedru f is 57.4. avg.p of proposedru avg.p is 57.4. r of proposedruweb+web r is 62.5. p of proposedruweb+web p is 49. f of proposedruweb+web f is 54.9?. avg.p of proposedruweb+web avg.p is 54.8. r of proposedruweb+pair r is 64.3. p of proposedruweb+pair p is 48.2. f of proposedruweb+pair f is 55.1?. avg.p of proposedruweb+pair avg.p is 55.3. r of proposedru+bk r is 67.4. p of proposedru+bk p is 52.3. f of proposedru+bk f is 58.9. avg.p of proposedru+bk avg.p is 59.9.
table 1 shows performance comparison of baseline vqa trained on vqa2.0, baseline vqa finetuned on convqa, and vqa trained using our ctm. perf con of vqa2.0 perf con is 36.25. avg con of vqa2.0 avg con is 71.36. top1 of vqa2.0 top1 is 70.34. perf con of vqa2.0 perf con is 26.13. avg con of vqa2.0 avg con is 59.61. top1 of vqa2.0 top1 is 60.03. yes/no of vqa2.0 yes/no is 65.49. num of vqa2.0 num is 31.39. perf con of cs-convqa perf con is 34.54. avg con of cs-convqa avg con is 70.39. top1 of cs-convqa top1 is 69.48. perf con of cs-convqa perf con is 26.39. avg con of cs-convqa avg con is 59.65. top1 of cs-convqa top1 is 60.07. yes/no of cs-convqa yes/no is 65.8. num of cs-convqa num is 35.92. perf con of l/cs-convqa perf con is 54.68. avg con of l/cs-convqa avg con is 83.42. top1 of l/cs-convqa top1 is 83.16. perf con of l/cs-convqa perf con is 24.7. avg con of l/cs-convqa avg con is 59.3. top1 of l/cs-convqa top1 is 59.6. yes/no of l/cs-convqa yes/no is 65.14. num of l/cs-convqa num is 33.33. perf con of l/cs-convqa perf con is 54.6. avg con of l/cs-convqa avg con is 83.23. top1 of l/cs-convqa top1 is 82.79. perf con of l/cs-convqa perf con is 25.94. avg con of l/cs-convqa avg con is 60.39. top1 of l/cs-convqa top1 is 60.78. yes/no of l/cs-convqa yes/no is 66.63. num of l/cs-convqa num is 36.89. perf con of l/cs-convqa,vg perf con is 36.4. avg con of l/cs-convqa,vg avg con is 71.6. top1 of l/cs-convqa,vg top1 is 70.94. perf con of l/cs-convqa,vg perf con is 25.22. avg con of l/cs-convqa,vg avg con is 59.19. top1 of l/cs-convqa,vg top1 is 59.56. yes/no of l/cs-convqa,vg yes/no is 65.3. num of l/cs-convqa,vg num is 31.39. perf con of l/cs-convqa,vg perf con is 51.41. avg con of l/cs-convqa,vg avg con is 81.66. top1 of l/cs-convqa,vg top1 is 81.37. perf con of l/cs-convqa,vg perf con is 27.49. avg con of l/cs-convqa,vg avg con is 59.75. top1 of l/cs-convqa,vg top1 is 60.15. yes/no of l/cs-convqa,vg yes/no is 66.41. num of l/cs-convqa,vg num is 34.95.
table 1 shows results on the validation set of opensquad. em of single-sentence em is 34.8. f1 of single-sentence f1 is 44.4. em of length-50 em is 35.5. f1 of length-50 f1 is 45.2. em of length-100 em is 35.7. f1 of length-100 f1 is 45.7. em of length-200 em is 34.8. f1 of length-200 f1 is 44.7. em of w/o sliding-window (same as (3)) em is 35.7. f1 of w/o sliding-window (same as (3)) f1 is 45.7. em of w/ sliding-window em is 40.4. f1 of w/ sliding-window f1 is 49.8. em of w/o passage ranker (same as (6)) em is 40.4. f1 of w/o passage ranker (same as (6)) f1 is 49.8. em of w/ passage ranker em is 41.3. f1 of w/ passage ranker f1 is 51.7. em of w/ passage scores em is 42.8. f1 of w/ passage scores f1 is 53.4. em of bert+qanet em is 18.3. f1 of bert+qanet f1 is 27.8. em of bert+qanet (fix bert) em is 35.5. f1 of bert+qanet (fix bert) f1 is 45.9. em of bert+qanet (init. from (11)) em is 36.2. f1 of bert+qanet (init. from (11)) f1 is 46.4.
table 2 shows evaluation results of our models on development (dev) and testing (test) sets. r1_f1 of dev r1_f1 is 43.9. r2_f1 of dev r2_f1 is 28.5. rl_f1 of dev rl_f1 is 46.3. bleu of dev bleu is 12.6. r1_f1 of test r1_f1 is 39.7. r2_f1 of test r2_f1 is 22.9. rl_f1 of test rl_f1 is 42.2. bleu of test bleu is 9. r1_f1 of dev r1_f1 is 45.4. r2_f1 of dev r2_f1 is 29.8. rl_f1 of dev rl_f1 is 47.4. bleu of dev bleu is 14. r1_f1 of test r1_f1 is 55.7. r2_f1 of test r2_f1 is 41.8. rl_f1 of test rl_f1 is 57.6. bleu of test bleu is 20.8. r1_f1 of dev r1_f1 is 44.3. r2_f1 of dev r2_f1 is 28.5. rl_f1 of dev rl_f1 is 46.4. bleu of dev bleu is 13.1. r1_f1 of test r1_f1 is 40. r2_f1 of test r2_f1 is 23. rl_f1 of test rl_f1 is 42.3. bleu of test bleu is 9.4.
table 4 shows the results on word-in-context (wic) data. accuracy (%) of lee and chen (2017) accuracy (%) is 52.14. accuracy (%) of neelakantan et al. (2015) accuracy (%) is 54. accuracy (%) of mancini et al. (2016) accuracy (%) is 54.56. accuracy (%) of guo et al. (2019) accuracy (%) is 55.27. accuracy (%) of chang et al. (2018) accuracy (%) is 57. accuracy (%) of pilehvar and collier (2016) accuracy (%) is 58.55. accuracy (%) of proposed (bert-base) accuracy (%) is 68.64.
table 7 shows comparison of copying accuracies. accuracy of ms uedin  accuracy is 64.63. accuracy of copynet  accuracy is 64.72. accuracy of ours  accuracy is 65.61.
table 5 shows evaluation iii: informativeness: the values represent percentage of times instructions generated by the model is chosen by a human evaluator. % of set2multipleseq % is 30. % of set2multipleseq+opt % is 63. % of ambigous % is 7.
table 2 shows performance of the three proposed approaches in comparison with the baseline. prec of baseline  prec is 49.80%. rec of baseline  rec is  —. f1 of baseline  f1 is  —. prec of wd  prec is 67.30%. rec of wd  rec is 93.00%. f1 of wd  f1 is 78.10%. prec of ner  prec is 71.80%. rec of ner  rec is 81.30%. f1 of ner  f1 is 76.20%. prec of blstm  prec is 86.90%. rec of blstm  rec is 85.30%. f1 of blstm  f1 is 86.10%.
table 1 shows experimental results. precision of baseline  precision is 92.75. recall of baseline  recall is 92.15. f-score of baseline  f-score is 92.45. precision of ve-p  precision is 93.11. recall of ve-p  recall is 91.4. f-score of ve-p  f-score is 92.25. precision of hanpane-p  precision is 92.71. recall of hanpane-p  recall is 91.94. f-score of hanpane-p  f-score is 92.32. precision of ve+p  precision is 93.15. recall of ve+p  recall is 91.79. f-score of ve+p  f-score is 92.47. precision of hanpane+p (proposed)  precision is 92.81. recall of hanpane+p (proposed)  recall is 92.33. f-score of hanpane+p (proposed)  f-score is 92.57.
table 4 shows performance of the average f1. f1 of sentence-level + structural b is 75.4. f1 of sentence-level + structural  i is 92.8. f1 of sentence-level + structural  o is 62.1. f1 of sentence-level + structural  macro is  *76.8. f1 of sentence-level + structural  v is  *80.7. f1 of sentence-level + structural  r is 61.5. f1 of sentence-level + structural  p is  *16.0. f1 of sentence-level + structural  t is 43.1. f1 of sentence-level + structural  f is 33.2. f1 of sentence-level + structural  macro is  *49.2. f1 of sentence-level b is 75.6. f1 of sentence-level  i is 92.8. f1 of sentence-level  o is 61. f1 of sentence-level  macro is 76.5. f1 of sentence-level  v is 80.2. f1 of sentence-level  r is 60.9. f1 of sentence-level  p is 13. f1 of sentence-level  t is 41.3. f1 of sentence-level  f is 31.3. f1 of sentence-level  macro is 47.7. f1 of post-level b is 67.8. f1 of post-level  i is 92.9. f1 of post-level  o is  *64.7. f1 of post-level  macro is 75.1. f1 of post-level  v is 79.9. f1 of post-level  r is 49.4. f1 of post-level  p is 2.1. f1 of post-level  t is 35.3. f1 of post-level  f is 32.2. f1 of post-level  macro is 43.8.
table 1 shows results on the yelp and amazon test sets. acc of shen et al. (2017) acc is 74.5. bleu of shen et al. (2017) bleu is 6.79. acc of shen et al. (2017) acc is 74.4. bleu of shen et al. (2017) bleu is 1.57. acc of fu et al. (2018) acc is 46.8. bleu of fu et al. (2018) bleu is 11.24. acc of fu et al. (2018) acc is 70.3. bleu of fu et al. (2018) bleu is 7.87. acc of li et al. (2018) acc is 88.3. bleu of li et al. (2018) bleu is 12.61. acc of li et al. (2018) acc is 53.4. bleu of li et al. (2018) bleu is 27.12. acc of this work acc is 88.5. bleu of this work bleu is 12.13. acc of this work acc is 53.8. bleu of this work bleu is 15.95. acc of w/o lsentiment acc is 3.4. bleu of w/o lsentiment bleu is 24.06. acc of w/o lsentiment acc is 18.2. bleu of w/o lsentiment bleu is 42.65. acc of w/o lcontent acc is 86.4. bleu of w/o lcontent bleu is 10.08. acc of w/o lcontent acc is 53.9. bleu of w/o lcontent bleu is 14.77. acc of w/o lalignment acc is 84.7. bleu of w/o lalignment bleu is 11.94. acc of w/o lalignment acc is 51.6. bleu of w/o lalignment bleu is 16.51. acc of only lsentiment acc is 85.4. bleu of only lsentiment bleu is 10.05. acc of only lsentiment acc is 53.4. bleu of only lsentiment bleu is 14.76.
table 1 shows overall results for each dataset and model. accuracy of ft-br bbc is 39.72. accuracy of ft-br etc is 52.24. accuracy of ft-br mftc is 51.19. accuracy of mtl bbc is 48.57. accuracy of mtl etc is 53.32. accuracy of mtl mftc is 53.97. accuracy of ft-lp bbc is 36.20. accuracy of ft-lp etc is 53.57. accuracy of ft-lp mftc is 55.11. accuracy of mtl-lp bbc is 55.60. accuracy of mtl-lp etc is 55.37. accuracy of mtl-lp mftc is 62.98. accuracy of mtl-xld bbc is 51.33. accuracy of mtl-xld etc is 52.22. accuracy of mtl-xld mftc is 60.94. accuracy of sobhani (seq2seq) bbc is na. accuracy of sobhani (seq2seq) etc is 54.81. accuracy of sobhani (seq2seq) mftc is na.
table 3 shows results on total term prediction(%). s of cnn s is 67.24. em of cnn em is 8.41. acc@0.1 of cnn acc@0.1 is 16.96. acc@0.2 of cnn acc@0.2 is 35.58. s of rnn s is 67.27. em of rnn em is 8.04. acc@0.1 of rnn acc@0.1 is 16.79. acc@0.2 of rnn acc@0.2 is 35.11. s of rcnn s is 69.56. em of rcnn em is 8.54. acc@0.1 of rcnn acc@0.1 is 17.57. acc@0.2 of rcnn acc@0.2 is 35.75. s of dgn s is 75.74. em of dgn em is 8.64. acc@0.1 of dgn acc@0.1 is 19.32. acc@0.2 of dgn acc@0.2 is 40.43.
table 2 shows results of parsing abstract structure precision of background precision is 74.6. recall of background recall is 77.2. f-measure of background f-measure is 75.8. precision of objective precision is 85.2. recall of objective recall is 81.8. f-measure of objective f-measure is 83.5. precision of data precision is 82.6. recall of data recall is 76.8. f-measure of data f-measure is 79.6. precision of design precision is 68. recall of design recall is 64.8. f-measure of design f-measure is 66.3. precision of method precision is 80.4. recall of method recall is 80.1. f-measure of method f-measure is 80.2. precision of result precision is 90.8. recall of result recall is 93.3. f-measure of result f-measure is 92. precision of conclusion precision is 93.8. recall of conclusion recall is 92. f-measure of conclusion f-measure is 92.9. accuracy of all accuracy is 86.6.
table 3 shows results on classifying trajectories accuracy of random all is 50.3. accuracy of random bio is 47.2. accuracy of random phy is 47.8. accuracy of random chm is 50.9. accuracy of random neu is 51.2. accuracy of majority all is 56.1. accuracy of majority bio is 56.3. accuracy of majority phy is 81.6. accuracy of majority chm is 74.3. accuracy of majority neu is 56.6. accuracy of lr all is 74.2. accuracy of lr bio is 81. accuracy of lr phy is 83.3. accuracy of lr chm is 81.9. accuracy of lr neu is 74.8. accuracy of lr - ld-r all is 71.3. accuracy of lr - ld-r bio is 77.7. accuracy of lr - ld-r phy is 81.6. accuracy of lr - ld-r chm is 73.1. accuracy of lr - ld-r neu is 70.5.
table 4 shows results on predicting trajectory accuracy on all of ld-% + ld-delta accuracy on all is 72.1. accuracy on all of ld-% only accuracy on all is 71. accuracy on all of ld-delta only accuracy on all is 60.4.
table 1 shows performance of the crf and alternative neural network structures on the public fce dataset for token-level error detection in learner writing. p of crf p is 62.2. r of crf r is 13.6. f0.5 of crf f0.5 is 36.3. predicted of crf predicted is 914. correct of crf correct is 516. p of crf p is 56.5. r of crf r is 8.2. f0.5 of crf f0.5 is 25.9. p of cnn p is 52.4. r of cnn r is 24.9. f0.5 of cnn f0.5 is 42.9. predicted of cnn predicted is 3518. correct of cnn correct is 1620. p of cnn p is 46. r of cnn r is 25.7. f0.5 of cnn f0.5 is 39.8. p of deep cnn p is 48.4. r of deep cnn r is 26.2. f0.5 of deep cnn f0.5 is 41.4. predicted of deep cnn predicted is 3992. correct of deep cnn correct is 1651. p of deep cnn p is 41.4. r of deep cnn r is 26.2. f0.5 of deep cnn f0.5 is 37.1. p of bi-rnn p is 63.9. r of bi-rnn r is 18. f0.5 of bi-rnn f0.5 is 42.3. predicted of bi-rnn predicted is 2333. correct of bi-rnn correct is 1196. p of bi-rnn p is 51.3. r of bi-rnn r is 19. f0.5 of bi-rnn f0.5 is 38.2. p of deep bi-rnn p is 60.3. r of deep bi-rnn r is 17.6. f0.5 of deep bi-rnn f0.5 is 40.6. predicted of deep bi-rnn predicted is 2543. correct of deep bi-rnn correct is 1255. p of deep bi-rnn p is 49.4. r of deep bi-rnn r is 19.9. f0.5 of deep bi-rnn f0.5 is 38.1. p of bi-lstm p is 54.5. r of bi-lstm r is 28.2. f0.5 of bi-lstm f0.5 is 46. predicted of bi-lstm predicted is 3898. correct of bi-lstm correct is 1798. p of bi-lstm p is 46.1. r of bi-lstm r is 28.5. f0.5 of bi-lstm f0.5 is 41.1. p of deep bi-lstm p is 56.7. r of deep bi-lstm r is 21.3. f0.5 of deep bi-lstm f0.5 is 42.5. predicted of deep bi-lstm predicted is 2822. correct of deep bi-lstm correct is 1359. p of deep bi-lstm p is 48.2. r of deep bi-lstm r is 21.6. f0.5 of deep bi-lstm f0.5 is 38.6.
table 2 shows results on the public fce test set when incrementally providing more training data to the error detection model. f0.5 of fce-public f0.5 is 46. f0.5 of fce-public f0.5 is 41.1. f0.5 of +nucle a4 f0.5 is 39. f0.5 of +nucle a4 f0.5 is 41. f0.5 of +ielts f0.5 is 45.6. f0.5 of +ielts f0.5 is 50.7. f0.5 of +fce f0.5 is 57.2. f0.5 of +fce f0.5 is 61.1. f0.5 of +cpe f0.5 is 59. f0.5 of +cpe f0.5 is 62.1. f0.5 of +cae f0.5 is 60.7. f0.5 of +cae f0.5 is 64.3.
table 7 shows results (in percentage) on the conll2009 test sets for chinese, german and spanish. p of pathlstm p is 83.2. r of pathlstm r is 75.9. f1 of pathlstm f1 is 79.4. p of bjorkelund et al. (2009) p is 82.4. r of bjorkelund et al. (2009) r is 75.1. f1 of bjorkelund et al. (2009) f1 is 78.6. p of zhao et al. (2009) p is 80.4. r of zhao et al. (2009) r is 75.2. f1 of zhao et al. (2009) f1 is 77.7. p of pathlstm p is 81.8. r of pathlstm r is 78.5. f1 of pathlstm f1 is 80.1. p of bjorkelund et al. (2009) p is 81.2. r of bjorkelund et al. (2009) r is 78.3. f1 of bjorkelund et al. (2009) f1 is 79.7. p of che et al. (2009) p is 82.1. r of che et al. (2009) r is 75.4. f1 of che et al. (2009) f1 is 78.6. p of zhao et al. (2009) p is 83.1. r of zhao et al. (2009) r is 78. f1 of zhao et al. (2009) f1 is 80.5. p of pathlstm p is 83.2. r of pathlstm r is 77.4. f1 of pathlstm f1 is 80.2. p of bjorkelund et al. (2009) p is 78.9. r of bjorkelund et al. (2009) r is 74.3. f1 of bjorkelund et al. (2009) f1 is 76.5.
table 1 shows overall performance with gold-standard entities, timex, and values, the candidate arguments are annotated in ace 2005. p of jet p is 67.6. r of jet r is 53.5. f1 of jet f1 is 59.7. p of jet p is 46.5. r of jet r is 37.2. f1 of jet f1 is 41.3. p of jet p is 41. p of jet p is 32.8. f1 of jet f1 is 36.5. p of cross-event p is 68.7. r of cross-event r is 68.9. f1 of cross-event f1 is 68.8. p of cross-event p is 50.9. r of cross-event r is 49.7. f1 of cross-event f1 is 50.3. p of cross-event p is 45.1. p of cross-event p is 44.1. f1 of cross-event f1 is 44.6. p of cross-entity p is 72.9. r of cross-entity r is 64.3. f1 of cross-entity f1 is 68.3. p of cross-entity p is 53.4. r of cross-entity r is 52.9. f1 of cross-entity f1 is 53.1. p of cross-entity p is 51.6. p of cross-entity p is 45.5. f1 of cross-entity f1 is 48.3. p of joint p is 73.7. r of joint r is 62.3. f1 of joint f1 is 67.5. p of joint p is 69.8. r of joint r is 47.9. f1 of joint f1 is 56.8. p of joint p is 64.7. p of joint p is 44.4. f1 of joint f1 is 52.7. p of dmcnn p is 75.6. r of dmcnn r is 63.6. f1 of dmcnn f1 is 69.1. p of dmcnn p is 68.8. r of dmcnn r is 51.9. f1 of dmcnn f1 is 59.1. p of dmcnn p is 62.2. p of dmcnn p is 46.9. f1 of dmcnn f1 is 53.5. p of rbpb(jet) p is 62.3. r of rbpb(jet) r is 59.9. f1 of rbpb(jet) f1 is 61.1. p of rbpb(jet) p is 50.4. r of rbpb(jet) r is 45.8. f1 of rbpb(jet) f1 is 48.0. p of rbpb(jet) p is 41.9. p of rbpb(jet) p is 36.5. f1 of rbpb(jet) f1 is 39.0. p of rbpb(jet) + et p is 66.7. r of rbpb(jet) + et r is 65.9. f1 of rbpb(jet) + et f1 is 66.3. p of rbpb(jet) + et p is 60.6. r of rbpb(jet) + et r is 56.7. f1 of rbpb(jet) + et f1 is 58.6. p of rbpb(jet) + et p is 49.2. p of rbpb(jet) + et p is 48.3. f1 of rbpb(jet) + et f1 is 48.7. p of rbpb(jet) + regu p is 67.2. r of rbpb(jet) + regu r is 61.7. f1 of rbpb(jet) + regu f1 is 64.3. p of rbpb(jet) + regu p is 62.8. r of rbpb(jet) + regu r is 57.5. f1 of rbpb(jet) + regu f1 is 60.0. p of rbpb(jet) + regu p is 52.6. p of rbpb(jet) + regu p is 48.4. f1 of rbpb(jet) + regu f1 is 50.4. p of rbpb(jet) + et + regu p is 70.3. r of rbpb(jet) + et + regu r is 67.5. f1 of rbpb(jet) + et + regu f1 is 68.9. p of rbpb(jet) + et + regu p is 63.2. r of rbpb(jet) + et + regu r is 59.4. f1 of rbpb(jet) + et + regu f1 is 61.2. p of rbpb(jet) + et + regu p is 54.1. p of rbpb(jet) + et + regu p is 53.5. f1 of rbpb(jet) + et + regu f1 is 53.8.
table 2 shows overall performance with predicted entities, timex, and values, the candidate arguments are extracted by jet. f1 of jet f1 is 59.7. f1 of jet f1 is 42.5. f1 of jet f1 is 36.6. f1 of cross-document f1 is 67.3. f1 of cross-document f1 is 46.2. f1 of cross-document f1 is 42.6. f1 of joint f1 is 65.6. f1 of joint f1 is 41.8. f1 of rbpb(jet) f1 is 60.4. f1 of rbpb(jet) f1 is 44.3. f1 of rbpb(jet) f1 is 37.1. f1 of rbpb(jet) + et f1 is 66. f1 of rbpb(jet) + et f1 is 47.8. f1 of rbpb(jet) + et f1 is 39.7. f1 of rbpb(jet) + regu f1 is 64.8. f1 of rbpb(jet) + regu f1 is 54.6. f1 of rbpb(jet) + regu f1 is 42. f1 of rbpb(jet) + et + regu f1 is 67.8. f1 of rbpb(jet) + et + regu f1 is 55.4. f1 of rbpb(jet) + et + regu f1 is 43.8.
table 4 shows system performance on test data (* indicates statistical significance) precision of lucene precision is 0.47. recall of lucene recall is 0.48. f-score of lucene f-score is 0.47*. precision of lucene precision is 0.16. recall of lucene recall is 0.22. f-score of lucene f-score is 0.19. precision of edits precision is 0.22. recall of edits recall is 0.57. f-score of edits f-score is 0.32. precision of edits precision is 0.23. recall of edits recall is 0.21. f-score of edits f-score is 0.20. precision of tie precision is 0.66. recall of tie recall is 0.21. f-score of tie f-score is 0.31. precision of tie precision is 0.43. recall of tie recall is 0.01. f-score of tie f-score is 0.02. precision of ent precision is 0.77. recall of ent recall is 0.26. f-score of ent f-score is 0.39. precision of ent precision is 0.42. recall of ent recall is 0.15. f-score of ent f-score is 0.23*.
table 3 shows performance of translation extraction acc1 of cue(bilda) k=100 is 0.024. acc1 of cue(bilda) k=400 is 0.056. acc1 of cue(bilda) k=2000 is 0.101. acc10 of cue(bilda) k=100 is 0.093. acc10 of cue(bilda) k=400 is 0.170. acc10 of cue(bilda) k=2000 is 0.281. acc1 of cue(bistm) k=100 is 0.055. acc1 of cue(bistm) k=400 is 0.112. acc1 of cue(bistm) k=2000 is 0.184. acc10 of cue(bistm) k=100 is 0.218. acc10 of cue(bistm) k=400 is 0.286. acc10 of cue(bistm) k=2000 is 0.410. acc1 of cue(bistm+ts) k=100 is 0.052. acc1 of cue(bistm+ts) k=400 is 0.107. acc1 of cue(bistm+ts) k=2000 is 0.176. acc10 of cue(bistm+ts) k=100 is 0.196. acc10 of cue(bistm+ts) k=400 is 0.274. acc10 of cue(bistm+ts) k=2000 is 0.398. acc1 of liu(bilda) k=100 is 0.206. acc1 of liu(bilda) k=400 is 0.345. acc1 of liu(bilda) k=2000 is 0.426. acc10 of liu(bilda) k=100 is 0.463. acc10 of liu(bilda) k=400 is 0.550. acc10 of liu(bilda) k=2000 is 0.603. acc1 of liu(bistm) k=100 is 0.287. acc1 of liu(bistm) k=400 is 0.414. acc1 of liu(bistm) k=2000 is 0.479. acc10 of liu(bistm) k=100 is 0.531. acc10 of liu(bistm) k=400 is 0.625. acc10 of liu(bistm) k=2000 is 0.671. acc1 of liu(bistm+ts) k=100 is 0.283. acc1 of liu(bistm+ts) k=400 is 0.406. acc1 of liu(bistm+ts) k=2000 is 0.467. acc10 of liu(bistm+ts) k=100 is 0.536. acc10 of liu(bistm+ts) k=400 is 0.612. acc10 of liu(bistm+ts) k=2000 is 0.667.
table 3 shows comparison with results published in the literature, where ‘∗’ refers to models from nguyen and grishman (2015). f1 of svm (rink and harabagiu 2010) f1 is 82.2. f1 of rnn (socher et al. 2012) f1 is 77.6. f1 of mvrnn (socher et al. 2012) f1 is 82.4. f1 of fcm (yu et al. 2014) f1 is 83. f1 of hybrid fcm (yu et al. 2014) f1 is 83.4. f1 of sdp-lstm (xu et al. 2015b) f1 is 83.7. f1 of drnns (xu et al. 2016) f1 is 85.8. f1 of sptree (miwa and bansal 2016) f1 is 84.5. f1 of cnn+ softmax (zeng et al. 2014) f1 is 82.7. f1 of cr-cnn (dos santos et al. 2015) f1 is 84.1. f1 of depnn (liu et al. 2015) f1 is 83.6. f1 of deplcnn+ns (xu et al. 2015a) f1 is 85.6. f1 of stack-forward* f1 is 83.4. f1 of vote-bidirect* f1 is 84.1. f1 of vote-backward* f1 is 84.1. f1 of att-input-cnn f1 is 87.5. f1 of att-pooling-cnn f1 is 88.
table 4 shows comparison between the main model and variants. f1 of att-input-cnn (main) f1 is 87.5. f1 of att-input-cnn (variant-1) f1 is 87.2. f1 of att-input-cnn (variant-2) f1 is 87.3.
table 3 shows precision/recall/f1 for the three models. p of first party collection/use p is 0.73. r of first party collection/use r is 0.67. f of first party collection/use f is 0.7. p of first party collection/use p is 0.76. r of first party collection/use r is 0.73. f of first party collection/use f is 0.75. p of first party collection/use p is 0.69. r of first party collection/use r is 0.76. f of first party collection/use f is 0.72. p of third party sharing/collection p is 0.64. r of third party sharing/collection r is 0.63. f of third party sharing/collection f is 0.63. p of third party sharing/collection p is 0.67. r of third party sharing/collection r is 0.73. f of third party sharing/collection f is 0.7. p of third party sharing/collection p is 0.63. r of third party sharing/collection r is 0.61. f of third party sharing/collection f is 0.62. p of user choice/control p is 0.45. r of user choice/control r is 0.62. f of user choice/control f is 0.52. p of user choice/control p is 0.65. r of user choice/control r is 0.58. f of user choice/control f is 0.61. p of user choice/control p is 0.47. r of user choice/control r is 0.33. f of user choice/control f is 0.39. p of introductory/generic* p is 0.51. r of introductory/generic* r is 0.5. f of introductory/generic* f is 0.5. p of introductory/generic* p is 0.58. r of introductory/generic* r is 0.49. f of introductory/generic* f is 0.53. p of introductory/generic* p is 0.54. r of introductory/generic* r is 0.49. f of introductory/generic* f is 0.51. p of data security p is 0.48. r of data security r is 0.75. f of data security f is 0.59. p of data security p is 0.66. r of data security r is 0.67. f of data security f is 0.67. p of data security p is 0.67. r of data security r is 0.53. f of data security f is 0.59. p of internat’l and specific audiences p is 0.49. r of internat’l and specific audiences r is 0.69. f of internat’l and specific audiences f is 0.57. p of internat’l and specific audiences p is 0.7. r of internat’l and specific audiences r is 0.7. f of internat’l and specific audiences f is 0.7. p of internat’l and specific audiences p is 0.67. r of internat’l and specific audiences r is 0.66. f of internat’l and specific audiences f is 0.66. p of privacy contact information* p is 0.34. r of privacy contact information* r is 0.72. f of privacy contact information* f is 0.46. p of privacy contact information* p is 0.6. r of privacy contact information* r is 0.68. f of privacy contact information* f is 0.64. p of privacy contact information* p is 0.48. r of privacy contact information* r is 0.59. f of privacy contact information* f is 0.53. p of user access, edit, and deletion p is 0.47. r of user access, edit, and deletion r is 0.71. f of user access, edit, and deletion f is 0.57. p of user access, edit, and deletion p is 0.67. r of user access, edit, and deletion r is 0.56. f of user access, edit, and deletion f is 61. p of user access, edit, and deletion p is 0.48. r of user access, edit, and deletion r is 0.42. f of user access, edit, and deletion f is 0.45. p of practice not covered* p is 0.2. r of practice not covered* r is 0.47. f of practice not covered* f is 0.28. p of practice not covered* p is 0.19. r of practice not covered* r is 0.26. f of practice not covered* f is 0.22. p of practice not covered* p is 0.15. r of practice not covered* r is 0.12. f of practice not covered* f is 0.13. p of policy change p is 0.59. r of policy change r is 0.83. f of policy change f is 0.69. p of policy change p is 0.66. r of policy change r is 0.88. f of policy change f is 0.75. p of policy change p is 0.52. r of policy change r is 0.68. f of policy change f is 0.59. p of data retention p is 0.1. r of data retention r is 0.35. f of data retention f is 0.16. p of data retention p is 0.12. r of data retention r is 0.12. f of data retention f is 0.12. p of data retention p is 0.08. r of data retention r is 0.12. f of data retention f is 0.09. p of do not track p is 0.45. r of do not track r is 1. f of do not track f is 0.62. p of do not track p is 1. r of do not track r is 1. f of do not track f is 1. p of do not track p is 0.45. r of do not track r is 0.4. f of do not track f is 0.41. p of - p is 0.53. r of - r is 0.65. f of - f is 0.58. p of - p is 0.66. r of - r is 0.66. f of - f is 0.66. p of - p is 0.6. r of - r is 0.59. f of - f is 0.6.
table 2 shows test results over different domains on spo dataset. accuracy of spo basketball is 46.3. accuracy of spo social is 48.2. accuracy of spo publication is 59. accuracy of spo blocks is 41.9. accuracy of spo calendar is 74.4. accuracy of spo housing is 54. accuracy of spo restaurants is 75.9. accuracy of spo avg is 57.1. accuracy of lfp basketball is 73.1. accuracy of lfp social is 70.2. accuracy of lfp publication is 72. accuracy of lfp blocks is 55.4. accuracy of lfp calendar is 71.4. accuracy of lfp housing is 61.9. accuracy of lfp restaurants is 76.5. accuracy of lfp avg is 68.6. accuracy of cfp basketball is 80.3. accuracy of cfp social is 79.5. accuracy of cfp publication is 70.2. accuracy of cfp blocks is 54.1. accuracy of cfp calendar is 73.2. accuracy of cfp housing is 63.5. accuracy of cfp restaurants is 71.1. accuracy of cfp avg is 70.3. accuracy of dsp basketball is 71.6. accuracy of dsp social is 67.5. accuracy of dsp publication is 64. accuracy of dsp blocks is 53.9. accuracy of dsp calendar is 64.3. accuracy of dsp housing is 55. accuracy of dsp restaurants is 76.8. accuracy of dsp avg is 64.7. accuracy of dsp-c basketball is 80.5. accuracy of dsp-c social is 80. accuracy of dsp-c publication is 75.8. accuracy of dsp-c blocks is 55.6. accuracy of dsp-c calendar is 75. accuracy of dsp-c housing is 61.9. accuracy of dsp-c restaurants is 80.1. accuracy of dsp-c avg is 72.7. accuracy of dsp-cl basketball is 80.6. accuracy of dsp-cl social is 77.6. accuracy of dsp-cl publication is 70.2. accuracy of dsp-cl blocks is 53.1. accuracy of dsp-cl calendar is 75. accuracy of dsp-cl housing is 59.3. accuracy of dsp-cl restaurants is 74.4. accuracy of dsp-cl avg is 70.
table 3 shows results of feature validation r-1 of rf r-1 is 0.38559. r-2 of rf r-2 is 0.11887. r-su4 of rf r-su4 is 0.14907. r-1 of rf - w/o novel r-1 is 0.37297. r-2 of rf - w/o novel r-2 is 0.10964. r-su4 of rf - w/o novel r-su4 is 0.14021. r-1 of rf - w/o trad. r-1 is 0.36314. r-2 of rf - w/o trad. r-2 is 0.0991. r-su4 of rf - w/o trad. r-su4 is 0.13102.
table 2 shows translation results. bleu of base ed is 15. bleu of base ef is 26.76. bleu of base ec is 29.42. bleu of base ej is 37.1. bleu of mers ed is 15.62. bleu of mers ef is 27.33. bleu of mers ec is 29.75. bleu of mers ej is 37.76. bleu of csrs ed is 16.15. bleu of csrs ef is 28.05. bleu of csrs ec is 30.12. bleu of csrs ej is 37.83. bleu of mers-mini ed is 15.77. bleu of mers-mini ef is 28.13. bleu of mers-mini ec is 30.53. bleu of mers-mini ej is 38.14. bleu of csrs-mini ed is 16.49. bleu of csrs-mini ef is 28.3. bleu of csrs-mini ec is 31.63. bleu of csrs-mini ej is 38.32.
table 2 shows comparisons of results on the test sets. uas of o3-adding uas is 93.20. las of o3-adding las is 92.12. cm of o3-adding cm is 48.92. uas of o3-adding uas is 93.42. las of o3-adding las is 91.29. cm of o3-adding cm is 50.37. uas of o3-adding uas is 93.14. las of o3-adding las is 90.07. cm of o3-adding cm is 43.38. uas of o3-adding uas is 87.55. las of o3-adding las is 86.19. cm of o3-adding cm is 35.65. uas of o3-perceptron uas is 93.31. las of o3-perceptron las is 92.23. cm of o3-perceptron cm is 50.00. uas of o3-perceptron uas is 93.42. las of o3-perceptron las is 91.26. cm of o3-perceptron cm is 49.92. uas of o3-perceptron uas is 93.12. las of o3-perceptron las is 89.53. cm of o3-perceptron cm is 43.83. uas of o3-perceptron uas is 87.65. las of o3-perceptron las is 86.17. cm of o3-perceptron cm is 36.07. uas of pei et al. (2015) uas is 93.29. las of pei et al. (2015) las is 92.13. cm of pei et al. (2015) cm is –. uas of pei et al. (2015) uas is –. las of pei et al. (2015) las is –. cm of pei et al. (2015) cm is –. uas of pei et al. (2015) uas is –. las of pei et al. (2015) las is –. cm of pei et al. (2015) cm is –. uas of pei et al. (2015) uas is –. las of pei et al. (2015) las is –. cm of pei et al. (2015) cm is –. uas of fonseca and aluı́sio (2015) uas is –. las of fonseca and aluı́sio (2015) las is –. cm of fonseca and aluı́sio (2015) cm is –. uas of fonseca and aluı́sio (2015) uas is –. las of fonseca and aluı́sio (2015) las is –. cm of fonseca and aluı́sio (2015) cm is –. uas of fonseca and aluı́sio (2015) uas is 91.6–. las of fonseca and aluı́sio (2015) las is 88.9–. cm of fonseca and aluı́sio (2015) cm is –. uas of fonseca and aluı́sio (2015) uas is –. las of fonseca and aluı́sio (2015) las is –. cm of fonseca and aluı́sio (2015) cm is –. uas of zhang and zhao (2015) uas is –. las of zhang and zhao (2015) las is –. cm of zhang and zhao (2015) cm is –. uas of zhang and zhao (2015) uas is –. las of zhang and zhao (2015) las is –. cm of zhang and zhao (2015) cm is –. uas of zhang and zhao (2015) uas is 92.52. las of zhang and zhao (2015) las is –. cm of zhang and zhao (2015) cm is 41.10. uas of zhang and zhao (2015) uas is 86.01. las of zhang and zhao (2015) las is –. cm of zhang and zhao (2015) cm is 31.88. uas of koo and collins (2010) uas is 93.04. las of koo and collins (2010) las is –. cm of koo and collins (2010) cm is –. uas of koo and collins (2010) uas is –. las of koo and collins (2010) las is –. cm of koo and collins (2010) cm is –. uas of koo and collins (2010) uas is –. las of koo and collins (2010) las is –. cm of koo and collins (2010) cm is –. uas of koo and collins (2010) uas is –. las of koo and collins (2010) las is –. cm of koo and collins (2010) cm is –. uas of martins et al. (2013) uas is 93.07. las of martins et al. (2013) las is –. cm of martins et al. (2013) cm is –. uas of martins et al. (2013) uas is 92.82. las of martins et al. (2013) las is –. cm of martins et al. (2013) cm is –. uas of martins et al. (2013) uas is –. las of martins et al. (2013) las is –. cm of martins et al. (2013) cm is –. uas of martins et al. (2013) uas is –. las of martins et al. (2013) las is –. cm of martins et al. (2013) cm is –. uas of ma and zhao (2015) uas is 93.0–. las of ma and zhao (2015) las is –. cm of ma and zhao (2015) cm is 48.8–. uas of ma and zhao (2015) uas is –. las of ma and zhao (2015) las is –. cm of ma and zhao (2015) cm is –. uas of ma and zhao (2015) uas is –. las of ma and zhao (2015) las is –. cm of ma and zhao (2015) cm is –. uas of ma and zhao (2015) uas is 87.2–. las of ma and zhao (2015) las is –. cm of ma and zhao (2015) cm is 37.0–. uas of chen and manning (2014) uas is –. las of chen and manning (2014) las is –. cm of chen and manning (2014) cm is –. uas of chen and manning (2014) uas is 91.8–. las of chen and manning (2014) las is 89.6–. cm of chen and manning (2014) cm is –. uas of chen and manning (2014) uas is 92.0–. las of chen and manning (2014) las is 90.7–. cm of chen and manning (2014) cm is –. uas of chen and manning (2014) uas is 83.9–. las of chen and manning (2014) las is 82.4–. cm of chen and manning (2014) cm is –. uas of dyer et al. (2015) uas is –. las of dyer et al. (2015) las is –. cm of dyer et al. (2015) cm is –. uas of dyer et al. (2015) uas is 93.1–. las of dyer et al. (2015) las is 90.9–. cm of dyer et al. (2015) cm is –. uas of dyer et al. (2015) uas is –. las of dyer et al. (2015) las is –. cm of dyer et al. (2015) cm is –. uas of dyer et al. (2015) uas is 87.2–. las of dyer et al. (2015) las is 85.7–. cm of dyer et al. (2015) cm is –. uas of weiss et al. (2015) uas is –. las of weiss et al. (2015) las is –. cm of weiss et al. (2015) cm is –. uas of weiss et al. (2015) uas is 93.99. las of weiss et al. (2015) las is 92.05. cm of weiss et al. (2015) cm is –. uas of weiss et al. (2015) uas is –. las of weiss et al. (2015) las is –. cm of weiss et al. (2015) cm is –. uas of weiss et al. (2015) uas is –. las of weiss et al. (2015) las is –. cm of weiss et al. (2015) cm is –. uas of zhou et al. (2015) uas is 93.28. las of zhou et al. (2015) las is 92.35. cm of zhou et al. (2015) cm is –. uas of zhou et al. (2015) uas is –. las of zhou et al. (2015) las is –. cm of zhou et al. (2015) cm is –. uas of zhou et al. (2015) uas is –. las of zhou et al. (2015) las is –. cm of zhou et al. (2015) cm is –. uas of zhou et al. (2015) uas is –. las of zhou et al. (2015) las is –. cm of zhou et al. (2015) cm is –.
table 3 shows results of grsemi-crf with external information, measured in f1 score. f1 of none conll 2000 is 93.92. f1 of none conll 2003 is 84.66. f1 of brown(nyt) conll 2000 is 94.18. f1 of brown(nyt) conll 2003 is 86.57. f1 of brown(rcv1) conll 2000 is 94.05. f1 of brown(rcv1) conll 2003 is 88.22.
table 4 shows f1 scores of grsemi-crf with scalar or vectorial gating coefficients. f1 of scalars conll 2000 is 94.47. f1 of scalars conll 2003 is 89.27. f1 of vectors conll 2000 is 95.01. f1 of vectors conll 2003 is 89.44.
table 4 shows loss function runtime comparison (seconds per epoch) of the dnn models. runtime of random ce is 124. runtime of random hinge is 230. runtime of random ce is 710. runtime of random hinge is 783. runtime of mix ce is 20755. runtime of mix hinge is 21045. runtime of mix ce is 25928. runtime of mix hinge is 26380. runtime of max ce is 39338. runtime of max hinge is 41867. runtime of max ce is 49583. runtime of max hinge is 49427.
table 4 shows comparison of original and shufﬂed character-based word representation on decoding pos tag. correlation of russian raw is 0.906. correlation of russian shuf. is 0.671. correlation of slovenian raw is 0.8. correlation of slovenian shuf. is 0.653.
table 5 shows comparison of morpho-phonological knowledge transfer on different language pairs. correlation of bigram type overlap. fa is 0.176. correlation of bigram type overlap. ud is 0.761. correlation of bigram type overlap. en is 0.891. correlation of bigram type overlap. shuf en is 0.864. correlation of bigram type overlap. rand is 0.648. correlation of bigram token overlap. fa is 0.689. correlation of bigram token overlap. ud is 0.881. correlation of bigram token overlap. en is 0.999. correlation of bigram token overlap. shuf en is 0.993. correlation of bigram token overlap. rand is 0.65. correlation of trigram type overlap. fa is 0.523. correlation of trigram type overlap. ud is 0.522. correlation of trigram type overlap. en is 0.665. correlation of trigram type overlap. shuf en is 0.449. correlation of trigram type overlap. rand is 0.078. correlation of trigram token overlap. fa is 0.526. correlation of trigram token overlap. ud is 0.585. correlation of trigram token overlap. en is 0.978. correlation of trigram token overlap. shuf en is 0.796. correlation of trigram token overlap. rand is 0.078.
table 4 shows emotion classification results (one vs. accuracy of #anger wang (2012) is 0.72. accuracy of #anger roberts (2012) is 0.64. accuracy of #anger qadir (2013) is 0.44. accuracy of #anger mohammad (2014) is 0.28. accuracy of #anger this work is 0.80. accuracy of #disgust wang (2012) is –. accuracy of #disgust roberts (2012) is 0.67. accuracy of #disgust qadir (2013) is –. accuracy of #disgust mohammad (2014) is 0.19. accuracy of #disgust this work is 0.92. accuracy of #fear wang (2012) is 0.44. accuracy of #fear roberts (2012) is 0.74. accuracy of #fear qadir (2013) is 0.54. accuracy of #fear mohammad (2014) is 0.51. accuracy of #fear this work is 0.77. accuracy of #joy wang (2012) is 0.72. accuracy of #joy roberts (2012) is 0.68. accuracy of #joy qadir (2013) is 0.59. accuracy of #joy mohammad (2014) is 0.62. accuracy of #joy this work is 0.79. accuracy of #sadness wang (2012) is 0.65. accuracy of #sadness roberts (2012) is 0.69. accuracy of #sadness qadir (2013) is 0.46. accuracy of #sadness mohammad (2014) is 0.39. accuracy of #sadness this work is 0.62. accuracy of #surprise wang (2012) is 0.14. accuracy of #surprise roberts (2012) is 0.61. accuracy of #surprise qadir (2013) is –. accuracy of #surprise mohammad (2014) is 0.45. accuracy of #surprise this work is 0.64. accuracy of all wang (2012) is –. accuracy of all roberts (2012) is 0.67. accuracy of all qadir (2013) is 0.53. accuracy of all mohammad (2014) is 0.49. accuracy of all this work is 0.78.
table 4 shows correlation results on ukpconvargrank. pearson’s r of svm pearson’s r is .351. spearman’s ρ of svm spearman’s ρ is .402. pearson’s r of blstm pearson’s r is .270. spearman’s ρ of blstm spearman’s ρ is .354.
table 3 shows testing performance of lcsts, where “rnn” is canonical enc-dec, and “rnn context” its attentive variant. r-1 of rnn (hu et al. 2015) +c r-1 is 21.5. r-2 of rnn (hu et al. 2015) +c r-2 is 8.9. r-l of rnn (hu et al. 2015) +c r-l is 18.6. r-1 of rnn (hu et al. 2015) +w r-1 is 17.7. r-2 of rnn (hu et al. 2015) +w r-2 is 8.5. r-l of rnn (hu et al. 2015) +w r-l is 15.8. r-1 of rnn context (hu et al. 2015) +c r-1 is 29.9. r-2 of rnn context (hu et al. 2015) +c r-2 is 17.4. r-l of rnn context (hu et al. 2015) +c r-l is 27.2. r-1 of rnn context (hu et al. 2015) +w r-1 is 26.8. r-2 of rnn context (hu et al. 2015) +w r-2 is 16.1. r-l of rnn context (hu et al. 2015) +w r-l is 24.1. r-1 of copynet +c r-1 is 34.4. r-2 of copynet +c r-2 is 21.6. r-l of copynet +c r-l is 31.3. r-1 of copynet +w r-1 is 35. r-2 of copynet +w r-2 is 22.3. r-l of copynet +w r-l is 32.
table 5 shows subjective evaluation of mle and mrt on chinese-english translation. percentage of mle < mrt evaluator 1 is 54%. percentage of mle < mrt evaluator 2 is 53%. percentage of mle = mrt evaluator 1 is 24%. percentage of mle = mrt evaluator 2 is 22%. percentage of mle > mrt evaluator 1 is 22%. percentage of mle > mrt evaluator 2 is 25%.
table 7 shows comparison with previous work on english-french translation. vocab of mle vocab is 30000. bleu of mle bleu is 28.45. vocab of mle vocab is 30000. bleu of mle bleu is 29.97. vocab of mle vocab is 30000. bleu of mle bleu is 33.08. vocab of mle vocab is 40000. bleu of mle bleu is 29.50. vocab of mle vocab is 40000. bleu of mle bleu is 31.80. vocab of mle vocab is 40000. bleu of mle bleu is 30.40. vocab of mle vocab is 40000. bleu of mle bleu is 32.70. vocab of mle vocab is 80000. bleu of mle bleu is 30.59. vocab of mle vocab is 30000. bleu of mle bleu is 29.88. vocab of mrt vocab is 30000. bleu of mrt bleu is 31.30. vocab of mrt vocab is 30000. bleu of mrt bleu is 34.23.
table 2 shows bleu scores obtained on the wmt14 test set. bleu of baseline small is 10.7. bleu of baseline medium is 15.2. bleu of baseline full is 16.7. bleu of +source small is 10.7. bleu of +source medium is 16. bleu of +source full is 17.3. bleu of +target small is 11.2. bleu of +target medium is 16.4. bleu of +target full is 17.5.
table 2 shows evaluation metrics bleu-1 of monolingual bleu-1 is 0.715. bleu-2 of monolingual bleu-2 is 0.573. bleu-3 of monolingual bleu-3 is 0.468. bleu-4 of monolingual bleu-4 is 0.379. rouge-l of monolingual rouge-l is 0.616. cider-d of monolingual cider-d is 0.58. bleu-1 of alternate bleu-1 is 0.709. bleu-2 of alternate bleu-2 is 0.565. bleu-3 of alternate bleu-3 is 0.46. bleu-4 of alternate bleu-4 is 0.37. rouge-l of alternate rouge-l is 0.611. cider-d of alternate cider-d is 0.568. bleu-1 of transfer bleu-1 is 0.717. bleu-2 of transfer bleu-2 is 0.574. bleu-3 of transfer bleu-3 is 0.469. bleu-4 of transfer bleu-4 is 0.38. rouge-l of transfer rouge-l is 0.619. cider-d of transfer cider-d is 0.625.
table 3 shows image captioning results bleu of bing bleu is 0.101. meteor of bing meteor is 0.151. bleu of ms coco bleu is 0.291. meteor of ms coco meteor is 0.247.
table 5 shows parsing performance on learner english. r of petrov (2010) r is 0.863. p of petrov (2010) p is 0.865. cmr of petrov (2010) cmr is 0.358. r of stanford r is 0.812. p of stanford p is 0.832. f of stanford f is 0.822. cmr of stanford cmr is 0.398. r of charniak-johnson r is 0.845. p of charniak-johnson p is 0.865. f of charniak-johnson f is 0.855. cmr of charniak-johnson cmr is 0.465.
table 6 shows average performance across all ten folds for the gl model and for different feature sets. rmse of fluency bow is 1.1571. rmse of fluency shallow is 1.1181. rmse of fluency baselinem is 1.1462. rmse of conciseness bow is 1.2622. rmse of conciseness shallow is 1.1968. rmse of conciseness baselinem is 1.2456. rmse of completeness bow is 0.8408. rmse of completeness shallow is 0.7945. rmse of completeness baselinem is 0.813. rmse of referencing bow is 0.7047. rmse of referencing shallow is 0.6613. rmse of referencing baselinem is 0.7048. rmse of descriptiveness bow is 0.926. rmse of descriptiveness shallow is 0.873. rmse of descriptiveness baselinem is 0.9073. rmse of novelty bow is 0.7994. rmse of novelty shallow is 0.7607. rmse of novelty baselinem is 0.7797. rmse of richness bow is 0.9866. rmse of richness shallow is 0.9454. rmse of richness baselinem is 0.9568. rmse of attractiveness bow is 0.7048. rmse of attractiveness shallow is 0.6702. rmse of attractiveness baselinem is 0.6907. rmse of formality bow is 0.7025. rmse of formality shallow is 0.6691. rmse of formality baselinem is 0.692. rmse of popularity bow is 0.8329. rmse of popularity shallow is 0.7825. rmse of popularity baselinem is 0.825. rmse of technicality bow is 0.7923. rmse of technicality shallow is 0.7409. rmse of technicality baselinem is 0.7907. rmse of subjectivity bow is 0.875. rmse of subjectivity shallow is 0.8283. rmse of subjectivity baselinem is 0.9094. rmse of polarity bow is 0.8109. rmse of polarity shallow is 0.778. rmse of polarity baselinem is 0.8009. rmse of sentimentality bow is 0.817. rmse of sentimentality shallow is 0.7668. rmse of sentimentality baselinem is 0.8046.
table 3 shows sentence boundary detection results (f1) on test sets. f1 of marmot wsj is 97.64. f1 of marmot switchboard is 71.87. f1 of marmot wsj* is 53.02. f1 of nosyntax wsj is 98.21. f1 of nosyntax switchboard is 76.31†. f1 of nosyntax wsj* is 55.15. f1 of joint wsj is 98.21. f1 of joint switchboard is 76.65†. f1 of joint wsj* is 65.34†‡.
table 2 shows results for rst discourse treebank (carlson et al., 2001). rouge-1 of first k words rouge-1 is 23.5. rouge-2 of first k words rouge-2 is 8.3. rouge-1 of tree knapsack rouge-1 is 25.1. rouge-2 of tree knapsack rouge-2 is 8.7. rouge-1 of full rouge-1 is 26.3. rouge-2 of full rouge-2 is 8.
table 6 shows weighted f-score performance on supersense prediction for the development set and two test sets provided by johannsen et al. f-score of most frequent sense tw-r-dev is 47.54. f-score of most frequent sense tw-r-eval is 44.98. f-score of most frequent sense tw-j-eval is 38.65. f-score of inter-annotator agreement tw-r-eval is 69.15. f-score of inter-annotator agreement tw-j-eval is 61.15. f-score of (ciaramita and altun 2006)â€  tw-r-dev is 48.96. f-score of (ciaramita and altun 2006)â€  tw-r-eval is 45.03. f-score of (ciaramita and altun 2006)â€  tw-j-eval is 39.65. f-score of searn (johannsen et al. 2014) tw-r-dev is 56.59. f-score of searn (johannsen et al. 2014) tw-r-eval is 50.89. f-score of searn (johannsen et al. 2014) tw-j-eval is 40.50. f-score of hmm (johannsen et al. 2014) tw-r-dev is 57.14. f-score of hmm (johannsen et al. 2014) tw-r-eval is 50.98. f-score of hmm (johannsen et al. 2014) tw-j-eval is 41.84. f-score of ours semcor tw-r-dev is 54.47. f-score of ours semcor tw-r-eval is 50.30. f-score of ours semcor tw-j-eval is 35.61. f-score of searn (johannsen et al. 2014) tw-r-dev is 67.72. f-score of searn (johannsen et al. 2014) tw-r-eval is 57.14. f-score of searn (johannsen et al. 2014) tw-j-eval is 42.42. f-score of hmm (johannsen et al. 2014) tw-r-dev is 60.66. f-score of hmm (johannsen et al. 2014) tw-r-eval is 51.40. f-score of hmm (johannsen et al. 2014) tw-j-eval is 41.60. f-score of ours twitter tw-r-dev is 61.12. f-score of ours twitter tw-r-eval is 57.16. f-score of ours twitter tw-j-eval is 41.97.
table 3 shows performance on eval for the gen task. meteor of ir meteor is 7.9 (6.1). bleu-4 of ir bleu-4 is 13.7 (12.6). meteor of moses meteor is 9.1 (9.7). bleu-4 of moses bleu-4 is 11.6 (11.5). meteor of sum-nn meteor is 10.6 (10.3). bleu-4 of sum-nn bleu-4 is 19.3 (18.2). meteor of code-nn meteor is 12.3 (13.4). bleu-4 of code-nn bleu-4 is 20.5 (20.4). meteor of ir meteor is 6.3 (8.0). bleu-4 of ir bleu-4 is 13.5 (13.0). meteor of moses meteor is 8.3 (9.7). bleu-4 of moses bleu-4 is 15.4 (15.9). meteor of sum-nn meteor is 6.4 (8.7). bleu-4 of sum-nn bleu-4 is 13.3 (14.2). meteor of code-nn meteor is 10.9 (14.0). bleu-4 of code-nn bleu-4 is 18.4 (17.0).
table 6 shows automatic evaluations of events from fn. pre of ace-ann-fn pre is 77.2. rec of ace-ann-fn rec is 63.5. f1 of ace-ann-fn f1 is 69.7. pre of ace-sf-fn pre is 73.2. rec of ace-sf-fn rec is 64.1. f1 of ace-sf-fn f1 is 68.4. pre of ace-rf-fn pre is 72.6. rec of ace-rf-fn rec is 63.9. f1 of ace-rf-fn f1 is 68.0. pre of ace-sl-fn pre is 77.5. rec of ace-sl-fn rec is 64.3. f1 of ace-sl-fn f1 is 70.3. pre of ace-psl-fn pre is 77.6. rec of ace-psl-fn rec is 65.2. f1 of ace-psl-fn f1 is 70.7.
table 2 shows model evaluation with standard metric and our new metric. size of corpus size is 20%. p of corpus p is 90.04. r of corpus r is 89.9. f1 of corpus f1 is 89.97. pb of corpus pb is 45.22. rb of corpus rb is 43.37. fb of corpus fb is 44.28. size of corpus size is 50%. p of corpus p is 92.87. r of corpus r is 91.58. f1 of corpus f1 is 92.22. pb of corpus pb is 54.24. rb of corpus rb is 49.12. fb of corpus fb is 51.55. size of corpus size is 80%. p of corpus p is 94.07. r of corpus r is 92.21. f1 of corpus f1 is 93.13. pb of corpus pb is 61.8. rb of corpus rb is 54.74. fb of corpus fb is 58.05. size of corpus size is 100%. p of corpus p is 94.03. r of corpus r is 92.91. f1 of corpus f1 is 93.47. pb of corpus pb is 64.22. rb of corpus rb is 59.16. fb of corpus fb is 61.59. size of corpus size is 20%. p of corpus p is 92.93. r of corpus r is 92.58. f1 of corpus f1 is 92.76. pb of corpus pb is 45.76. rb of corpus rb is 44.13. fb of corpus fb is 44.93. size of corpus size is 50%. p of corpus p is 95.22. r of corpus r is 95.18. f1 of corpus f1 is 95.2. pb of corpus pb is 63. rb of corpus rb is 62.22. fb of corpus fb is 62.6. size of corpus size is 80%. p of corpus p is 95.68. r of corpus r is 95.74. f1 of corpus f1 is 95.71. pb of corpus pb is 67.26. rb of corpus rb is 66.96. fb of corpus fb is 67.11. size of corpus size is 100%. p of corpus p is 96.19. r of corpus r is 96.02. f1 of corpus f1 is 96.11. pb of corpus pb is 70.8. rb of corpus rb is 69.45. fb of corpus fb is 70.12. size of corpus size is 20%. p of corpus p is 87.32. r of corpus r is 86.37. f1 of corpus f1 is 86.84. pb of corpus pb is 42.16. rb of corpus rb is 40.23. fb of corpus fb is 41.17. size of corpus size is 50%. p of corpus p is 89.34. r of corpus r is 89.03. f1 of corpus f1 is 89.19. pb of corpus pb is 50.31. rb of corpus rb is 49.26. fb of corpus fb is 49.78. size of corpus size is 80%. p of corpus p is 91.42. r of corpus r is 91.1. f1 of corpus f1 is 91.26. pb of corpus pb is 60.48. rb of corpus rb is 59.25. fb of corpus fb is 59.86. size of corpus size is 100%. p of corpus p is 92. r of corpus r is 91.77. f1 of corpus f1 is 91.89. pb of corpus pb is 63.72. rb of corpus rb is 62.7. fb of corpus fb is 63.2. size of corpus size is 20%. p of corpus p is 89.7. r of corpus r is 89.31. f1 of corpus f1 is 89.5. pb of corpus pb is 43.53. rb of corpus rb is 42.35. fb of corpus fb is 42.93. size of corpus size is 50%. p of corpus p is 93.04. r of corpus r is 92.42. f1 of corpus f1 is 92.73. pb of corpus pb is 56.21. rb of corpus rb is 54.27. fb of corpus fb is 55.23. size of corpus size is 80%. p of corpus p is 94.45. r of corpus r is 93.94. f1 of corpus f1 is 94.19. pb of corpus pb is 64.55. rb of corpus rb is 62.5. fb of corpus fb is 63.51. size of corpus size is 100%. p of corpus p is 94.89. r of corpus r is 94.61. f1 of corpus f1 is 94.75. pb of corpus pb is 68.1. rb of corpus rb is 66.63. fb of corpus fb is 67.36.
table 3 shows performance of various models using 25 dimensional ce features, a:disease name recognition, b: disease classification task precision of nn+ce precision is 76.98. recall of nn+ce recall is 75.80. f1 score of nn+ce f1 score is 76.39. precision of nn+ce precision is 78.51. recall of nn+ce recall is 72.75. f1 score of nn+ce f1 score is 75.52. precision of bi-rnn+ce precision is 71.96. recall of bi-rnn+ce recall is 74.90. f1 score of bi-rnn+ce f1 score is 73.40. precision of bi-rnn+ce precision is 74.14. recall of bi-rnn+ce recall is 72.12. f1 score of bi-rnn+ce f1 score is 73.11. precision of bi-gru+ce precision is 76.28. recall of bi-gru+ce recall is 74.14. f1 score of bi-gru+ce f1 score is 75.19. precision of bi-gru+ce precision is 76.03. recall of bi-gru+ce recall is 69.81. f1 score of bi-gru+ce f1 score is 72.79. precision of bi-lstm+ce precision is 81.52. recall of bi-lstm+ce recall is 72.86. f1 score of bi-lstm+ce f1 score is 76.94. precision of bi-lstm+ce precision is 76.98. recall of bi-lstm+ce recall is 75.80. f1 score of bi-lstm+ce f1 score is 76.39. precision of nn+ce precision is 67.27. recall of nn+ce recall is 53.45. f1 score of nn+ce f1 score is 59.57. precision of nn+ce precision is 67.90. recall of nn+ce recall is 49.95. f1 score of nn+ce f1 score is 57.56. precision of bi-rnn+ce precision is 61.34. recall of bi-rnn+ce recall is 56.32. f1 score of bi-rnn+ce f1 score is 58.72. precision of bi-rnn+ce precision is 60.32. recall of bi-rnn+ce recall is 57.28. f1 score of bi-rnn+ce f1 score is 58.76. precision of bi-gru+ce precision is 61.94. recall of bi-gru+ce recall is 59.11. f1 score of bi-gru+ce f1 score is 60.49. precision of bi-gru+ce precision is 62.56. recall of bi-gru+ce recall is 56.50. f1 score of bi-gru+ce f1 score is 59.38. precision of bi-lstm+ce precision is 61.82. recall of bi-lstm+ce recall is 57.03. f1 score of bi-lstm+ce f1 score is 59.33. precision of bi-lstm+ce precision is 64.74. recall of bi-lstm+ce recall is 55.53. f1 score of bi-lstm+ce f1 score is 59.78.
table 3 shows comparison with previous state-of-the-art models on penn-ym, penn-sd and ctb5. uas of (zhang and nivre 2011) uas is 92.9. las of (zhang and nivre 2011) las is 91.8. uas of (zhang and nivre 2011) uas is 86.0. las of (zhang and nivre 2011) las is 84.4. uas of (bernd bohnet 2012) uas is 93.39. las of (bernd bohnet 2012) las is 92.38. uas of (bernd bohnet 2012) uas is 87.5. las of (bernd bohnet 2012) las is 85.9. uas of (zhang and mcdonald 2014) uas is 93.57. las of (zhang and mcdonald 2014) las is 92.48. uas of (zhang and mcdonald 2014) uas is 93.01. las of (zhang and mcdonald 2014) las is 90.64. uas of (zhang and mcdonald 2014) uas is 87.96. las of (zhang and mcdonald 2014) las is 86.34. uas of (dyer et al. 2015) uas is 93.1. las of (dyer et al. 2015) las is 90.9. uas of (dyer et al. 2015) uas is 87.2. las of (dyer et al. 2015) las is 85.7. uas of (weiss et al. 2015) uas is 93.99. las of (weiss et al. 2015) las is 92.05. uas of our basic model + segment uas is 93.51. las of our basic model + segment las is 92.45. uas of our basic model + segment uas is 94.08. las of our basic model + segment las is 91.82. uas of our basic model + segment uas is 87.55. las of our basic model + segment las is 86.23.
table 4 shows model performance of different way to learn segment embeddings. uas of average peen-ym is 93.23. uas of average peen-sd is 93.83. uas of average ctb5 is 87.24. uas of lstm-minus peen-ym is 93.51. uas of lstm-minus peen-sd is 94.08. uas of lstm-minus ctb5 is 87.55.
table 1 shows results on the test set. average f1 of berant et al. (2013) average f1 is 35.7. average f1 of yao and van durme (2014) average f1 is 33.0. average f1 of xu et al. (2014) average f1 is 39.1. average f1 of berant and liang (2014) average f1 is 39.9. average f1 of bao et al. (2014) average f1 is 37.5. average f1 of bordes et al. (2014) average f1 is 39.2. average f1 of dong et al. (2015) average f1 is 40.8. average f1 of yao (2015) average f1 is 44.3. average f1 of bast and haussmann (2015) average f1 is 49.4. average f1 of berant and liang (2015) average f1 is 49.7. average f1 of reddy et al. (2016) average f1 is 50.3. average f1 of yih et al. (2015) average f1 is 52.5. average f1 of structured average f1 is 44.1. average f1 of structured + joint average f1 is 47.1. average f1 of structured + unstructured average f1 is 47.0. average f1 of structured + joint + unstructured average f1 is 53.3.
table 2 shows overall performance comparison against baselines. perplexity of standard smt (koehn et al., 2003) perplexity is 128. bleu of standard smt (koehn et al., 2003) bleu is 21.68. human evaluation (syntactic) of standard smt (koehn et al., 2003) human evaluation (syntactic) is 0.563. human evaluation (semantic) of standard smt (koehn et al., 2003) human evaluation (semantic) is 0.248. human evaluation (overall) of standard smt (koehn et al., 2003) human evaluation (overall) is 0.811. perplexity of couplet smt (jiang and zhou, 2008) perplexity is 97. bleu of couplet smt (jiang and zhou, 2008) bleu is 28.71. human evaluation (syntactic) of couplet smt (jiang and zhou, 2008) human evaluation (syntactic) is 0.916. human evaluation (semantic) of couplet smt (jiang and zhou, 2008) human evaluation (semantic) is 0.503. human evaluation (overall) of couplet smt (jiang and zhou, 2008) human evaluation (overall) is 1.419. perplexity of lstm-rnn (sutskever et al., 2014) perplexity is 85. bleu of lstm-rnn (sutskever et al., 2014) bleu is 24.23. human evaluation (syntactic) of lstm-rnn (sutskever et al., 2014) human evaluation (syntactic) is 0.648. human evaluation (semantic) of lstm-rnn (sutskever et al., 2014) human evaluation (semantic) is 0.233. human evaluation (overall) of lstm-rnn (sutskever et al., 2014) human evaluation (overall) is 0.881. perplexity of ipoet (yan et al., 2013) perplexity is 143. bleu of ipoet (yan et al., 2013) bleu is 13.77. human evaluation (syntactic) of ipoet (yan et al., 2013) human evaluation (syntactic) is 0.228. human evaluation (semantic) of ipoet (yan et al., 2013) human evaluation (semantic) is 0.435. human evaluation (overall) of ipoet (yan et al., 2013) human evaluation (overall) is 0.663. perplexity of poetry smt (he et al., 2012) perplexity is 121. bleu of poetry smt (he et al., 2012) bleu is 23.11. human evaluation (syntactic) of poetry smt (he et al., 2012) human evaluation (syntactic) is 0.802. human evaluation (semantic) of poetry smt (he et al., 2012) human evaluation (semantic) is 0.516. human evaluation (overall) of poetry smt (he et al., 2012) human evaluation (overall) is 1.318. perplexity of rnnpg (zhang and lapata, 2014) perplexity is 99. bleu of rnnpg (zhang and lapata, 2014) bleu is 25.83. human evaluation (syntactic) of rnnpg (zhang and lapata, 2014) human evaluation (syntactic) is 0.853. human evaluation (semantic) of rnnpg (zhang and lapata, 2014) human evaluation (semantic) is 0.6. human evaluation (overall) of rnnpg (zhang and lapata, 2014) human evaluation (overall) is 1.453. perplexity of neural couplet machine (ncm) perplexity is 68. bleu of neural couplet machine (ncm) bleu is 32.62. human evaluation (syntactic) of neural couplet machine (ncm) human evaluation (syntactic) is 0.925. human evaluation (semantic) of neural couplet machine (ncm) human evaluation (semantic) is 0.631. human evaluation (overall) of neural couplet machine (ncm) human evaluation (overall) is 1.556.
table 2 shows accuracy of all models on the cnn and daily mail datasets. accuracy of frame-semantic model† dev is 36.3. accuracy of frame-semantic model† test is 40.2. accuracy of frame-semantic model† dev is 35.5. accuracy of frame-semantic model† test is 35.5. accuracy of word distance model† dev is 50.5. accuracy of word distance model† test is 50.9. accuracy of word distance model† dev is 56.4. accuracy of word distance model† test is 55.5. accuracy of deep lstm reader† dev is 55.0. accuracy of deep lstm reader† test is 57.0. accuracy of deep lstm reader† dev is 63.3. accuracy of deep lstm reader† test is 62.2. accuracy of attentive reader† dev is 61.6. accuracy of attentive reader† test is 63.0. accuracy of attentive reader† dev is 70.5. accuracy of attentive reader† test is 69.0. accuracy of impatient reader† dev is 61.8. accuracy of impatient reader† test is 63.8. accuracy of impatient reader† dev is 69.0. accuracy of impatient reader† test is 68.0. accuracy of memnns (window memory)‡ dev is 58.0. accuracy of memnns (window memory)‡ test is 60.6. accuracy of memnns (window memory)‡ dev is n/a. accuracy of memnns (window memory)‡ test is n/a. accuracy of memnns (window memory+self-sup.)‡ dev is 63.4. accuracy of memnns (window memory+self-sup.)‡ test is 66.8. accuracy of memnns (window memory+self-sup.)‡ dev is n/a. accuracy of memnns (window memory+self-sup.)‡ test is n/a. accuracy of memnns (ensemble)‡ dev is 66.2∗. accuracy of memnns (ensemble)‡ test is 69.4∗. accuracy of memnns (ensemble)‡ dev is n/a. accuracy of memnns (ensemble)‡ test is n/a. accuracy of ours: classifier dev is 67.1. accuracy of ours: classifier test is 67.9. accuracy of ours: classifier dev is 69.1. accuracy of ours: classifier test is 68.3. accuracy of ours: neural net dev is 72.4. accuracy of ours: neural net test is 72.4. accuracy of ours: neural net dev is 76.9. accuracy of ours: neural net test is 75.8.
table 4 shows performance scores of our method compared to the path-based baselines and the state-of-the-art distributional methods for hypernymy detection, on both variations of the dataset – with lexical and random split to train / test / validation. precision of snow precision is 0.843. recall of snow recall is 0.452. f1 of snow f1 is 0.589. precision of snow precision is 0.760. recall of snow recall is 0.438. f1 of snow f1 is 0.556. precision of snow + gen precision is 0.852. recall of snow + gen recall is 0.561. f1 of snow + gen f1 is 0.676. precision of snow + gen precision is 0.759. recall of snow + gen recall is 0.530. f1 of snow + gen f1 is 0.624. precision of hypenet path-based precision is 0.811. recall of hypenet path-based recall is 0.716. f1 of hypenet path-based f1 is 0.761. precision of hypenet path-based precision is 0.691. recall of hypenet path-based recall is 0.632. f1 of hypenet path-based f1 is 0.660.
table 2 shows performance of different rule integration methods on sst2. accuracy (%) of cnn (kim, 2014) accuracy (%) is 87.2. accuracy (%) of -but-clause accuracy (%) is 87.3. accuracy (%) of -l2-reg accuracy (%) is 87.5. accuracy (%) of -project accuracy (%) is 87.9. accuracy (%) of -opt-project accuracy (%) is 88.3. accuracy (%) of -pipeline accuracy (%) is 87.9. accuracy (%) of -rule-p accuracy (%) is 88.8. accuracy (%) of -rule-q accuracy (%) is 89.3.
table 1 shows statistical evaluation of the prediction of the on-line gp systems with respect to subj rating. prec. of fail prec. is 1.00. recall of fail recall is 0.52. f-measure of fail f-measure is 0.68. number of fail number is 204. prec. of suc. prec. is 0.95. recall of suc. recall is 1.00. f-measure of suc. f-measure is 0.97. number of suc. number is 1892. prec. of total prec. is 0.96. recall of total recall is 0.95. f-measure of total f-measure is 0.95. number of total number is 2096.
table 1 shows final pos tagging test set results on english wsj and treebank union as well as conll’09. accuracy of linear crf en is 97.17. accuracy of linear crf en-union is 97.60. accuracy of linear crf en-union is 94.58. accuracy of linear crf en-union is 96.04. accuracy of linear crf ca is 98.81. accuracy of linear crf ch is 94.45. accuracy of linear crf cz is 98.90. accuracy of linear crf en is 97.50. accuracy of linear crf ge is 97.14. accuracy of linear crf ja is 97.90. accuracy of linear crf sp is 98.79. accuracy of linear crf - is 97.17. accuracy of ling et al. (2015) en is 97.78. accuracy of ling et al. (2015) en-union is 97.44. accuracy of ling et al. (2015) en-union is 94.03. accuracy of ling et al. (2015) en-union is 96.18. accuracy of ling et al. (2015) ca is 98.77. accuracy of ling et al. (2015) ch is 94.38. accuracy of ling et al. (2015) cz is 99.00. accuracy of ling et al. (2015) en is 97.60. accuracy of ling et al. (2015) ge is 97.84. accuracy of ling et al. (2015) ja is 97.06. accuracy of ling et al. (2015) sp is 98.71. accuracy of ling et al. (2015) - is 97.16. accuracy of our local (b=1) en is 97.44. accuracy of our local (b=1) en-union is 97.66. accuracy of our local (b=1) en-union is 94.46. accuracy of our local (b=1) en-union is 96.59. accuracy of our local (b=1) ca is 98.91. accuracy of our local (b=1) ch is 94.56. accuracy of our local (b=1) cz is 98.96. accuracy of our local (b=1) en is 97.36. accuracy of our local (b=1) ge is 97.35. accuracy of our local (b=1) ja is 98.02. accuracy of our local (b=1) sp is 98.88. accuracy of our local (b=1) - is 97.29. accuracy of our local (b=8) en is 97.45. accuracy of our local (b=8) en-union is 97.69. accuracy of our local (b=8) en-union is 94.46. accuracy of our local (b=8) en-union is 96.64. accuracy of our local (b=8) ca is 98.88. accuracy of our local (b=8) ch is 94.56. accuracy of our local (b=8) cz is 98.96. accuracy of our local (b=8) en is 97.40. accuracy of our local (b=8) ge is 97.35. accuracy of our local (b=8) ja is 98.02. accuracy of our local (b=8) sp is 98.89. accuracy of our local (b=8) - is 97.30. accuracy of our global (b=8) en is 97.44. accuracy of our global (b=8) en-union is 97.77. accuracy of our global (b=8) en-union is 94.80. accuracy of our global (b=8) en-union is 96.86. accuracy of our global (b=8) ca is 99.03. accuracy of our global (b=8) ch is 94.72. accuracy of our global (b=8) cz is 99.02. accuracy of our global (b=8) en is 97.65. accuracy of our global (b=8) ge is 97.52. accuracy of our global (b=8) ja is 98.37. accuracy of our global (b=8) sp is 98.97. accuracy of our global (b=8) - is 97.47. accuracy of parsey mcparseface en-union is 97.52. accuracy of parsey mcparseface en-union is 94.24. accuracy of parsey mcparseface en-union is 96.45.
table 4 shows sentence compression results on news data. a of filippova et al. (2015) a is 35.36. f1 of filippova et al. (2015) f1 is 82.83. readability of filippova et al. (2015) readability is 4.66. informativeness of filippova et al. (2015) informativeness is 4.03. readability of automatic readability is 4.31. informativeness of automatic informativeness is 3.77. a of our local (b=1) a is 30.51. f1 of our local (b=1) f1 is 78.72. readability of our local (b=1) readability is 4.58. informativeness of our local (b=1) informativeness is 4.03. a of our local (b=8) a is 31.19. f1 of our local (b=8) f1 is 75.69. a of our global (b=8) a is 35.16. f1 of our global (b=8) f1 is 81.41. readability of our global (b=8) readability is 4.67. informativeness of our global (b=8) informativeness is 4.07.
table 1 shows performance comparison between different embeddings style. accuracy of w/o embed alarm is 97.25. accuracy of w/o embed apps is 89.16. accuracy of w/o embed calendar is 91.34. accuracy of w/o embed communication is 99.1. accuracy of w/o embed finance is 90.44. accuracy of w/o embed flights is 94.19. accuracy of w/o embed games is 90.16. accuracy of w/o embed hotel is 93.23. accuracy of w/o embed livemovie is 90.88. accuracy of w/o embed livetv is 83.14. accuracy of w/o embed movies is 93.27. accuracy of w/o embed music is 87.87. accuracy of w/o embed mystuff is 94.2. accuracy of w/o embed note is 97.62. accuracy of w/o embed ondevice is 97.51. accuracy of w/o embed places is 97.29. accuracy of w/o embed reminder is 98.72. accuracy of w/o embed sports is 76.96. accuracy of w/o embed timer is 91.1. accuracy of w/o embed travel is 81.58. accuracy of w/o embed tv is 91.42. accuracy of w/o embed weather is 97.31. accuracy of w/o embed average is 91.99. accuracy of 6b-50d alarm is 97.68. accuracy of 6b-50d apps is 91.07. accuracy of 6b-50d calendar is 92.43. accuracy of 6b-50d communication is 99.13. accuracy of 6b-50d finance is 90.84. accuracy of 6b-50d flights is 92.99. accuracy of 6b-50d games is 91.79. accuracy of 6b-50d hotel is 94.21. accuracy of 6b-50d livemovie is 92.64. accuracy of 6b-50d livetv is 85.02. accuracy of 6b-50d movies is 94.01. accuracy of 6b-50d music is 90.37. accuracy of 6b-50d mystuff is 94.4. accuracy of 6b-50d note is 98.36. accuracy of 6b-50d ondevice is 97.77. accuracy of 6b-50d places is 97.68. accuracy of 6b-50d reminder is 98.96. accuracy of 6b-50d sports is 78.53. accuracy of 6b-50d timer is 91.79. accuracy of 6b-50d travel is 82.57. accuracy of 6b-50d tv is 94.11. accuracy of 6b-50d weather is 97.33. accuracy of 6b-50d average is 92.89. accuracy of 840b-300d alarm is 97.5. accuracy of 840b-300d apps is 92.52. accuracy of 840b-300d calendar is 92.32. accuracy of 840b-300d communication is 99.08. accuracy of 840b-300d finance is 90.72. accuracy of 840b-300d flights is 93.99. accuracy of 840b-300d games is 92.09. accuracy of 840b-300d hotel is 93.97. accuracy of 840b-300d livemovie is 92.8. accuracy of 840b-300d livetv is 84.67. accuracy of 840b-300d movies is 93.97. accuracy of 840b-300d music is 90.9. accuracy of 840b-300d mystuff is 94.51. accuracy of 840b-300d note is 98.36. accuracy of 840b-300d ondevice is 97.6. accuracy of 840b-300d places is 97.68. accuracy of 840b-300d reminder is 98.94. accuracy of 840b-300d sports is 78.38. accuracy of 840b-300d timer is 91.33. accuracy of 840b-300d travel is 82.43. accuracy of 840b-300d tv is 94.91. accuracy of 840b-300d weather is 97.4. accuracy of 840b-300d average is 93.00. accuracy of sent alarm is 97.68. accuracy of sent apps is 94.24. accuracy of sent calendar is 92.53. accuracy of sent communication is 99.08. accuracy of sent finance is 90.76. accuracy of sent flights is 94.59. accuracy of sent games is 93.08. accuracy of sent hotel is 94.7. accuracy of sent livemovie is 93.28. accuracy of sent livetv is 85.41. accuracy of sent movies is 94.75. accuracy of sent music is 91.75. accuracy of sent mystuff is 94.51. accuracy of sent note is 98.49. accuracy of sent ondevice is 97.77. accuracy of sent places is 98.01. accuracy of sent reminder is 98.96. accuracy of sent sports is 78.7. accuracy of sent timer is 92.33. accuracy of sent travel is 83.64. accuracy of sent tv is 95.19. accuracy of sent weather is 97.4. accuracy of sent average is 93.49. accuracy of sent+ alarm is 97.74. accuracy of sent+ apps is 94.3. accuracy of sent+ calendar is 92.43. accuracy of sent+ communication is 99.12. accuracy of sent+ finance is 90.82. accuracy of sent+ flights is 94.59. accuracy of sent+ games is 92.92. accuracy of sent+ hotel is 94.78. accuracy of sent+ livemovie is 93.37. accuracy of sent+ livetv is 85.86. accuracy of sent+ movies is 95.16. accuracy of sent+ music is 91.33. accuracy of sent+ mystuff is 94.95. accuracy of sent+ note is 98.52. accuracy of sent+ ondevice is 97.84. accuracy of sent+ places is 97.75. accuracy of sent+ reminder is 98.96. accuracy of sent+ sports is 79.44. accuracy of sent+ timer is 92.61. accuracy of sent+ travel is 82.81. accuracy of sent+ tv is 95.47. accuracy of sent+ weather is 97.47. accuracy of sent+ average is 93.56.
table 2 shows performance for selected domains as the number of unlabeled data increases. accuracy of 0 apps is 89.16. accuracy of 0 music is 87.87. accuracy of 0 tv is 91.42. accuracy of 10% apps is 89.83. accuracy of 10% music is 89.12. accuracy of 10% tv is 92.28. accuracy of 20% apps is 90.04. accuracy of 20% music is 89.61. accuracy of 20% tv is 92.83. accuracy of 30% apps is 90.26. accuracy of 30% music is 90.4. accuracy of 30% tv is 93.61. accuracy of 40% apps is 90.88. accuracy of 40% music is 90.83. accuracy of 40% tv is 93.96. accuracy of 50% apps is 91.9. accuracy of 50% music is 91.26. accuracy of 50% tv is 94.67. accuracy of 60% apps is 92.41. accuracy of 60% music is 91.31. accuracy of 60% tv is 94.91. accuracy of 70% apps is 92.41. accuracy of 70% music is 91.33. accuracy of 70% tv is 95.12. accuracy of 80% apps is 92.95. accuracy of 80% music is 91.38. accuracy of 80% tv is 95.34. accuracy of 90% apps is 93.72. accuracy of 90% music is 91.33. accuracy of 90% tv is 95.44. accuracy of 100% apps is 94.3. accuracy of 100% music is 91.33. accuracy of 100% tv is 95.47.
table 2 shows development and test set results for shiftreduce dependency parser on penn treebank using only (s1, s0, q0) positional features. uas of c & m 2014 uas is 92.0. las of c & m 2014 las is 89.7. uas of c & m 2014 uas is 91.8. las of c & m 2014 las is 89.6. uas of dyer et al. 2015 uas is 93.2. las of dyer et al. 2015 las is 90.9. uas of dyer et al. 2015 uas is 93.1. las of dyer et al. 2015 las is 90.9. uas of weiss et al. 2015 uas is 93.19. las of weiss et al. 2015 las is 91.18. uas of + percept./beam uas is 93.99. las of + percept./beam las is 92.05. uas of bi-lstm uas is 93.31. las of bi-lstm las is 91.01. uas of bi-lstm uas is 93.21. las of bi-lstm las is 91.16. uas of 2-layer bi-lstm uas is 93.67. las of 2-layer bi-lstm las is 91.48. uas of 2-layer bi-lstm uas is 93.42. las of 2-layer bi-lstm las is 91.36.
table 3 shows results on chinese event detection. p of maxent p is 50.0. r of maxent r is 77.0. f of maxent f is 60.6. p of maxent p is 47.5. r of maxent r is 73.1. f of maxent f is 57.6. p of rich-c p is 62.2. r of rich-c r is 71.9. f of rich-c f is 66.7. p of rich-c p is 58.9. r of rich-c r is 68.1. f of rich-c f is 63.2. p of hnn p is 74.2. r of hnn r is 63.1. f of hnn f is 68.2. p of hnn p is 77.1. r of hnn r is 53.1. f of hnn f is 63.0.
table 3 shows per category performance. p (%) of brand p (%) is 91.79. r (%) of brand r (%) is 87.93. f1 of brand f1 is 89.82. p (%) of brand p (%) is 89.3. r (%) of brand r (%) is 89.3. f1 of brand f1 is 89.3. p (%) of brand p (%) is 93.99. r (%) of brand r (%) is 91.20. f1 of brand f1 is 92.57. p (%) of brand p (%) is 95.15. r (%) of brand r (%) is 92.29. f1 of brand f1 is 93.70. p (%) of model p (%) is 86.28. r (%) of model r (%) is 80.71. f1 of model f1 is 83.40. p (%) of model p (%) is 80.7. r (%) of model r (%) is 78.9. f1 of model f1 is 79.8. p (%) of model p (%) is 85.56. r (%) of model r (%) is 80.89. f1 of model f1 is 83.16. p (%) of model p (%) is 87.25. r (%) of model r (%) is 85.90. f1 of model f1 is 86.57. p (%) of product p (%) is 87.85. r (%) of product r (%) is 88.16. f1 of product f1 is 88.00. p (%) of product p (%) is 83.4. r (%) of product r (%) is 85.0. f1 of product f1 is 84.2. p (%) of product p (%) is 87.90. r (%) of product r (%) is 87.92. f1 of product f1 is 87.91. p (%) of product p (%) is 91.94. r (%) of product r (%) is 90.98. f1 of product f1 is 91.46. p (%) of product family p (%) is 89.27. r (%) of product family r (%) is 81.41. f1 of product family f1 is 85.16. p (%) of product family p (%) is 81.4. r (%) of product family r (%) is 79.0. f1 of product family f1 is 80.2. p (%) of product family p (%) is 88.12. r (%) of product family r (%) is 82.17. f1 of product family f1 is 85.04. p (%) of product family p (%) is 87.98. r (%) of product family r (%) is 87.47. f1 of product family f1 is 87.73. p (%) of overall p (%) is 88.86. r (%) of overall r (%) is 86.29. f1 of overall f1 is 87.55. p (%) of overall p (%) is 84.3. r (%) of overall r (%) is 84.5. f1 of overall f1 is 84.4. p (%) of overall p (%) is 89.18. r (%) of overall r (%) is 87.10. f1 of overall f1 is 88.13. p (%) of overall p (%) is 91.61. r (%) of overall r (%) is 90.24. f1 of overall f1 is 90.92.
table 4 shows evaluation of the programs with the highest f1 score in the beam (abest prec. of no curriculum prec. is 79.1. rec. of no curriculum rec. is 91.1. f1 of no curriculum f1 is 78.5. acc. of no curriculum acc. is 67.2. prec. of curriculum prec. is 88.6. rec. of curriculum rec. is 96.1. f1 of curriculum f1 is 89.5. acc. of curriculum acc. is 79.8.
table 4 shows graphquestions results. f1 of sempre (berant et al. 2013) f1 is 10.80. f1 of parasempre (berant and liang 2014) f1 is 12.79. f1 of jacana (yao and van durme 2014) f1 is 5.08. f1 of neural baseline f1 is 16.24. f1 of scanner f1 is 17.02.
table 6 shows spades results. f1 of unsupervised ccg (bisk et al. 2016) f1 is 24.8. f1 of semi-supervised ccg (bisk et al. 2016) f1 is 28.4. f1 of neural baseline f1 is 28.6. f1 of supervised ccg (bisk et al. 2016) f1 is 30.9. f1 of rule-based system (bisk et al. 2016) f1 is 31.4. f1 of scanner f1 is 31.5.
table 8 shows evaluation of predicates induced by scanner against easyccg. f1 of - scanner is 51.2. f1 of - baseline is 45.5. f1 of conj (1422) scanner is 56.1. f1 of conj (1422) baseline is 66.4. f1 of control (132) scanner is 28.3. f1 of control (132) baseline is 40.5. f1 of pp (3489) scanner is 46.2. f1 of pp (3489) baseline is 23.1. f1 of subord (76) scanner is 37.9. f1 of subord (76) baseline is 52.9. f1 of - scanner is 42.1. f1 of - baseline is 25.5. f1 of - scanner is 11.9. f1 of - baseline is 15.3.
table 2 shows results of all three tasks on the kbp 2016 evaluation sets. muc of kbp2016 muc is 26.37. b3 of kbp2016 b3 is 37.49. ceafe of kbp2016 ceafe is 34.21. blanc of kbp2016 blanc is 22.25. avg-f of kbp2016 avg-f is 30.08. trigger of kbp2016 trigger is 46.99. muc of indep. muc is 22.71. b3 of indep. b3 is 40.72. ceafe of indep. ceafe is 39. blanc of indep. blanc is 22.71. avg-f of indep. avg-f is 31.28. trigger of indep. trigger is 48.82. anaphoricity of indep. anaphoricity is 27.35. muc of joint muc is 27.41. b3 of joint b3 is 40.9. ceafe of joint ceafe is 39. blanc of joint blanc is 25. avg-f of joint avg-f is 33.08. trigger of joint trigger is 49.3. anaphoricity of joint anaphoricity is 31.94. muc of delta over indep. muc is 4.7. b3 of delta over indep. b3 is 0.18. ceafe of delta over indep. ceafe is 0. blanc of delta over indep. blanc is 2.29. avg-f of delta over indep. avg-f is 1.8. trigger of delta over indep. trigger is 0.48. anaphoricity of delta over indep. anaphoricity is 4.59.
table 3 shows results of model ablations on the kbp 2016 evaluation sets. f-score of coref indep. is 31.28. delta f-score of coref indep.+coreftrigger is 0.39. delta f-score of coref indep.+corefanaph is 0.4. delta f-score of coref indep.+triggeranaph is 0.11. delta f-score of coref joint-coreftrigger is 0.56. delta f-score of coref joint-corefanaph is 0.63. delta f-score of coref joint-triggeranaph is 1.89. delta f-score of coref joint is 1.8. f-score of trigger indep. is 48.82. delta f-score of trigger indep.+coreftrigger is 0.42. delta f-score of trigger indep.+corefanaph is -0.08. delta f-score of trigger indep.+triggeranaph is 0.38. delta f-score of trigger joint-coreftrigger is -0.06. delta f-score of trigger joint-corefanaph is 0.66. delta f-score of trigger joint-triggeranaph is 0.5. delta f-score of trigger joint is 0.48. f-score of anaph indep. is 27.35. delta f-score of anaph indep.+coreftrigger is -0.05. delta f-score of anaph indep.+corefanaph is 3.45. delta f-score of anaph indep.+triggeranaph is 1.35. delta f-score of anaph joint-coreftrigger is 4.41. delta f-score of anaph joint-corefanaph is 1.46. delta f-score of anaph joint-triggeranaph is 4.01. delta f-score of anaph joint is 4.59. f-score of coref indep. is 25.84. delta f-score of coref indep.+coreftrigger is 0.95. delta f-score of coref indep.+corefanaph is 0.37. delta f-score of coref indep.+triggeranaph is 0.14. delta f-score of coref joint-coreftrigger is 0.19. delta f-score of coref joint-corefanaph is 1.5. delta f-score of coref joint-triggeranaph is 1.65. delta f-score of coref joint is 1.95. f-score of trigger indep. is 39.82. delta f-score of trigger indep.+coreftrigger is 0.56. delta f-score of trigger indep.+corefanaph is 0.04. delta f-score of trigger indep.+triggeranaph is 0.52. delta f-score of trigger joint-coreftrigger is 0.35. delta f-score of trigger joint-corefanaph is 0.88. delta f-score of trigger joint-triggeranaph is 0.5. delta f-score of trigger joint is 0.71. f-score of anaph indep. is 19.31. delta f-score of anaph indep.+coreftrigger is -0.37. delta f-score of anaph indep.+corefanaph is -0.11. delta f-score of anaph indep.+triggeranaph is 0.02. delta f-score of anaph joint-coreftrigger is 3.34. delta f-score of anaph joint-corefanaph is 0.28. delta f-score of anaph joint-triggeranaph is 1.79. delta f-score of anaph joint is 4.02.
table 6 shows evaluation results of aes on three datasets. qwk score of svr-basic prompt 1 is 0.554. qwk score of svr-basic prompt 2 is 0.468. qwk score of svr-basic prompt 3 is 0.457. qwk score of svr-basic+mode prompt 1 is 0.6. qwk score of svr-basic+mode prompt 2 is 0.501. qwk score of svr-basic+mode prompt 3 is 0.481. qwk score of blrr-basic prompt 1 is 0.683. qwk score of blrr-basic prompt 2 is 0.557. qwk score of blrr-basic prompt 3 is 0.513. qwk score of blrr-basic+mode prompt 1 is 0.696. qwk score of blrr-basic+mode prompt 2 is 0.565. qwk score of blrr-basic+mode prompt 3 is 0.527.
table 1 shows accuracy of encoders with position features (wrd+pos) and without (wrd) in terms of bleu and perplexity (ppl) on iwslt’14 german to english translation; results include unknown word replacement. bleu of phrase-based wrd is 28.4. bleu of lstm wrd+pos is 27.4. bleu of lstm wrd is 27.3. ppl of lstm wrd+pos is 10.8. bleu of bilstm wrd+pos is 29.7. bleu of bilstm wrd is 29.8. ppl of bilstm wrd+pos is 9.9. bleu of pooling wrd+pos is 26.1. bleu of pooling wrd is 19.7. ppl of pooling wrd+pos is 11. bleu of convolutional wrd+pos is 29.9. bleu of convolutional wrd is 20.1. ppl of convolutional wrd+pos is 9.1. bleu of deep convolutional 6/3 wrd+pos is 30.4. bleu of deep convolutional 6/3 wrd is 25.2. ppl of deep convolutional 6/3 wrd+pos is 8.9.
table 2 shows accuracy on three wmt tasks, including results published in previous work. vocabulary size of bigru vocabulary size is 90k. bleu of bigru bleu is 28.1. vocabulary size of bilstm vocabulary size is 80k. bleu of bilstm bleu is 27.5. vocabulary size of convolutional vocabulary size is 80k. bleu of convolutional bleu is 27.1. vocabulary size of deep convolutional 8/4 vocabulary size is 80k. bleu of deep convolutional 8/4 bleu is 27.8.
table 3 shows bleu results for amr generation. bleu of giga-20m dev is 33.1. bleu of giga-20m test is 33.8. bleu of giga-2m dev is 31.8. bleu of giga-2m test is 32.3. bleu of giga-200k dev is 27.2. bleu of giga-200k test is 27.4. bleu of amr-only dev is 21.7. bleu of amr-only test is 22. bleu of pbmt* (pourdamghani et al. 2016) dev is 27.2. bleu of pbmt* (pourdamghani et al. 2016) test is 26.9. bleu of tsp (song et al. 2016) dev is 21.1. bleu of tsp (song et al. 2016) test is 22.4. bleu of treetostr (flanigan et al. 2016) dev is 23. bleu of treetostr (flanigan et al. 2016) test is 23.
table 5 shows performance of morse against morfessor on the non-canonical version of sd17 p of morfessor p is 65.95. r of morfessor r is 51.13. f1 of morfessor f1 is 57.60. p of morse p is 75.35. r of morse r is 83.60. f1 of morse f1 is 79.26. p of morse-cv p is 84.6. r of morse-cv r is 78.36. f1 of morse-cv f1 is 81.29.
table 2 shows error rates (%) on larger datasets in comparison with previous models. error rates of dpcnn + unsupervised embed. yelp.p is 2.64. error rates of dpcnn + unsupervised embed. yelp.f is 30.58. error rates of dpcnn + unsupervised embed. yahoo is 23.9. error rates of dpcnn + unsupervised embed. ama.f is 34.81. error rates of dpcnn + unsupervised embed. ama.p is 3.32. error rates of shallowcnn + unsup. embed. [jz16] yelp.p is 2.9. error rates of shallowcnn + unsup. embed. [jz16] yelp.f is 32.39. error rates of shallowcnn + unsup. embed. [jz16] yahoo is 24.85. error rates of shallowcnn + unsup. embed. [jz16] ama.f is 36.24. error rates of shallowcnn + unsup. embed. [jz16] ama.p is 3.79. error rates of hierarchical attention net [yydhsh16] yahoo is 24.2. error rates of hierarchical attention net [yydhsh16] ama.f is 36.4. error rates of [csbl16] char-level cnn: best yelp.p is 4.28. error rates of [csbl16] char-level cnn: best yelp.f is 35.28. error rates of [csbl16] char-level cnn: best yahoo is 26.57. error rates of [csbl16] char-level cnn: best ama.f is 37. error rates of [csbl16] char-level cnn: best ama.p is 4.28. error rates of fasttext bigrams (joulin et al., 2016) yelp.p is 4.3. error rates of fasttext bigrams (joulin et al., 2016) yelp.f is 36.1. error rates of fasttext bigrams (joulin et al., 2016) yahoo is 27.7. error rates of fasttext bigrams (joulin et al., 2016) ama.f is 39.8. error rates of fasttext bigrams (joulin et al., 2016) ama.p is 5.4. error rates of [zzl15] char-level cnn: best yelp.p is 4.88. error rates of [zzl15] char-level cnn: best yelp.f is 37.95. error rates of [zzl15] char-level cnn: best yahoo is 28.8. error rates of [zzl15] char-level cnn: best ama.f is 40.43. error rates of [zzl15] char-level cnn: best ama.p is 4.93. error rates of [zzl15] word-level cnn: best yelp.p is 4.6. error rates of [zzl15] word-level cnn: best yelp.f is 39.58. error rates of [zzl15] word-level cnn: best yahoo is 28.84. error rates of [zzl15] word-level cnn: best ama.f is 42.39. error rates of [zzl15] word-level cnn: best ama.p is 5.51. error rates of [zzl15] linear model: best yelp.p is 4.36. error rates of [zzl15] linear model: best yelp.f is 40.14. error rates of [zzl15] linear model: best yahoo is 28.96. error rates of [zzl15] linear model: best ama.f is 44.74. error rates of [zzl15] linear model: best ama.p is 7.98.
table 2 shows accuracy on the simplequestions and webqsp relation detection tasks (test sets). accuracy of words simplequestions is 91.3. accuracy of char-3-gram simplequestions is 90.0. accuracy of char-3-gram webqsp is 77.74. accuracy of words simplequestions is 91.2. accuracy of words webqsp is 79.32. accuracy of rel_names simplequestions is 88.9. accuracy of rel_names webqsp is 78.96. accuracy of words + rel_names simplequestions is 93.3. accuracy of words + rel_names webqsp is 82.53.
table 1 shows performance on ace05 test dataset. p of li and ji (2014) p is 0.852. r of li and ji (2014) r is 0.769. f1 of li and ji (2014) f1 is 0.808. p of li and ji (2014) p is 0.689. r of li and ji (2014) r is 0.419. f1 of li and ji (2014) f1 is 0.521. p of li and ji (2014) p is 0.654. r of li and ji (2014) r is 0.398. f1 of li and ji (2014) f1 is 0.495. p of sptree p is 0.829. r of sptree r is 0.839. f1 of sptree f1 is 0.834. p of sptree p is 0.572. r of sptree r is 0.54. f1 of sptree f1 is 0.556. p of sptree1 p is 0.823. r of sptree1 r is 0.839. f1 of sptree1 f1 is 0.831. p of sptree1 p is 0.605. r of sptree1 r is 0.553. f1 of sptree1 f1 is 0.578. p of sptree1 p is 0.578. r of sptree1 r is 0.529. f1 of sptree1 f1 is 0.553. p of our model p is 0.84. r of our model r is 0.813. f1 of our model f1 is 0.826. p of our model p is 0.579. r of our model r is 0.54. f1 of our model f1 is 0.559. p of our model p is 0.555. r of our model r is 0.518. f1 of our model f1 is 0.536.
table 5 shows feature ablation analysis of the paragraph representations of our document reader. f1 of full f1 is 78.8. f1 of no ftoken f1 is 78.0. f1 of no fexact match f1 is 77.3. f1 of no faligned f1 is 77.3. f1 of no faligned and fexact match f1 is 59.4.
table 6 shows full wikipedia results. accuracy of squad (all wikipedia) - is n/a. accuracy of squad (all wikipedia) squad is 27.1. accuracy of squad (all wikipedia) +fine-tune (ds) is 28.4. accuracy of squad (all wikipedia) +multitask (ds) is 29.8. accuracy of curatedtrec - is 31.3. accuracy of curatedtrec squad is 19.7. accuracy of curatedtrec +fine-tune (ds) is 25.7. accuracy of curatedtrec +multitask (ds) is 25.4. accuracy of webquestions - is 39.8. accuracy of webquestions squad is 11.8. accuracy of webquestions +fine-tune (ds) is 19.5. accuracy of webquestions +multitask (ds) is 20.7. accuracy of wikimovies - is n/a. accuracy of wikimovies squad is 24.5. accuracy of wikimovies +fine-tune (ds) is 34.3. accuracy of wikimovies +multitask (ds) is 36.5.
table 3 shows comparison with previous work on spanish-french and german-french translation tasks from the europarl corpus. bleu of pivot es→ fr is 29.79. bleu of pivot de→ fr is 23.70. bleu of hard es→ fr is 29.93. bleu of hard de→ fr is 23.88. bleu of soft es→ fr is 30.57. bleu of soft de→ fr is 23.79. bleu of likelihood es→ fr is 32.59. bleu of likelihood de→ fr is 25.93. bleu of ours sent-beam es→ fr is 31.64. bleu of ours sent-beam de→ fr is 24.39. bleu of word-sampling es→ fr is 33.86. bleu of word-sampling de→ fr is 27.03.
table 2 shows comparison of the methods in terms of the semantic similarity task. accuracy of sgd-sgns ws-sim is 0.719. accuracy of sgd-sgns ws-rel is 0.57. accuracy of sgd-sgns ws-full is 0.662. accuracy of sgd-sgns simlex is 0.288. accuracy of sgd-sgns men is 0.645. accuracy of svd-sppmi ws-sim is 0.722. accuracy of svd-sppmi ws-rel is 0.585. accuracy of svd-sppmi ws-full is 0.669. accuracy of svd-sppmi simlex is 0.317. accuracy of svd-sppmi men is 0.686. accuracy of ro-sgns ws-sim is 0.729. accuracy of ro-sgns ws-rel is 0.597. accuracy of ro-sgns ws-full is 0.677. accuracy of ro-sgns simlex is 0.322. accuracy of ro-sgns men is 0.683. accuracy of sgd-sgns ws-sim is 0.733. accuracy of sgd-sgns ws-rel is 0.584. accuracy of sgd-sgns ws-full is 0.677. accuracy of sgd-sgns simlex is 0.317. accuracy of sgd-sgns men is 0.664. accuracy of svd-sppmi ws-sim is 0.747. accuracy of svd-sppmi ws-rel is 0.625. accuracy of svd-sppmi ws-full is 0.694. accuracy of svd-sppmi simlex is 0.347. accuracy of svd-sppmi men is 0.71. accuracy of ro-sgns ws-sim is 0.757. accuracy of ro-sgns ws-rel is 0.647. accuracy of ro-sgns ws-full is 0.708. accuracy of ro-sgns simlex is 0.353. accuracy of ro-sgns men is 0.701. accuracy of sgd-sgns ws-sim is 0.738. accuracy of sgd-sgns ws-rel is 0.6. accuracy of sgd-sgns ws-full is 0.688. accuracy of sgd-sgns simlex is 0.35. accuracy of sgd-sgns men is 0.712. accuracy of svd-sppmi ws-sim is 0.765. accuracy of svd-sppmi ws-rel is 0.639. accuracy of svd-sppmi ws-full is 0.707. accuracy of svd-sppmi simlex is 0.38. accuracy of svd-sppmi men is 0.737. accuracy of ro-sgns ws-sim is 0.767. accuracy of ro-sgns ws-rel is 0.654. accuracy of ro-sgns ws-full is 0.715. accuracy of ro-sgns simlex is 0.383. accuracy of ro-sgns men is 0.732.
table 1 shows evaluation results of word similarity computation. correlation of cbow wordsim-240 is 57.7. correlation of cbow wordsim-297 is 61.1. correlation of glove wordsim-240 is 59.8. correlation of glove wordsim-297 is 58.7. correlation of skip-gram wordsim-240 is 58.5. correlation of skip-gram wordsim-297 is 63.3. correlation of ssa wordsim-240 is 58.9. correlation of ssa wordsim-297 is 64. correlation of sac wordsim-240 is 59. correlation of sac wordsim-297 is 63.1. correlation of mst wordsim-240 is 59.2. correlation of mst wordsim-297 is 62.8. correlation of sat wordsim-240 is 63.2. correlation of sat wordsim-297 is 65.6.
table 2 shows evaluation results of word analogy inference. accuracy of cbow capital is 49.8. accuracy of cbow city is 85.7. accuracy of cbow relationship is 86. accuracy of cbow all is 64.2. mean rank of cbow capital is 36.98. mean rank of cbow city is 1.23. mean rank of cbow relationship is 62.64. mean rank of cbow all is 37.62. accuracy of glove capital is 57.3. accuracy of glove city is 74.3. accuracy of glove relationship is 81.6. accuracy of glove all is 65.8. mean rank of glove capital is 19.09. mean rank of glove city is 1.71. mean rank of glove relationship is 3.58. mean rank of glove all is 12.63. accuracy of skip-gram capital is 66.8. accuracy of skip-gram city is 93.7. accuracy of skip-gram relationship is 76.8. accuracy of skip-gram all is 73.4. mean rank of skip-gram capital is 137.19. mean rank of skip-gram city is 1.07. mean rank of skip-gram relationship is 2.95. mean rank of skip-gram all is 83.51. accuracy of ssa capital is 62.3. accuracy of ssa city is 93.7. accuracy of ssa relationship is 81.6. accuracy of ssa all is 71.9. mean rank of ssa capital is 45.74. mean rank of ssa city is 1.06. mean rank of ssa relationship is 3.33. mean rank of ssa all is 28.52. accuracy of sac capital is 61.6. accuracy of sac city is 95.4. accuracy of sac relationship is 77.9. accuracy of sac all is 70.8. mean rank of sac capital is 19.08. mean rank of sac city is 1.02. mean rank of sac relationship is 2.18. mean rank of sac all is 12.18. accuracy of mst capital is 65.7. accuracy of mst city is 95.4. accuracy of mst relationship is 82.7. accuracy of mst all is 74.5. mean rank of mst capital is 50.29. mean rank of mst city is 1.05. mean rank of mst relationship is 2.48. mean rank of mst all is 31.05. accuracy of sat capital is 83.2. accuracy of sat city is 98.9. accuracy of sat relationship is 82.4. accuracy of sat all is 85.3. mean rank of sat capital is 14.42. mean rank of sat city is 1.01. mean rank of sat relationship is 2.63. mean rank of sat all is 9.48.
table 2 shows results of chinese srl tested on cpb and csb with automatic pos tagging, using standard lstm rnn model (experiment 1). pr. (%) of csb pr. (%) is 75.8. rec. (%) of csb rec. (%) is 73.45. f1 (%) of csb f1 (%) is 74.61. pr. (%) of cpb pr. (%) is 76.75. rec. (%) of cpb rec. (%) is 73.03. f1 (%) of cpb f1 (%) is 74.84.
table 3 shows result comparison on cpb dataset. f1 (%) of xue (2008) me f1 (%) is 71.9. f1 (%) of collobert and weston (2008) mtl f1 (%) is 74.05. f1 (%) of ding and chang (2009) crf f1 (%) is 72.64. f1 (%) of yang et al. (2014) multi-predicate f1 (%) is 75.31. f1 (%) of wang et al. (2015) bi-lstm f1 (%) is 77.09 (+0.00). f1 (%) of sha et al. (2016) bi-lstm+qom f1 (%) is 77.69. f1 (%) of wang et al. (2015) +gigaword embedding f1 (%) is 77.21. f1 (%) of wang et al. (2015) +netbank embedding f1 (%) is 77.59. f1 (%) of guo et al. (2016) +relataion classification f1 (%) is 75.46. f1 (%) of bi-lstm+csb embedding f1 (%) is 77.68 (+0.59). f1 (%) of two-column finetuning f1 (%) is 78.42 (+1.33). f1 (%) of two-column progressive (ours) f1 (%) is 79.30 (+2.21). f1 (%) of two-column progressive+gra (ours) f1 (%) is 79.67 (+2.58).
table 2 shows results on semeval textual similarity datasets (pearson’s r × 100) when experimenting with different regularization techniques. correlation of none oracle is 68.5. correlation of none 2016 sts is 68.4. correlation of dropout oracle is 68.4. correlation of dropout 2016 sts is 68.3. correlation of word dropout oracle is 68.3. correlation of word dropout 2016 sts is 68.3. correlation of none oracle is 60.6. correlation of none 2016 sts is 59.3. correlation of l2 oracle is 60.3. correlation of l2 2016 sts is 56.5. correlation of dropout oracle is 58.1. correlation of dropout 2016 sts is 55.3. correlation of word dropout oracle is 66.2. correlation of word dropout 2016 sts is 65.3. correlation of scrambling oracle is 66.3. correlation of scrambling 2016 sts is 65.1. correlation of dropout + scrambling oracle is 68.4. correlation of dropout + scrambling 2016 sts is 68.4. correlation of none oracle is 67.7. correlation of none 2016 sts is 67.5. correlation of dropout + scrambling oracle is 69.2. correlation of dropout + scrambling 2016 sts is 68.6. correlation of dropout + scrambling oracle is 69.4. correlation of dropout + scrambling 2016 sts is 68.7.
table 3 shows results on semeval textual similarity datasets (pearson’s r × 100) for the gran architectures. correlation of gran (no reg.) oracle is 68. correlation of gran (no reg.) sts 2016 is 68. correlation of gran oracle is 69.5. correlation of gran sts 2016 is 68.9. correlation of gran-2 oracle is 68.8. correlation of gran-2 sts 2016 is 68.1. correlation of gran-3 oracle is 69. correlation of gran-3 sts 2016 is 67.2. correlation of gran-4 oracle is 68.6. correlation of gran-4 sts 2016 is 68.1. correlation of gran-5 oracle is 66.1. correlation of gran-5 sts 2016 is 64.8. correlation of bigran oracle is 69.7. correlation of bigran sts 2016 is 68.4.
table 6 shows results from supervised training on the sts and sick datasets (pearson’s r × 100) for the gran architectures. r of gran sts is 81.6. r of gran sick is 85.3. r of gran avg. is 83.5. r of gran-2 sts is 77.4. r of gran-2 sick is 85.1. r of gran-2 avg. is 81.3. r of gran-3 sts is 81.3. r of gran-3 sick is 85.4. r of gran-3 avg. is 83.4. r of gran-4 sts is 80.1. r of gran-4 sick is 85.5. r of gran-4 avg. is 82.8. r of gran-5 sts is 70.9. r of gran-5 sick is 83. r of gran-5 avg. is 77.
table 1 shows results on belinkov et al. acc. of syntactic-sg acc. is 88.7. acc. of glove acc. is 84.3. acc. of glove-retro acc. is 84.8. acc. of glove-extended acc. is 89.7.
table 2 shows results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments. full uas of rbg full uas is 94.17. ppa acc. of rbg ppa acc. is 88.51. full uas of rbg + hpcd (full) full uas is 94.19. ppa acc. of rbg + hpcd (full) ppa acc. is 89.59. full uas of rbg + lstm-pp full uas is 94.14. ppa acc. of rbg + lstm-pp ppa acc. is 86.35. full uas of rbg + ontolstm-pp full uas is 94.3. ppa acc. of rbg + ontolstm-pp ppa acc. is 90.11. full uas of rbg + oracle pp full uas is 94.6. ppa acc. of rbg + oracle pp ppa acc. is 98.97.
table 2 shows performance of alternative sequence labeling architectures on ner and chunking datasets, measured using conll standard entity-level f1 score. f1 of baseline dev is 92.92. f1 of baseline test is 92.67. f1 of baseline dev is 90.85. f1 of baseline test is 85.63. f1 of baseline dev is 83.63. f1 of baseline test is 84.51. f1 of baseline dev is 77.13. f1 of baseline test is 72.79. f1 of + dropout dev is 93.4. f1 of + dropout test is 93.15. f1 of + dropout dev is 91.14. f1 of + dropout test is 86. f1 of + dropout dev is 84.78. f1 of + dropout test is 85.67. f1 of + dropout dev is 77.61. f1 of + dropout test is 73.16. f1 of + lmcost dev is 94.22. f1 of + lmcost test is 93.88. f1 of + lmcost dev is 91.48. f1 of + lmcost test is 86.26. f1 of + lmcost dev is 85.45. f1 of + lmcost test is 86.27. f1 of + lmcost dev is 78.51. f1 of + lmcost test is 73.83.
table 6 shows result of end-to-end problem solving correct of dev correct is 27.60%. timeout of dev timeout is 10.90%. wrong of dev wrong is 12.10%. no rcf of dev no rcf is 12.10%. parse failure of dev parse failure is 37.40%. correct of test correct is 11.40%. timeout of test timeout is 1.80%. wrong of test wrong is 11.40%. no rcf of test no rcf is 6.80%. parse failure of test parse failure is 68.60%.
table 2 shows the detailed comparison of e-e and e-t against relation types to mirza and tonelli (2016) (micro-average overall f1-score) on test data. f1-score of after e-d is 0.582. f1-score of after e-d is 0.466. f1-score of after e-e is 0.44. f1-score of after e-e is 0.43. f1-score of before e-d is 0.634. f1-score of before e-d is 0.671. f1-score of before e-e is 0.46. f1-score of before e-e is 0.471. f1-score of includes e-d is 0.056. f1-score of includes e-d is 0.25. f1-score of includes e-e is 0.025. f1-score of includes e-e is 0.049. f1-score of is includ. e-d is 0.595. f1-score of is includ. e-d is 0.6. f1-score of is includ. e-e is 0.17. f1-score of is includ. e-e is 0.25. f1-score of vague e-d is 0.526. f1-score of vague e-d is 0.502. f1-score of vague e-e is 0.624. f1-score of vague e-e is 0.613. f1-score of overall e-d is 0.546. f1-score of overall e-d is 0.534. f1-score of overall e-e is 0.529. f1-score of overall e-e is 0.519.
table 1 shows single-source parsing results in terms of average accuracy % over 3 runs. accuracy of - en is 84.4. accuracy of - de is 70.24. accuracy of - el is 74.4. accuracy of - th is 72.86. accuracy of - avg. is 75.48. accuracy of - en is 81.85. accuracy of - id is 74.85. accuracy of - zh is 73.66. accuracy of - avg. is 76.79. accuracy of separate en is 85. accuracy of separate de is 71.19. accuracy of separate el is 75.12. accuracy of separate th is 72.26. accuracy of separate avg. is 75.89. accuracy of separate en is 81.4. accuracy of separate id is 74.03. accuracy of separate zh is 75.89. accuracy of separate avg. is 77.11. accuracy of shared en is 85.48. accuracy of shared de is 72.86. accuracy of shared el is 75.6. accuracy of shared th is 73.33. accuracy of shared avg. is 76.82. accuracy of shared en is 81.77. accuracy of shared id is 75.45. accuracy of shared zh is 73.96. accuracy of shared avg. is 77.06.
table 6 shows single-source parsing results showing the accuracy of the 3 runs. accuracy of 1 en is 87.14. accuracy of 1 de is 70.00. accuracy of 1 el is 76.43. accuracy of 1 th is 72.50. accuracy of 1 en is 84.60. accuracy of 1 id is 75.67. accuracy of 1 zh is 74.33. accuracy of 2 en is 83.57. accuracy of 2 de is 70.36. accuracy of 2 el is 72.50. accuracy of 2 th is 73.57. accuracy of 2 en is 79.24. accuracy of 2 id is 74.55. accuracy of 2 zh is 73.66. accuracy of 3 en is 82.50. accuracy of 3 de is 70.36. accuracy of 3 el is 74.29. accuracy of 3 th is 72.50. accuracy of 3 en is 81.70. accuracy of 3 id is 74.33. accuracy of 3 zh is 72.99. accuracy of 1 en is 85.71. accuracy of 1 de is 71.79. accuracy of 1 el is 77.14. accuracy of 1 th is 72.14. accuracy of 1 en is 82.14. accuracy of 1 id is 75.67. accuracy of 1 zh is 74.11. accuracy of 2 en is 83.93. accuracy of 2 de is 71.79. accuracy of 2 el is 72.14. accuracy of 2 th is 72.14. accuracy of 2 en is 81.03. accuracy of 2 id is 72.54. accuracy of 2 zh is 76.12. accuracy of 3 en is 85.36. accuracy of 3 de is 70.00. accuracy of 3 el is 76.07. accuracy of 3 th is 72.50. accuracy of 3 en is 81.03. accuracy of 3 id is 73.88. accuracy of 3 zh is 77.46. accuracy of 1 en is 85.36. accuracy of 1 de is 73.57. accuracy of 1 el is 76.43. accuracy of 1 th is 72.50. accuracy of 1 en is 82.59. accuracy of 1 id is 76.56. accuracy of 1 zh is 75.67. accuracy of 2 en is 83.93. accuracy of 2 de is 73.93. accuracy of 2 el is 74.64. accuracy of 2 th is 71.07. accuracy of 2 en is 80.36. accuracy of 2 id is 75.45. accuracy of 2 zh is 72.54. accuracy of 3 en is 87.14. accuracy of 3 de is 71.07. accuracy of 3 el is 75.71. accuracy of 3 th is 76.43. accuracy of 3 en is 82.37. accuracy of 3 id is 74.33. accuracy of 3 zh is 73.66.
table 1 shows results on rte performance without (init) and with prior compound splitting. acc of init acc is 64.13. p of init p is 62.50. r of init r is 74.57. f1 of init f1 is 68.00. p of init p is 66.67. r of init r is 53.20. f1 of init f1 is 59.18. acc of manual splitting* acc is 67.88. p of manual splitting* p is 65.08. r of manual splitting* r is 80.20. f1 of manual splitting* f1 is 71.85. p of manual splitting* p is 72.64. r of manual splitting* r is 54.99. f1 of manual splitting* f1 is 62.59. acc of zvdp2016 acc is 66.63. p of zvdp2016 p is 64.55. r of zvdp2016 r is 77.02. f1 of zvdp2016 f1 is 70.23. p of zvdp2016 p is 69.87. r of zvdp2016 r is 55.75. f1 of zvdp2016 f1 is 62.02. acc of ff2010* acc is 67.38. p of ff2010* p is 65.48. r of ff2010* r is 76.53. f1 of ff2010* f1 is 70.58. p of ff2010* p is 70.19. r of ff2010* r is 57.80. f1 of ff2010* f1 is 63.39. acc of wh2012 acc is 66.00. p of wh2012 p is 63.73. r of wh2012 r is 77.75. f1 of wh2012 f1 is 70.04. p of wh2012 p is 69.77. r of wh2012 r is 53.71. f1 of wh2012 f1 is 60.69.
table 2 shows bleu results for the low-resource experiments (news commentary v8) bleu of bpe2bpe newstest2015 is 13.81. bleu of bpe2bpe newstest2016 is 14.16. bleu of bpe2tree newstest2015 is 14.55. bleu of bpe2tree newstest2016 is 16.13. bleu of bpe2bpe ens. newstest2015 is 14.42. bleu of bpe2bpe ens. newstest2016 is 15.07. bleu of bpe2tree ens. newstest2015 is 15.69. bleu of bpe2tree ens. newstest2016 is 17.21. bleu of bpe2bpe newstest2015 is 12.58. bleu of bpe2bpe newstest2016 is 11.37. bleu of bpe2tree newstest2015 is 12.92. bleu of bpe2tree newstest2016 is 11.94. bleu of bpe2bpe ens. newstest2015 is 13.36. bleu of bpe2bpe ens. newstest2016 is 11.91. bleu of bpe2tree ens. newstest2015 is 13.66. bleu of bpe2tree ens. newstest2016 is 12.89. bleu of bpe2bpe newstest2015 is 10.85. bleu of bpe2bpe newstest2016 is 11.23. bleu of bpe2tree newstest2015 is 11.54. bleu of bpe2tree newstest2016 is 11.65. bleu of bpe2bpe ens. newstest2015 is 11.46. bleu of bpe2bpe ens. newstest2016 is 11.77. bleu of bpe2tree ens. newstest2015 is 12.43. bleu of bpe2tree ens. newstest2016 is 12.68.
table 4 shows test set results f1 of svm f1 is 0.66. f1 of svm f1 is 0.60. macro f of svm macro f is 0.64. f1 of nrc f1 is 0.58. f1 of nrc f1 is 0.69. macro f of nrc macro f is 0.64. f1 of stanford f1 is 0.54. f1 of stanford f1 is 0.73. macro f of stanford macro f is 0.67. f1 of autoslog (aslog) f1 is 0.11. f1 of autoslog (aslog) f1 is 0.68. macro f of autoslog (aslog) macro f is 0.53. f1 of retrained stanford f1 is 0.53. f1 of retrained stanford f1 is 0.73. macro f of retrained stanford macro f is 0.67.
table 3 shows mean accuracy and standard deviation results. accuracy of majority train is 56.37. accuracy of majority dev is 55.31. accuracy of majority test-p is 56.16. accuracy of majority test-u is 55.43. accuracy of text only train is 58.36±0.6. accuracy of text only dev is 56.61±0.5. accuracy of text only test-p is 57.18±0.6. accuracy of text only test-u is 56.21±0.4. accuracy of image only train is 56.79±1.3. accuracy of image only dev is 55.35±0.1. accuracy of image only test-p is 56.05±0.3. accuracy of image only test-u is 55.33±0.3. accuracy of maxent train is 99.99. accuracy of maxent dev is 68.04. accuracy of maxent test-p is 67.68. accuracy of maxent test-u is 67.82. accuracy of mlp train is 96.15±1.3. accuracy of mlp dev is 67.50±0.5. accuracy of mlp test-p is 66.28±0.4. accuracy of mlp test-u is 65.32±0.4. accuracy of image features+rnn train is 59.71±1.0. accuracy of image features+rnn dev is 57.72±1.4. accuracy of image features+rnn test-p is 57.62±1.3. accuracy of image features+rnn test-u is 56.29±0.9. accuracy of cnn+rnn train is 58.85±0.2. accuracy of cnn+rnn dev is 56.59±0.3. accuracy of cnn+rnn test-p is 58.01±0.3. accuracy of cnn+rnn test-u is 56.30±0.6. accuracy of nmn train is 98.37±0.6. accuracy of nmn dev is 63.06±0.1. accuracy of nmn test-p is 66.12±0.4. accuracy of nmn test-u is 61.99±0.8.
table 2 shows readability evaluation by human subjects score of n=1 score is 4.55. score of n=2 score is 4.58. score of n=1 score is 3.88. score of n=2 score is 4.07.
table 3 shows results on the atomic and full datasets. val. acc of theano val. acc is 36.81%. bleu of theano bleu is 9.5. bleu of theano bleu is 7.1. val. acc of keras val. acc is 45.76%. bleu of keras bleu is 13.7. bleu of keras bleu is 7.8. val. acc of youtube-dl val. acc is 50.84%. bleu of youtube-dl bleu is 16.4. bleu of youtube-dl bleu is 17.5. val. acc of node val. acc is 52.46%. bleu of node bleu is 7.8. bleu of node bleu is 7.7. val. acc of angular val. acc is 44.39%. bleu of angular bleu is 13.9. bleu of angular bleu is 11.7. val. acc of react val. acc is 49.44%. bleu of react bleu is 11.4. bleu of react bleu is 10.7. val. acc of opencv val. acc is 50.77%. bleu of opencv bleu is 11.2. bleu of opencv bleu is 9.0. val. acc of cntk val. acc is 48.88%. bleu of cntk bleu is 17.9. bleu of cntk bleu is 11.8. val. acc of bitcoin val. acc is 50.04%. bleu of bitcoin bleu is 17.9. bleu of bitcoin bleu is 13.0. val. acc of corenlp val. acc is 63.20%. bleu of corenlp bleu is 28.5. bleu of corenlp bleu is 10.1. val. acc of elasticsearch val. acc is 36.53%. bleu of elasticsearch bleu is 11.8. bleu of elasticsearch bleu is 5.2. val. acc of guava val. acc is 65.52%. bleu of guava bleu is 29.8. bleu of guava bleu is 19.5.
table 3 shows final results comparing translated language features (trans) to benchmark lexical generalisation features (lex). p of base p is 60.4. r of base r is 24.1. f of base f is 34.4. p of base+lex p is 66.8. r of base+lex r is 42.6. f of base+lex f is 52.0. p of base+trans p is 59.6. r of base+trans r is 45.8. f of base+trans f is 51.8. p of base+lex+trans p is 67.9. r of base+lex+trans r is 46.2. f of base+lex+trans f is 55.0.
table 5 shows precision, recall and f1 of different methods on yahoo! answers factoid qa dataset. p of aqqu p is 0.116. r of aqqu r is 0.117. f of aqqu f1 is 0.116. p of text2kb p is 0.170. r of text2kb r is 0.170. f of text2kb f1 is 0.170. p of askmsr (entities) p is 0.175. r of askmsr (entities) r is 0.319. f of askmsr (entities) f1 is 0.226. p of memn2n p is 0.072. r of memn2n r is 0.131. f of memn2n f1 is 0.092. p of kv memn2n p is 0.126. r of kv memn2n r is 0.228. f of kv memn2n f1 is 0.162. p of evinets (text) p is 0.210. r of evinets (text) r is 0.383. f of evinets (text) f1 is 0.271. p of evinets (text+kb) p is 0.226. r of evinets (text+kb) r is 0.409. f of evinets (text+kb) f1 is 0.291. p of oracle p is 0.622. r of oracle r is 1.0. f of oracle f1 is 0.767.
table 3 shows results on our corpus. p of unstructured p is 50.0. r of unstructured r is 67.2. f of unstructured f1 is 52.9. p of + pairs p is 53.3. r of + pairs r is 64.1. f of + pairs f1 is 54.3. p of + graph p is 53.9. r of + graph r is 63.9. f of + graph f1 is 54.5. p of unstructured p is 42.6. r of unstructured r is 58.9. f of unstructured f1 is 44.4. p of + pairs p is 46.5. r of + pairs r is 54.1. f of + pairs f1 is 45.6. p of + graph p is 47.0. r of + graph r is 53.6. f of + graph f1 is 45.6.
table 2 shows number of wikidata entities as subjects (#s) of each predicate (p), and evaluation results on manually annotated randomly selected subjects that have at least an object. #s of has part (creative work series) #s is 261. p of has part (creative work series) p is 0.050. p of has part (creative work series) p is 0.333. r of has part (creative work series) r is 0.316. f1 of has part (creative work series) f1 is 0.324. p of has part (creative work series) p is 0.353. r of has part (creative work series) r is 0.316. f1 of has part (creative work series) f1 is 0.333. #s of contains admin. terr. entity #s is 18000. p of contains admin. terr. entity p is 0.034. p of contains admin. terr. entity p is 0.390. r of contains admin. terr. entity r is 0.188. f1 of contains admin. terr. entity f1 is 0.254. p of contains admin. terr. entity p is 0.548. r of contains admin. terr. entity r is 0.200. f1 of contains admin. terr. entity f1 is 0.293. #s of spouse #s is 45917. p of spouse p is 0. p of spouse p is 0.014. r of spouse r is 0.011. f1 of spouse f1 is 0.013. p of spouse p is 0.028. r of spouse r is 0.017. f1 of spouse f1 is 0.021. #s of child #s is 35057. p of child p is 0.112. p of child p is 0.151. r of child r is 0.129. f1 of child f1 is 0.139. p of child p is 0.320. r of child r is 0.219. f1 of child f1 is 0.260. #s of child (manual ground truth) #s is 6408. p of child (manual ground truth) p is 0.374. r of child (manual ground truth) r is 0.309. f1 of child (manual ground truth) f1 is 0.338. p of child (manual ground truth) p is 0.452. r of child (manual ground truth) r is 0.315. f1 of child (manual ground truth) f1 is 0.371.
table 1 shows test-set accuracies obtained; results except the agt are drawn from (lei et al., 2015). accuracy of agt accuracy is 50.5. accuracy of high-order cnn accuracy is 51.2. accuracy of tree-lstm accuracy is 51.0. accuracy of drnn accuracy is 49.8. accuracy of pvec accuracy is 48.7. accuracy of dcnn accuracy is 48.5. accuracy of dan accuracy is 48.2. accuracy of cnn-mc accuracy is 47.4. accuracy of cnn accuracy is 47.2. accuracy of rntn accuracy is 45.7. accuracy of nbow accuracy is 44.5. accuracy of rnn accuracy is 43.2. accuracy of svm accuracy is 38.3.
table 1 shows translation results (bleu score) for different machine translation and system combination methods. bleu of pbmt mt03 is 37.47. bleu of pbmt mt04 is 41.20. bleu of pbmt mt05 is 36.41. bleu of pbmt mt06 is 36.03. bleu of pbmt average is 37.78. bleu of hpmt mt03 is 38.05. bleu of hpmt mt04 is 41.47. bleu of hpmt mt05 is 36.86. bleu of hpmt mt06 is 36.04. bleu of hpmt average is 38.10. bleu of nmt mt03 is 37.91. bleu of nmt mt04 is 38.95. bleu of nmt mt05 is 36.02. bleu of nmt mt06 is 36.65. bleu of nmt average is 37.38. bleu of jane (freitag et al. 2014) mt03 is 39.83. bleu of jane (freitag et al. 2014) mt04 is 42.75. bleu of jane (freitag et al. 2014) mt05 is 38.63. bleu of jane (freitag et al. 2014) mt06 is 39.10. bleu of jane (freitag et al. 2014) average is 40.08. bleu of multi mt03 is 40.64. bleu of multi mt04 is 44.81. bleu of multi mt05 is 38.80. bleu of multi mt06 is 38.26. bleu of multi average is 40.63. bleu of multi+source mt03 is 42.16. bleu of multi+source mt04 is 45.51. bleu of multi+source mt05 is 40.28. bleu of multi+source mt06 is 39.03. bleu of multi+source average is 41.75. bleu of multi+ensemble mt03 is 41.67. bleu of multi+ensemble mt04 is 45.95. bleu of multi+ensemble mt05 is 40.37. bleu of multi+ensemble mt06 is 39.02. bleu of multi+ensemble average is 41.75. bleu of multi+source+ensemble mt03 is 43.55. bleu of multi+source+ensemble mt04 is 47.09. bleu of multi+source+ensemble mt05 is 42.02. bleu of multi+source+ensemble mt06 is 41.10. bleu of multi+source+ensemble average is 43.44.
table 2 shows experimental results of japanese caption generation. bleu-1 of en-generator → mt bleu-1 is 0.565. bleu-2 of en-generator → mt bleu-2 is 0.330. bleu-3 of en-generator → mt bleu-3 is 0.204. bleu-4 of en-generator → mt bleu-4 is 0.127. rouge_l of en-generator → mt rouge_l is 0.449. cider of en-generator → mt cider is 0.324. bleu-1 of ja-generator bleu-1 is 0.763. bleu-2 of ja-generator bleu-2 is 0.614. bleu-3 of ja-generator bleu-3 is 0.492. bleu-4 of ja-generator bleu-4 is 0.385. rouge_l of ja-generator rouge_l is 0.553. cider of ja-generator cider is 0.883.
table 3 shows comparison of di algorithms. correlation of none an is 0.35. correlation of none nn is 0.50. correlation of none vo is 0.39. correlation of none avg is 0.41. correlation of none vo is 0.22. correlation of standard di an is 0.48. correlation of standard di nn is 0.51. correlation of standard di vo is 0.43. correlation of standard di avg is 0.47. correlation of standard di vo is 0.29. correlation of offset inference an is 0.49. correlation of offset inference nn is 0.52. correlation of offset inference vo is 0.44. correlation of offset inference avg is 0.48. correlation of offset inference vo is 0.31.
table 2 shows spearman’s rank correlation performance for the word similarity task on scws. correlation of sge + c (mikolov et al. 2013a) 100 is 0.59. correlation of sge + c (mikolov et al. 2013a) 300 is 0.59. correlation of sge + c (mikolov et al. 2013a) 600 is 0.62. correlation of mssg (neelakantan et al. 2014) 100 is 0.60. correlation of mssg (neelakantan et al. 2014) 300 is 0.61. correlation of mssg (neelakantan et al. 2014) 600 is 0.64. correlation of htle 100 is 0.63. correlation of htle 300 is 0.56. correlation of htle 600 is 0.55. correlation of htle add 100 is 0.61. correlation of htle add 300 is 0.61. correlation of htle add 600 is 0.58. correlation of stle 100 is 0.59. correlation of stle 300 is 0.58. correlation of stle 600 is 0.55.
table 1 shows metric-based evaluation. average of lm average is 0.360. greedy of lm greedy is 0.348. extrema of lm extrema is 0.310. average of hred average is 0.429. greedy of hred greedy is 0.466. extrema of hred extrema is 0.383. average of sphred average is 0.468. greedy of sphred greedy is 0.478. extrema of sphred extrema is 0.434. average of vhred average is 0.403. greedy of vhred greedy is 0.432. extrema of vhred extrema is 0.374. accuracy of scene1-a accuracy is 90.9%. average of scene1-b average is 0.426. greedy of scene1-b greedy is 0.432. extrema of scene1-b extrema is 0.396. accuracy of scene1-b accuracy is 86.9%. average of scene2-a average is 0.465. greedy of scene2-a greedy is 0.440. extrema of scene2-a extrema is 0.428. accuracy of scene2-a accuracy is 99.8%. average of scene2-b average is 0.463. greedy of scene2-b greedy is 0.437. extrema of scene2-b extrema is 0.420. accuracy of scene2-b accuracy is 99.2%.
table 4 shows results on sick after finetuning. accuracy of - accuracy is 77.96. accuracy of squad-t accuracy is 81.49. accuracy of squad accuracy is 82.86. accuracy of squad* accuracy is 84.38. accuracy of snli accuracy is 83.20. accuracy of squad-t + snli accuracy is 85.00. accuracy of squad + snli accuracy is 86.63. accuracy of squad + snli* accuracy is 88.22. accuracy of yin et al. (2016) accuracy is 86.2. accuracy of lai and hockenmaier (2014) accuracy is 84.57. accuracy of zhao et al. (2014) accuracy is 83.64. accuracy of jimenez et al. (2014) accuracy is 83.05. accuracy of mou et al. (2016) accuracy is 70.9. accuracy of mou et al. (2016) (pretrained on snli) accuracy is 77.6.
table 1 shows comparison of our model variants on the maptask corpus. accuracy of no attn. without hmm is 60.97%. accuracy of no attn. gate bias hmm is 64.60%. accuracy of no attn. gate all hmm is 63.55%. accuracy of traditional without hmm is 61.72%. accuracy of traditional gate bias hmm is 64.73%. accuracy of traditional gate all hmm is 65.19%. accuracy of gated attn. without hmm is 62.21%. accuracy of gated attn. gate bias hmm is 65.94%. accuracy of gated attn. gate all hmm is 65.94%.
table 4 shows overall performance (%). r of president r is 94.6. p of president p is 89.9. f of president f is 92.2. r of president r is 87.2. p of president p is 80.4. f of president f is 83.7. r of company r is 86.6. p of company p is 95.8. f of company f is 91.0. r of company r is 90.8. p of company p is 85.2. f of company f is 87.9. r of university r is 96.7. p of university p is 96.4. f of university f is 96.5. r of university r is 96.9. p of university p is 92.0. f of university f is 94.4. r of state r is 96.2. p of state p is 92.1. f of state f is 94.1. r of state r is 95.0. p of state p is 58.6. f of state f is 72.5. r of character r is 92.5. p of character p is 61.3. f of character f is 73.7. r of character r is 92.8. p of character p is 52.2. f of character f is 66.8. r of brand r is 89.6. p of brand p is 90.2. f of brand f is 89.9. r of brand r is 86.7. p of brand p is 83.2. f of brand f is 84.9. r of restaurant r is 87.0. p of restaurant p is 81.4. f of restaurant f is 84.1. r of restaurant r is 86.9. p of restaurant p is 88.1. f of restaurant f is 87.5. r of overall r is 95.2. p of overall p is 93.4. f of overall f is 94.3. r of overall r is 93.1. p of overall p is 81.6. f of overall f is 87.0.
table 1 shows results of comparing several segmentation strategies. bleu of unseg tst11 is 25.7. bleu of unseg tst12 is 28.2. bleu of unseg tst13 is 27.3. bleu of unseg tst14 is 23.9. bleu of unseg avg. is 26.3. bleu of unseg tst11 is 15.8. bleu of unseg tst12 is 17.1. bleu of unseg tst13 is 18.1. bleu of unseg tst14 is 15.5. bleu of unseg avg. is 16.6. bleu of morph tst11 is 29.2. bleu of morph tst12 is 33. bleu of morph tst13 is 32.9. bleu of morph tst14 is 28.3. bleu of morph avg. is 30.9. bleu of morph tst11 is 16.5. bleu of morph tst12 is 18.8. bleu of morph tst13 is 20.4. bleu of morph tst14 is 17.2. bleu of morph avg. is 18.2. bleu of ccnn tst11 is 29. bleu of ccnn tst12 is 32. bleu of ccnn tst13 is 32.5. bleu of ccnn tst14 is 27.8. bleu of ccnn avg. is 30.2. bleu of ccnn tst11 is 14.3. bleu of ccnn tst12 is 12.8. bleu of ccnn tst13 is 13.6. bleu of ccnn tst14 is 12.6. bleu of ccnn avg. is 13.3. bleu of char tst11 is 28.8. bleu of char tst12 is 31.8. bleu of char tst13 is 32.5. bleu of char tst14 is 27.8. bleu of char avg. is 30.2. bleu of char tst11 is 15.3. bleu of char tst12 is 17.1. bleu of char tst13 is 18. bleu of char tst14 is 15.3. bleu of char avg. is 16.4. bleu of bpe tst11 is 29.7. bleu of bpe tst12 is 32.5. bleu of bpe tst13 is 33.6. bleu of bpe tst14 is 28.4. bleu of bpe avg. is 31.1. bleu of bpe tst11 is 17.5. bleu of bpe tst12 is 18. bleu of bpe tst13 is 20. bleu of bpe tst14 is 16.6. bleu of bpe avg. is 18.
table 4 shows comparison with previous models. f1 of (zhao and kit 2008c) f1 is 95.4. f1 of (zhao and kit 2008c) f1 is 97.6. f1 + pre-train of (chen et al. 2015a) f1 + pre-train is 94.5. f1 of (chen et al. 2015a) f1 is 94.4. training (hours) of (chen et al. 2015a) training (hours) is 50. test (sec.) of (chen et al. 2015a) test (sec.) is 105. f1 + pre-train of (chen et al. 2015a) f1 + pre-train is 95.4. f1 of (chen et al. 2015a) f1 is 95.1. training (hours) of (chen et al. 2015a) training (hours) is 100. test (sec.) of (chen et al. 2015a) test (sec.) is 120. f1 + pre-train of (chen et al. 2015b) f1 + pre-train is 94.8. f1 of (chen et al. 2015b) f1 is 94.3. training (hours) of (chen et al. 2015b) training (hours) is 58. test (sec.) of (chen et al. 2015b) test (sec.) is 105. f1 + pre-train of (chen et al. 2015b) f1 + pre-train is 95.6. f1 of (chen et al. 2015b) f1 is 95.0. training (hours) of (chen et al. 2015b) training (hours) is 117. test (sec.) of (chen et al. 2015b) test (sec.) is 120. f1 of (ma and hinrichs 2015) f1 is 95.1. training (hours) of (ma and hinrichs 2015) training (hours) is 1.5. test (sec.) of (ma and hinrichs 2015) test (sec.) is 24. f1 of (ma and hinrichs 2015) f1 is 96.6. training (hours) of (ma and hinrichs 2015) training (hours) is 3. test (sec.) of (ma and hinrichs 2015) test (sec.) is 28. f1 + pre-train of (zhang et al. 2016) f1 + pre-train is 95.1. training (hours) of (zhang et al. 2016) training (hours) is 6. test (sec.) of (zhang et al. 2016) test (sec.) is 110. f1 + pre-train of (zhang et al. 2016) f1 + pre-train is 97.0. training (hours) of (zhang et al. 2016) training (hours) is 13. test (sec.) of (zhang et al. 2016) test (sec.) is 125. f1 + pre-train of (liu et al. 2016) f1 + pre-train is 93.91. f1 + pre-train of (liu et al. 2016) f1 + pre-train is 95.21. f1 + pre-train of (cai and zhao 2016) f1 + pre-train is 95.5. f1 of (cai and zhao 2016) f1 is 95.2. training (hours) of (cai and zhao 2016) training (hours) is 48. test (sec.) of (cai and zhao 2016) test (sec.) is 95. f1 + pre-train of (cai and zhao 2016) f1 + pre-train is 96.5. f1 of (cai and zhao 2016) f1 is 96.4. training (hours) of (cai and zhao 2016) training (hours) is 96. test (sec.) of (cai and zhao 2016) test (sec.) is 105. f1 + pre-train of our results f1 + pre-train is 95.8. f1 of our results f1 is 95.4. training (hours) of our results training (hours) is 3. test (sec.) of our results test (sec.) is 25. f1 + pre-train of our results f1 + pre-train is 97.1. f1 of our results f1 is 97.0. training (hours) of our results training (hours) is 6. test (sec.) of our results test (sec.) is 30.
table 3 shows final results. accuracy of dssm‡ val is 60.4. accuracy of dssm‡ test is 58.5. accuracy of uw (schwartz et al. 2017b) test is 75.2. accuracy of uw (ending only) test is 72.4. accuracy of trigram lm (estimated from stories) val is 52.4. accuracy of trigram lm (estimated from stories) test is 53.6. accuracy of trigram lm (estimated from endings) val is 53.8. accuracy of trigram lm (estimated from endings) test is 54.6. accuracy of our model (hier encplotend att) test is 74.7. accuracy of our model (ending only) test is 72.5. accuracy of human‡ (story + ending) val is 100. accuracy of human‡ (story + ending) test is 100. accuracy of human (ending only) val is 78.
table 1 shows results of our model and baseline systems. rouge-1 of rnn (w) (hu et al. 2015) rouge-1 is 17.7. rouge-2 of rnn (w) (hu et al. 2015) rouge-2 is 8.5. rouge-l of rnn (w) (hu et al. 2015) rouge-l is 15.8. rouge-1 of rnn (c) (hu et al. 2015) rouge-1 is 21.5. rouge-2 of rnn (c) (hu et al. 2015) rouge-2 is 8.9. rouge-l of rnn (c) (hu et al. 2015) rouge-l is 18.6. rouge-1 of rnn context (w) (hu et al. 2015) rouge-1 is 26.8. rouge-2 of rnn context (w) (hu et al. 2015) rouge-2 is 16.1. rouge-l of rnn context (w) (hu et al. 2015) rouge-l is 24.1. rouge-1 of rnn context (c) (hu et al. 2015) rouge-1 is 29.9. rouge-2 of rnn context (c) (hu et al. 2015) rouge-2 is 17.4. rouge-l of rnn context (c) (hu et al. 2015) rouge-l is 27.2. rouge-1 of rnn context + srb (c) rouge-1 is 32.1. rouge-2 of rnn context + srb (c) rouge-2 is 18.9. rouge-l of rnn context + srb (c) rouge-l is 29.2. rouge-1 of +attention (c) rouge-1 is 33.3. rouge-2 of +attention (c) rouge-2 is 20.0. rouge-l of +attention (c) rouge-l is 30.1.
table 2 shows results of our model and state-of-the-art systems. r-1 of word r-1 is 26.8. r-2 of word r-2 is 16.1. r-l of word r-l is 24.1. r-1 of char r-1 is 29.9. r-2 of char r-2 is 17.4. r-l of char r-l is 27.2. r-1 of word r-1 is 35.0. r-2 of word r-2 is 22.3. r-l of word r-l is 32.0. r-1 of char r-1 is 34.4. r-2 of char r-2 is 21.6. r-l of char r-l is 31.3. r-1 of char r-1 is 33.3. r-2 of char r-2 is 20.0. r-l of char r-l is 30.1.
table 2 shows classification results: predicting suspicion and verified posts reported as a – accuracy, ap – average precision, roc – the area under the receiver operator characteristics curve, and inferring types of suspicious news reported using f1 micro and f1 macro scores. a of tweets a is 0.65. roc of tweets roc is 0.70. ap of tweets ap is 0.68. f1 of tweets f1 is 0.82. f1 macro of tweets f1 macro is 0.40. a of  + network a is 0.72. roc of  + network roc is 0.80. ap of  + network ap is 0.82. f1 of  + network f1 is 0.88. f1 macro of  + network f1 macro is 0.57. a of  + cues a is 0.69. roc of  + cues roc is 0.74. ap of  + cues ap is 0.73. f1 of  + cues f1 is 0.83. f1 macro of  + cues f1 macro is 0.46. a of all a is 0.75. roc of all roc is 0.84. ap of all ap is 0.84. f1 of all f1 is 0.88. f1 macro of all f1 macro is 0.59. a of tweets a is 0.72. roc of tweets roc is 0.81. ap of tweets ap is 0.81. f1 of tweets f1 is 0.84. f1 macro of tweets f1 macro is 0.48. a of  + network a is 0.78. roc of  + network roc is 0.87. ap of  + network ap is 0.88. f1 of  + network f1 is 0.88. f1 macro of  + network f1 macro is 0.59. a of  + cues a is 0.75. roc of  + cues roc is 0.85. ap of  + cues ap is 0.85. f1 of  + cues f1 is 0.86. f1 macro of  + cues f1 macro is 0.49. a of all a is 0.79. roc of all roc is 0.88. ap of all ap is 0.89. f1 of all f1 is 0.89. f1 macro of all f1 macro is 0.59. a of tweets a is 0.78. roc of tweets roc is 0.87. ap of tweets ap is 0.88. f1 of tweets f1 is 0.90. f1 macro of tweets f1 macro is 0.63. a of  + network a is 0.83. roc of  + network roc is 0.91. ap of  + network ap is 0.92. f1 of  + network f1 is 0.92. f1 macro of  + network f1 macro is 0.71. a of  + cues a is 0.93. roc of  + cues roc is 0.98. ap of  + cues ap is 0.99. f1 of  + cues f1 is 0.90. f1 macro of  + cues f1 macro is 0.63. a of  + syntax a is 0.93. roc of  + syntax roc is 0.96. ap of  + syntax ap is 0.96. f1 of  + syntax f1 is 0.90. f1 macro of  + syntax f1 macro is 0.64. a of all a is 0.95. roc of all roc is 0.99. ap of all ap is 0.99. f1 of all f1 is 0.91. f1 macro of all f1 macro is 0.66. a of tweets a is 0.76. roc of tweets roc is 0.85. ap of tweets ap is 0.87. f1 of tweets f1 is 0.91. f1 macro of tweets f1 macro is 0.63. a of  + network a is 0.81. roc of  + network roc is 0.9. ap of  + network ap is 0.91. f1 of  + network f1 is 0.92. f1 macro of  + network f1 macro is 0.70. a of  + cues a is 0.93. roc of  + cues roc is 0.98. ap of  + cues ap is 0.98. f1 of  + cues f1 is 0.90. f1 macro of  + cues f1 macro is 0.61. a of all a is 0.95. roc of all roc is 0.98. ap of all ap is 0.99. f1 of all f1 is 0.91. f1 macro of all f1 macro is 0.64.
table 3 shows performance of classifiers precision of cf parser precision is 0.7131. recall of cf parser recall is 0.8365. f1 of cf parser f1 is 0.7699. precision of rules only precision is 0.5864. recall of rules only recall is 0.9134. f1 of rules only f1 is 0.7143. precision of svm precision is 0.2381. recall of svm recall is 0.9135. f1 of svm f1 is 0.3777.
table 4 shows word similarity evaluation on foreign languages. similarity of ws353 fasttext is 38.2. similarity of ws353 w2g is 16.73. similarity of ws353 w2gm is 20.09. similarity of ws353 pft-g is 41. similarity of ws353 pft-gm is 41.3. similarity of gur350 fasttext is 70. similarity of gur350 w2g is 65.01. similarity of gur350 w2gm is 69.26. similarity of gur350 pft-g is 77.6. similarity of gur350 pft-gm is 78.2. similarity of gur65 fasttext is 81. similarity of gur65 w2g is 74.94. similarity of gur65 w2gm is 76.89. similarity of gur65 pft-g is 81.8. similarity of gur65 pft-gm is 85.2. similarity of ws353 fasttext is 57.1. similarity of ws353 w2g is 56.02. similarity of ws353 w2gm is 61.09. similarity of ws353 pft-g is 60.2. similarity of ws353 pft-gm is 62.5. similarity of sl-999 fasttext is 29.3. similarity of sl-999 w2g is 29.44. similarity of sl-999 w2gm is 34.91. similarity of sl-999 pft-g is 29.3. similarity of sl-999 pft-gm is 33.7.
table 1 shows comparison with baselines and nonce2vec (herbelot and baroni, 2017) on few-shot embedding tasks. mean recip. rank of word2vec mean recip. rank is 0.00007. med.rank of word2vec med. rank is 111012. spearman correlation 2 sent. of word2vec spearman correlation 2 sent. is 0.1459. spearman correlation 4 sent. of word2vec spearman correlation 4 sent. is 0.2457. spearman correlation 6 sent. of word2vec spearman correlation 6 sent. is 0.2498. mean recip. rank of additive mean recip. rank is 0.00945. med.rank of additive med. rank is 3381. spearman correlation 2 sent. of additive spearman correlation 2 sent. is 0.3627. spearman correlation 4 sent. of additive spearman correlation 4 sent. is 0.3701. spearman correlation 6 sent. of additive spearman correlation 6 sent. is 0.3595. mean recip. rank of additive, no stop words mean recip. rank is 0.03686. med.rank of additive, no stop words med. rank is 861. spearman correlation 2 sent. of additive, no stop words spearman correlation 2 sent. is 0.3376. spearman correlation 4 sent. of additive, no stop words spearman correlation 4 sent. is 0.3624. spearman correlation 6 sent. of additive, no stop words spearman correlation 6 sent. is 0.408. mean recip. rank of nonce2vec mean recip. rank is 0.04907. med.rank of nonce2vec med. rank is 623. spearman correlation 2 sent. of nonce2vec spearman correlation 2 sent. is 0.332. spearman correlation 4 sent. of nonce2vec spearman correlation 4 sent. is 0.3668. spearman correlation 6 sent. of nonce2vec spearman correlation 6 sent. is 0.389. mean recip. rank of a la carte mean recip. rank is 0.07058. med.rank of a la carte med. rank is 165.5. spearman correlation 2 sent. of a la carte spearman correlation 2 sent. is 0.3634. spearman correlation 4 sent. of a la carte spearman correlation 4 sent. is 0.3844. spearman correlation 6 sent. of a la carte spearman correlation 6 sent. is 0.3941.
table 4 shows performance of document embeddings built using `a la carte n-gram vectors and recent unsupervised word-level approaches on classification tasks, with the character lstm of (radford et al., 2017) shown for comparison. accuracy of bong mr is 77.1. accuracy of bong cr is 77. accuracy of bong subj is 91. accuracy of bong mpqa is 85.1. accuracy of bong trec is 86.8. accuracy of bong sst (±1) is 80.7. accuracy of bong sst is 36.8. accuracy of bong imdb is 88.3. accuracy of bong mr is 77.8. accuracy of bong cr is 78.1. accuracy of bong subj is 91.8. accuracy of bong mpqa is 85.8. accuracy of bong trec is 90. accuracy of bong sst (±1) is 80.9. accuracy of bong sst is 39. accuracy of bong imdb is 90. accuracy of bong mr is 77.8. accuracy of bong cr is 78.3. accuracy of bong subj is 91.4. accuracy of bong mpqa is 85.6. accuracy of bong trec is 89.8. accuracy of bong sst (±1) is 80.1. accuracy of bong sst is 42.3. accuracy of bong imdb is 89.8. accuracy of a la carte mr is 79.8. accuracy of a la carte cr is 81.3. accuracy of a la carte subj is 92.6. accuracy of a la carte mpqa is 87.4. accuracy of a la carte trec is 85.6. accuracy of a la carte sst (±1) is 84.1. accuracy of a la carte sst is 46.7. accuracy of a la carte imdb is 89. accuracy of a la carte mr is 81.3. accuracy of a la carte cr is 83.7. accuracy of a la carte subj is 93.5. accuracy of a la carte mpqa is 87.6. accuracy of a la carte trec is 89. accuracy of a la carte sst (±1) is 85.8. accuracy of a la carte sst is 47.8. accuracy of a la carte imdb is 90.3. accuracy of a la carte mr is 81.8. accuracy of a la carte cr is 84.3. accuracy of a la carte subj is 93.8. accuracy of a la carte mpqa is 87.6. accuracy of a la carte trec is 89. accuracy of a la carte sst (±1) is 86.7. accuracy of a la carte sst is 48.1. accuracy of a la carte imdb is 90.9. accuracy of sent2vec mr is 76.3. accuracy of sent2vec cr is 79.1. accuracy of sent2vec subj is 91.2. accuracy of sent2vec mpqa is 87.2. accuracy of sent2vec trec is 85.8. accuracy of sent2vec sst (±1) is 80.2. accuracy of sent2vec sst is 31. accuracy of sent2vec imdb is 85.5. accuracy of disc mr is 80.1. accuracy of disc cr is 81.5. accuracy of disc subj is 92.6. accuracy of disc mpqa is 87.9. accuracy of disc trec is 90. accuracy of disc sst (±1) is 85.5. accuracy of disc sst is 46.7. accuracy of disc imdb is 89.6. accuracy of skip-thoughts mr is 80.3. accuracy of skip-thoughts cr is 83.8. accuracy of skip-thoughts subj is 94.2. accuracy of skip-thoughts mpqa is 88.9. accuracy of skip-thoughts trec is 93. accuracy of skip-thoughts sst (±1) is 85.1. accuracy of skip-thoughts sst is 45.8. accuracy of sdae mr is 74.6. accuracy of sdae cr is 78. accuracy of sdae subj is 90.8. accuracy of sdae mpqa is 86.9. accuracy of sdae trec is 78.4. accuracy of cnn-lstm mr is 77.8. accuracy of cnn-lstm cr is 82. accuracy of cnn-lstm subj is 93.6. accuracy of cnn-lstm mpqa is 89.4. accuracy of cnn-lstm trec is 92.6. accuracy of mc-qt mr is 82.4. accuracy of mc-qt cr is 86. accuracy of mc-qt subj is 94.8. accuracy of mc-qt mpqa is 90.2. accuracy of mc-qt trec is 92.4. accuracy of mc-qt sst (±1) is 87.6. accuracy of byte mlstm mr is 86.8. accuracy of byte mlstm cr is 90.6. accuracy of byte mlstm subj is 94.7. accuracy of byte mlstm mpqa is 88.8. accuracy of byte mlstm trec is 90.4. accuracy of byte mlstm sst (±1) is 91.7. accuracy of byte mlstm sst is 54.6. accuracy of byte mlstm imdb is 92.2.
table 1 shows results for the relation induction task. diff of acc diff is 90. conc of acc  conc is 89. avg of acc avg is 89.9. r1ik of acc r1ik is 90. r2ik of acc r2ik is 92.3. r3ik of acc r3ik is 90.9. r4ik of acc r4ik is 90.4. diff of pre diff is 81.6. conc of pre  conc is 78.7. avg of pre avg is 80.8. r1ik of pre r1ik is 79.9. r2ik of pre r2ik is 87.1. r3ik of pre r3ik is 83.2. r4ik of pre r4ik is 81.1. diff of rec diff is 82.6. conc of rec  conc is 83.9. avg of rec avg is 83.9. r1ik of rec r1ik is 86. r2ik of rec r2ik is 84.8. r3ik of rec r3ik is 84.8. r4ik of rec r4ik is 85.5. diff of f1 diff is 82.1. conc of f1  conc is 81.2. avg of f1 avg is 82.3. r1ik of f1 r1ik is 82.8. r2ik of f1 r2ik is 85.9. r3ik of f1 r3ik is 84. r4ik of f1 r4ik is 83.3.
table 2 shows performance (ρ) on sl and sv for ercnt models trained with different constraints. sl of synonyms only sl is 0.465. sv of synonyms only sv is 0.339. sl of antonyms only sl is 0.451. sv of antonyms only sv is 0.317. sl of synonyms + antonyms sl is 0.582. sv of synonyms + antonyms sv is 0.439.
table 2 shows the translation performance on english-german, english-french and chinese-to-english test sets. bleu of supervised en-de is 24.07. bleu of supervised de-en is 26.99. bleu of supervised en-fr is 30.5. bleu of supervised fr-en is 30.21. bleu of supervised zh-en is 40.02. bleu of word-by-word en-de is 5.85. bleu of word-by-word de-en is 9.34. bleu of word-by-word en-fr is 3.6. bleu of word-by-word fr-en is 6.8. bleu of word-by-word zh-en is 5.09. bleu of lample et al. (2017) en-de is 9.64. bleu of lample et al. (2017) de-en is 13.33. bleu of lample et al. (2017) en-fr is 15.05. bleu of lample et al. (2017) fr-en is 14.31. bleu of the proposed approach en-de is 10.86. bleu of the proposed approach de-en is 14.62. bleu of the proposed approach en-fr is 16.97. bleu of the proposed approach fr-en is 15.58. bleu of the proposed approach zh-en is 14.52.
table 5 shows comparison of different segmentation algorithms (wmt14 en!de) bleu of word bleu is 23.12. bleu of character (512 nodes) bleu is 22.62. bleu of mixed word/character bleu is 24.17. bleu of bpe bleu is 24.53. bleu of unigram w/o sr (l = 1) bleu is 24.5. bleu of unigram w/ sr (l = 64 alpha = 0.1) bleu is 25.04.
table 2 shows results on wmt14 en→de. test bleu of gnmt test bleu is 24.67. test bleu of convs2s test bleu is 25.01 }0.17. epochs of convs2s epochs is 38. training time of convs2s training
table 4 shows ablation results of rnmt+ and the transformer big model on wmt’14 en → fr. bleu of baseline rnmt+ is 41. bleu of baseline trans. big is 40.73. bleu of -label smoothing rnmt+ is 40.33. bleu of -label smoothing trans. big is 40.49. bleu of -multi-head attention rnmt+ is 40.44. bleu of -multi-head attention trans. big is 39.83. bleu of -layer norm. rnmt+ is *. bleu of -layer norm. trans. big is *. bleu of -sync. training rnmt+ is 39.68. bleu of -sync. training trans. big is *.
table 3 shows performance of our model and attentivener (shimaoka et al., 2017) on the new entity typing benchmark, using same training data. mrr of attentivener mrr is 0.221. p of attentivener p is 53.7. r of attentivener r is 15. f1 of attentivener f1 is 23.5. mrr of attentivener mrr is 0.223. p of attentivener p is 54.2. r of attentivener r is 15.2. f1 of attentivener f1 is 23.7. mrr of our model mrr is 0.229. p of our model p is 48.1. r of our model r is 23.2. f1 of our model f1 is 31.3. mrr of our model mrr is 0.234. p of our model p is 47.1. r of our model r is 24.2. f1 of our model f1 is 32.
table 4 shows results on the development set for different type granularity and for different supervision data with our model. mrr of all mrr is 0.229. p of all p is 48.1. r of all r is 23.2. f1 of all f1 is 31.3. p of all p is 60.3. r of all r is 61.6. f1 of all f1 is 61. p of all p is 40.4. r of all r is 38.4. f1 of all f1 is 39.4. p of all p is 42.8. r of all r is 8.8. f1 of all f1 is 14.6. mrr of -crowd mrr is 0.173. p of -crowd p is 40.1. r of -crowd r is 14.8. f1 of -crowd f1 is 21.6. p of -crowd p is 53.7. r of -crowd r is 45.6. f1 of -crowd f1 is 49.3. p of -crowd p is 20.8. r of -crowd r is 18.5. f1 of -crowd f1 is 19.6. p of -crowd p is 54.4. r of -crowd r is 4.6. f1 of -crowd f1 is 8.4. mrr of -head mrr is 0.22. p of -head p is 50.3. r of -head r is 19.6. f1 of -head f1 is 28.2. p of -head p is 58.8. r of -head r is 62.8. f1 of -head f1 is 60.7. p of -head p is 44.4. r of -head r is 29.8. f1 of -head f1 is 35.6. p of -head p is 46.2. r of -head r is 4.7. f1 of -head f1 is 8.5. mrr of -el mrr is 0.225. p of -el p is 48.4. r of -el r is 22.3. f1 of -el f1 is 30.6. p of -el p is 62.2. r of -el r is 60.1. f1 of -el f1 is 61.2. p of -el p is 40.3. r of -el r is 26.1. f1 of -el f1 is 31.7. p of -el p is 41.4. r of -el r is 9.9. f1 of -el f1 is 16.
table 6 shows results on the ontonotes fine-grained entity typing test set. acc. of attentivener++ acc. is 51.7. ma-f1 of attentivener++ ma-f1 is 70.9. mi-f1 of attentivener++ mi-f1 is 64.9. acc. of afet (ren et al., 2016a) acc. is 55.1. ma-f1 of afet (ren et al., 2016a) ma-f1 is 71.1. mi-f1 of afet (ren et al., 2016a) mi-f1 is 64.7. acc. of lnr (ren et al., 2016b) acc. is 57.2. ma-f1 of lnr (ren et al., 2016b) ma-f1 is 71.5. mi-f1 of lnr (ren et al., 2016b) mi-f1 is 66.1. acc. of ours (onto+wiki+head) acc. is 59.5. ma-f1 of ours (onto+wiki+head) ma-f1 is 76.8. mi-f1 of ours (onto+wiki+head) mi-f1 is 71.8.
table 6 shows map of entity-level typing in wikipedia data using typenet. map of cnn low data is 51.72. map of cnn full data is 68.15. map of cnn + hierarchy low data is 54.82. map of cnn + hierarchy full data is 75.56. map of cnn + transitive low data is 57.68. map of cnn + transitive full data is 77.21. map of cnn + hierarchy + transitive low data is 58.74. map of cnn + hierarchy + transitive full data is 78.59. map of cnn+complex low data is 50.51. map of cnn+complex full data is 69.83. map of cnn+complex + hierarchy low data is 55.3. map of cnn+complex + hierarchy full data is 72.86. map of cnn+complex + transitive low data is 53.71. map of cnn+complex + transitive full data is 72.18. map of cnn+complex + hierarchy + transitive low data is 58.81. map of cnn+complex + hierarchy + transitive full data is 77.21.
table 3 shows interannotator agreement rates (pairwise averages) on little prince sample (216 tokens) with different levels of hierarchy coarsening according to figure 2 (“exact” means no coarsening). agreement of exact role is 74.40%. agreement of exact function is 81.30%. agreement of depth-3 role is 75.00%. agreement of depth-3 function is 81.80%. agreement of depth-2 role is 79.90%. agreement of depth-2 function is 87.40%. agreement of depth-1 role is 92.60%. agreement of depth-1 function is 93.90%.
table 5 shows performance of predicting veracity. accuracy of style all is 0.55. precision of style fake is 0.42. precision of style real is 0.62. recall of style fake is 0.41. recall of style real is 0.64. f1 of style fake is 0.41. f1 of style real is 0.63. accuracy of topic all is 0.52. precision of topic fake is 0.41. precision of topic real is 0.62. recall of topic fake is 0.48. recall of topic real is 0.55. f1 of topic fake is 0.44. f1 of topic real is 0.58. accuracy of style all is 0.55. precision of style fake is 0.43. precision of style real is 0.64. recall of style fake is 0.49. recall of style real is 0.59. f1 of style fake is 0.46. f1 of style real is 0.61. accuracy of topic all is 0.58. precision of topic fake is 0.46. precision of topic real is 0.65. recall of topic fake is 0.45. recall of topic real is 0.66. f1 of topic fake is 0.46. f1 of topic real is 0.66. accuracy of all-fake all is 0.39. precision of all-fake fake is 0.39. recall of all-fake fake is 1. recall of all-fake real is 0. f1 of all-fake fake is 0.56. accuracy of all-real all is 0.61. precision of all-real real is 0.61. recall of all-real fake is 0. recall of all-real real is 1. f1 of all-real real is 0.76.
table 1 shows results for amr generation on the test set. bleu of s2s bleu is 21.7. bleu of s2s chrf++ is 49.1. bleu of s2s #params is 28.4m. bleu of s2s (-s) bleu is 18.4. bleu of s2s (-s) chrf++ is 46.3. bleu of s2s (-s) #params is  28.4m. bleu of g2s bleu is 23.3. bleu of g2s chrf++ is 50.4. bleu of g2s #params is 28.3m. bleu of s2s bleu is 26.6. bleu of s2s chrf++ is 52.5. bleu of s2s #params is 142m. bleu of s2s (-s) bleu is 22. bleu of s2s (-s) chrf++ is 48.9. bleu of s2s (-s) #params is 142m. bleu of g2s bleu is 27.5. bleu of g2s chrf++ is 53.5. bleu of g2s #params is 141m. bleu of kiycz17 bleu is 22. bleu of kiycz17 bleu is 33.8. bleu of pkh16 bleu is 26.9. bleu of spzwg17 bleu is 25.6. bleu of fdsc16 bleu is 22.
table 1 shows test classification accuracy (and the number of parameters used). accuracy of hard roc is 62.2 (4k). accuracy of hard sst is 75.5 (6k). accuracy of hard amazon is 88.5 (67k). accuracy of dan roc is 64.3 (91k). accuracy of dan sst is 83.1 (91k). accuracy of dan amazon is 85.4 (91k). accuracy of bilstm roc is 65.2 (844k). accuracy of bilstm sst is 84.8 (1.5m). accuracy of bilstm amazon is 90.8 (844k). accuracy of cnn roc is 64.3 (155k). accuracy of cnn sst is 82.2 (62k). accuracy of cnn amazon is 90.2 (305k). accuracy of sopa roc is 66.5 (255k). accuracy of sopa sst is 85.6 (255k). accuracy of sopa amazon is 90.5 (256k).
table 2 shows movie review dev results of s-lstm time(s) of +0 dummy node time (s) is 56. acc of +0 dummy node acc is 81.76. #param of +0 dummy node # param is 7216k. time(s) of +1 dummy node time (s) is 65. acc of +1 dummy node acc is 82.64. #param of +1 dummy node # param is 8768k. time(s) of +2 dummy node time (s) is 76. acc of +2 dummy node acc is 82.24. #param of +2 dummy node # param is 10321k. time(s) of hidden size 100 time (s) is 42. acc of hidden size 100 acc is 81.75. #param of hidden size 100 # param is 4891k. time(s) of hidden size 200 time (s) is 54. acc of hidden size 200 acc is 82.04. #param of hidden size 200 # param is 6002k. time(s) of hidden size 300 time (s) is 65. acc of hidden size 300 acc is 82.64. #param of hidden size 300 # param is 8768k. time(s) of hidden size 600 time (s) is 175. acc of hidden size 600 acc is 81.84. #param of hidden size 600 # param is 17648k. time(s) of hidden size 900 time (s) is 235. acc of hidden size 900 acc is 81.66. #param of hidden size 900 # param is 33942k. time(s) of without s /s time (s) is 63. acc of without s /s acc is 82.36. #param of without s /s # param is 8768k. time(s) of with s /s time (s) is 65. acc of with s /s acc is 82.64. #param of with s /s # param is 8768k.
table 3 shows movie review development results time(s) of lstm time (s) is 67. acc of lstm acc is 80.72. #param of lstm # param is 5,977k. time(s) of bilstm time (s) is 106. acc of bilstm acc is 81.73. #param of bilstm # param is 7,059k. time(s) of 2 stacked bilstm time (s) is 207. acc of 2 stacked bilstm acc is 81.97. #param of 2 stacked bilstm # param is 9,221k. time(s) of 3 stacked bilstm time (s) is 310. acc of 3 stacked bilstm acc is 81.53. #param of 3 stacked bilstm # param is 11,383k. time(s) of 4 stacked bilstm time (s) is 411. acc of 4 stacked bilstm acc is 81.37. #param of 4 stacked bilstm # param is 13,546k. time(s) of s-lstm time (s) is 65. acc of s-lstm acc is 82.64*. #param of s-lstm # param is 8,768k. time(s) of cnn time (s) is 34. acc of cnn acc is 80.35. #param of cnn # param is 5,637k. time(s) of 2 stacked cnn time (s) is 40. acc of 2 stacked cnn acc is 80.97. #param of 2 stacked cnn # param is 5,717k. time(s) of 3 stacked cnn time (s) is 47. acc of 3 stacked cnn acc is 81.46. #param of 3 stacked cnn # param is 5,808k. time(s) of 4 stacked cnn time (s) is 51. acc of 4 stacked cnn acc is 81.39. #param of 4 stacked cnn # param is 5,855k. time(s) of transformer (n=6) time (s) is 138. acc of transformer (n=6) acc is 81.03. #param of transformer (n=6) # param is 7,234k. time(s) of transformer (n=8) time (s) is 174. acc of transformer (n=8) acc is 81.86. #param of transformer (n=8) # param is 7,615k. time(s) of transformer (n=10) time (s) is 214. acc of transformer (n=10) acc is 81.63. #param of transformer (n=10) # param is 8,004k.
table 1 shows performances of different approaches on the wikisql dataset. acclf of attentional seq2seq acclf is 23.3%. accex of attentional seq2seq accex is 37.0%. acclf of attentional seq2seq acclf is 23.4%. accex of attentional seq2seq accex is 35.9%. acclf of aug.pntnet (zhong et al. 2017) acclf is 44.1%. accex of aug.pntnet (zhong et al. 2017) accex is 53.8%. acclf of aug.pntnet (zhong et al. 2017) acclf is 43.3%. accex of aug.pntnet (zhong et al. 2017) accex is 53.3%. acclf of aug.pntnet (re-implemented by us) acclf is 51.5%. accex of aug.pntnet (re-implemented by us) accex is 58.9%. acclf of aug.pntnet (re-implemented by us) acclf is 52.1%. accex of aug.pntnet (re-implemented by us) accex is 59.2%. acclf of seq2sql (no rl) (zhong et al. 2017) acclf is 48.2%. accex of seq2sql (no rl) (zhong et al. 2017) accex is 58.1%. acclf of seq2sql (no rl) (zhong et al. 2017) acclf is 47.4%. accex of seq2sql (no rl) (zhong et al. 2017) accex is 57.1%. acclf of seq2sql (zhong et al. 2017) acclf is 49.5%. accex of seq2sql (zhong et al. 2017) accex is 60.8%. acclf of seq2sql (zhong et al. 2017) acclf is 48.3%. accex of seq2sql (zhong et al. 2017) accex is 59.4%. accex of sqlnet (xu et al. 2017) accex is 69.8%. accex of sqlnet (xu et al. 2017) accex is 68.0%. accex of guo and gao (2018) accex is 71.1%. accex of guo and gao (2018) accex is 69.0%. acclf of stamp (w/o cell) acclf is 58.6%. accex of stamp (w/o cell) accex is 67.8%. acclf of stamp (w/o cell) acclf is 58.0%. accex of stamp (w/o cell) accex is 67.4%. acclf of stamp (w/o column-cell relation) acclf is 59.3%. accex of stamp (w/o column-cell relation) accex is 71.8%. acclf of stamp (w/o column-cell relation) acclf is 58.4%. accex of stamp (w/o column-cell relation) accex is 70.6%. acclf of stamp acclf is 61.5%. accex of stamp accex is 74.8%. acclf of stamp acclf is 60.7%. accex of stamp accex is 74.4%. acclf of stamp+rl acclf is 61.7%. accex of stamp+rl accex is 75.1%. acclf of stamp+rl acclf is 61.0%. accex of stamp+rl accex is 74.6%.
table 6 shows performances on two datasets. accuracy of wang et al. (2017) (linear) is 19.70%. accuracy of wang et al. (2017) (all) is 14.60%. accuracy of wang et al. (2017) (linear) is 10.20%. accuracy of seq2seq equ (linear) is 26.80%. accuracy of seq2seq equ (all) is 20.10%. accuracy of seq2seq equ (linear) is 13.10%. accuracy of seq2seq lf (linear) is 50.80%. accuracy of seq2seq lf (all) is 45.20%. accuracy of seq2seq lf (linear) is 13.90%. accuracy of seq2seq lf+attreg (linear) is 56.70%. accuracy of seq2seq lf+attreg (all) is 54.00%. accuracy of seq2seq lf+attreg (linear) is 15.10%. accuracy of seq2seq lf+attreg+iter (linear) is 61.60%. accuracy of seq2seq lf+attreg+iter (all) is 57.10%. accuracy of seq2seq lf+attreg+iter (linear) is 16.80%. accuracy of shi et al. (2015) (linear) is 63.60%. accuracy of shi et al. (2015) (all) is 60.20%. accuracy of shi et al. (2015) (linear) is n/a. accuracy of huang et al. (2017) (linear) is 20.80%. accuracy of huang et al. (2017) (all) is n/a. accuracy of huang et al. (2017) (linear) is 28.40%.
table 7 shows the comparisons of gen+adv with gen and the data augmentation model (gen+aug). f1 of gen case is 91.5. f1 of gen zero is 56.2. f1 of gen+aug case is 91.2. f1 of gen+aug zero is 57. f1 of gen+adv case is 92.0ö. f1 of gen+adv zero is 58.4ö.
table 2 shows results of different models in nyt dataset and webnlg dataset. precision of noveltagging precision is 0.624. recall of noveltagging recall is 0.317. f1 of noveltagging f1 is 0.42. precision of noveltagging precision is 0.525. recall of noveltagging recall is 0.193. f1 of noveltagging f1 is 0.283. precision of onedecoder precision is 0.594. recall of onedecoder recall is 0.531. f1 of onedecoder f1 is 0.56. precision of onedecoder precision is 0.322. recall of onedecoder recall is 0.289. f1 of onedecoder f1 is 0.305. precision of multidecoder precision is 0.61. recall of multidecoder recall is 0.566. f1 of multidecoder f1 is 0.587. precision of multidecoder precision is 0.377. recall of multidecoder recall is 0.364. f1 of multidecoder f1 is 0.371.
table 1 shows trigger identification performance p (%) of joint (local+global) p (%) is 76.9. r (%) of joint (local+global) r (%) is 65. f (%) of joint (local+global) f (%) is 70.4. p (%) of msep-emd p (%) is 75.6. r (%) of msep-emd r (%) is 69.8. f (%) of msep-emd f (%) is 72.6. p (%) of dm-cnn p (%) is 80.4. r (%) of dm-cnn r (%) is 67.7. f (%) of dm-cnn f (%) is 73.5. p (%) of dm-cnn* p (%) is 79.7. r (%) of dm-cnn* r (%) is 69.6. f (%) of dm-cnn* f (%) is 74.3. p (%) of bi-rnn p (%) is 68.5. r (%) of bi-rnn r (%) is 75.7. f (%) of bi-rnn f (%) is 71.9. p (%) of hybrid: bi-lstm+cnn p (%) is 80.8. r (%) of hybrid: bi-lstm+cnn r (%) is 71.5. f (%) of hybrid: bi-lstm+cnn f (%) is 75.9. p (%) of self: bi-lstm+gan p (%) is 75.3. r (%) of self: bi-lstm+gan r (%) is 78.8. f (%) of self: bi-lstm+gan f (%) is 77.
table 1 shows results on the test set. micro-f1 of caevo (not nn model) micro-f1 is 0.507. micro-f1 of catena (not nn model) micro-f1 is 0.511. micro-f1 of cheng et al. 2017 micro-f1 is 0.5203. macro-f1 of meng et al. 2017 macro-f1 is 0.519. micro-f1 of pairwise micro-f1 is 0.535. macro-f1 of pairwise macro-f1 is 0.528. micro-f1 of two more hidden layers micro-f1 is 0.539. macro-f1 of two more hidden layers macro-f1 is 0.532. micro-f1 of gcl w/ state-tracking controller micro-f1 is 0.545. macro-f1 of gcl w/ state-tracking controller macro-f1 is 0.538. micro-f1 of gcl w/ stateless controller micro-f1 is 0.546. macro-f1 of gcl w/ stateless controller macro-f1 is 0.538. micro-f1 of gcl w/ pre-trained output layer micro-f1 is 0.541. macro-f1 of gcl w/ pre-trained output layer macro-f1 is 0.536.
table 5 shows results on timebank corpus acc.(%) of choubey and huang (2017) acc.(%) is 51.2. acc.(%) of choubey and huang (2017) + cp score acc.(%) is 52.3.
table 4 shows performance of our model with different random seeds. min f of our model min f is 56.5. median f of our model median f is 57.1. max f of our model max f is 57.5. σ of our model σ is 0.00253.
table 2 shows full length rouge f1 evaluation (%) on cnn/daily mail test set. rouge-1 of lead3 rouge-1 is 40.24-. rouge-2 of lead3 rouge-2 is 17.70-. rouge-l of lead3 rouge-l is 36.45-. rouge-1 of textrank rouge-1 is 40.20-. rouge-2 of textrank rouge-2 is 17.56-. rouge-l of textrank rouge-l is 36.44-. rouge-1 of crsum rouge-1 is 40.52-. rouge-2 of crsum rouge-2 is 18.08-. rouge-l of crsum rouge-l is 36.81-. rouge-1 of nn-se rouge-1 is 41.13-. rouge-2 of nn-se rouge-2 is 18.59-. rouge-l of nn-se rouge-l is 37.40-. rouge-1 of pgn ‡ rouge-1 is 39.53-. rouge-2 of pgn ‡ rouge-2 is 17.28-. rouge-l of pgn ‡ rouge-l is 36.38-. rouge-1 of lead3 ‡ * rouge-1 is 39.2. rouge-2 of lead3 ‡ * rouge-2 is 15.7. rouge-l of lead3 ‡ * rouge-l is 35.5. rouge-1 of summarunner ‡ * rouge-1 is 39.6. rouge-2 of summarunner ‡ * rouge-2 is 16.2. rouge-l of summarunner ‡ * rouge-l is 35.3. rouge-1 of neusum rouge-1 is 41.59. rouge-2 of neusum rouge-2 is 19.01. rouge-l of neusum rouge-l is 37.98.
table 5 shows speed comparison with see et al. total time (hr) of (see et al., 2017) total time (hr) is 12.9. words / sec of (see et al., 2017) words / sec is 14.8. total time (hr) of rnn-ext + abs + rl total time (hr) is 0.68. words / sec of rnn-ext + abs + rl words / sec is 361.3. total time (hr) of rnn-ext + abs + rl + rerank total time (hr) is 2.00 (1.46 +0.54). words / sec of rnn-ext + abs + rl + rerank words / sec is 109.8.
table 4 shows gigaword human evaluation: pairwise comparison between our 3-way multi-task (mtl) model w.r.t. relevance of mtl wins relevance is 33. readability of mtl wins readability is 32. total of mtl wins total is 65. relevance of baseline wins relevance is 22. readability of baseline wins readability is 22. total of baseline wins total is 44. relevance of non-distinguish relevance is 45. readability of non-distinguish readability is 46. total of non-distinguish total is 91.
table 6 shows performance of our pointer-based entailment generation (eg) models compared with previous sota work. m of pasunuru&bansal (2017) m is 29.6. c of pasunuru&bansal (2017) c is 117.8. r of pasunuru&bansal (2017) r is 62.4. b of pasunuru&bansal (2017) b is 40.6. m of our 1-layer pointer eg m is 32.4. c of our 1-layer pointer eg c is 139.3. r of our 1-layer pointer eg r is 65.1. b of our 1-layer pointer eg b is 43.6. m of our 2-layer pointer eg m is 32.3. c of our 2-layer pointer eg c is 140.0. r of our 2-layer pointer eg r is 64.4. b of our 2-layer pointer eg b is 43.7.
table 9 shows entailment classification results of our baseline vs. average entailment probability of baseline average entailment probability is 0.907. average entailment probability of multi-task (eg) average entailment probability is 0.912.
table 6 shows overview of macro-weighted average f1 scores of svm and psl models. f1 of svm bow mfd is 18.7. f1 of psl bow mfd is 21.88. f1 of majority vote mfd is 12.5. f1 of majority vote ar is 10.86. f1 of m1 (unigrams) mfd is 7.17. f1 of m1 (unigrams) ar is 8.68. f1 of m3 (+ political info) mfd is 22.01. f1 of m3 (+ political info) ar is 30.45. f1 of m5 (+ frames) mfd is 28.94. f1 of m5 (+ frames) ar is 37.44. f1 of m9 (+ bigrams) mfd is 67.93. f1 of m9 (+ bigrams) ar is 66.5. f1 of m13 (all features) mfd is 72.49. f1 of m13 (all features) ar is 69.38.
table 9 shows overview of macro-weighted average f1 scores of joint psl model m13. f1 of baseline mfd is 55.49. f1 of baseline ar is 55.88. f1 of joint mfd is 51.22. f1 of joint ar is 58.75. f1 of skyline mfd is 72.49. f1 of skyline ar is 69.38.
table 3 shows django results. accuracy of retrieval system accuracy is 14.7. accuracy of phrasal smt accuracy is 31.5. accuracy of hierarchical smt accuracy is 9.5. accuracy of seq2seq+unk replacement accuracy is 45.1. accuracy of seq2tree+unk replacement accuracy is 39.4. accuracy of lpn+copy (ling et al. 2016) accuracy is 62.3. accuracy of snm+copy (yin and neubig 2017) accuracy is 71.6. accuracy of onestage accuracy is 69.5. accuracy of coarse2fine accuracy is 74.1. accuracy of coarse2fine - sketch encoder accuracy is 72.1. accuracy of coarse2fine + oracle sketch accuracy is 83.
table 5 shows importance scores of confidence metrics (normalized by maximum value on each dataset). dout of ifttt dout is 0.39. noise of ifttt noise is 1. pr of ifttt pr is 0.89. ppl of ifttt ppl is 0.27. lm of ifttt lm is 0.26. #unk of ifttt #unk is 0.46. var of ifttt var is 0.43. ent of ifttt ent is 0.34. dout of django dout is 1. noise of django noise is 0.59. pr of django pr is 0.22. ppl of django ppl is 0.58. lm of django lm is 0.49. #unk of django #unk is 0.14. var of django var is 0.24. ent of django ent is 0.25.
table 3 shows accuracy (%) of the proposed method in comparison with previous work. accuracy of mikolov et al. (2013) en-it is 34.93†. accuracy of mikolov et al. (2013) en-de is 35.00†. accuracy of mikolov et al. (2013) en-fi is 25.91†. accuracy of mikolov et al. (2013) en-es is 27.73†. accuracy of faruqui and dyer (2014) en-it is 38.40*. accuracy of faruqui and dyer (2014) en-de is 37.13*. accuracy of faruqui and dyer (2014) en-fi is 27.60*. accuracy of faruqui and dyer (2014) en-es is 26.80*. accuracy of shigeto et al. (2015) en-it is 41.53†. accuracy of shigeto et al. (2015) en-de is 43.07†. accuracy of shigeto et al. (2015) en-fi is 31.04†. accuracy of shigeto et al. (2015) en-es is 33.73†. accuracy of dinu et al. (2015) en-it is 37.7. accuracy of dinu et al. (2015) en-de is 38.93*. accuracy of dinu et al. (2015) en-fi is 29.14*. accuracy of dinu et al. (2015) en-es is 30.40*. accuracy of lazaridou et al. (2015) en-it is 40.2. accuracy of xing et al. (2015) en-it is 36.87†. accuracy of xing et al. (2015) en-de is 41.27†. accuracy of xing et al. (2015) en-fi is 28.23†. accuracy of xing et al. (2015) en-es is 31.20†. accuracy of zhang et al. (2016) en-it is 36.73†. accuracy of zhang et al. (2016) en-de is 40.80†. accuracy of zhang et al. (2016) en-fi is 28.16†. accuracy of zhang et al. (2016) en-es is 31.07†. accuracy of artetxe et al. (2016) en-it is 39.27. accuracy of artetxe et al. (2016) en-de is 41.87* . accuracy of artetxe et al. (2016) en-fi is 30.62*. accuracy of artetxe et al. (2016) en-es is 31.40*. accuracy of artetxe et al. (2017) en-it is 39.67. accuracy of artetxe et al. (2017) en-de is 40.87. accuracy of artetxe et al. (2017) en-fi is 28.72. accuracy of smith et al. (2017) en-it is 43.1. accuracy of smith et al. (2017) en-de is 43.33†. accuracy of smith et al. (2017) en-fi is 29.42†. accuracy of smith et al. (2017) en-es is 35.13†. accuracy of artetxe et al. (2018a) en-it is 45.27. accuracy of artetxe et al. (2018a) en-de is 44.13. accuracy of artetxe et al. (2018a) en-fi is 32.94. accuracy of artetxe et al. (2018a) en-es is 36.6. accuracy of artetxe et al. (2017) en-it is 37.27. accuracy of artetxe et al. (2017) en-de is 39.6. accuracy of artetxe et al. (2017) en-fi is 28.16. accuracy of smith et al. (2017) cognates en-it is 39.9. accuracy of artetxe et al. (2017) num. en-it is 39.4. accuracy of artetxe et al. (2017) num. en-de is 40.27. accuracy of artetxe et al. (2017) num. en-fi is 26.47. accuracy of zhang et al. (2017a) λ = 1 en-it is 0.00*. accuracy of zhang et al. (2017a) λ = 1 en-de is 0.00*. accuracy of zhang et al. (2017a) λ = 1 en-fi is 0.00*. accuracy of zhang et al. (2017a) λ = 1 en-es is 0.00*. accuracy of zhang et al. (2017a) λ = 10 en-it is 0.00*. accuracy of zhang et al. (2017a) λ = 10 en-de is 0.00*. accuracy of zhang et al. (2017a) λ = 10 en-fi is 0.01*. accuracy of zhang et al. (2017a) λ = 10 en-es is 0.01*. accuracy of conneau et al. (2018) code‡ en-it is 45.15*. accuracy of conneau et al. (2018) code‡ en-de is 46.83*. accuracy of conneau et al. (2018) code‡ en-fi is 0.38*. accuracy of conneau et al. (2018) code‡ en-es is 35.38*. accuracy of conneau et al. (2018) paper‡ en-it is 45.1. accuracy of conneau et al. (2018) paper‡ en-de is 0.01*. accuracy of conneau et al. (2018) paper‡ en-fi is 0.01*. accuracy of conneau et al. (2018) paper‡ en-es is 35.44*. accuracy of proposed method en-it is 48.13. accuracy of proposed method en-de is 48.19. accuracy of proposed method en-fi is 32.63. accuracy of proposed method en-es is 37.33.
table 6 shows performance comparison between models with different components (c: character embedding; l: shared lstm; s: language-specific layer; h: highway networks; d: dropout). f-score of basic 0 is 2.06. f-score of basic 10 is 20.03. f-score of basic 100 is 47.98. f-score of basic 200 is 51.52. f-score of basic all is 77.63. f-score of basic + c 0 is 1.69. f-score of basic + c 10 is 24.22. f-score of basic + c 100 is 48.53. f-score of basic + c 200 is 56.26. f-score of basic + c all is 83.38. f-score of basic + cl 0 is 9.62. f-score of basic + cl 10 is 25.97. f-score of basic + cl 100 is 49.54. f-score of basic + cl 200 is 56.29. f-score of basic + cl all is 83.37. f-score of basic + cls 0 is 3.21. f-score of basic + cls 10 is 25.43. f-score of basic + cls 100 is 50.67. f-score of basic + cls 200 is 56.34. f-score of basic + cls all is 84.02. f-score of basic + clsh 0 is 7.7. f-score of basic + clsh 10 is 30.48. f-score of basic + clsh 100 is 53.73. f-score of basic + clsh 200 is 58.09. f-score of basic + clsh all is 84.68. f-score of basic + clshd 0 is 12.12. f-score of basic + clshd 10 is 35.82. f-score of basic + clshd 100 is 57.33. f-score of basic + clshd 200 is 63.27. f-score of basic + clshd all is 86.
table 2 shows we report f1 results for medical bli with the cosine similarity and the classifier based systems. f1 (top) of baseline f1 (top) is 13.43. f1 (all) of baseline f1 (all) is 9.84. f1 (top) of baseline f1 (top) is 37.73. f1 (all) of baseline f1 (all) is 36.61. f1 (top) of baseline bnc lexicon f1 (top) is 20.73. f1 (all) of baseline bnc lexicon f1 (all) is 21.78. f1 (top) of adapted medical lexicon f1 (top) is 14.18. f1 (all) of adapted medical lexicon f1 (all) is 14.15. f1 (top) of adapted medical lexicon f1 (top) is 40.71. f1 (all) of adapted medical lexicon f1 (all) is 38.09. f1 (top) of adapted bnc lexicon f1 (top) is 16.29. f1 (all) of adapted bnc lexicon f1 (all) is 16.71. f1 (top) of adapted bnc lexicon f1 (top) is 22.1. f1 (all) of adapted bnc lexicon f1 (all) is 21.5.
table 5 shows results with the semi-supervised system for bli. f1 (top) of baseline+bnc f1 (top) is 35.04 (-0.66). f1 (all) of baseline+bnc f1 (all) is 34.98 (-1.40). f1 (top) of baseline+medical f1 (top) is 36.20 (0.50). f1 (all) of baseline+medical f1 (all) is 36.55 (0.16). f1 (top) of adapted+bnc f1 (top) is 41.01 (0.30). f1 (all) of adapted+bnc f1 (all) is 39.04 (0.95). f1 (top) of adapted+medical f1 (top) is 41.44 (0.73). f1 (all) of adapted+medical f1 (all) is 37.51 (-0.57).
table 4 shows results for different combinations of interactions between document (d) and question (q) context (ctx) and context + knowledge (ctx+kn) representations. accuracy of dctx qctx (w/o know) dev is 75.5. accuracy of dctx qctx (w/o know) test is 70.3. accuracy of dctx qctx (w/o know) dev is 68.2. accuracy of dctx qctx (w/o know) test is 64.8. accuracy of dctx+kn qctx+kn dev is 76.45. accuracy of dctx+kn qctx+kn test is 69.68. accuracy of dctx+kn qctx+kn dev is 70.85. accuracy of dctx+kn qctx+kn test is 66.32. accuracy of dctx qctx+kn dev is 77.1. accuracy of dctx qctx+kn test is 69.72. accuracy of dctx qctx+kn dev is 70.8. accuracy of dctx qctx+kn test is 66.32. accuracy of dctx+kn qctx dev is 75.65. accuracy of dctx+kn qctx test is 70.88. accuracy of dctx+kn qctx dev is 71.2. accuracy of dctx+kn qctx test is 67.96. accuracy of full model dev is 76.8. accuracy of full model test is 70.24. accuracy of full model dev is 71.85. accuracy of full model test is 67.64. accuracy of w/o dctx qctx dev is 75.95. accuracy of w/o dctx qctx test is 70.24. accuracy of w/o dctx qctx dev is 70.65. accuracy of w/o dctx qctx test is 67.12. accuracy of w/o dctx+kn qctx+kn dev is 76.2. accuracy of w/o dctx+kn qctx+kn test is 69.8. accuracy of w/o dctx+kn qctx+kn dev is 70.75. accuracy of w/o dctx+kn qctx+kn test is 67. accuracy of w/o dctx qctx+kn dev is 76.55. accuracy of w/o dctx qctx+kn test is 70.52. accuracy of w/o dctx qctx+kn dev is 71.75. accuracy of w/o dctx qctx+kn test is 66.32. accuracy of w/o dctx+kn qctx dev is 76.05. accuracy of w/o dctx+kn qctx test is 70.84. accuracy of w/o dctx+kn qctx dev is 70.8. accuracy of w/o dctx+kn qctx test is 66.8.
table 6 shows comparison of knreader to existing endto-end neural models on the benchmark datasets. accuracy of human (ctx + q) test is 81.6. accuracy of human (ctx + q) test is 81.6. accuracy of lstms (ctx + q) (hill et al. 2015) dev is 51.2. accuracy of lstms (ctx + q) (hill et al. 2015) test is 41.8. accuracy of lstms (ctx + q) (hill et al. 2015) dev is 62.6. accuracy of lstms (ctx + q) (hill et al. 2015) test is 56. accuracy of as reader dev is 73.8. accuracy of as reader test is 68.6. accuracy of as reader dev is 68.8. accuracy of as reader test is 63.4. accuracy of as reader (our impl) dev is 75.5. accuracy of as reader (our impl) test is 70.3. accuracy of as reader (our impl) dev is 68.2. accuracy of as reader (our impl) test is 64.8. accuracy of knreader (ours) dev is 77.4. accuracy of knreader (ours) test is 71.4. accuracy of knreader (ours) dev is 71.8. accuracy of knreader (ours) test is 67.6. accuracy of memnns (weston et al. 2015) dev is 70.4. accuracy of memnns (weston et al. 2015) test is 66.6. accuracy of memnns (weston et al. 2015) dev is 64.2. accuracy of memnns (weston et al. 2015) test is 63. accuracy of epireader (trischler et al. 2016) dev is 74.9. accuracy of epireader (trischler et al. 2016) test is 69. accuracy of epireader (trischler et al. 2016) dev is 71.5. accuracy of epireader (trischler et al. 2016) test is 67.4. accuracy of ga reader (dhingra et al. 2017) dev is 77.2. accuracy of ga reader (dhingra et al. 2017) test is 71.4. accuracy of ga reader (dhingra et al. 2017) dev is 71.6. accuracy of ga reader (dhingra et al. 2017) test is 68. accuracy of iaa reader (sordoni et al. 2016) dev is 75.3. accuracy of iaa reader (sordoni et al. 2016) test is 69.7. accuracy of iaa reader (sordoni et al. 2016) dev is 72.1. accuracy of iaa reader (sordoni et al. 2016) test is 69.2. accuracy of aoa reader (cui et al. 2017) dev is 75.2. accuracy of aoa reader (cui et al. 2017) test is 68.6. accuracy of aoa reader (cui et al. 2017) dev is 72.2. accuracy of aoa reader (cui et al. 2017) test is 69.4. accuracy of ga reader (+feat) dev is 77.8. accuracy of ga reader (+feat) test is 72. accuracy of ga reader (+feat) dev is 74.4. accuracy of ga reader (+feat) test is 70.7. accuracy of nse (munkhdalai and yu 2016) dev is 77. accuracy of nse (munkhdalai and yu 2016) test is 71.4. accuracy of nse (munkhdalai and yu 2016) dev is 74.3. accuracy of nse (munkhdalai and yu 2016) test is 71.9.
table 2 shows results on cross-lingual image description retrieval. r@1 of dpcca (variant a) en→de is 0.795. r@1 of dpcca (variant a) de→en is 0.779. bleu+1 of dpcca (variant a) en→de is 0.836. bleu+1 of dpcca (variant a) de→en is 0.827. r@1 of dpcca (variant b) en→de is 0.809. r@1 of dpcca (variant b) de→en is 0.794. bleu+1 of dpcca (variant b) en→de is 0.848. bleu+1 of dpcca (variant b) de→en is 0.839. r@1 of dpcca(b)+dcca noi (concat) en→de is 0.826. r@1 of dpcca(b)+dcca noi (concat) de→en is 0.791. bleu+1 of dpcca(b)+dcca noi (concat) en→de is 0.863. bleu+1 of dpcca(b)+dcca noi (concat) de→en is 0.837. r@1 of dcca noi (wang et al. 2015b) en→de is 0.812. r@1 of dcca noi (wang et al. 2015b) de→en is 0.788. bleu+1 of dcca noi (wang et al. 2015b) en→de is 0.849. bleu+1 of dcca noi (wang et al. 2015b) de→en is 0.83. r@1 of dcca sdl (chang et al. 2017) en→de is 0.507. r@1 of dcca sdl (chang et al. 2017) de→en is 0.487. bleu+1 of dcca sdl (chang et al. 2017) en→de is 0.552. bleu+1 of dcca sdl (chang et al. 2017) de→en is 0.533. r@1 of dcca (wang et al. 2015a) en→de is 0.619. r@1 of dcca (wang et al. 2015a) de→en is 0.621. bleu+1 of dcca (wang et al. 2015a) en→de is 0.664. bleu+1 of dcca (wang et al. 2015a) de→en is 0.673. r@1 of dccae (wang et al. 2015a) en→de is 0.564. r@1 of dccae (wang et al. 2015a) de→en is 0.542. bleu+1 of dccae (wang et al. 2015a) en→de is 0.607. bleu+1 of dccae (wang et al. 2015a) de→en is 0.598. r@1 of img pivot (gella et al. 2017) en→de is 0.772. r@1 of img pivot (gella et al. 2017) de→en is 0.763. bleu+1 of img pivot (gella et al. 2017) en→de is 0.789. bleu+1 of img pivot (gella et al. 2017) de→en is 0.781. r@1 of bcn (rajendran et al. 2016) en→de is 0.579. r@1 of bcn (rajendran et al. 2016) de→en is 0.57. bleu+1 of bcn (rajendran et al. 2016) en→de is 0.628. bleu+1 of bcn (rajendran et al. 2016) de→en is 0.629. r@1 of pcca (rao 1969) en→de is 0.785. r@1 of pcca (rao 1969) de→en is 0.737. bleu+1 of pcca (rao 1969) en→de is 0.825. bleu+1 of pcca (rao 1969) de→en is 0.787. r@1 of cca (hotelling 1936) en→de is 0.764. r@1 of cca (hotelling 1936) de→en is 0.704. bleu+1 of cca (hotelling 1936) en→de is 0.803. bleu+1 of cca (hotelling 1936) de→en is 0.754. r@1 of gcca (funaki and nakayama 2015) en→de is 0.699. r@1 of gcca (funaki and nakayama 2015) de→en is 0.69. bleu+1 of gcca (funaki and nakayama 2015) en→de is 0.742. bleu+1 of gcca (funaki and nakayama 2015) de→en is 0.743. r@1 of ncca (michaeli et al. 2016) en→de is 0.157. r@1 of ncca (michaeli et al. 2016) de→en is 0.165. bleu+1 of ncca (michaeli et al. 2016) en→de is 0.205. bleu+1 of ncca (michaeli et al. 2016) de→en is 0.213. r@1 of ppcca (mukuta and harada 2014) en→de is 0.035. r@1 of ppcca (mukuta and harada 2014) de→en is 0.05. bleu+1 of ppcca (mukuta and harada 2014) en→de is 0.063. bleu+1 of ppcca (mukuta and harada 2014) de→en is 0.086.
table 3 shows results on en and de simlex-999 (pos-based evaluation). correlation of dpcca (variant a) en-adj is 0.64. correlation of dpcca (variant a) en-verbs is 0.311. correlation of dpcca (variant a) en-nouns is 0.369. correlation of dpcca (variant a) de-adj is 0.43. correlation of dpcca (variant a) de-verbs is 0.321. correlation of dpcca (variant a) de-nouns is 0.404. correlation of dpcca (variant b) en-adj is 0.626. correlation of dpcca (variant b) en-verbs is 0.316. correlation of dpcca (variant b) en-nouns is 0.382. correlation of dpcca (variant b) de-adj is 0.462. correlation of dpcca (variant b) de-verbs is 0.319. correlation of dpcca (variant b) de-nouns is 0.399. correlation of dcca noi (wang et al. 2015b) en-adj is 0.611. correlation of dcca noi (wang et al. 2015b) en-verbs is 0.308. correlation of dcca noi (wang et al. 2015b) en-nouns is 0.361. correlation of dcca noi (wang et al. 2015b) de-adj is 0.441. correlation of dcca noi (wang et al. 2015b) de-verbs is 0.297. correlation of dcca noi (wang et al. 2015b) de-nouns is 0.398. correlation of dcca (wang et al. 2015a) en-adj is 0.618. correlation of dcca (wang et al. 2015a) en-verbs is 0.261. correlation of dcca (wang et al. 2015a) en-nouns is 0.327. correlation of dcca (wang et al. 2015a) de-adj is 0.404. correlation of dcca (wang et al. 2015a) de-verbs is 0.29. correlation of dcca (wang et al. 2015a) de-nouns is 0.362. correlation of pcca (rao 1969) en-adj is 0.614. correlation of pcca (rao 1969) en-verbs is 0.296. correlation of pcca (rao 1969) en-nouns is 0.34. correlation of pcca (rao 1969) de-adj is 0.305. correlation of pcca (rao 1969) de-verbs is 0.143. correlation of pcca (rao 1969) de-nouns is 0.34. correlation of cca (hotelling 1936) en-adj is 0.557. correlation of cca (hotelling 1936) en-verbs is 0.297. correlation of cca (hotelling 1936) en-nouns is 0.321. correlation of cca (hotelling 1936) de-adj is 0.284. correlation of cca (hotelling 1936) de-verbs is 0.157. correlation of cca (hotelling 1936) de-nouns is 0.346. correlation of gcca (funaki and nakayama 2015) en-adj is 0.636. correlation of gcca (funaki and nakayama 2015) en-verbs is 0.28. correlation of gcca (funaki and nakayama 2015) en-nouns is 0.378. correlation of gcca (funaki and nakayama 2015) de-adj is 0.446. correlation of gcca (funaki and nakayama 2015) de-verbs is 0.277. correlation of gcca (funaki and nakayama 2015) de-nouns is 0.398. correlation of init emb en-adj is 0.582. correlation of init emb en-verbs is 0.16. correlation of init emb en-nouns is 0.306. correlation of init emb de-adj is 0.407. correlation of init emb de-verbs is 0.164. correlation of init emb de-nouns is 0.285.
table 4 shows results (spearman rank correlation) of our models and the strongest baselines on multilingual simlex-999 (all data). correlation of dpcca (a) en is 0.398. correlation of dpcca (a) de is 0.4. correlation of dpcca (a) en is 0.412. correlation of dpcca (a) it is 0.429. correlation of dpcca (a) en is 0.404. correlation of dpcca (a) ru is 0.407. correlation of dpcca (b) en is 0.405. correlation of dpcca (b) de is 0.4. correlation of dpcca (b) en is 0.413. correlation of dpcca (b) it is 0.427. correlation of dpcca (b) en is 0.413. correlation of dpcca (b) ru is 0.402. correlation of pcca en is 0.374. correlation of pcca de is 0.301. correlation of pcca en is 0.37. correlation of pcca it is 0.386. correlation of pcca en is 0.374. correlation of pcca ru is 0.374. correlation of dcca noi en is 0.39. correlation of dcca noi de is 0.398. correlation of dcca noi en is 0.413. correlation of dcca noi it is 0.422. correlation of dcca noi en is 0.407. correlation of dcca noi ru is 0.398. correlation of gcca en is 0.395. correlation of gcca de is 0.386. correlation of gcca en is 0.414. correlation of gcca it is 0.407. correlation of gcca en is 0.412. correlation of gcca ru is 0.396. correlation of init emb en is 0.321. correlation of init emb de is 0.278. correlation of init emb en is 0.321. correlation of init emb it is 0.361. correlation of init emb en is 0.321. correlation of init emb ru is 0.385.
table 3 shows simlex-999 results (spearman’s ⇢). spearman’s of glove all is 40.8. spearman’s of glove adjs is 62.2. spearman’s of glove nouns is 42.8. spearman’s of glove verbs is 19.6. spearman’s of glove conc-q1 is 43.3. spearman’s of glove conc-q2 is 41.6. spearman’s of glove conc-q3 is 42.3. spearman’s of glove conc-q4 is 40.2. spearman’s of glove hard is 27.2. spearman’s of picturebook all is 37.3. spearman’s of picturebook adjs is 11.7. spearman’s of picturebook nouns is 48.2. spearman’s of picturebook verbs is 17.3. spearman’s of picturebook conc-q1 is 14.4. spearman’s of picturebook conc-q2 is 27.5. spearman’s of picturebook conc-q3 is 46.2. spearman’s of picturebook conc-q4 is 60.7. spearman’s of picturebook hard is 28.8. spearman’s of glove + picturebook all is 45.5. spearman’s of glove + picturebook adjs is 46.2. spearman’s of glove + picturebook nouns is 52.1. spearman’s of glove + picturebook verbs is 22.8. spearman’s of glove + picturebook conc-q1 is 36.7. spearman’s of glove + picturebook conc-q2 is 41.7. spearman’s of glove + picturebook conc-q3 is 50.4. spearman’s of glove + picturebook conc-q4 is 57.3. spearman’s of glove + picturebook hard is 32.5. spearman’s of picturebook (visual) all is 31.3. spearman’s of picturebook (visual) adjs is 11.1. spearman’s of picturebook (visual) nouns is 38.8. spearman’s of picturebook (visual) verbs is 20.4. spearman’s of picturebook (visual) conc-q1 is 13.9. spearman’s of picturebook (visual) conc-q2 is 26.1. spearman’s of picturebook (visual) conc-q3 is 38.7. spearman’s of picturebook (visual) conc-q4 is 47.7. spearman’s of picturebook (visual) hard is 23.9. spearman’s of picturebook (semantic) all is 37.3. spearman’s of picturebook (semantic) adjs is 11.7. spearman’s of picturebook (semantic) nouns is 48.2. spearman’s of picturebook (semantic) verbs is 17.3. spearman’s of picturebook (semantic) conc-q1 is 14.4. spearman’s of picturebook (semantic) conc-q2 is 27.5. spearman’s of picturebook (semantic) conc-q3 is 46.2. spearman’s of picturebook (semantic) conc-q4 is 60.7. spearman’s of picturebook (semantic) hard is 28.8. spearman’s of picturebook (1) all is 24.5. spearman’s of picturebook (1) adjs is 2.6. spearman’s of picturebook (1) nouns is 33.5. spearman’s of picturebook (1) verbs is 12.1. spearman’s of picturebook (1) conc-q1 is 4.7. spearman’s of picturebook (1) conc-q2 is 17.8. spearman’s of picturebook (1) conc-q3 is 32.8. spearman’s of picturebook (1) conc-q4 is 47.8. spearman’s of picturebook (1) hard is 13.6. spearman’s of picturebook (2) all is 28.4. spearman’s of picturebook (2) adjs is 6.5. spearman’s of picturebook (2) nouns is 38.9. spearman’s of picturebook (2) verbs is 9. spearman’s of picturebook (2) conc-q1 is 5. spearman’s of picturebook (2) conc-q2 is 21.3. spearman’s of picturebook (2) conc-q3 is 34.3. spearman’s of picturebook (2) conc-q4 is 55.1. spearman’s of picturebook (2) hard is 15.7. spearman’s of picturebook (3) all is 30.3. spearman’s of picturebook (3) adjs is 11.9. spearman’s of picturebook (3) nouns is 41.9. spearman’s of picturebook (3) verbs is 3.1. spearman’s of picturebook (3) conc-q1 is 2.6. spearman’s of picturebook (3) conc-q2 is 24.3. spearman’s of picturebook (3) conc-q3 is 37.5. spearman’s of picturebook (3) conc-q4 is 58.3. spearman’s of picturebook (3) hard is 18.4. spearman’s of picturebook (5) all is 34.4. spearman’s of picturebook (5) adjs is 6.8. spearman’s of picturebook (5) nouns is 44.5. spearman’s of picturebook (5) verbs is 18. spearman’s of picturebook (5) conc-q1 is 9. spearman’s of picturebook (5) conc-q2 is 27.9. spearman’s of picturebook (5) conc-q3 is 42.8. spearman’s of picturebook (5) conc-q4 is 58.3. spearman’s of picturebook (5) hard is 25.9. spearman’s of picturebook (10) all is 37.3. spearman’s of picturebook (10) adjs is 11.7. spearman’s of picturebook (10) nouns is 48.2. spearman’s of picturebook (10) verbs is 17.3. spearman’s of picturebook (10) conc-q1 is 14.4. spearman’s of picturebook (10) conc-q2 is 27.5. spearman’s of picturebook (10) conc-q3 is 46.2. spearman’s of picturebook (10) conc-q4 is 60.7. spearman’s of picturebook (10) hard is 28.8.
table 4 shows classification accuracies are reported for snli and mulitnli. accuracy of glove (bow) dev is 85.2. acucracy of glove (bow) test is 84.2. accuracy of glove (bow) dev-mat is 70.5. accuracy of glove (bow) dev-mis is 69.9. accuracy of glove (bow) test-p is 86.8. accuracy of glove (bow) test-s is 79.8. accuracy of glove (bow) test-mse is 25.2. accuracy of picturebook (bow) dev is 84. acucracy of picturebook (bow) test is 83.8. accuracy of picturebook (bow) dev-mat is 67.9. accuracy of picturebook (bow) dev-mis is 67.1. accuracy of picturebook (bow) test-p is 85.8. accuracy of picturebook (bow) test-s is 79.3. accuracy of picturebook (bow) test-mse is 27. accuracy of glove + picturebook (bow) dev is 86.2. acucracy of glove + picturebook (bow) test is 85.2. accuracy of glove + picturebook (bow) dev-mat is 71.3. accuracy of glove + picturebook (bow) dev-mis is 70.9. accuracy of glove + picturebook (bow) test-p is 87.2. accuracy of glove + picturebook (bow) test-s is 80.9. accuracy of glove + picturebook (bow) test-mse is 24.4. accuracy of bilstm-max (conneau et al. 2017a) dev is 85. acucracy of bilstm-max (conneau et al. 2017a) test is 84.5. accuracy of glove dev is 86.8. acucracy of glove test is 86.3. accuracy of glove dev-mat is 74.1. accuracy of glove dev-mis is 74.5. accuracy of picturebook dev is 85.2. acucracy of picturebook test is 85.1. accuracy of picturebook dev-mat is 70.7. accuracy of picturebook dev-mis is 70.3. accuracy of glove + picturebook dev is 86.7. acucracy of glove + picturebook test is 86.1. accuracy of glove + picturebook dev-mat is 73.7. accuracy of glove + picturebook dev-mis is 73.7. accuracy of glove + picturebook + contextual gating dev is 86.9. acucracy of glove + picturebook + contextual gating test is 86.5. accuracy of glove + picturebook + contextual gating dev-mat is 74.2. accuracy of glove + picturebook + contextual gating dev-mis is 74.4.
table 6 shows coco test-set results for image-sentence retrieval experiments. r@1 of vse++ (faghri et al. 2017) r@1 is 64.6. r@10 of vse++ (faghri et al. 2017) r@10 is 95.7. med r of vse++ (faghri et al. 2017) med r is 1. r@1 of vse++ (faghri et al. 2017) r@1 is 52. r@10 of vse++ (faghri et al. 2017) r@10 is 92. med r of vse++ (faghri et al. 2017) med r is 1. r@1 of glove r@1 is 64.6. r@5 of glove r@5 is 88.9. r@10 of glove r@10 is 95.5. med r of glove med r is 1. r@1 of glove r@1 is 53.7. r@5 of glove r@5 is 86.5. r@10 of glove r@10 is 94.4. med r of glove med r is 1. r@1 of picturebook r@1 is 62.4. r@5 of picturebook r@5 is 90.2. r@10 of picturebook r@10 is 95.3. med r of picturebook med r is 1. r@1 of picturebook r@1 is 54.2. r@5 of picturebook r@5 is 86.4. r@10 of picturebook r@10 is 94.3. med r of picturebook med r is 1. r@1 of glove + picturebook r@1 is 61.8. r@5 of glove + picturebook r@5 is 89.2. r@10 of glove + picturebook r@10 is 95. med r of glove + picturebook med r is 1. r@1 of glove + picturebook r@1 is 54.1. r@5 of glove + picturebook r@5 is 86.7. r@10 of glove + picturebook r@10 is 94.7. med r of glove + picturebook med r is 1. r@1 of glove + picturebook + contextual gating r@1 is 63.4. r@5 of glove + picturebook + contextual gating r@5 is 90.3. r@10 of glove + picturebook + contextual gating r@10 is 96.5. med r of glove + picturebook + contextual gating med r is 1. r@1 of glove + picturebook + contextual gating r@1 is 55.2. r@5 of glove + picturebook + contextual gating r@5 is 87.2. r@10 of glove + picturebook + contextual gating r@10 is 94.4. med r of glove + picturebook + contextual gating med r is 1.
table 7 shows machine translation results on the multi30k english ! german task. bleu of bpe (caglayan et al. 2017) bleu is 38.1. meteor of bpe (caglayan et al. 2017) meteor is 57.3. bleu of bpe (caglayan et al. 2017) bleu is 30.8. meteor of bpe (caglayan et al. 2017) meteor is 51.6. bleu of bpe (caglayan et al. 2017) bleu is 26.4. meteor of bpe (caglayan et al. 2017) meteor is 46.8. bleu of baseline bleu is 38.9. meteor of baseline meteor is 56.5. bleu of baseline bleu is 32.6. meteor of baseline meteor is 50.7. bleu of baseline bleu is 26.8. meteor of baseline meteor is 45.4. bleu of picturebook bleu is 39.6. meteor of picturebook meteor is 56.9. bleu of picturebook bleu is 31.8. meteor of picturebook meteor is 50.1. bleu of picturebook bleu is 27.7. meteor of picturebook meteor is 45.8. bleu of picturebook + inverse picturebook bleu is 40.2. meteor of picturebook + inverse picturebook meteor is 57.2. bleu of picturebook + inverse picturebook bleu is 32.3. meteor of picturebook + inverse picturebook meteor is 50.7. bleu of picturebook + inverse picturebook bleu is 27.8. meteor of picturebook + inverse picturebook meteor is 46.3. bleu of picturebook + inverse picturebook + gating bleu is 40. meteor of picturebook + inverse picturebook + gating meteor is 57.3. bleu of picturebook + inverse picturebook + gating bleu is 33. meteor of picturebook + inverse picturebook + gating meteor is 51.1. bleu of picturebook + inverse picturebook + gating bleu is 27.9. meteor of picturebook + inverse picturebook + gating meteor is 46.5.
table 3 shows experimental results (%). acc of svm acc is 70.49\. acc of svm acc is 80.16\. acc of svm acc is 63.40?. macro-f1 of svm macro-f1 is 63.30?. acc of adarnn acc is 66.30\. macro-f1 of adarnn macro-f1 is 65.90\. acc of ae-lstm acc is 68.90\. acc of ae-lstm acc is 76.60\. acc of atae-lstm acc is 68.70\ -. acc of atae-lstm acc is 77.20\. acc of ian acc is 72.10\ -. acc of ian acc is 78.60\. acc of cnn-asp acc is 72.46. macro-f1 of cnn-asp macro-f1 is 65.31. acc of cnn-asp acc is 77.82. macro-f1 of cnn-asp macro-f1 is 65.11. acc of cnn-asp acc is 73.27. macro-f1 of cnn-asp macro-f1 is 71.77. acc of td-lstm acc is 71.83. macro-f1 of td-lstm macro-f1 is 68.43. acc of td-lstm acc is 78. macro-f1 of td-lstm macro-f1 is 66.73. acc of td-lstm acc is 66.62. macro-f1 of td-lstm macro-f1 is 64.01. acc of memnet acc is 70.33. macro-f1 of memnet macro-f1 is 64.09. acc of memnet acc is 78.16. macro-f1 of memnet macro-f1 is 65.83. acc of memnet acc is 68.5. macro-f1 of memnet macro-f1 is 66.91. acc of bilstm-att-g acc is 74.37. macro-f1 of bilstm-att-g macro-f1 is 69.9. acc of bilstm-att-g acc is 80.38. macro-f1 of bilstm-att-g macro-f1 is 70.78. acc of bilstm-att-g acc is 72.7. macro-f1 of bilstm-att-g macro-f1 is 70.84. acc of ram acc is 75.01. macro-f1 of ram macro-f1 is 70.51. acc of ram acc is 79.79. macro-f1 of ram macro-f1 is 68.86. acc of ram acc is 71.88. macro-f1 of ram macro-f1 is 70.33. acc of lstm-att-cnn acc is 73.37. macro-f1 of lstm-att-cnn macro-f1 is 68.03. acc of lstm-att-cnn acc is 78.95. macro-f1 of lstm-att-cnn macro-f1 is 68.71. acc of lstm-att-cnn acc is 70.09. macro-f1 of lstm-att-cnn macro-f1 is 67.68. acc of lstm-fc-cnn-lf acc is 75.59. macro-f1 of lstm-fc-cnn-lf macro-f1 is 70.6. acc of lstm-fc-cnn-lf acc is 80.41. macro-f1 of lstm-fc-cnn-lf macro-f1 is 70.23. acc of lstm-fc-cnn-lf acc is 73.7. macro-f1 of lstm-fc-cnn-lf macro-f1 is 72.82. acc of lstm-fc-cnn-as acc is 75.78. macro-f1 of lstm-fc-cnn-as macro-f1 is 70.72. acc of lstm-fc-cnn-as acc is 80.23. macro-f1 of lstm-fc-cnn-as macro-f1 is 70.06. acc of lstm-fc-cnn-as acc is 74.28. macro-f1 of lstm-fc-cnn-as macro-f1 is 72.6. acc of tnet w/o transformation acc is 73.3. macro-f1 of tnet w/o transformation macro-f1 is 68.25. acc of tnet w/o transformation acc is 78.9. macro-f1 of tnet w/o transformation macro-f1 is 65.86. acc of tnet w/o transformation acc is 72.1. macro-f1 of tnet w/o transformation macro-f1 is 70.57. acc of tnet w/o context acc is 73.91. macro-f1 of tnet w/o context macro-f1 is 68.87. acc of tnet w/o context acc is 80.07. macro-f1 of tnet w/o context macro-f1 is 69.01. acc of tnet w/o context acc is 74.51. macro-f1 of tnet w/o context macro-f1 is 73.05. acc of tnet-lf w/o position acc is 75.13. macro-f1 of tnet-lf w/o position macro-f1 is 70.63. acc of tnet-lf w/o position acc is 79.86. macro-f1 of tnet-lf w/o position macro-f1 is 69.69. acc of tnet-lf w/o position acc is 73.83. macro-f1 of tnet-lf w/o position macro-f1 is 72.49. acc of tnet-as w/o position acc is 75.27. macro-f1 of tnet-as w/o position macro-f1 is 70.03. acc of tnet-as w/o position acc is 79.79. macro-f1 of tnet-as w/o position macro-f1 is 69.78. acc of tnet-as w/o position acc is 73.84. macro-f1 of tnet-as w/o position macro-f1 is 72.47. acc of tnet-lf acc is 76.01†,‡. macro-f1 of tnet-lf macro-f1 is 71.47†,‡ . acc of tnet-lf acc is 80.79†,‡. macro-f1 of tnet-lf macro-f1 is  70.84‡. acc of tnet-lf acc is 74.68†,‡. macro-f1 of tnet-lf macro-f1 is 73.36†,‡. acc of tnet-as acc is 76.54†,‡. macro-f1 of tnet-as macro-f1 is 71.75†,‡. acc of tnet-as acc is  80.69†,‡. macro-f1 of tnet-as macro-f1 is 71.27†,‡ . acc of tnet-as acc is 74.97†,‡. macro-f1 of tnet-as macro-f1 is 73.60†,‡.
table 2 shows human evaluations of the proposed method and baselines. sentiment of caae (shen et al. 2017) sentiment is 7.67. semantic of caae (shen et al. 2017) semantic is 3.87. g-score of caae (shen et al. 2017) g-score is 5.45. sentiment of mdal (fu et al. 2018) sentiment is 7.12. semantic of mdal (fu et al. 2018) semantic is 3.68. g-score of mdal (fu et al. 2018) g-score is 5.12. sentiment of proposed method sentiment is 6.99. semantic of proposed method semantic is 5.08. g-score of proposed method g-score is 5.96. sentiment of caae (shen et al. 2017) sentiment is 8.61. semantic of caae (shen et al. 2017) semantic is 3.15. g-score of caae (shen et al. 2017) g-score is 5.21. sentiment of mdal (fu et al. 2018) sentiment is 7.93. semantic of mdal (fu et al. 2018) semantic is 3.22. g-score of mdal (fu et al. 2018) g-score is 5.05. sentiment of proposed method sentiment is 7.92. semantic of proposed method semantic is 4.67. g-score of proposed method g-score is 6.08.
table 3 shows experimental results on reddit datasets. p of nbow p is 67.33. r of nbow r is 66.56. f1 of nbow f1 is 66.82. acc of nbow acc is 67.52. p of nbow p is 65.45. r of nbow r is 65.62. f1 of nbow f1 is 65.52. acc of nbow acc is 66.55. p of vanilla cnn p is 65.97. r of vanilla cnn r is 65.97. f1 of vanilla cnn f1 is 65.97. acc of vanilla cnn acc is 66.24. p of vanilla cnn p is 65.88. r of vanilla cnn r is 62.9. f1 of vanilla cnn f1 is 62.85. acc of vanilla cnn acc is 66.8. p of vanilla lstm p is 67.57. r of vanilla lstm r is 67.67. f1 of vanilla lstm f1 is 67.32. acc of vanilla lstm acc is 67.34. p of vanilla lstm p is 66.94. r of vanilla lstm r is 67.22. f1 of vanilla lstm f1 is 67.03. acc of vanilla lstm acc is 67.92. p of attention lstm p is 68.11. r of attention lstm r is 67.87. f1 of attention lstm f1 is 67.94. acc of attention lstm acc is 68.37. p of attention lstm p is 68.2. r of attention lstm r is 68.78. f1 of attention lstm f1 is 67.44. acc of attention lstm acc is 67.22. p of grnn (zhang et al.) p is 66.16. r of grnn (zhang et al.) r is 66.16. f1 of grnn (zhang et al.) f1 is 66.16. acc of grnn (zhang et al.) acc is 66.42. p of grnn (zhang et al.) p is 66.56. r of grnn (zhang et al.) r is 66.73. f1 of grnn (zhang et al.) f1 is 66.66. acc of grnn (zhang et al.) acc is 67.65. p of cnn-lstm-dnn (ghosh and veale) p is 68.27. r of cnn-lstm-dnn (ghosh and veale) r is 67.87. f1 of cnn-lstm-dnn (ghosh and veale) f1 is 67.95. acc of cnn-lstm-dnn (ghosh and veale) acc is 68.5. p of cnn-lstm-dnn (ghosh and veale) p is 66.14. r of cnn-lstm-dnn (ghosh and veale) r is 66.73. f1 of cnn-lstm-dnn (ghosh and veale) f1 is 65.74. acc of cnn-lstm-dnn (ghosh and veale) acc is 66. p of siarn (this paper) p is 69.59. r of siarn (this paper) r is 69.48. f1 of siarn (this paper) f1 is 69.52. acc of siarn (this paper) acc is 69.84. p of siarn (this paper) p is 69.35. r of siarn (this paper) r is 70.05. f1 of siarn (this paper) f1 is 69.22. acc of siarn (this paper) acc is 69.57. p of miarn (this paper) p is 69.68. r of miarn (this paper) r is 69.37. f1 of miarn (this paper) f1 is 69.54. acc of miarn (this paper) acc is 69.9. p of miarn (this paper) p is 68.97. r of miarn (this paper) r is 69.3. f1 of miarn (this paper) f1 is 69.09. acc of miarn (this paper) acc is 69.91.
table 4 shows experimental results on debates datasets. p of nbow p is 57.17. r of nbow r is 57.03. f1 of nbow f1 is 57. acc of nbow acc is 57.51. p of nbow p is 66.01. r of nbow r is 66.03. f1 of nbow f1 is 66.02. acc of nbow acc is 66.09. p of vanilla cnn p is 58.21. r of vanilla cnn r is 58. f1 of vanilla cnn f1 is 57.95. acc of vanilla cnn acc is 58.55. p of vanilla cnn p is 68.45. r of vanilla cnn r is 68.18. f1 of vanilla cnn f1 is 68.21. acc of vanilla cnn acc is 68.56. p of vanilla lstm p is 54.87. r of vanilla lstm r is 54.89. f1 of vanilla lstm f1 is 54.84. acc of vanilla lstm acc is 54.92. p of vanilla lstm p is 68.3. r of vanilla lstm r is 63.96. f1 of vanilla lstm f1 is 60.78. acc of vanilla lstm acc is 62.66. p of attention lstm p is 58.98. r of attention lstm r is 57.93. f1 of attention lstm f1 is 57.23. acc of attention lstm acc is 59.07. p of attention lstm p is 70.04. r of attention lstm r is 69.62. f1 of attention lstm f1 is 69.63. acc of attention lstm acc is 69.96. p of grnn (zhang et al.) p is 56.21. r of grnn (zhang et al.) r is 56.21. f1 of grnn (zhang et al.) f1 is 55.96. acc of grnn (zhang et al.) acc is 55.96. p of grnn (zhang et al.) p is 62.26. r of grnn (zhang et al.) r is 61.87. f1 of grnn (zhang et al.) f1 is 61.21. acc of grnn (zhang et al.) acc is 61.37. p of cnn-lstm-dnn (ghosh and veale) p is 55.5. r of cnn-lstm-dnn (ghosh and veale) r is 54.6. f1 of cnn-lstm-dnn (ghosh and veale) f1 is 53.31. acc of cnn-lstm-dnn (ghosh and veale) acc is 55.96. p of cnn-lstm-dnn (ghosh and veale) p is 64.31. r of cnn-lstm-dnn (ghosh and veale) r is 64.33. f1 of cnn-lstm-dnn (ghosh and veale) f1 is 64.31. acc of cnn-lstm-dnn (ghosh and veale) acc is 64.38. p of siarn (this paper) p is 63.94. r of siarn (this paper) r is 63.45. f1 of siarn (this paper) f1 is 62.52. acc of siarn (this paper) acc is 62.69. p of siarn (this paper) p is 72.17. r of siarn (this paper) r is 71.81. f1 of siarn (this paper) f1 is 71.85. acc of siarn (this paper) acc is 72.1. p of miarn (this paper) p is 63.88. r of miarn (this paper) r is 63.71. f1 of miarn (this paper) f1 is 63.18. acc of miarn (this paper) acc is 63.21. p of miarn (this paper) p is 72.92. r of miarn (this paper) r is 72.93. f1 of miarn (this paper) f1 is 72.75. acc of miarn (this paper) acc is 72.75.
table 2 shows performance of seq2seq for gec with different learning (row) and inference (column) methods on conll-2014 dataset. p of normal seq2seq p is 61.06. r of normal seq2seq r is 18.49. f0.5 of normal seq2seq f0.5 is 41.81. p of normal seq2seq p is 61.56. r of normal seq2seq r is 18.85. f0.5 of normal seq2seq f0.5 is 42.37. p of normal seq2seq p is 61.75. r of normal seq2seq r is 23.3. f0.5 of normal seq2seq f0.5 is 46.42. p of normal seq2seq p is 61.94. r of normal seq2seq r is 23.7. f0.5 of normal seq2seq f0.5 is 46.83. p of back-boost p is 61.66. r of back-boost r is 19.54. f0.5 of back-boost f0.5 is 43.09. p of back-boost p is 61.43. r of back-boost r is 19.61. f0.5 of back-boost f0.5 is 43.07. p of back-boost p is 61.47. r of back-boost r is 24.74. f0.5 of back-boost f0.5 is 47.4. p of back-boost p is 61.24. r of back-boost r is 25.01. f0.5 of back-boost f0.5 is 47.48. p of self-boost p is 61.64. r of self-boost r is 19.83. f0.5 of self-boost f0.5 is 43.35. p of self-boost p is 61.5. r of self-boost r is 19.9. f0.5 of self-boost f0.5 is 43.36. p of self-boost p is 62.13. r of self-boost r is 24.45. f0.5 of self-boost f0.5 is 47.49. p of self-boost p is 61.67. r of self-boost r is 24.76. f0.5 of self-boost f0.5 is 47.51. p of dual-boost p is 62.03. r of dual-boost r is 20.82. f0.5 of dual-boost f0.5 is 44.44. p of dual-boost p is 61.64. r of dual-boost r is 21.19. f0.5 of dual-boost f0.5 is 44.61. p of dual-boost p is 62.22. r of dual-boost r is 25.49. f0.5 of dual-boost f0.5 is 48.3. p of dual-boost p is 61.64. r of dual-boost r is 26.45. f0.5 of dual-boost f0.5 is 48.69. p of back-boost (+native) p is 63.93. r of back-boost (+native) r is 22.03. f0.5 of back-boost (+native) f0.5 is 46.31. p of back-boost (+native) p is 63.95. r of back-boost (+native) r is 22.12. f0.5 of back-boost (+native) f0.5 is 46.4. p of back-boost (+native) p is 62.04. r of back-boost (+native) r is 27.43. f0.5 of back-boost (+native) f0.5 is 49.54. p of back-boost (+native) p is 61.98. r of back-boost (+native) r is 27.7. f0.5 of back-boost (+native) f0.5 is 49.68. p of self-boost (+native) p is 64.33. r of self-boost (+native) r is 22.1. f0.5 of self-boost (+native) f0.5 is 46.54. p of self-boost (+native) p is 64.14. r of self-boost (+native) r is 22.19. f0.5 of self-boost (+native) f0.5 is 46.54. p of self-boost (+native) p is 62.18. r of self-boost (+native) r is 27.59. f0.5 of self-boost (+native) f0.5 is 49.71. p of self-boost (+native) p is 61.64. r of self-boost (+native) r is 28.37. f0.5 of self-boost (+native) f0.5 is 49.93. p of dual-boost (+native) p is 65.77. r of dual-boost (+native) r is 21.92. f0.5 of dual-boost (+native) f0.5 is 46.98. p of dual-boost (+native) p is 65.82. r of dual-boost (+native) r is 22.14. f0.5 of dual-boost (+native) f0.5 is 47.19. p of dual-boost (+native) p is 62.64. r of dual-boost (+native) r is 27.4. f0.5 of dual-boost (+native) f0.5 is 49.83. p of dual-boost (+native) p is 62.7. r of dual-boost (+native) r is 27.69. f0.5 of dual-boost (+native) f0.5 is 50.04. p of back-boost (+native)★ p is 67.37. r of back-boost (+native)★ r is 24.31. f0.5 of back-boost (+native)★ f0.5 is 49.75. p of back-boost (+native)★ p is 67.25. r of back-boost (+native)★ r is 24.35. f0.5 of back-boost (+native)★ f0.5 is 49.73. p of back-boost (+native)★ p is 64.61. r of back-boost (+native)★ r is 28.44. f0.5 of back-boost (+native)★ f0.5 is 51.51. p of back-boost (+native)★ p is 64.46. r of back-boost (+native)★ r is 28.78. f0.5 of back-boost (+native)★ f0.5 is 51.66. p of self-boost (+native)★ p is 66.52. r of self-boost (+native)★ r is 25.13. f0.5 of self-boost (+native)★ f0.5 is 50.03. p of self-boost (+native)★ p is 66.78. r of self-boost (+native)★ r is 25.33. f0.5 of self-boost (+native)★ f0.5 is 50.31. p of self-boost (+native)★ p is 63.82. r of self-boost (+native)★ r is 30.15. f0.5 of self-boost (+native)★ f0.5 is 52.17. p of self-boost (+native)★ p is 63.34. r of self-boost (+native)★ r is 31.63. f0.5 of self-boost (+native)★ f0.5 is 52.21. p of dual-boost (+native)★ p is 66.34. r of dual-boost (+native)★ r is 25.39. f0.5 of dual-boost (+native)★ f0.5 is 50.16. p of dual-boost (+native)★ p is 66.45. r of dual-boost (+native)★ r is 25.51. f0.5 of dual-boost (+native)★ f0.5 is 50.3. p of dual-boost (+native)★ p is 64.72. r of dual-boost (+native)★ r is 30.06. f0.5 of dual-boost (+native)★ f0.5 is 52.59. p of dual-boost (+native)★ p is 64.47. r of dual-boost (+native)★ r is 30.48. f0.5 of dual-boost (+native)★ f0.5 is 52.72.
table 1 shows experimental results of dam and other comparison approaches on ubuntu corpus v1 and douban conversation corpus. r2@1 of dualencoderlstm r2@1 is 0.901. r10@1 of dualencoderlstm r10@1 is 0.638. r10@2 of dualencoderlstm r10@2 is 0.784. r10@5 of dualencoderlstm r10@5 is 0.949. map of dualencoderlstm map is 0.485. mrr of dualencoderlstm mrr is 0.527. p@1 of dualencoderlstm p@1 is 0.32. r10@1 of dualencoderlstm r10@1 is 0.187. r10@2 of dualencoderlstm r10@2 is 0.343. r10@5 of dualencoderlstm r10@5 is 0.72. r2@1 of dualencoderbilstm r2@1 is 0.895. r10@1 of dualencoderbilstm r10@1 is 0.63. r10@2 of dualencoderbilstm r10@2 is 0.78. r10@5 of dualencoderbilstm r10@5 is 0.944. map of dualencoderbilstm map is 0.479. mrr of dualencoderbilstm mrr is 0.514. p@1 of dualencoderbilstm p@1 is 0.313. r10@1 of dualencoderbilstm r10@1 is 0.184. r10@2 of dualencoderbilstm r10@2 is 0.33. r10@5 of dualencoderbilstm r10@5 is 0.716. r2@1 of mv-lstm r2@1 is 0.906. r10@1 of mv-lstm r10@1 is 0.653. r10@2 of mv-lstm r10@2 is 0.804. r10@5 of mv-lstm r10@5 is 0.946. map of mv-lstm map is 0.498. mrr of mv-lstm mrr is 0.538. p@1 of mv-lstm p@1 is 0.348. r10@1 of mv-lstm r10@1 is 0.202. r10@2 of mv-lstm r10@2 is 0.351. r10@5 of mv-lstm r10@5 is 0.71. r2@1 of match-lstm r2@1 is 0.904. r10@1 of match-lstm r10@1 is 0.653. r10@2 of match-lstm r10@2 is 0.799. r10@5 of match-lstm r10@5 is 0.944. map of match-lstm map is 0.5. mrr of match-lstm mrr is 0.537. p@1 of match-lstm p@1 is 0.345. r10@1 of match-lstm r10@1 is 0.202. r10@2 of match-lstm r10@2 is 0.348. r10@5 of match-lstm r10@5 is 0.72. r2@1 of multiview r2@1 is 0.908. r10@1 of multiview r10@1 is 0.662. r10@2 of multiview r10@2 is 0.801. r10@5 of multiview r10@5 is 0.951. map of multiview map is 0.505. mrr of multiview mrr is 0.543. p@1 of multiview p@1 is 0.342. r10@1 of multiview r10@1 is 0.202. r10@2 of multiview r10@2 is 0.35. r10@5 of multiview r10@5 is 0.729. r2@1 of dl2r r2@1 is 0.899. r10@1 of dl2r r10@1 is 0.626. r10@2 of dl2r r10@2 is 0.783. r10@5 of dl2r r10@5 is 0.944. map of dl2r map is 0.488. mrr of dl2r mrr is 0.527. p@1 of dl2r p@1 is 0.33. r10@1 of dl2r r10@1 is 0.193. r10@2 of dl2r r10@2 is 0.342. r10@5 of dl2r r10@5 is 0.705. r2@1 of smndynamic r2@1 is 0.926. r10@1 of smndynamic r10@1 is 0.726. r10@2 of smndynamic r10@2 is 0.847. r10@5 of smndynamic r10@5 is 0.961. map of smndynamic map is 0.529. mrr of smndynamic mrr is 0.569. p@1 of smndynamic p@1 is 0.397. r10@1 of smndynamic r10@1 is 0.233. r10@2 of smndynamic r10@2 is 0.396. r10@5 of smndynamic r10@5 is 0.724. r2@1 of dam r2@1 is 0.938. r10@1 of dam r10@1 is 0.767. r10@2 of dam r10@2 is 0.874. r10@5 of dam r10@5 is 0.969. map of dam map is 0.55. mrr of dam mrr is 0.601. p@1 of dam p@1 is 0.427. r10@1 of dam r10@1 is 0.254. r10@2 of dam r10@2 is 0.41. r10@5 of dam r10@5 is 0.757. r2@1 of damfirst r2@1 is 0.927. r10@1 of damfirst r10@1 is 0.736. r10@2 of damfirst r10@2 is 0.854. r10@5 of damfirst r10@5 is 0.962. map of damfirst map is 0.528. mrr of damfirst mrr is 0.579. p@1 of damfirst p@1 is 0.4. r10@1 of damfirst r10@1 is 0.229. r10@2 of damfirst r10@2 is 0.396. r10@5 of damfirst r10@5 is 0.741. r2@1 of damlast r2@1 is 0.932. r10@1 of damlast r10@1 is 0.752. r10@2 of damlast r10@2 is 0.861. r10@5 of damlast r10@5 is 0.965. map of damlast map is 0.539. mrr of damlast mrr is 0.583. p@1 of damlast p@1 is 0.408. r10@1 of damlast r10@1 is 0.242. r10@2 of damlast r10@2 is 0.407. r10@5 of damlast r10@5 is 0.748. r2@1 of damself r2@1 is 0.931. r10@1 of damself r10@1 is 0.741. r10@2 of damself r10@2 is 0.859. r10@5 of damself r10@5 is 0.964. map of damself map is 0.527. mrr of damself mrr is 0.574. p@1 of damself p@1 is 0.382. r10@1 of damself r10@1 is 0.221. r10@2 of damself r10@2 is 0.403. r10@5 of damself r10@5 is 0.75. r2@1 of damcross r2@1 is 0.932. r10@1 of damcross r10@1 is 0.749. r10@2 of damcross r10@2 is 0.863. r10@5 of damcross r10@5 is 0.966. map of damcross map is 0.535. mrr of damcross mrr is 0.585. p@1 of damcross p@1 is 0.4. r10@1 of damcross r10@1 is 0.234. r10@2 of damcross r10@2 is 0.411. r10@5 of damcross r10@5 is 0.733.
table 2 shows test set performance comparison on the ctb dataset lp of charniak (2000) lp is 82.1. lr of charniak (2000) lr is 79.6. f1 of charniak (2000) f1 is 80.8. lp of zhu et al. (2013) lp is 84.3. lr of zhu et al. (2013) lr is 82.1. f1 of zhu et al. (2013) f1 is 83.2. f1 of wang et al. (2015) f1 is 83.2. f1 of watanabe and sumita (2015) f1 is 84.3. f1 of dyer et al. (2016) f1 is 84.6. lp of liu and zhang (2017b) lp is 85.9. lr of liu and zhang (2017b) lr is 85.2. f1 of liu and zhang (2017b) f1 is 85.5. f1 of liu and zhang (2017a) f1 is 86.1. lp of - lp is 86.6. lr of - lr is 86.4. f1 of - f1 is 86.5. lp of zhu et al. (2013) lp is 86.8. lr of zhu et al. (2013) lr is 84.4. f1 of zhu et al. (2013) f1 is 85.6. f1 of wang and xue (2014) f1 is 86.3. f1 of wang et al. (2015) f1 is 86.6. lp of charniak and johnson (2005) lp is 83.8. lr of charniak and johnson (2005) lr is 80.8. f1 of charniak and johnson (2005) f1 is 82.3. f1 of dyer et al. (2016) f1 is 86.9.
table 4 shows performance of rsp on qbankdev. rec. of 0 rec. is 91.07. prec. of 0 prec. is 88.77. f1 of 0 f1 is 89.91. rec. of 2k rec. is 94.44. prec. of 2k prec. is 96.23. f1 of 2k f1 is 95.32. rec. of 2k rec. is 95.84. prec. of 2k prec. is 97.02. f1 of 2k f1 is 96.43. rec. of 50 rec. is 93.85. prec. of 50 prec. is 95.91. f1 of 50 f1 is 94.87. rec. of 100 rec. is 95.08. prec. of 100 prec. is 96.06. f1 of 100 f1 is 95.57. rec. of 400 rec. is 94.94. prec. of 400 prec. is 97.05. f1 of 400 f1 is 95.99.
table 5 shows performance of rsp on geniadev. rec. of 0 rec. is 72.51. prec. of 0 prec. is 88.84. f1 of 0 f1 is 79.85. rec. of 14k rec. is 88.04. prec. of 14k prec. is 92.3. f1 of 14k f1 is 90.12. rec. of 14k rec. is 88.24. prec. of 14k prec. is 92.33. f1 of 14k f1 is 90.24. rec. of 50 rec. is 82.3. prec. of 50 prec. is 90.55. f1 of 50 f1 is 86.23. rec. of 100 rec. is 83.94. prec. of 100 prec. is 89.97. f1 of 100 f1 is 86.85. rec. of 400 rec. is 85.52. prec. of 400 prec. is 91.01. f1 of 400 f1 is 88.18.
table 2 shows results of the proposed method and the baselines on the semeval 2013 task. f1 of sfs (versley 2013) isomorphic is 23.1. f1 of sfs (versley 2013) non-isomorphic is 17.9. f1 of iiith (surtani et al. 2013) isomorphic is 23.1. f1 of iiith (surtani et al. 2013) non-isomorphic is 25.8. f1 of melodi (van de cruys et al. 2013) isomorphic is 13. f1 of melodi (van de cruys et al. 2013) non-isomorphic is 54.8. f1 of semeval 2013 baseline (hendrickx et al. 2013) isomorphic is 13.8. f1 of semeval 2013 baseline (hendrickx et al. 2013) non-isomorphic is 40.6. f1 of baseline isomorphic is 3.8. f1 of baseline non-isomorphic is 16.1. f1 of our method isomorphic is 28.2. f1 of our method non-isomorphic is 28.4.
table 4 shows classification results. f1 of tratz and hovy (2010) f1 is 0.739. f1 of dima (2016) f1 is 0.725. f1 of shwartz and waterson (2018) f1 is 0.714. f1 of distributional f1 is 0.677. f1 of paraphrase f1 is 0.505. f1 of integrated f1 is 0.673. f1 of tratz and hovy (2010) f1 is 0.34. f1 of dima (2016) f1 is 0.334. f1 of shwartz and waterson (2018) f1 is 0.429. f1 of distributional f1 is 0.356. f1 of paraphrase f1 is 0.333. f1 of integrated f1 is 0.37. f1 of tratz and hovy (2010) f1 is 0.76. f1 of dima (2016) f1 is 0.775. f1 of shwartz and waterson (2018) f1 is 0.736. f1 of distributional f1 is 0.689. f1 of paraphrase f1 is 0.557. f1 of integrated f1 is 0.7. f1 of tratz and hovy (2010) f1 is 0.391. f1 of dima (2016) f1 is 0.372. f1 of shwartz and waterson (2018) f1 is 0.478. f1 of distributional f1 is 0.37. f1 of paraphrase f1 is 0.345. f1 of integrated f1 is 0.393.
table 3 shows with and without sentiment accuracy of with amazon is 81.8. accuracy of with rt is 75.2. accuracy of with - is 90.7. accuracy of with - is 83.1. accuracy of without amazon is 76.1. accuracy of without rt is 67.2. accuracy of without - is 87.8. accuracy of without - is 82.6. accuracy of with amazon is 85.5. accuracy of with rt is 78.0. accuracy of with - is 90.3. accuracy of with - is 82.5. accuracy of without amazon is 79.8. accuracy of without rt is 71.0. accuracy of without - is 89.1. accuracy of without - is 82.2. accuracy of - amazon is 76.1. accuracy of - rt is 62.2. accuracy of - - is 80.1. accuracy of - - is 71.5.
table 4 shows comparison of sentiment-infused word embeddings on sentiment classification task accuracy of amazon instant video - is 84.1. accuracy of amazon instant video - is 84.1. accuracy of amazon instant video - is 81.9. accuracy of amazon instant video spherical is 84.9*. accuracy of amazon instant video logistic is 84.9*. accuracy of amazon instant video - is 87.8. accuracy of amazon instant video - is 87.8. accuracy of amazon instant video - is 86.9. accuracy of amazon instant video spherical is 88.1. accuracy of amazon instant video logistic is 88.2. accuracy of android apps - is 83.0. accuracy of android apps - is 83.0. accuracy of android apps - is 80.9. accuracy of android apps spherical is 84.0*. accuracy of android apps logistic is 84.0*. accuracy of android apps - is 86.3. accuracy of android apps - is 86.3. accuracy of android apps - is 85.0. accuracy of android apps spherical is 86.6. accuracy of android apps logistic is 86.5. accuracy of automotive - is 80.7. accuracy of automotive - is 80.7. accuracy of automotive - is 78.8. accuracy of automotive spherical is 81.0. accuracy of automotive logistic is 81.3. accuracy of automotive - is 85.1. accuracy of automotive - is 85.1. accuracy of automotive - is 83.8. accuracy of automotive spherical is 84.9. accuracy of automotive logistic is 85.0. accuracy of baby - is 80.9. accuracy of baby - is 80.9. accuracy of baby - is 78.6. accuracy of baby spherical is 82.1. accuracy of baby logistic is 82.2*. accuracy of baby - is 84.2. accuracy of baby - is 84.2. accuracy of baby - is 82.8. accuracy of baby spherical is 84.4. accuracy of baby logistic is 84.6. accuracy of beauty - is 81.8. accuracy of beauty - is 81.8. accuracy of beauty - is 79.8. accuracy of beauty spherical is 82.4. accuracy of beauty logistic is 82.7*. accuracy of beauty - is 85.2. accuracy of beauty - is 85.2. accuracy of beauty - is 83.5. accuracy of beauty spherical is 85.2. accuracy of beauty logistic is 85.4. accuracy of books - is 80.9. accuracy of books - is 80.9. accuracy of books - is 78.9. accuracy of books spherical is 81.0. accuracy of books logistic is 81.3. accuracy of books - is 85.3. accuracy of books - is 85.3. accuracy of books - is 83.6. accuracy of books spherical is 85.3. accuracy of books logistic is 85.5. accuracy of cd & vinyl - is 79.4. accuracy of cd & vinyl - is 79.4. accuracy of cd & vinyl - is 77.6. accuracy of cd & vinyl spherical is 79.4. accuracy of cd & vinyl logistic is 79.9. accuracy of cd & vinyl - is 83.5. accuracy of cd & vinyl - is 83.5. accuracy of cd & vinyl - is 81.9. accuracy of cd & vinyl spherical is 83.7. accuracy of cd & vinyl logistic is 83.6. accuracy of cell phones - is 82.2. accuracy of cell phones - is 82.2. accuracy of cell phones - is 80.0. accuracy of cell phones spherical is 82.9. accuracy of cell phones logistic is 83.0*. accuracy of cell phones - is 86.8. accuracy of cell phones - is 86.8. accuracy of cell phones - is 85.3. accuracy of cell phones spherical is 86.8. accuracy of cell phones logistic is 87.0. accuracy of clothing - is 82.6. accuracy of clothing - is 82.6. accuracy of clothing - is 80.7. accuracy of clothing spherical is 83.8. accuracy of clothing logistic is 84.0*. accuracy of clothing - is 86.3. accuracy of clothing - is 86.3. accuracy of clothing - is 84.7. accuracy of clothing spherical is 86.4. accuracy of clothing logistic is 86.8. accuracy of digital music - is 82.3. accuracy of digital music - is 82.3. accuracy of digital music - is 80.5. accuracy of digital music spherical is 82.8. accuracy of digital music logistic is 83.0*. accuracy of digital music - is 86.3. accuracy of digital music - is 86.3. accuracy of digital music - is 84.6. accuracy of digital music spherical is 86.1. accuracy of digital music logistic is 86.3. accuracy of electronics - is 81.0. accuracy of electronics - is 81.0. accuracy of electronics - is 78.8. accuracy of electronics spherical is 80.9. accuracy of electronics logistic is 81.3. accuracy of electronics - is 85.2. accuracy of electronics - is 85.2. accuracy of electronics - is 83.6. accuracy of electronics spherical is 85.3. accuracy of electronics logistic is 85.3. accuracy of grocery & food - is 81.7. accuracy of grocery & food - is 81.7. accuracy of grocery & food - is 79.4. accuracy of grocery & food spherical is 83.1*. accuracy of grocery & food logistic is 83.1*. accuracy of grocery & food - is 85.0. accuracy of grocery & food - is 85.0. accuracy of grocery & food - is 83.7. accuracy of grocery & food spherical is 85.1. accuracy of grocery & food logistic is 85.6*. accuracy of health - is 79.7. accuracy of health - is 79.7. accuracy of health - is 77.9. accuracy of health spherical is 80.4*. accuracy of health logistic is 80.4. accuracy of health - is 84.0. accuracy of health - is 84.0. accuracy of health - is 82.3. accuracy of health spherical is 84.0. accuracy of health logistic is 84.3. accuracy of home & kitchen - is 81.6. accuracy of home & kitchen - is 81.6. accuracy of home & kitchen - is 79.5. accuracy of home & kitchen spherical is 82.1. accuracy of home & kitchen logistic is 82.1. accuracy of home & kitchen - is 85.4. accuracy of home & kitchen - is 85.4. accuracy of home & kitchen - is 83.9. accuracy of home & kitchen spherical is 85.3. accuracy of home & kitchen logistic is 85.4. accuracy of kindle store - is 84.7. accuracy of kindle store - is 84.7. accuracy of kindle store - is 83.2. accuracy of kindle store spherical is 85.2. accuracy of kindle store logistic is 85.4*. accuracy of kindle store - is 88.3. accuracy of kindle store - is 88.3. accuracy of kindle store - is 87.2. accuracy of kindle store spherical is 88.3. accuracy of kindle store logistic is 88.6. accuracy of movies & tv - is 81.4. accuracy of movies & tv - is 81.4. accuracy of movies & tv - is 78.5. accuracy of movies & tv spherical is 81.9. accuracy of movies & tv logistic is 81.9. accuracy of movies & tv - is 85.2. accuracy of movies & tv - is 85.2. accuracy of movies & tv - is 83.5. accuracy of movies & tv spherical is 85.4. accuracy of movies & tv logistic is 85.5. accuracy of musical instruments - is 81.7. accuracy of musical instruments - is 81.6. accuracy of musical instruments - is 79.7. accuracy of musical instruments spherical is 82.4. accuracy of musical instruments logistic is 82.4. accuracy of musical instruments - is 85.8. accuracy of musical instruments - is 85.8. accuracy of musical instruments - is 84.1. accuracy of musical instruments spherical is 85.9. accuracy of musical instruments logistic is 85.7. accuracy of office - is 82.0. accuracy of office - is 82.0. accuracy of office - is 80.0. accuracy of office spherical is 83.0*. accuracy of office logistic is 82.9. accuracy of office - is 86.1. accuracy of office - is 86.1. accuracy of office - is 84.5. accuracy of office spherical is 86.4. accuracy of office logistic is 86.5*. accuracy of garden - is 80.4. accuracy of garden - is 80.4. accuracy of garden - is 77.9. accuracy of garden spherical is 81.0. accuracy of garden logistic is 81.5. accuracy of garden - is 84.1. accuracy of garden - is 84.1. accuracy of garden - is 82.5. accuracy of garden spherical is 84.3. accuracy of garden logistic is 84.6*. accuracy of pet supplies - is 79.7. accuracy of pet supplies - is 79.7. accuracy of pet supplies - is 77.5. accuracy of pet supplies spherical is 80.4. accuracy of pet supplies logistic is 80.2. accuracy of pet supplies - is 83.2. accuracy of pet supplies - is 83.2. accuracy of pet supplies - is 81.5. accuracy of pet supplies spherical is 83.4. accuracy of pet supplies logistic is 83.8. accuracy of sports & outdoors - is 80.8. accuracy of sports & outdoors - is 80.8. accuracy of sports & outdoors - is 79.1. accuracy of sports & outdoors spherical is 81.3*. accuracy of sports & outdoors logistic is 81.2. accuracy of sports & outdoors - is 84.6. accuracy of sports & outdoors - is 84.6. accuracy of sports & outdoors - is 83.1. accuracy of sports & outdoors spherical is 84.3. accuracy of sports & outdoors logistic is 84.7. accuracy of tools - is 81.0. accuracy of tools - is 81.0. accuracy of tools - is 79.3. accuracy of tools spherical is 81.0. accuracy of tools logistic is 81.3. accuracy of tools - is 84.7. accuracy of tools - is 84.7. accuracy of tools - is 83.2. accuracy of tools spherical is 84.8. accuracy of tools logistic is 84.9. accuracy of toys & games - is 83.8. accuracy of toys & games - is 83.8. accuracy of toys & games - is 82.0. accuracy of toys & games spherical is 84.7. accuracy of toys & games logistic is 84.9*. accuracy of toys & games - is 87.2. accuracy of toys & games - is 87.2. accuracy of toys & games - is 85.7. accuracy of toys & games spherical is 87.1. accuracy of toys & games logistic is 87.5. accuracy of video games - is 80.3. accuracy of video games - is 80.3. accuracy of video games - is 77.4. accuracy of video games spherical is 81.5. accuracy of video games logistic is 81.7*. accuracy of video games - is 84.9. accuracy of video games - is 84.9. accuracy of video games - is 83.2. accuracy of video games spherical is 85.0. accuracy of video games logistic is 84.9. accuracy of - - is 81.6. accuracy of - - is 81.6. accuracy of - - is 79.5. accuracy of - spherical is 82.2. accuracy of - logistic is 82.4. accuracy of - - is 85.4. accuracy of - - is 85.4. accuracy of - - is 83.9. accuracy of - spherical is 85.5. accuracy of - logistic is 85.7. accuracy of - - is 75.6. accuracy of - - is 75.6. accuracy of - - is 73.4. accuracy of - spherical is 75.8*. accuracy of - logistic is 75.4. accuracy of - - is 77.9. accuracy of - - is 77.9. accuracy of - - is 76.7. accuracy of - spherical is 77.7. accuracy of - logistic is 77.9.
table 5 shows comparison of word embeddings on subjectivity and topic classification tasks accuracy of computers - is 79.8. accuracy of computers - is 79.8. accuracy of computers - is 79.6. accuracy of computers spherical is 79.6. accuracy of computers logistic is 79.8. accuracy of computers - is 79.8. accuracy of computers - is 79.8. accuracy of computers - is 79.8. accuracy of computers spherical is 79.7. accuracy of computers logistic is 79.7. accuracy of misc - is 89.8. accuracy of misc - is 89.8. accuracy of misc - is 89.7. accuracy of misc spherical is 89.8. accuracy of misc logistic is 90.0. accuracy of misc - is 90.4. accuracy of misc - is 90.4. accuracy of misc - is 90.6. accuracy of misc spherical is 90.4. accuracy of misc logistic is 90.3. accuracy of politics - is 84.6. accuracy of politics - is 84.6. accuracy of politics - is 84.4. accuracy of politics spherical is 84.5. accuracy of politics logistic is 84.6. accuracy of politics - is 83.8. accuracy of politics - is 83.8. accuracy of politics - is 83.5. accuracy of politics spherical is 83.6. accuracy of politics logistic is 83.5. accuracy of recreation - is 83.4. accuracy of recreation - is 83.4. accuracy of recreation - is 83.1. accuracy of recreation spherical is 83.1. accuracy of recreation logistic is 83.2. accuracy of recreation - is 82.6. accuracy of recreation - is 82.6. accuracy of recreation - is 82.5. accuracy of recreation spherical is 82.7. accuracy of recreation logistic is 82.8. accuracy of religion - is 84.6. accuracy of religion - is 84.6. accuracy of religion - is 84.5. accuracy of religion spherical is 84.5. accuracy of religion logistic is 84.6. accuracy of religion - is 84.2. accuracy of religion - is 84.2. accuracy of religion - is 84.2. accuracy of religion spherical is 84.1. accuracy of religion logistic is 84.2. accuracy of science - is 78.2. accuracy of science - is 78.2. accuracy of science - is 78.2. accuracy of science spherical is 78.1. accuracy of science logistic is 78.3. accuracy of science - is 76.4. accuracy of science - is 76.4. accuracy of science - is 76.1. accuracy of science spherical is 76.7. accuracy of science logistic is 76.6. accuracy of - - is 83.4. accuracy of - - is 83.4. accuracy of - - is 83.2. accuracy of - spherical is 83.3. accuracy of - logistic is 83.4. accuracy of - - is 82.8. accuracy of - - is 82.8. accuracy of - - is 82.8. accuracy of - spherical is 82.9. accuracy of - logistic is 82.8. accuracy of - - is 90.6. accuracy of - - is 90.6. accuracy of - - is 90.0. accuracy of - spherical is 90.6. accuracy of - logistic is 90.6. accuracy of - - is 90.6. accuracy of - - is 90.6. accuracy of - - is 90.3. accuracy of - spherical is 90.7. accuracy of - logistic is 90.8.
table 1 shows metaphor identification results. p of shutova et al. (2016) p is 0.67. r of shutova et al. (2016) r is 0.76. f1 of shutova et al. (2016) f1 is 0.71. p of rei et al. (2017) p is 0.74. r of rei et al. (2017) r is 0.76. f1 of rei et al. (2017) f1 is 0.74. p of sim-cbowi+o p is 0.66. r of sim-cbowi+o r is 0.78. f1 of sim-cbowi+o f1 is 0.72. p of sim-sgi+o p is 0.68. r of sim-sgi+o r is 0.82. f1 of sim-sgi+o f1 is 0.74*. p of melamud et al. (2016) p is 0.60. r of melamud et al. (2016) r is 0.80. f1 of melamud et al. (2016) f1 is 0.69. p of sim-sgi p is 0.56. r of sim-sgi r is 0.95. f1 of sim-sgi f1 is 0.70. p of sim-sgi+o p is 0.62. r of sim-sgi+o r is 0.89. f1 of sim-sgi+o f1 is 0.73. p of sim-cbowi p is 0.59. r of sim-cbowi r is 0.91. f1 of sim-cbowi f1 is 0.72. p of sim-cbowi+o p is 0.66. r of sim-cbowi+o r is 0.88. f1 of sim-cbowi+o f1 is 0.75*.
table 2 shows performance comparison (%) of our lmms and the baselines on two basic nlp tasks (word similarity & syntactic analogy) and one downstream task (text classification). accuracy of wordsim-353 cbow is 58.77. accuracy of wordsim-353 skip-gram is 61.94. accuracy of wordsim-353 glove is 49.40. accuracy of wordsim-353 emm is 60.01. accuracy of wordsim-353 lmm-a is 62.05. accuracy of wordsim-353 lmm-s is 63.13. accuracy of wordsim-353 lmm-m is 61.54. accuracy of rw cbow is 40.58. accuracy of rw skip-gram is 36.42. accuracy of rw glove is 33.40. accuracy of rw emm is 40.83. accuracy of rw lmm-a is 43.12. accuracy of rw lmm-s is 42.14. accuracy of rw lmm-m is 40.51. accuracy of rg-65 cbow is 56.50. accuracy of rg-65 skip-gram is 62.81. accuracy of rg-65 glove is 59.92. accuracy of rg-65 emm is 60.85. accuracy of rg-65 lmm-a is 62.51. accuracy of rg-65 lmm-s is 62.49. accuracy of rg-65 lmm-m is 63.07. accuracy of scws cbow is 63.13. accuracy of scws skip-gram is 60.20. accuracy of scws glove is 47.98. accuracy of scws emm is 60.28. accuracy of scws lmm-a is 61.86. accuracy of scws lmm-s is 61.71. accuracy of scws lmm-m is 63.02. accuracy of men-3k cbow is 68.07. accuracy of men-3k skip-gram is 66.30. accuracy of men-3k glove is 60.56. accuracy of men-3k emm is 66.76. accuracy of men-3k lmm-a is 66.26. accuracy of men-3k lmm-s is 68.36. accuracy of men-3k lmm-m is 64.65. accuracy of ws-353-rel cbow is 49.72. accuracy of ws-353-rel skip-gram is 57.05. accuracy of ws-353-rel glove is 47.46. accuracy of ws-353-rel emm is 54.48. accuracy of ws-353-rel lmm-a is 56.14. accuracy of ws-353-rel lmm-s is 58.47. accuracy of ws-353-rel lmm-m is 55.19. accuracy of syntactic analogy cbow is 13.46. accuracy of syntactic analogy skip-gram is 13.14. accuracy of syntactic analogy glove is 13.94. accuracy of syntactic analogy emm is 17.34. accuracy of syntactic analogy lmm-a is 20.38. accuracy of syntactic analogy lmm-s is 17.59. accuracy of syntactic analogy lmm-m is 18.30. accuracy of text classification cbow is 78.26. accuracy of text classification skip-gram is 79.40. accuracy of text classification glove is 77.01. accuracy of text classification emm is 80.00. accuracy of text classification lmm-a is 80.67. accuracy of text classification lmm-s is 80.59. accuracy of text classification lmm-m is 81.28.
table 4 shows our memory-to-context source memory nmt variants vs. bleu of jean et al. (2017) - is 21.95. bleu of jean et al. (2017) nc-11 is 6.04. bleu of jean et al. (2017) nc-16 is 10.26. bleu of jean et al. (2017) - is 21.67. meteor of jean et al. (2017) - is 24.10. meteor of jean et al. (2017) nc-11 is 11.61. meteor of jean et al. (2017) nc-16 is 15.56. meteor of jean et al. (2017) - is 25.77. bleu of wang et al. (2017) - is 21.87. bleu of wang et al. (2017) nc-11 is 5.49. bleu of wang et al. (2017) nc-16 is 10.14. bleu of wang et al. (2017) - is 22.06. meteor of wang et al. (2017) - is 24.13. meteor of wang et al. (2017) nc-11 is 11.05. meteor of wang et al. (2017) nc-16 is 15.20. meteor of wang et al. (2017) - is 26.00. bleu of s-nmt - is 20.85. bleu of s-nmt nc-11 is 5.24. bleu of s-nmt nc-16 is 9.18. bleu of s-nmt - is 20.42. meteor of s-nmt - is 23.27. meteor of s-nmt nc-11 is 10.90. meteor of s-nmt nc-16 is 14.35. meteor of s-nmt - is 24.65. bleu of s-nmt + src mem - is 21.91. bleu of s-nmt + src mem nc-11 is 6.26. bleu of s-nmt + src mem nc-16 is 10.20. bleu of s-nmt + src mem - is 22.10. meteor of s-nmt + src mem - is 24.04. meteor of s-nmt + src mem nc-11 is 11.52. meteor of s-nmt + src mem nc-16 is 15.45. meteor of s-nmt + src mem - is 25.92. bleu of s-nmt + both mems - is 22.00. bleu of s-nmt + both mems nc-11 is 6.57. bleu of s-nmt + both mems nc-16 is 10.54. bleu of s-nmt + both mems - is 22.32. meteor of s-nmt + both mems - is 24.40. meteor of s-nmt + both mems nc-11 is 12.24. meteor of s-nmt + both mems nc-16 is 16.18. meteor of s-nmt + both mems - is 26.34.
table 1 shows results on lgl, wiktor (wik) and geovirus (geo). area under curve of camcoder lgl is 22 (18). area under curve of camcoder wik is 33 (37). area under curve of camcoder geo is 31 (32). average error of camcoder lgl is 7 (5). average error of camcoder wik is 11 (9). average error of camcoder geo is 3 (3). accuracy@161km of camcoder lgl is 76 (83). accuracy@161km of camcoder wik is 65 (57). accuracy@161km of camcoder geo is 82 (80). area under curve of edinburgh lgl is 25 (22). area under curve of edinburgh wik is 53 (58). area under curve of edinburgh geo is 33 (34). average error of edinburgh lgl is 8 (8). average error of edinburgh wik is 31 (30). average error of edinburgh geo is 5 (4). accuracy@161km of edinburgh lgl is 76 (80). accuracy@161km of edinburgh wik is 42 (36). accuracy@161km of edinburgh geo is 78 (78). area under curve of yahoo! lgl is 34 (35). area under curve of yahoo! wik is 44 (53). area under curve of yahoo! geo is 40 (44). average error of yahoo! lgl is 6 (5). average error of yahoo! wik is 23 (25). average error of yahoo! geo is 3 (3). accuracy@161km of yahoo! lgl is 72 (75). accuracy@161km of yahoo! wik is 52 (39). accuracy@161km of yahoo! geo is 70 (65). area under curve of population lgl is 27 (22). area under curve of population wik is 68 (71). area under curve of population geo is 32 (32). average error of population lgl is 12 (10). average error of population wik is 45 (42). average error of population geo is 5 (3). accuracy@161km of population lgl is 70 (79). accuracy@161km of population wik is 22 (14). accuracy@161km of population geo is 80 (80). area under curve of clavin lgl is 26 (20). area under curve of clavin wik is 70 (69). area under curve of clavin geo is 32 (33). average error of clavin lgl is 13 (9). average error of clavin wik is 43 (39). average error of clavin geo is 6 (5). accuracy@161km of clavin lgl is 71 (80). accuracy@161km of clavin wik is 16 (16). accuracy@161km of clavin geo is 79 (80). area under curve of geotxt lgl is 29 (21). area under curve of geotxt wik is 70 (71). area under curve of geotxt geo is 33 (34). average error of geotxt lgl is 14 (9). average error of geotxt wik is 47 (45). average error of geotxt geo is 6 (5). accuracy@161km of geotxt lgl is 68 (80). accuracy@161km of geotxt wik is 18 (14). accuracy@161km of geotxt geo is 79 (79). area under curve of topocluster lgl is 38 (36). area under curve of topocluster wik is 63 (66). area under curve of topocluster geo is na. average error of topocluster lgl is 12 (8). average error of topocluster wik is 38 (35). average error of topocluster geo is na. accuracy@161km of topocluster lgl is 63 (71). accuracy@161km of topocluster wik is 26 (20). accuracy@161km of topocluster geo is na. area under curve of santos et al. lgl is na. area under curve of santos et al. wik is na. area under curve of santos et al. geo is na. average error of santos et al. lgl is 8. average error of santos et al. wik is na. average error of santos et al. geo is na. accuracy@161km of santos et al. lgl is 71. accuracy@161km of santos et al. wik is na. accuracy@161km of santos et al. geo is na.
table 2 shows the dependency parsing results. las of baseline las is 90.83. las of ensemble las is 92.73. las of distill (reference alpha=1.0) las is 91.99. las of distill (exploration t=1.0) las is 92.00. las of distill (both) las is 92.14. las of ballesteros et al. (2016) (dyn. oracle) las is 91.42. las of andor et al. (2016) (local b=1) las is 91.02. las of buckman et al. (2016) (local b=8) las is 91.19. las of andor et al. (2016) (local b=32) las is 91.70. las of andor et al. (2016) (global b=32) las is 92.79. las of dozat and manning (2016) las is 94.08. las of kuncoro et al. (2016) las is 92.06. las of kuncoro et al. (2017) las is 94.60.
table 3 shows the machine translation results. bleu of baseline bleu is 22.79. bleu of ensemble bleu is 26.26. bleu of distill (reference alpha=0.8) bleu is 24.76. bleu of distill (exploration t=0.1) bleu is 24.64. bleu of distill (both) bleu is 25.44. bleu of mixer bleu is 20.73. bleu of bso (local b=1) bleu is 22.53. bleu of bso (global b=1) bleu is 23.83.
table 4 shows the ranking performance of parsers’ output distributions evaluated in map on “problematic” states. map of baseline optimal-yet-ambiguous is 68.59. map of baseline non-optimal is 89.59. map of ensemble optimal-yet-ambiguous is 74.19. map of ensemble non-optimal is 90.90. map of distill (both) optimal-yet-ambiguous is 81.15. map of distill (both) non-optimal is 91.38.
table 1 shows uas and las of four versions of our model on test sets for three languages, together with topperforming parsing systems. uas of t uas is 91.8. las of t las is 89.6. uas of t uas is 83.9. las of t las is 82.4. uas of t uas is 91.63. las of t las is 89.44. uas of t uas is 85.30. las of t las is 83.72. uas of t uas is 88.83. las of t las is 86.10. uas of t uas is 93.1. las of t las is 90.9. uas of t uas is 87.2. las of t las is 85.7. uas of t uas is 93.33. las of t las is 21.22. uas of t uas is 87.3. las of t las is 85.9. uas of t uas is 91.4. las of t las is 89.4. uas of t uas is 93.56. las of t las is 91.42. uas of t uas is 87.65. las of t las is 86.21. uas of t uas is 93.9. las of t las is 91.9. uas of t uas is 87.6. las of t las is 86.1. uas of t uas is 94.26. las of t las is 92.41. uas of t uas is 94.61. las of t las is 92.79. uas of t uas is 90.91. las of t las is 89.15. uas of g uas is 93.1. las of g las is 91.0. uas of g uas is 86.6. las of g las is 85.1. uas of g uas is 94.08. las of g las is 91.82. uas of g uas is 87.55. las of g las is 86.23. uas of g uas is 94.10. las of g las is 91.49. uas of g uas is 88.1. las of g las is 85.7. uas of g uas is 94.26. las of g las is 92.06. uas of g uas is 88.87. las of g las is 87.30. uas of g uas is 91.60. las of g las is 89.24. uas of g uas is 94.88. las of g las is 92.98. uas of g uas is 89.05. las of g las is 87.74. uas of g uas is 92.58. las of g las is 90.54. uas of g uas is 95.74. las of g las is 94.08. uas of g uas is 89.30. las of g las is 88.23. uas of g uas is 93.46. las of g las is 91.44. uas of g uas is 95.84. las of g las is 94.21. uas of g uas is 90.43. las of g las is 89.14. uas of g uas is 93.85. las of g las is 92.32. uas of t uas is 95.77. las of t las is 94.12. uas of t uas is 90.48. las of t las is 89.19. uas of t uas is 93.59. las of t las is 92.06. uas of t uas is 95.78. las of t las is 94.12. uas of t uas is 90.49. las of t las is 89.19. uas of t uas is 93.65. las of t las is 92.12. uas of t uas is 95.85. las of t las is 94.18. uas of t uas is 90.43. las of t las is 89.15. uas of t uas is 93.76. las of t las is 92.21. uas of t uas is 95.87. las of t las is 94.19. uas of t uas is 90.59. las of t las is 89.29. uas of t uas is 93.65. las of t las is 92.11.
table 2 shows parsing performance on the test data of ptb with different versions of pos tags. uas of gold uas is 96.12±0.03. las of gold las is 95.06±0.05. ucm of gold ucm is 62.22±0.33. lcm of gold lcm is 55.74±0.44. uas of pred uas is 95.87±0.04. las of pred las is 94.19±0.04. ucm of pred ucm is 61.43±0.49. lcm of pred lcm is 49.68±0.47. uas of none uas is 95.90±0.05. las of none las is 94.21±0.04. ucm of none ucm is 61.58±0.39. lcm of none lcm is 49.87±0.46.
table 4 shows uas and las on both the development and test datasets of 12 treebanks from ud treebanks, together with biaf for comparison. uas of bg uas is 93.92±0.13. las of bg las is 89.05±0.11. uas of bg uas is 94.09±0.16. las of bg las is 89.17±0.14. uas of bg uas is 94.30±0.16. las of bg las is 90.04±0.16. uas of bg uas is 94.31±0.06. las of bg las is 89.96±0.07. uas of ca uas is 94.21±0.05. las of ca las is 91.97±0.06. uas of ca uas is 94.47±0.02. las of ca las is 92.51±0.05. uas of ca uas is 94.36±0.06. las of ca las is 92.05±0.07. uas of ca uas is 94.47±0.02. las of ca las is 92.39±0.02. uas of cs uas is 94.14±0.03. las of cs las is 90.89±0.04. uas of cs uas is 94.33±0.04. las of cs las is 91.24±0.05. uas of cs uas is 94.06±0.04. las of cs las is 90.60±0.05. uas of cs uas is 94.21±0.06. las of cs las is 90.94±0.07. uas of de uas is 91.89±0.11. las of de las is 88.39±0.17. uas of de uas is 92.26±0.11. las of de las is 88.79±0.15. uas of de uas is 90.26±0.19. las of de las is 86.11±0.25. uas of de uas is 90.26±0.07. las of de las is 86.16±0.01. uas of en uas is 92.51±0.08. las of en las is 90.50±0.07. uas of en uas is 92.47±0.03. las of en las is 90.46±0.02. uas of en uas is 91.91±0.17. las of en las is 89.82±0.16. uas of en uas is 91.93±0.07. las of en las is 89.83±0.06. uas of es uas is 93.46±0.05. las of es las is 91.13±0.07. uas of es uas is 93.54±0.06. las of es las is 91.34±0.05. uas of es uas is 93.72±0.07. las of es las is 91.33±0.08. uas of es uas is 93.77±0.07. las of es las is 91.52±0.07. uas of fr uas is 95.05±0.04. las of fr las is 92.76±0.07. uas of fr uas is 94.97±0.04. las of fr las is 92.57±0.06. uas of fr uas is 92.62±0.15. las of fr las is 89.51±0.14. uas of fr uas is 92.90±0.20. las of fr las is 89.88±0.23. uas of it uas is 94.89±0.12. las of it las is 92.58±0.12. uas of it uas is 94.93±0.09. las of it las is 92.90±0.10. uas of it uas is 94.75±0.12. las of it las is 92.72±0.12. uas of it uas is 94.70±0.07. las of it las is 92.55±0.09. uas of nl uas is 93.39±0.08. las of nl las is 90.90±0.07. uas of nl uas is 93.94±0.11. las of nl las is 91.67±0.08. uas of nl uas is 93.44±0.09. las of nl las is 91.04±0.06. uas of nl uas is 93.98±0.05. las of nl las is 91.73±0.07. uas of no uas is 95.44±0.05. las of no las is 93.73±0.05. uas of no uas is 95.52±0.08. las of no las is 93.80±0.08. uas of no uas is 95.28±0.05. las of no las is 93.58±0.05. uas of no uas is 95.33±0.03. las of no las is 93.62±0.03. uas of ro uas is 91.97±0.13. las of ro las is 85.38±0.03. uas of ro uas is 92.06±0.08. las of ro las is 85.58±0.12. uas of ro uas is 91.94±0.07. las of ro las is 85.61±0.13. uas of ro uas is 91.80±0.11. las of ro las is 85.34±0.21. uas of ru uas is 93.81±0.05. las of ru las is 91.85±0.06. uas of ru uas is 94.11±0.07. las of ru las is 92.29±0.10. uas of ru uas is 94.40±0.03. las of ru las is 92.68±0.04. uas of ru uas is 94.69±0.04. las of ru las is 93.07±0.03.
table 2 shows number agreement error rates for various lstm language models, broken down by the number of attractors. error of random n=0 is 50.0. error of random n=1 is 50.0. error of random n=2 is 50.0. error of random n=3 is 50.0. error of random n=4 is 50.0. error of majority n=0 is 32.0. error of majority n=1 is 32.0. error of majority n=2 is 32.0. error of majority n=3 is 32.0. error of majority n=4 is 32.0. error of lstm h=50 n=0 is 6.8. error of lstm h=50 n=1 is 32.6. error of lstm h=50 n=2 is ?50. error of lstm h=50 n=3 is ?65. error of lstm h=50 n=4 is ?70. error of our lstm h=50 n=0 is 2.4. error of our lstm h=50 n=1 is 8.0. error of our lstm h=50 n=2 is 15.7. error of our lstm h=50 n=3 is 26.1. error of our lstm h=50 n=4 is 34.65. error of our lstm h=150 n=0 is 1.5. error of our lstm h=150 n=1 is 4.5. error of our lstm h=150 n=2 is 9.0. error of our lstm h=150 n=3 is 14.3. error of our lstm h=150 n=4 is 17.6. error of our lstm h=250 n=0 is 1.4. error of our lstm h=250 n=1 is 3.3. error of our lstm h=250 n=2 is 5.9. error of our lstm h=250 n=3 is 9.7. error of our lstm h=250 n=4 is 13.9. error of our lstm h=350 n=0 is 1.3. error of our lstm h=350 n=1 is 3.0. error of our lstm h=350 n=2 is 5.7. error of our lstm h=350 n=3 is 9.7. error of our lstm h=350 n=4 is 13.8. error of 1b word lstm (repl) n=0 is 2.8. error of 1b word lstm (repl) n=1 is 8.0. error of 1b word lstm (repl) n=2 is 14.0. error of 1b word lstm (repl) n=3 is 21.8. error of 1b word lstm (repl) n=4 is 20.0. error of char lstm n=0 is 1.2. error of char lstm n=1 is 5.5. error of char lstm n=2 is 11.8. error of char lstm n=3 is 20.4. error of char lstm n=4 is 27.8.
table 5 shows evaluation on in-car assistant. bleu of human* bleu is 13.5. ent. f1 of human* ent. f1 is 60.7. sch. f1 of human* sch. f1 is 64.3. wea. f1 of human* wea. f1 is 61.6. nav. f1 of human* nav. f1 is 55.2. bleu of rule-based* bleu is 6.6. ent. f1 of rule-based* ent. f1 is 43.8. sch. f1 of rule-based* sch. f1 is 61.3. wea. f1 of rule-based* wea. f1 is 39.5. nav. f1 of rule-based* nav. f1 is 40.4. bleu of kv retrieval net* bleu is 13.2. ent. f1 of kv retrieval net* ent. f1 is 48.0. sch. f1 of kv retrieval net* sch. f1 is 62.9. wea. f1 of kv retrieval net* wea. f1 is 47.0. nav. f1 of kv retrieval net* nav. f1 is 41.3. bleu of seq2seq bleu is 8.4. ent. f1 of seq2seq ent. f1 is 10.3. sch. f1 of seq2seq sch. f1 is 09.7. wea. f1 of seq2seq wea. f1 is 14.1. nav. f1 of seq2seq nav. f1 is 07.0. bleu of +attn bleu is 9.3. ent. f1 of +attn ent. f1 is 19.9. sch. f1 of +attn sch. f1 is 23.4. wea. f1 of +attn wea. f1 is 25.6. nav. f1 of +attn nav. f1 is 10.8. bleu of ptr-unk bleu is 8.3. ent. f1 of ptr-unk ent. f1 is 22.7. sch. f1 of ptr-unk sch. f1 is 26.9. wea. f1 of ptr-unk wea. f1 is 26.7. nav. f1 of ptr-unk nav. f1 is 14.9. bleu of mem2seq h1 bleu is 11.6. ent. f1 of mem2seq h1 ent. f1 is 32.4. sch. f1 of mem2seq h1 sch. f1 is 39.8. wea. f1 of mem2seq h1 wea. f1 is 33.6. nav. f1 of mem2seq h1 nav. f1 is 24.6. bleu of mem2seq h3 bleu is 12.6. ent. f1 of mem2seq h3 ent. f1 is 33.4. sch. f1 of mem2seq h3 sch. f1 is 49.3. wea. f1 of mem2seq h3 wea. f1 is 32.8. nav. f1 of mem2seq h3 nav. f1 is 20.0. bleu of mem2seq h6 bleu is 9.9. ent. f1 of mem2seq h6 ent. f1 is 23.6. sch. f1 of mem2seq h6 sch. f1 is 34.3. wea. f1 of mem2seq h6 wea. f1 is 33.0. nav. f1 of mem2seq h6 nav. f1 is 4.4.
table 3 shows evaluation results on factoid question answering dialogues. accuracy (%) of lstm accuracy (%) is 7.8. recall (%) of lstm recall (%) is 7.5. accuracy (%) of hred accuracy (%) is 3.7. recall (%) of hred recall (%) is 3.9. accuracy (%) of gends accuracy (%) is 70.3. recall (%) of gends recall (%) is 63.1. accuracy (%) of nkd-ori accuracy (%) is 67.0. recall (%) of nkd-ori recall (%) is 56.2. accuracy (%) of nkd-gated accuracy (%) is 77.6. recall (%) of nkd-gated recall (%) is 77.3. accuracy (%) of nkd-atte accuracy (%) is 55.1. recall (%) of nkd-atte recall (%) is 46.6.
table 4 shows evaluation results on entire dataset. accuracy (%) of lstm accuracy (%) is 2.6. recall (%) of lstm recall (%) is 2.5. entity number of lstm entity number is 1.65. accuracy (%) of hred accuracy (%) is 1.4. recall (%) of hred recall (%) is 1.5. entity number of hred entity number is 1.79. accuracy (%) of gends accuracy (%) is 20.9. recall (%) of gends recall (%) is 17.4. entity number of gends entity number is 1.34. accuracy (%) of nkd-ori accuracy (%) is 22.9. recall (%) of nkd-ori recall (%) is 19.7. entity number of nkd-ori entity number is 2.55. accuracy (%) of nkd-gated accuracy (%) is 24.8. recall (%) of nkd-gated recall (%) is 25.6. entity number of nkd-gated entity number is 1.59. accuracy (%) of nkd-atte accuracy (%) is 18.4. recall (%) of nkd-atte recall (%) is 16.0. entity number of nkd-atte entity number is 3.41.
table 5 shows human evaluation result. fluency of lstm fluency is 2.52. appropriateness of knowledge of lstm appropriateness of knowledge is 0.88. entire correctness of lstm entire correctness is 0.8. fluency of hred fluency is 2.48. appropriateness of knowledge of hred appropriateness of knowledge is 0.36. entire correctness of hred entire correctness is 0.32. fluency of gends fluency is 2.76. appropriateness of knowledge of gends appropriateness of knowledge is 1.36. entire correctness of gends entire correctness is 1.34. fluency of nkd-ori fluency is 2.42. appropriateness of knowledge of nkd-ori appropriateness of knowledge is 1.92. entire correctness of nkd-ori entire correctness is 1.58. fluency of nkd-gated fluency is 2.08. appropriateness of knowledge of nkd-gated appropriateness of knowledge is 1.72. entire correctness of nkd-gated entire correctness is 1.44. fluency of nkd-atte fluency is 2.7. appropriateness of knowledge of nkd-atte appropriateness of knowledge is 1.54. entire correctness of nkd-atte entire correctness is 1.38.
table 3 shows roundtrip translation (mean/median accuracy) and sentiment analysis (f1) results for wordbased (word) and character-based (char) multilingual embeddings. µ of rtsimple µ is 33. md of rtsimple md is 24. µ of rtsimple µ is 37. md of rtsimple md is 36. µ of rtsimple µ is . md of rtsimple md is . µ of rtsimple µ is . md of rtsimple md is . n of rtsimple n is 67. µ of rtsimple µ is 24. md of rtsimple md is 13. µ of rtsimple µ is 32. md of rtsimple md is 21. µ of rtsimple µ is . md of rtsimple md is . µ of rtsimple µ is . md of rtsimple md is . n of rtsimple n is 70. pos of rtsimple pos is . neg of rtsimple neg is . pos of rtsimple pos is . neg of rtsimple neg is . µ of bow µ is 7. md of bow md is 5. µ of bow µ is 8. md of bow md is 7. µ of bow µ is 13. md of bow md is 12. µ of bow µ is 26. md of bow md is 28. n of bow n is 69. µ of bow µ is 3. md of bow md is 2. µ of bow µ is 3. md of bow md is 2. µ of bow µ is 5. md of bow md is 4. µ of bow µ is 10. md of bow md is 11. n of bow n is 70. pos of bow pos is 33. neg of bow neg is 81. pos of bow pos is 13. neg of bow neg is 83. µ of s-id µ is 46. md of s-id md is 46. µ of s-id µ is 52. md of s-id md is 55. µ of s-id µ is 63. md of s-id md is 76. µ of s-id µ is 79. md of s-id md is 91. n of s-id n is 65. µ of s-id µ is 9. md of s-id md is 5. µ of s-id µ is 9. md of s-id md is 5. µ of s-id µ is 14. md of s-id md is 9. µ of s-id µ is 25. md of s-id md is 22. n of s-id n is 70. pos of s-id pos is 79. neg of s-id neg is 88. pos of s-id pos is 65. neg of s-id neg is 86. µ of sample µ is 33. md of sample md is 23. µ of sample µ is 43. md of sample md is 42. µ of sample µ is 54. md of sample md is 59. µ of sample µ is 82. md of sample md is 96. n of sample n is 65. µ of sample µ is 53. md of sample md is 59. µ of sample µ is 59. md of sample md is 72. µ of sample µ is 67. md of sample md is 85. µ of sample µ is 79. md of sample md is 99. n of sample n is 58. pos of sample pos is 82. neg of sample neg is 89. pos of sample pos is 77. neg of sample neg is 89. µ of clique µ is 43. md of clique md is 36. µ of clique µ is 59. md of clique md is 63. µ of clique µ is 67. md of clique md is 77. µ of clique µ is 93. md of clique md is 99. n of clique n is 69. µ of clique µ is 42. md of clique md is 46. µ of clique µ is 48. md of clique md is 55. µ of clique µ is 60. md of clique md is 76. µ of clique µ is 73. md of clique md is 98. n of clique n is 53. pos of clique pos is 84. neg of clique neg is 89. pos of clique pos is 69. neg of clique neg is 88. µ of n(t) µ is 54. md of n(t) md is 59. µ of n(t) µ is 61. md of n(t) md is 69. µ of n(t) µ is 80. md of n(t) md is 87. µ of n(t) µ is 94. md of n(t) md is 100. n of n(t) n is 69. µ of n(t) µ is 50. md of n(t) md is 53. µ of n(t) µ is 54. md of n(t) md is 59. µ of n(t) µ is 73. md of n(t) md is 82. µ of n(t) µ is 90. md of n(t) md is 99. n of n(t) n is 66. pos of n(t) pos is 82. neg of n(t) neg is 89. pos of n(t) pos is 87. neg of n(t) neg is 90. µ of n(t)-clique µ is 11. md of n(t)-clique md is 0. µ of n(t)-clique µ is 11. md of n(t)-clique md is 0. µ of n(t)-clique µ is 16. md of n(t)-clique md is 0. µ of n(t)-clique µ is 22. md of n(t)-clique md is 0. n of n(t)-clique n is 18. µ of n(t)-clique µ is 39. md of n(t)-clique md is 45. µ of n(t)-clique µ is 41. md of n(t)-clique md is 47. µ of n(t)-clique µ is 58. md of n(t)-clique md is 74. µ of n(t)-clique µ is 76. md of n(t)-clique md is 94. n of n(t)-clique n is 56. pos of n(t)-clique pos is 22. neg of n(t)-clique neg is 84. pos of n(t)-clique pos is 61. neg of n(t)-clique neg is 84. µ of n(t)-cc µ is 3. md of n(t)-cc md is 0. µ of n(t)-cc µ is 3. md of n(t)-cc md is 0. µ of n(t)-cc µ is 5. md of n(t)-cc md is 0. µ of n(t)-cc µ is 7. md of n(t)-cc md is 0. n of n(t)-cc n is 5. µ of n(t)-cc µ is 11. md of n(t)-cc md is 0. µ of n(t)-cc µ is 11. md of n(t)-cc md is 0. µ of n(t)-cc µ is 16. md of n(t)-cc md is 0. µ of n(t)-cc µ is 25. md of n(t)-cc md is 0. n of n(t)-cc n is 21. pos of n(t)-cc pos is 4. neg of n(t)-cc neg is 84. pos of n(t)-cc pos is 40. neg of n(t)-cc neg is 83. µ of n(t)-edge µ is 35. md of n(t)-edge md is 30. µ of n(t)-edge µ is 43. md of n(t)-edge md is 36. µ of n(t)-edge µ is 56. md of n(t)-edge md is 55. µ of n(t)-edge µ is 7. md of n(t)-edge md is 94. n of n(t)-edge n is 69. µ of n(t)-edge µ is 39. md of n(t)-edge md is 29. µ of n(t)-edge µ is 49. md of n(t)-edge md is 52. µ of n(t)-edge µ is 64. md of n(t)-edge md is 78. µ of n(t)-edge µ is 88. md of n(t)-edge md is 100. n of n(t)-edge n is 63. pos of n(t)-edge pos is 84. neg of n(t)-edge neg is 90. pos of n(t)-edge pos is 84. neg of n(t)-edge neg is 89.
table 4 shows development results. p of word baseline p is 73.20. r of word baseline r is 57.05. f1 of word baseline f1 is 64.12. p of word+char lstm p is 71.98. r of word+char lstm r is 65.41. f1 of word+char lstm f1 is 68.54. p of word+char lstm p is 71.08. r of word+char lstm r is 65.83. f1 of word+char lstm f1 is 68.35. p of word+char+bichar lstm p is 72.63. r of word+char+bichar lstm r is 67.60. f1 of word+char+bichar lstm f1 is 70.03. p of word+char cnn p is 73.06. r of word+char cnn r is 66.29. f1 of word+char cnn f1 is 69.51. p of word+char+bichar cnn p is 72.01. r of word+char+bichar cnn r is 65.50. f1 of word+char+bichar cnn f1 is 68.60. p of char baseline p is 67.12. r of char baseline r is 58.42. f1 of char baseline f1 is 62.47. p of char+softword p is 69.30. r of char+softword r is 62.47. f1 of char+softword f1 is 65.71. p of char+bichar p is 71.67. r of char+bichar r is 64.02. f1 of char+bichar f1 is 67.63. p of char+bichar+softword p is 72.64. r of char+bichar+softword r is 66.89. f1 of char+bichar+softword f1 is 69.64. p of lattice p is 74.64. r of lattice r is 68.83. f1 of lattice f1 is 71.62.
table 3 shows performances of character-based methods on kbp2017eval trigger identification task. p of fbrnn(char) p is 57.97. r of fbrnn(char) r is 36.92. f1 of fbrnn(char) f1 is 45.11. p of npn(iob) p is 60.96. r of npn(iob) r is 47.39. f1 of npn(iob) f1 is 53.32. p of npn(task-specific) p is 64.32. r of npn(task-specific) r is 53.16. f1 of npn(task-specific) f1 is 58.21.
table 6 shows results of using different representation on trigger classification task on kbp2017eval. p of dmcnn(word) p is 54.81. r of dmcnn(word) r is 46.84. f1 of dmcnn(word) f1 is 50.51. p of npn(char) p is 56.19. r of npn(char) r is 43.88. f1 of npn(char) f1 is 49.28. p of npn(task-specific) p is 57.63. r of npn(task-specific) r is 47.63. f1 of npn(task-specific) f1 is 52.15.
table 2 shows test results. bleu of pbmt bleu is 26.9. bleu of snrg bleu is 25.6. bleu of tree2str bleu is 23.0. bleu of mseq2seq+anon bleu is 22.0. bleu of graph2seq+copy bleu is 22.7. bleu of graph2seq+charlstm+copy bleu is 23.3. bleu of mseq2seq+anon (200k) bleu is 27.4. bleu of mseq2seq+anon (2m) bleu is 32.3. bleu of seq2seq+charlstm+copy (200k) bleu is 27.4. bleu of seq2seq+charlstm+copy (2m) bleu is 31.7. bleu of graph2seq+charlstm+copy (200k) bleu is 28.2. bleu of graph2seq+charlstm+copy (2m) bleu is 33.0.
table 4 shows human evaluation results. correctness of blstm correctness is 2.25. grammar of blstm grammar is 2.33. fluency of blstm fluency is 2.29. correctness of blstm correctness is 1.53. grammar of blstm grammar is 1.71. fluency of blstm fluency is 1.68. correctness of blstm correctness is 1.54. grammar of blstm grammar is 1.84. fluency of blstm fluency is 1.84. correctness of smt correctness is 2.03. grammar of smt grammar is 2.11. fluency of smt fluency is 2.07. correctness of smt correctness is 1.36. grammar of smt grammar is 1.48. fluency of smt fluency is 1.44. correctness of smt correctness is 1.81. grammar of smt grammar is 1.99. fluency of smt fluency is 1.89. correctness of tff correctness is 1.77. grammar of tff grammar is 1.91. fluency of tff fluency is 1.88. correctness of tff correctness is 1.44. grammar of tff grammar is 1.69. fluency of tff fluency is 1.66. correctness of tff correctness is 1.71. grammar of tff grammar is 1.99. fluency of tff fluency is 1.96. correctness of tlstm correctness is 2.53. grammar of tlstm grammar is 2.61. fluency of tlstm fluency is 2.55. correctness of tlstm correctness is 1.75. grammar of tlstm grammar is 1.93. fluency of tlstm fluency is 1.86. correctness of tlstm correctness is 2.21. grammar of tlstm grammar is 2.38. fluency of tlstm fluency is 2.35. correctness of gtr-lstm correctness is 2.64. grammar of gtr-lstm grammar is 2.66. fluency of gtr-lstm fluency is 2.57. correctness of gtr-lstm correctness is 1.96. grammar of gtr-lstm grammar is 2.04. fluency of gtr-lstm fluency is 1.99. correctness of gtr-lstm correctness is 2.29. grammar of gtr-lstm grammar is 2.42. fluency of gtr-lstm fluency is 2.41.
table 4 shows crowd-sourced ablation evaluation of generations on tripadvisor. repetition of repetition only repetition is +0.63. contradiction of repetition only contradiction is +0.30. relevance of repetition only relevance is +0.37. clarity of repetition only clarity is +0.42. better of repetition only better is 50%. neither of repetition only neither is 23%. worse of repetition only worse is 27%. repetition of entailment only repetition is +0.01. contradiction of entailment only contradiction is +0.02. relevance of entailment only relevance is +0.05. clarity of entailment only clarity is -0.10. better of entailment only better is 39%. neither of entailment only neither is 20%. worse of entailment only worse is 41%. repetition of relevance only repetition is -0.19. contradiction of relevance only contradiction is +0.09. relevance of relevance only relevance is +0.10. clarity of relevance only clarity is +0.060. better of relevance only better is 36%. neither of relevance only neither is 22%. worse of relevance only worse is 42%. repetition of lexical style only repetition is +0.11. contradiction of lexical style only contradiction is +0.16. relevance of lexical style only relevance is +0.20. clarity of lexical style only clarity is +0.16. better of lexical style only better is 38%. neither of lexical style only neither is 25%. worse of lexical style only worse is 38%. repetition of all repetition is +0.23. contradiction of all contradiction is -0.02. relevance of all relevance is +0.19. clarity of all clarity is -0.03. better of all better is 47%. neither of all neither is 19%. worse of all worse is 34%.
table 3 shows performance of baselines and our model with different subsets of features as per various quantitative measures. bleu of temp bleu is 0.72. bleu-2 of temp bleu-2 is 20.77. diversity of temp diversity is 4.43. bleu of nn (m+t+s) bleu is 1.28. bleu-2 of nn (m+t+s) bleu-2 is 21.07. diversity of nn (m+t+s) diversity is 7.85. bleu of raw bleu is 1.13. bleu-2 of raw bleu-2 is 13.74. diversity of raw diversity is 2.37. bleu of gac-sparse bleu is 1.76. bleu-2 of gac-sparse bleu-2 is 21.49. diversity of gac-sparse diversity is 4.29. bleu of gac (m+t) bleu is 1.85. bleu-2 of gac (m+t) bleu-2 is 23.35. diversity of gac (m+t) diversity is 4.72. bleu of temp bleu is 16.17. bleu-2 of temp bleu-2 is 47.29. diversity of temp diversity is 1.16. bleu of nn (m+t) bleu is 5.98. bleu-2 of nn (m+t) bleu-2 is 42.97. diversity of nn (m+t) diversity is 4.52. bleu of raw bleu is 16.92. bleu-2 of raw bleu-2 is 47.72. diversity of raw diversity is 1.07. bleu of gac-sparse bleu is 14.98. bleu-2 of gac-sparse bleu-2 is 51.46. diversity of gac-sparse diversity is 2.63. bleu of gac(m+t+s) bleu is 16.94. bleu-2 of gac(m+t+s) bleu-2 is 47.65. diversity of gac(m+t+s) diversity is 1.01. bleu of nn (m) bleu is 1.28. bleu-2 of nn (m) bleu-2 is 24.49. diversity of nn (m) diversity is 6.97. bleu of raw bleu is 2.80. bleu-2 of raw bleu-2 is 23.26. diversity of raw diversity is 3.03. bleu of gac-sparse bleu is 3.58. bleu-2 of gac-sparse bleu-2 is 25.28. diversity of gac-sparse diversity is 2.18. bleu of gac(m+t) bleu is 3.51. bleu-2 of gac(m+t) bleu-2 is 29.48. diversity of gac(m+t) diversity is 3.64.
table 1 shows comparison between various rc datasets avg. word distance of movie qa avg. word distance is 20.67. avg. sentence distance of movie qa avg. sentence distance is 1.67. number of sentences for inferencing of movie qa number of sentences for inferencing is 2.3. % of instances where both query&answer entities were found in passage of movie qa % of instances where both query&answer entities were found in passage is 67.96. % of instances where only query entities were found in passage of movie qa % of instances where only query entities were found in passage is 59.61. % length of the longest common sequence of non-stop words in query (w.r.t query length) and plot of movie qa % length of the longest common sequence of non-stop words in query (w.r.t query length) and plot is 25. avg. word distance of narrativeqa over plot-summaries avg. word distance is 24.94. avg. sentence distance of narrativeqa over plot-summaries avg. sentence distance is 1.95. number of sentences for inferencing of narrativeqa over plot-summaries number of sentences for inferencing is 1.95. % of instances where both query&answer entities were found in passage of narrativeqa over plot-summaries % of instances where both query&answer entities were found in passage is 59.4. % of instances where only query entities were found in passage of narrativeqa over plot-summaries % of instances where only query entities were found in passage is 61.77. % length of the longest common sequence of non-stop words in query (w.r.t query length) and plot of narrativeqa over plot-summaries % length of the longest common sequence of non-stop words in query (w.r.t query length) and plot is 26.26. avg. word distance of selfrc avg. word distance is 13.4. avg. sentence distance of selfrc avg. sentence distance is 1.34. number of sentences for inferencing of selfrc number of sentences for inferencing is 1.51. % of instances where both query&answer entities were found in passage of selfrc % of instances where both query&answer entities were found in passage is 58.79. % of instances where only query entities were found in passage of selfrc % of instances where only query entities were found in passage is 63.39. % length of the longest common sequence of non-stop words in query (w.r.t query length) and plot of selfrc % length of the longest common sequence of non-stop words in query (w.r.t query length) and plot is 38. avg. word distance of paraphraserc avg. word distance is 45.3. avg. sentence distance of paraphraserc avg. sentence distance is 2.7. number of sentences for inferencing of paraphraserc number of sentences for inferencing is 2.47. % of instances where both query&answer entities were found in passage of paraphraserc % of instances where both query&answer entities were found in passage is 12.25. % of instances where only query entities were found in passage of paraphraserc % of instances where only query entities were found in passage is 47.05. % length of the longest common sequence of non-stop words in query (w.r.t query length) and plot of paraphraserc % length of the longest common sequence of non-stop words in query (w.r.t query length) and plot is 21.
table 3 shows performance of the spanmodel and genmodel on the span test subset and the full test set of the self and paraphraserc. acc. of spanmodel acc. is 46.14. f1 of spanmodel f1 is 57.49. bleu of spanmodel bleu is 22.98. acc. of spanmodel acc. is 37.53. f1 of spanmodel f1 is 50.56. bleu of spanmodel bleu is 7.47. acc. of genmodel (with augmented training data) acc. is 16.45. f1 of genmodel (with augmented training data) f1 is 26.97. bleu of genmodel (with augmented training data) bleu is 7.61. acc. of genmodel (with augmented training data) acc. is 15.31. f1 of genmodel (with augmented training data) f1 is 24.05. bleu of genmodel (with augmented training data) bleu is 5.50. acc. of spanmodel acc. is 17.93. f1 of spanmodel f1 is 26.27. bleu of spanmodel bleu is 9.39. acc. of spanmodel acc. is 9.78. f1 of spanmodel f1 is 16.33. bleu of spanmodel bleu is 2.60. acc. of spanmodel with preprocessed data acc. is 27.49. f1 of spanmodel with preprocessed data f1 is 35.10. bleu of spanmodel with preprocessed data bleu is 12.78. acc. of spanmodel with preprocessed data acc. is 14.92. f1 of spanmodel with preprocessed data f1 is 21.53. bleu of spanmodel with preprocessed data bleu is 2.75. acc. of genmodel (with augmented training data) acc. is 12.66. f1 of genmodel (with augmented training data) f1 is 19.48. bleu of genmodel (with augmented training data) bleu is 4.41. acc. of genmodel (with augmented training data) acc. is 5.42. f1 of genmodel (with augmented training data) f1 is 9.64. bleu of genmodel (with augmented training data) bleu is 1.75.
table 1 shows main results—comparison of different answer module architectures. em of standard 1-step em is 75.139. f1 of standard 1-step f1 is 83.367. em of fixed 5-step with memory network (prediction from final step) em is 75.033. f1 of fixed 5-step with memory network (prediction from final step) f1 is 83.327. em of fixed 5-step with memory network (prediction averaged from all steps) em is 75.256. f1 of fixed 5-step with memory network (prediction averaged from all steps) f1 is 83.215. em of dynamic steps (max 5) with reasonet em is 75.355. f1 of dynamic steps (max 5) with reasonet f1 is 83.360. em of stochastic answer network (san) fixed 5-step em is 76.235. f1 of stochastic answer network (san) fixed 5-step f1 is 84.056.
table 4 shows effect of number of steps: best and worst results are boldfaced. em of 1 step em is 75.38. f1 of 1 step f1 is 83.29. em of 2 step em is 75.43. f1 of 2 step f1 is 83.41. em of 3 step em is 75.89. f1 of 3 step f1 is 83.57. em of 4 step em is 75.92. f1 of 4 step f1 is 83.85. em of 5 step em is 76.24. f1 of 5 step f1 is 84.06. em of 6 step em is 75.99. f1 of 6 step f1 is 83.72. em of 7 step em is 76.04. f1 of 7 step f1 is 83.92. em of 8 step em is 76.03. f1 of 8 step f1 is 83.82. em of 9 step em is 75.95. f1 of 9 step f1 is 83.75. em of 10 step em is 76.04. f1 of 10 step f1 is 83.89.
table 5 shows test performance on the adversarial squad dataset in f1 score. f1 of lr (rajpurkar et al., 2016) addsent is 23.2. f1 of lr (rajpurkar et al., 2016) addonesent is 30.3. f1 of sedt (liu et al., 2017a) addsent is 33.9. f1 of sedt (liu et al., 2017a) addonesent is 44.8. f1 of bidaf (seo et al., 2016) addsent is 34.3. f1 of bidaf (seo et al., 2016) addonesent is 45.7. f1 of jnet (zhang et al., 2017) addsent is 37.9. f1 of jnet (zhang et al., 2017) addonesent is 47.0. f1 of reasonet(shen et al., 2017) addsent is 39.4. f1 of reasonet(shen et al., 2017) addonesent is 50.3. f1 of rasor(lee et al., 2016) addsent is 39.5. f1 of rasor(lee et al., 2016) addonesent is 49.5. f1 of mnemonic(hu et al., 2017) addsent is 46.6. f1 of mnemonic(hu et al., 2017) addonesent is 56.0. f1 of qanet(yu et al., 2018) addsent is 45.2. f1 of qanet(yu et al., 2018) addonesent is 55.7. f1 of standard 1-step in table 1 addsent is 45.4. f1 of standard 1-step in table 1 addonesent is 55.8. f1 of san addsent is 46.6. f1 of san addonesent is 56.5.
table 7 shows ms marco devset results. rouge of reasonet++(shen et al. 2017) rouge is 38.01. bleu of reasonet++(shen et al. 2017) bleu is 38.62. rouge of v-net(wang et al. 2018) rouge is 45.65. rouge of standard 1-step in table 1 rouge is 42.30. bleu of standard 1-step in table 1 bleu is 42.39. rouge of san rouge is 46.14. bleu of san bleu is 43.85.
table 5 shows results on the dev set of squad (first two) and newsqa (last). f1 of full f1 is 79.9. em of full em is 71. train sp of full train sp is x1.0. infer sp of full infer sp is x1.0. f1 of full f1 is 83.1. em of full em is 74.5. train sp of full train sp is x1.0. infer sp of full infer sp is x1.0. f1 of full f1 is 63.8. em of full em is 50.7. train sp of full train sp is x1.0. infer sp of full infer sp is x1.0. f1 of oracle f1 is 84.3. em of oracle em is 74.9. train sp of oracle train sp is x6.7. infer sp of oracle infer sp is x5.1. f1 of oracle f1 is 85.1. em of oracle em is 76. train sp of oracle train sp is x3.0. infer sp of oracle infer sp is x5.1. f1 of oracle f1 is 75.5. em of oracle em is 59.2. train sp of oracle train sp is x18.8. infer sp of oracle infer sp is x21.7. f1 of minimal(top k) f1 is 78.7. em of minimal(top k) em is 69.9. train sp of minimal(top k) train sp is x6.7. infer sp of minimal(top k) infer sp is x5.1. f1 of minimal(top k) f1 is 79.2. em of minimal(top k) em is 70.7. train sp of minimal(top k) train sp is x3.0. infer sp of minimal(top k) infer sp is x5.1. f1 of minimal(top k) f1 is 62.3. em of minimal(top k) em is 49.3. train sp of minimal(top k) train sp is x15.0. infer sp of minimal(top k) infer sp is x6.9. f1 of minimal(dyn) f1 is 79.8. em of minimal(dyn) em is 70.9. train sp of minimal(dyn) train sp is x6.7. infer sp of minimal(dyn) infer sp is x3.6. f1 of minimal(dyn) f1 is 80.6. em of minimal(dyn) em is 72. train sp of minimal(dyn) train sp is x3.0. infer sp of minimal(dyn) infer sp is x3.7. f1 of minimal(dyn) f1 is 63.2. em of minimal(dyn) em is 50.1. train sp of minimal(dyn) train sp is x15.0. infer sp of minimal(dyn) infer sp is x5.3. f1 of gnr f1 is 85.0a. em of gnr em is 66.6a. f1 of fastqa f1 is 78.5. em of fastqa em is 70.3. f1 of fastqa f1 is 56.1. em of fastqa em is 43.7. f1 of fusionnet f1 is 83.6. em of fusionnet em is 75.3.
table 8 shows results on the dev-full set of triviaqa (wikipedia) and the dev set of squad-open. n sent of - n sent is 69. acc of - acc is 95.9. sp of - sp is x1.0. f1 of - f1 is 59.6. em of - em is 53.5. n sent of - n sent is 124. acc of - acc is 76.9. sp of - sp is x1.0. f1 of - f1 is 41.0. em of - em is 33.1. n sent of tf-idf n sent is 5. acc of tf-idf acc is 73.0. sp of tf-idf sp is x13.8. f1 of tf-idf f1 is 51.9. em of tf-idf em is 45.8. n sent of tf-idf n sent is 5. acc of tf-idf acc is 46.1. sp of tf-idf sp is x12.4. f1 of tf-idf f1 is 36.6. em of tf-idf em is 29.6. n sent of tf-idf n sent is 10. acc of tf-idf acc is 79.9. sp of tf-idf sp is x6.9. f1 of tf-idf f1 is 57.2. em of tf-idf em is 51.5. n sent of tf-idf n sent is 10. acc of tf-idf acc is 54.3. sp of tf-idf sp is x6.2. f1 of tf-idf f1 is 39.8. em of tf-idf em is 32.5. n sent of our selector n sent is 5.0. acc of our selector acc is 84.9. sp of our selector sp is x13.8. f1 of our selector f1 is 59.5. em of our selector em is 54.0. n sent of our selector n sent is 5.3. acc of our selector acc is 58.9. sp of our selector sp is x11.7. f1 of our selector f1 is 42.3. em of our selector em is 34.6. n sent of our selector n sent is 10.5. acc of our selector acc is 90.9. sp of our selector sp is x6.6. f1 of our selector f1 is 60.5. em of our selector em is 54.9. n sent of our selector n sent is 10.7. acc of our selector acc is 64.0. sp of our selector sp is x5.8. f1 of our selector f1 is 42.5. em of our selector em is 34.7. f1 of - f1 is 56.0a. em of - em is 51.6a. n sent of - n sent is 2376a. acc of - acc is 77.8. em of - em is 29.8. f1 of - f1 is 55.1a. em of - em is 48.6a. f1 of - f1 is 37.5. em of - em is 29.1. f1 of - f1 is 52.9b. em of - em is 46.9a. n sent of - n sent is 2376a. acc of - acc is 77.8. em of - em is 28.4.
table 9 shows results on the dev set of squadadversarial. f1 of full f1 is 52.6. em of full em is 46.2. sp of full sp is x0.7. f1 of full f1 is 63.5. em of full em is 56.8. sp of full sp is x0.7. f1 of oracle f1 is 84.2. em of oracle em is 75.3. sp of oracle sp is x4.3. f1 of oracle f1 is 84.5. em of oracle em is 75.8. sp of oracle sp is x4.3. f1 of minimal f1 is 59.7. em of minimal em is 52.2. sp of minimal sp is x4.3. f1 of minimal f1 is 67.5. em of minimal em is 60.1. sp of minimal sp is x4.3. f1 of full f1 is 57.7. em of full em is 51.1. sp of full sp is x1.0. f1 of full f1 is 66.5. em of full em is 59.7. sp of full sp is x1.0. f1 of oracle f1 is 82.5. em of oracle em is 74.1. sp of oracle sp is x6.0. f1 of oracle f1 is 82.9. em of oracle em is 74.6. sp of oracle sp is x6.0. f1 of minimal f1 is 58.5. em of minimal em is 51.5. sp of minimal sp is x6.0. f1 of minimal f1 is 66.5. em of minimal em is 59.5. sp of minimal sp is x6.0. f1 of - f1 is 39.5. f1 of - f1 is 49.5. f1 of - f1 is 39.4. f1 of - f1 is 50.3. f1 of - f1 is 46.6. f1 of - f1 is 56.0.
table 4 shows results on ted test data for training with estimated (e) and direct (d) rewards from simulation (s), humans (h) and filtered (f) human ratings. bleu of - bleu is 27.0. meteor of - meteor is 30.7. beer of - beer is 59.48. bleu of s bleu is 32.5★ ±0.01. meteor of s meteor is 33.7★ ±0.01. beer of s beer is 63.47★ ±0.10. bleu of s bleu is 27.5★. meteor of s meteor is 30.9★. beer of s beer is 59.62★. bleu of s bleu is 28.2★ ±0.09. meteor of s meteor is 31.6★ ±0.04. beer of s beer is 60.23★ ±0.14. bleu of s bleu is 27.8★ ±0.01. meteor of s meteor is 31.2★ ±0.01. beer of s beer is 59.83★ ±0.04. bleu of h bleu is 27.5★. meteor of h meteor is 30.9★. beer of h beer is 59.72★. bleu of h bleu is 28.1★ ±0.01. meteor of h meteor is 31.5★ ±0.01. beer of h beer is 60.21★ ±0.12. bleu of h bleu is 27.8★ ±0.09. meteor of h meteor is 31.3★ ±0.09. beer of h beer is 59.88★ ±0.23. bleu of f bleu is 28.1★ ±0.20. meteor of f meteor is 31.6★ ±0.10. beer of f beer is 60.29★ ±0.13.
table 4 shows detokenized bleu scores for wmt17 translation tasks. bleu of en→de winner is 28.3. bleu of en→de transformer is 27.33. bleu of en→de our model is 27.22. bleu of en→de δd is -0.11. bleu of en→de winner is 28.9. bleu of en→de transformer is 27.92. bleu of en→de our model is 27.80. bleu of en→de δd is -0.12. bleu of de→en winner is 35.1. bleu of de→en transformer is 32.63. bleu of de→en our model is 32.73. bleu of de→en δd is +0.10. bleu of de→en winner is 36.5. bleu of de→en transformer is 34.06. bleu of de→en our model is 34.13. bleu of de→en δd is +0.07. bleu of en→fi winner is 20.7. bleu of en→fi transformer is 21.00. bleu of en→fi our model is 20.87. bleu of en→fi δd is -0.13. bleu of en→fi winner is 21.1. bleu of en→fi transformer is 21.54. bleu of en→fi our model is 21.47. bleu of en→fi δd is -0.07. bleu of fi→en winner is 20.5. bleu of fi→en transformer is 25.19. bleu of fi→en our model is 24.78. bleu of fi→en δd is -0.41. bleu of fi→en winner is 21.4. bleu of fi→en transformer is 26.22. bleu of fi→en our model is 25.74. bleu of fi→en δd is -0.48. bleu of en→lv winner is 21.1. bleu of en→lv transformer is 16.83. bleu of en→lv our model is 16.63. bleu of en→lv δd is -0.20. bleu of en→lv winner is 21.6. bleu of en→lv transformer is 17.42. bleu of en→lv our model is 17.23. bleu of en→lv δd is -0.19. bleu of lv→en winner is 21.9. bleu of lv→en transformer is 17.57. bleu of lv→en our model is 17.51. bleu of lv→en δd is -0.06. bleu of lv→en winner is 22.9. bleu of lv→en transformer is 18.48. bleu of lv→en our model is 18.30. bleu of lv→en δd is -0.18. bleu of en→ru winner is 29.8. bleu of en→ru transformer is 27.82. bleu of en→ru our model is 27.73. bleu of en→ru δd is -0.09. bleu of en→ru winner is 29.8. bleu of en→ru transformer is 27.83. bleu of en→ru our model is 27.74. bleu of en→ru δd is -0.09. bleu of ru→en winner is 34.7. bleu of ru→en transformer is 31.51. bleu of ru→en our model is 31.36. bleu of ru→en δd is -0.15. bleu of ru→en winner is 35.6. bleu of ru→en transformer is 32.59. bleu of ru→en our model is 32.36. bleu of ru→en δd is -0.23. bleu of en→tr winner is 18.1. bleu of en→tr transformer is 12.11. bleu of en→tr our model is 11.59. bleu of en→tr δd is -0.52. bleu of en→tr winner is 18.4. bleu of en→tr transformer is 12.56. bleu of en→tr our model is 12.03. bleu of en→tr δd is -0.53. bleu of tr→en winner is 20.1. bleu of tr→en transformer is 16.19. bleu of tr→en our model is 15.84. bleu of tr→en δd is -0.35. bleu of tr→en winner is 20.9. bleu of tr→en transformer is 16.93. bleu of tr→en our model is 16.57. bleu of tr→en δd is -0.36. bleu of en→cs winner is 23.5. bleu of en→cs transformer is 21.53. bleu of en→cs our model is 21.12. bleu of en→cs δd is -0.41. bleu of en→cs winner is 24.1. bleu of en→cs transformer is 22.07. bleu of en→cs our model is 21.66. bleu of en→cs δd is -0.41. bleu of cs→en winner is 30.9. bleu of cs→en transformer is 27.49. bleu of cs→en our model is 27.45. bleu of cs→en δd is -0.04. bleu of cs→en winner is 31.9. bleu of cs→en transformer is 28.41. bleu of cs→en our model is 28.33. bleu of cs→en δd is -0.08.
table 1 shows performance breakdown of each transition phase. accuracy of peng et al. (2018) shiftorpop is 0.87. accuracy of peng et al. (2018) pushindex is 0.87. accuracy of peng et al. (2018) arcbinary is 0.83. accuracy of peng et al. (2018) arclabel is 0.81. accuracy of soft+feats shiftorpop is 0.93. accuracy of soft+feats pushindex is 0.84. accuracy of soft+feats arcbinary is 0.91. accuracy of soft+feats arclabel is 0.75. accuracy of hard+feats shiftorpop is 0.94. accuracy of hard+feats pushindex is 0.85. accuracy of hard+feats arcbinary is 0.93. accuracy of hard+feats arclabel is 0.77.
table 4 shows comparison to other amr parsers. f of buys and blunsom (2017) f is 0.60. p of konstas et al. (2017) p is 0.60. r of konstas et al. (2017) r is 0.65. f of konstas et al. (2017) f is 0.62. f of ballesteros and al-onaizan (2017)* f is 0.64. f of damonte et al. (2017) f is 0.64. p of peng et al. (2018) p is 0.69. r of peng et al. (2018) r is 0.59. f of peng et al. (2018) f is 0.64. p of wang et al. (2015b) p is 0.64. r of wang et al. (2015b) r is 0.62. f of wang et al. (2015b) f is 0.63. p of wang et al. (2015a) p is 0.70. r of wang et al. (2015a) r is 0.63. f of wang et al. (2015a) f is 0.66. p of flanigan et al. (2016) p is 0.70. r of flanigan et al. (2016) r is 0.65. f of flanigan et al. (2016) f is 0.67. p of wang and xue (2017) p is 0.72. r of wang and xue (2017) r is 0.65. f of wang and xue (2017) f is 0.68. p of ours soft attention p is 0.68. r of ours soft attention r is 0.63. f of ours soft attention f is 0.65. p of ours hard attention p is 0.69. r of ours hard attention r is 0.64. f of ours hard attention f is 0.66.
table 2 shows test accuracy of sentiment classification on stanford sentiment treebank. accuracy (%) of bilstm accuracy (%) is 84.8. accuracy (%) of pipeline accuracy (%) is 85.7. accuracy (%) of ste accuracy (%) is 85.4. accuracy (%) of spigot accuracy (%) is 86.3.
table 3 shows syntactic parsing performance (in unlabeled attachment score, uas) and dm semantic parsing performance (in labeled f1) on different groups of the development data. uas of pipeline uas is 97.4. dm of pipeline dm is 94.0. uas of spigot uas is 97.4. dm of spigot dm is 94.3. uas of pipeline uas is 91.3. dm of pipeline dm is 88.1. uas of spigot uas is 89.6. dm of spigot dm is 89.2.
table 2 shows evaluation results for question generation. bleu-3 of baseline (du et al. 2017) (w/o answer) bleu-3 is 17.50. bleu-4 of baseline (du et al. 2017) (w/o answer) bleu-4 is 12.28. meteor of baseline (du et al. 2017) (w/o answer) meteor is 16.62. bleu-3 of baseline (du et al. 2017) (w/o answer) bleu-3 is 15.81. bleu-4 of baseline (du et al. 2017) (w/o answer) bleu-4 is 10.78. meteor of baseline (du et al. 2017) (w/o answer) meteor is 15.31. bleu-3 of seq2seq + copy (w/ answer) bleu-3 is 20.01. bleu-4 of seq2seq + copy (w/ answer) bleu-4 is 14.31. meteor of seq2seq + copy (w/ answer) meteor is 18.50. bleu-3 of seq2seq + copy (w/ answer) bleu-3 is 19.61. bleu-4 of seq2seq + copy (w/ answer) bleu-4 is 13.96. meteor of seq2seq + copy (w/ answer) meteor is 18.19. bleu-3 of contextnqg: seq2seq + copy (w/ full context + answer) bleu-3 is 20.31. bleu-4 of contextnqg: seq2seq + copy (w/ full context + answer) bleu-4 is 14.58. meteor of contextnqg: seq2seq + copy (w/ full context + answer) meteor is 18.84. bleu-3 of contextnqg: seq2seq + copy (w/ full context + answer) bleu-3 is 19.57. bleu-4 of contextnqg: seq2seq + copy (w/ full context + answer) bleu-4 is 14.05. meteor of contextnqg: seq2seq + copy (w/ full context + answer) meteor is 18.19. bleu-3 of corefnqg bleu-3 is 20.90. bleu-4 of corefnqg bleu-4 is 15.16. meteor of corefnqg meteor is 19.12. bleu-3 of corefnqg bleu-3 is 20.19. bleu-4 of corefnqg bleu-4 is 14.52. meteor of corefnqg meteor is 18.59. bleu-3 of corefnqg - gating bleu-3 is 20.68. bleu-4 of corefnqg - gating bleu-4 is 14.84. meteor of corefnqg - gating meteor is 18.98. bleu-3 of corefnqg - gating bleu-3 is 20.08. bleu-4 of corefnqg - gating bleu-4 is 14.40. meteor of corefnqg - gating meteor is 18.64. bleu-3 of corefnqg - mention-pair score bleu-3 is 20.56. bleu-4 of corefnqg - mention-pair score bleu-4 is 14.75. meteor of corefnqg - mention-pair score meteor is 18.85. bleu-3 of corefnqg - mention-pair score bleu-3 is 19.73. bleu-4 of corefnqg - mention-pair score bleu-4 is 14.13. meteor of corefnqg - mention-pair score meteor is 18.38.
table 6 shows performance of the neural machine reading comprehension model (no initialization with pretrained embeddings) on our generated corpus. exact match of docreader (chen et al. 2017) dev is 82.33. exact match of docreader (chen et al. 2017) test is 81.65. f-1 of docreader (chen et al. 2017) dev is 88.20. f-1 of docreader (chen et al. 2017) test is 87.79.
table 3 shows performance of our method and competing models on the ms-marco test set rouge-l of fastqa ext (weissenborn et al. 2017) rouge-l is 33.67. bleu-1 of fastqa ext (weissenborn et al. 2017) bleu-1 is 33.93. rouge-l of prediction (wang and jiang 2016) rouge-l is 37.33. bleu-1 of prediction (wang and jiang 2016) bleu-1 is 40.72. rouge-l of reasonet (shen et al. 2017) rouge-l is 38.81. bleu-1 of reasonet (shen et al. 2017) bleu-1 is 39.86. rouge-l of r-net (wang et al. 2017c) rouge-l is 42.89. bleu-1 of r-net (wang et al. 2017c) bleu-1 is 42.22. rouge-l of s-net (tan et al. 2017) rouge-l is 45.23. bleu-1 of s-net (tan et al. 2017) bleu-1 is 43.78. rouge-l of our model rouge-l is 46.15. bleu-1 of our model bleu-1 is 44.47. rouge-l of s-net (ensemble) rouge-l is 46.65. bleu-1 of s-net (ensemble) bleu-1 is 44.78. rouge-l of our model (ensemble) rouge-l is 46.66. bleu-1 of our model (ensemble) bleu-1 is 45.41. rouge-l of human rouge-l is 47. bleu-1 of human bleu-1 is 46.
table 2 shows component evaluation for the language model (“ppl” = perplexity), pentameter model (“stress acc”), and rhyme model (“rhyme f1”). ppl of lm ppl is 90.13. ppl of lm* ppl is 84.23. ppl of lm** ppl is 80.41. ppl of lm**-c ppl is 83.68. ppl of lm**+pm+rm ppl is 80.22. stress acc of lm**+pm+rm stress acc is 0.74. rhyme f1 of lm**+pm+rm rhyme f1 is 0.91. stress acc of stress-bl stress acc is 0.80. rhyme f1 of rhyme-bl rhyme f1 is 0.74. rhyme f1 of rhyme-em rhyme f1 is 0.71.
table 1 shows (1) accuracy (acc.) and string edit distance (sed) results in the prediction of all referring expressions; (2) accuracy (acc.), precision (prec.), recall (rec.) and f-score results in the prediction of pronominal forms; and (3) accuracy (acc.) and bleu score results of the texts with the generated referring expressions. acc. of onlynames acc. is 0.53d. sed of onlynames sed is 4.05d. acc. of onlynames acc. is 0.15d. bleu of onlynames bleu is 69.03d. acc. of ferreira acc. is 0.61c. sed of ferreira sed is 3.18c. acc. of ferreira acc. is 0.43b. prec. of ferreira prec. is 0.57. rec. of ferreira rec. is 0.54. f-score of ferreira f-score is 0.55. acc. of ferreira acc. is 0.19c. bleu of ferreira bleu is 72.78c. acc. of neuralreg+seq2seq acc. is 0.74a,b. sed of neuralreg+seq2seq sed is 2.32a,b. acc. of neuralreg+seq2seq acc. is 0.75a. prec. of neuralreg+seq2seq prec. is 0.77. rec. of neuralreg+seq2seq rec. is 0.78. f-score of neuralreg+seq2seq f-score is 0.78. acc. of neuralreg+seq2seq acc. is 0.28b. bleu of neuralreg+seq2seq bleu is 79.27a,b. acc. of neuralreg+catt acc. is 0.74a. sed of neuralreg+catt sed is 2.25a. acc. of neuralreg+catt acc. is 0.75a. prec. of neuralreg+catt prec. is 0.73. rec. of neuralreg+catt rec. is 0.78. f-score of neuralreg+catt f-score is 0.75. acc. of neuralreg+catt acc. is 0.30a. bleu of neuralreg+catt bleu is 79.39a. acc. of neuralreg+hieratt acc. is 0.73b. sed of neuralreg+hieratt sed is 2.36b. acc. of neuralreg+hieratt acc. is 0.73a. prec. of neuralreg+hieratt prec. is 0.74. rec. of neuralreg+hieratt rec. is 0.77. f-score of neuralreg+hieratt f-score is 0.75. acc. of neuralreg+hieratt acc. is 0.28a,b. bleu of neuralreg+hieratt bleu is 79.01b.
table 3 shows fluency, grammaticality and clarity results obtained in the human evaluation. fluency of onlynames fluency is 4.74c. grammar of onlynames grammar is 4.68b. clarity of onlynames clarity is 4.90b. fluency of ferreira fluency is 4.74c. grammar of ferreira grammar is 4.58b. clarity of ferreira clarity is 4.93b. fluency of neuralreg+seq2seq fluency is 4.95b,c. grammar of neuralreg+seq2seq grammar is 4.82a,b. clarity of neuralreg+seq2seq clarity is 4.97b. fluency of neuralreg+catt fluency is 5.23a,b. grammar of neuralreg+catt grammar is 4.95a,b. clarity of neuralreg+catt clarity is 5.26a,b. fluency of neuralreg+hieratt fluency is 5.07b,c. grammar of neuralreg+hieratt grammar is 4.90a,b. clarity of neuralreg+hieratt clarity is 5.13a,b. fluency of original fluency is 5.41a. grammar of original grammar is 5.17a. clarity of original clarity is 5.42a.
table 1 shows ned performance on the snapcaptionskb dataset at top-1, 3, 5, 10, 50 accuracies. accuracy (%) of m→e list top-1 is 51.2. accuracy (%) of m→e list top-3 is 60.4. accuracy (%) of m→e list top-5 is 66.5. accuracy (%) of m→e list top-10 is 66.9. accuracy (%) of m→e list top-50 is 66.9. accuracy (%) of 5-nn (lexical) top-1 is 35.2. accuracy (%) of 5-nn (lexical) top-3 is 43.3. accuracy (%) of 5-nn (lexical) top-5 is 45.0. accuracy (%) of 10-nn (lexical) top-1 is 31.9. accuracy (%) of 10-nn (lexical) top-3 is 40.1. accuracy (%) of 10-nn (lexical) top-5 is 44.5. accuracy (%) of 10-nn (lexical) top-10 is 50.7. accuracy (%) of m→e list top-1 is 48.7. accuracy (%) of m→e list top-3 is 57.3. accuracy (%) of m→e list top-5 is 66.3. accuracy (%) of m→e list top-10 is 66.9. accuracy (%) of m→e list top-50 is 66.9. accuracy (%) of n/a top-1 is 43.6. accuracy (%) of n/a top-3 is 63.8. accuracy (%) of n/a top-5 is 67.1. accuracy (%) of n/a top-10 is 70.5. accuracy (%) of n/a top-50 is 77.2. accuracy (%) of n/a top-1 is 67.0. accuracy (%) of n/a top-3 is 72.7. accuracy (%) of n/a top-5 is 74.8. accuracy (%) of n/a top-10 is 76.8. accuracy (%) of n/a top-50 is 85. accuracy (%) of n/a top-1 is 67.8. accuracy (%) of n/a top-3 is 73.5. accuracy (%) of n/a top-5 is 74.8. accuracy (%) of n/a top-10 is 76.2. accuracy (%) of n/a top-50 is 84.6. accuracy (%) of n/a top-1 is 67.2. accuracy (%) of n/a top-3 is 74.6. accuracy (%) of n/a top-5 is 77.7. accuracy (%) of n/a top-10 is 80.5. accuracy (%) of n/a top-50 is 88.1. accuracy (%) of n/a top-1 is 68.1. accuracy (%) of n/a top-3 is 75.5. accuracy (%) of n/a top-5 is 78.2. accuracy (%) of n/a top-10 is 80.9. accuracy (%) of n/a top-50 is 87.9.
table 2 shows mned performance (top-1, 5, 10 accuracies) on snapcaptionskb with varying qualities of kb embeddings. accuracy of trained with 1m entities top-1 is 68.1. accuracy of trained with 1m entities top-5 is 78.2. accuracy of trained with 1m entities top-10 is 80.9. accuracy of trained with 10k entities top-1 is 60.3. accuracy of trained with 10k entities top-5 is 72.5. accuracy of trained with 10k entities top-10 is 75.9. accuracy of random embeddings top-1 is 41.4. accuracy of random embeddings top-5 is 45.8. accuracy of random embeddings top-10 is 48.0.
table 1 shows ablation results on the validation set. r1 of - r1 is 49.2. r2 of - r2 is 18.9. r3 of - r3 is 9.8. r4 of - r4 is 6.0. rl of - rl is 43.8. avg. of - avg. is 25.5. r1 of - r1 is 53.3. r2 of - r2 is 19.7. r3 of - r3 is 10.4. r4 of - r4 is 6.4. rl of - rl is 47.2. avg. of - avg. is 27.4. r1 of - r1 is 55.0. r2 of - r2 is 21.6. r3 of - r3 is 11.7. r4 of - r4 is 7.5. rl of - rl is 48.9. avg. of - avg. is 28.9. r1 of - r1 is 55.3. r2 of - r2 is 21.3. r3 of - r3 is 11.4. r4 of - r4 is 7.2. rl of - rl is 49.0. avg. of - avg. is 28.8. r1 of - r1 is 54.8. r2 of - r2 is 21.1. r3 of - r3 is 11.3. r4 of - r4 is 7.2. rl of - rl is 48.6. avg. of - avg. is 28.6. r1 of title+caption r1 is 55.4. r2 of title+caption r2 is 21.8. r3 of title+caption r3 is 11.8. r4 of title+caption r4 is 7.5. rl of title+caption rl is 49.2. avg. of title+caption avg. is 29.2. r1 of title+fs r1 is 55.1. r2 of title+fs r2 is 21.6. r3 of title+fs r3 is 11.6. r4 of title+fs r4 is 7.4. rl of title+fs rl is 48.9. avg. of title+fs avg. is 28.9. r1 of caption+fs r1 is 55.3. r2 of caption+fs r2 is 21.5. r3 of caption+fs r3 is 11.5. r4 of caption+fs r4 is 7.3. rl of caption+fs rl is 49.0. avg. of caption+fs avg. is 28.9. r1 of title+caption+fs r1 is 55.4. r2 of title+caption+fs r2 is 21.5. r3 of title+caption+fs r3 is 11.6. r4 of title+caption+fs r4 is 7.4. rl of title+caption+fs rl is 49.1. avg. of title+caption+fs avg. is 29.0.
table 4 shows results (in percentage) for answer selection comparing our approaches (bottom part) to baselines (top): ap-cnn (dos santos et al., 2016), abcnn (yin et al., 2016), l.d.c (wang and jiang, 2017), kv-memnn (miller et al., 2016), and compaggr, a state-of-the-art system by wang et al. acc of wrd cnt acc is 77.84. map of wrd cnt map is 27.50. mrr of wrd cnt mrr is 27.77. acc of wrd cnt acc is 51.05. map of wrd cnt map is 48.91. mrr of wrd cnt mrr is 49.24. acc of wrd cnt acc is 44.67. map of wrd cnt map is 46.48. mrr of wrd cnt mrr is 46.91. acc of wrd cnt acc is 20.16. map of wrd cnt map is 19.37. mrr of wrd cnt mrr is 19.51. acc of wgt wrd cnt acc is 78.43. map of wgt wrd cnt map is 28.10. mrr of wgt wrd cnt mrr is 28.38. acc of wgt wrd cnt acc is 49.79. map of wgt wrd cnt map is 50.99. mrr of wgt wrd cnt mrr is 51.32. acc of wgt wrd cnt acc is 45.24. map of wgt wrd cnt map is 48.20. mrr of wgt wrd cnt mrr is 48.64. acc of wgt wrd cnt acc is 20.50. map of wgt wrd cnt map is 20.06. mrr of wgt wrd cnt mrr is 20.23. map of ap-cnn map is 68.86. mrr of ap-cnn mrr is 69.57. map of abcnn map is 69.21. mrr of abcnn mrr is 71.08. map of l.d.c map is 70.58. mrr of l.d.c mrr is 72.26. map of kv-memnn map is 70.69. mrr of kv-memnn mrr is 72.65. acc of localisf acc is 79.50. map of localisf map is 27.78. mrr of localisf mrr is 28.05. acc of localisf acc is 49.79. map of localisf map is 49.57. mrr of localisf mrr is 50.11. acc of localisf acc is 44.69. map of localisf map is 48.40. mrr of localisf mrr is 46.48. acc of localisf acc is 20.21. map of localisf map is 20.22. mrr of localisf mrr is 20.39. acc of isf acc is 78.85. map of isf map is 28.09. mrr of isf mrr is 28.36. acc of isf acc is 48.52. map of isf map is 46.53. mrr of isf mrr is 46.72. acc of isf acc is 45.61. map of isf map is 48.57. mrr of isf mrr is 48.99. acc of isf acc is 20.52. map of isf map is 20.07. mrr of isf mrr is 20.23. acc of paircnn acc is 32.53. map of paircnn map is 46.34. mrr of paircnn mrr is 46.35. acc of paircnn acc is 32.49. map of paircnn map is 39.87. mrr of paircnn mrr is 38.71. acc of paircnn acc is 25.67. map of paircnn map is 40.16. mrr of paircnn mrr is 39.89. acc of paircnn acc is 14.92. map of paircnn map is 34.62. mrr of paircnn mrr is 35.14. acc of compaggr acc is 85.52. map of compaggr map is 91.05. mrr of compaggr mrr is 91.05. acc of compaggr acc is 60.76. map of compaggr map is 73.12. mrr of compaggr mrr is 74.06. acc of compaggr acc is 54.54. map of compaggr map is 67.63. mrr of compaggr mrr is 68.21. acc of compaggr acc is 32.05. map of compaggr map is 52.82. mrr of compaggr mrr is 53.43. acc of xnet acc is 35.50. map of xnet map is 58.46. mrr of xnet mrr is 58.84. acc of xnet acc is 54.43. map of xnet map is 69.12. mrr of xnet mrr is 70.22. acc of xnet acc is 26.18. map of xnet map is 42.28. mrr of xnet mrr is 42.43. acc of xnet acc is 15.45. map of xnet map is 35.42. mrr of xnet mrr is 35.97. acc of xnettopk acc is 36.09. map of xnettopk map is 59.70. mrr of xnettopk mrr is 59.32. acc of xnettopk acc is 55.00. map of xnettopk map is 68.66. mrr of xnettopk mrr is 70.24. acc of xnettopk acc is 29.41. map of xnettopk map is 46.69. mrr of xnettopk mrr is 46.97. acc of xnettopk acc is 17.04. map of xnettopk map is 37.60. mrr of xnettopk mrr is 38.16. acc of lrxnet acc is 85.63. map of lrxnet map is 91.10. mrr of lrxnet mrr is 91.85. acc of lrxnet acc is 63.29. map of lrxnet map is 76.57. mrr of lrxnet mrr is 75.10. acc of lrxnet acc is 55.17. map of lrxnet map is 68.92. mrr of lrxnet mrr is 68.43. acc of lrxnet acc is 32.92. map of lrxnet map is 31.15. mrr of lrxnet mrr is 30.41. acc of xnet+ acc is 79.39. map of xnet+ map is 87.32. mrr of xnet+ mrr is 88.00. acc of xnet+ acc is 57.08. map of xnet+ map is 70.25. mrr of xnet+ mrr is 71.28. acc of xnet+ acc is 47.23. map of xnet+ map is 61.81. mrr of xnet+ mrr is 61.42. acc of xnet+ acc is 23.07. map of xnet+ map is 42.88. mrr of xnet+ mrr is 43.42.
table 1 shows performance of our various models in an unsupervised setting (i.e., without labels or covariates) on the imdb dataset using a 5,000-word vocabulary and 50 topics. ppl. of lda ppl. is 1508. npmi (int.) of lda npmi (int.) is 0.13. npmi (ext.) of lda npmi (ext.) is 0.14. sparsity of lda sparsity is 0. ppl. of sage ppl. is 1767. npmi (int.) of sage npmi (int.) is 0.12. npmi (ext.) of sage npmi (ext.) is 0.12. sparsity of sage sparsity is 0.79. ppl. of nvdm ppl. is 1748. npmi (int.) of nvdm npmi (int.) is 0.06. npmi (ext.) of nvdm npmi (ext.) is 0.04. sparsity of nvdm sparsity is 0. ppl. of scholar - b.g. ppl. is 1889. npmi (int.) of scholar - b.g. npmi (int.) is 0.09. npmi (ext.) of scholar - b.g. npmi (ext.) is 0.13. sparsity of scholar - b.g. sparsity is 0. ppl. of scholar ppl. is 1905. npmi (int.) of scholar npmi (int.) is 0.14. npmi (ext.) of scholar npmi (ext.) is 0.13. sparsity of scholar sparsity is 0. ppl. of scholar + w.v. ppl. is 1991. npmi (int.) of scholar + w.v. npmi (int.) is 0.18. npmi (ext.) of scholar + w.v. npmi (ext.) is 0.17. sparsity of scholar + w.v. sparsity is 0. ppl. of scholar + reg. ppl. is 2185. npmi (int.) of scholar + reg. npmi (int.) is 0.10. npmi (ext.) of scholar + reg. npmi (ext.) is 0.12. sparsity of scholar + reg. sparsity is 0.58.
table 4 shows results for span detection on the dense development dataset. p of bio p is 69.0. r of bio r is 75.9. f of bio f is 72.2. p of bio p is 80.4. r of bio r is 86.0. f of bio f is 83.1. p of span (tau = 0.5) p is 81.7. r of span (tau = 0.5) r is 80.9. f of span (tau = 0.5) f is 81.3. p of span (tau = 0.5) p is 87.5. r of span (tau = 0.5) r is 84.2. f of span (tau = 0.5) f is 85.8. p of span (tau = tau*) p is 80.0. r of span (tau = tau*) r is 84.7. f of span (tau = tau*) f is 82.2. p of span (tau = tau*) p is 83.8. r of span (tau = tau*) r is 93.0. f of span (tau = tau*) f is 88.1.
table 5 shows question generation results on the dense development set. em of local em is 44.2. pm of local pm is 62.0. sa of local sa is 83.2. em of seq. em is 47.2. pm of seq. pm is 62.3. sa of seq. sa is 82.9.
table 4 shows results on the chinese test set. p of zhao et al. (2009a) p is 80.4. r of zhao et al. (2009a) r is 75.2. f1 of zhao et al. (2009a) f1 is 77.7. p of bjorkelund et al. (2009) p is 82.4. r of bjorkelund et al. (2009) r is 75.1. f1 of bjorkelund et al. (2009) f1 is 78.6. p of roth and lapata (2016) p is 83.2. r of roth and lapata (2016) r is 75.9. f1 of roth and lapata (2016) f1 is 79.4. p of marcheggiani and titov (2017) p is 84.6. r of marcheggiani and titov (2017) r is 80.4. f1 of marcheggiani and titov (2017) f1 is 82.5. p of ours p is 84.2. r of ours r is 81.5. f1 of ours f1 is 82.8. p of marcheggiani et al. (2017) p is 83.4. r of marcheggiani et al. (2017) r is 79.1. f1 of marcheggiani et al. (2017) f1 is 81.2. p of ours p is 84.5. r of ours r is 79.3. f1 of ours f1 is 81.8.
table 5 shows srl results without predicate sense. p of 1st-order p is 84.4. r of 1st-order r is 82.6. f1 of 1st-order f1 is 83.5. p of 2nd-order p is 84.8. r of 2nd-order r is 83.0. f1 of 2nd-order f1 is 83.9. p of 3rd-order p is 85.1. r of 3rd-order r is 83.3. f1 of 3rd-order f1 is 84.2. p of marcheggiani and titov (2017) p is 85.2. r of marcheggiani and titov (2017) r is 81.6. f1 of marcheggiani and titov (2017) f1 is 83.3.
table 9 shows results on english test set, in terms of labeled attachment score for syntactic dependencies (las), semantic precision (p), semantic recall (r), semantic labeled f1 score (sem-f1), the ratio semf1/las. las (%) of zhao et al. (2009c) [srl-only] las (%) is 86.0. sem-f1 (%) of zhao et al. (2009c) [srl-only] sem-f1 (%) is 85.4. sem-f1/las (%) of zhao et al. (2009c) [srl-only] sem-f1/las (%) is 99.3. las (%) of zhao et al. (2009a) [joint] las (%) is 89.2. sem-f1 (%) of zhao et al. (2009a) [joint] sem-f1 (%) is 86.2. sem-f1/las (%) of zhao et al. (2009a) [joint] sem-f1/las (%) is 96.6. las (%) of bjorkelund et al. (2010) las (%) is 89.8. p (%) of bjorkelund et al. (2010) p (%) is 87.1. r (%) of bjorkelund et al. (2010) r (%) is 84.5. sem-f1 (%) of bjorkelund et al. (2010) sem-f1 (%) is 85.8. sem-f1/las (%) of bjorkelund et al. (2010) sem-f1/las (%) is 95.6. las (%) of lei et al. (2015) las (%) is 90.4. sem-f1 (%) of lei et al. (2015) sem-f1 (%) is 86.6. sem-f1/las (%) of lei et al. (2015) sem-f1/las (%) is 95.8. las (%) of roth and lapata (2016) las (%) is 89.8. p (%) of roth and lapata (2016) p (%) is 88.1. r (%) of roth and lapata (2016) r (%) is 85.3. sem-f1 (%) of roth and lapata (2016) sem-f1 (%) is 86.7. sem-f1/las (%) of roth and lapata (2016) sem-f1/las (%) is 96.5. las (%) of marcheggiani and titov (2017) las (%) is 90.3*. p (%) of marcheggiani and titov (2017) p (%) is 89.1. r (%) of marcheggiani and titov (2017) r (%) is 86.8. sem-f1 (%) of marcheggiani and titov (2017) sem-f1 (%) is 88.0. sem-f1/las (%) of marcheggiani and titov (2017) sem-f1/las (%) is 97.5. las (%) of ours + conll-2009 predicted las (%) is 86.0. p (%) of ours + conll-2009 predicted p (%) is 89.7. r (%) of ours + conll-2009 predicted r (%) is 89.3. sem-f1 (%) of ours + conll-2009 predicted sem-f1 (%) is 89.5. sem-f1/las (%) of ours + conll-2009 predicted sem-f1/las (%) is 104.0. las (%) of ours + auto syntax las (%) is 90.0. p (%) of ours + auto syntax p (%) is 90.5. r (%) of ours + auto syntax r (%) is 89.3. sem-f1 (%) of ours + auto syntax sem-f1 (%) is 89.9. sem-f1/las (%) of ours + auto syntax sem-f1/las (%) is 99.9. las (%) of ours + gold syntax las (%) is 100. p (%) of ours + gold syntax p (%) is 91.0. r (%) of ours + gold syntax r (%) is 89.7. sem-f1 (%) of ours + gold syntax sem-f1 (%) is 90.3. sem-f1/las (%) of ours + gold syntax sem-f1/las (%) is 90.3.
table 1 shows ms-coco ’s test set evaluation measures. bleu-1 of - bleu-1 is 70.63. bleu-4 of - bleu-4 is 30.14. cider of - cider is 93.59. bleu-1 of - bleu-1 is 73.40. bleu-4 of - bleu-4 is 33.11. cider of - cider is 101.63. bleu-1 of - bleu-1 is 70.79. bleu-4 of - bleu-4 is 30.29. cider of - cider is 93.61. bleu-1 of - bleu-1 is 72.68. bleu-4 of - bleu-4 is 32.15. cider of - cider is 99.77. bleu-1 of - bleu-1 is 71.94. bleu-4 of - bleu-4 is 31.27. cider of - cider is 95.79. bleu-1 of - bleu-1 is 73.49. bleu-4 of - bleu-4 is 32.93. cider of - cider is 102.33. bleu-1 of - bleu-1 is 72.39. bleu-4 of - bleu-4 is 31.76. cider of - cider is 97.47. bleu-1 of - bleu-1 is 74.01. bleu-4 of - bleu-4 is 33.25. cider of - cider is 102.81. bleu-1 of v bleu-1 is 71.76. bleu-4 of v bleu-4 is 31.16. cider of v cider is 96.37. bleu-1 of v bleu-1 is 73.12. bleu-4 of v bleu-4 is 32.71. cider of v cider is 101.25. bleu-1 of vbatch bleu-1 is 71.46. bleu-4 of vbatch bleu-4 is 31.15. cider of vbatch cider is 96.53. bleu-1 of vbatch bleu-1 is 73.26. bleu-4 of vbatch bleu-4 is 32.73. cider of vbatch cider is 101.90. bleu-1 of vrefs bleu-1 is 71.80. bleu-4 of vrefs bleu-4 is 31.63. cider of vrefs cider is 96.22. bleu-1 of vrefs bleu-1 is 73.53. bleu-4 of vrefs bleu-4 is 32.59. cider of vrefs cider is 102.33. bleu-1 of v bleu-1 is 70.81. bleu-4 of v bleu-4 is 30.43. cider of v cider is 94.26. bleu-1 of v bleu-1 is 73.29. bleu-4 of v bleu-4 is 32.81. cider of v cider is 101.58. bleu-1 of vbatch bleu-1 is 71.85. bleu-4 of vbatch bleu-4 is 31.13. cider of vbatch cider is 96.65. bleu-1 of vbatch bleu-1 is 73.43. bleu-4 of vbatch bleu-4 is 32.95. cider of vbatch cider is 102.03. bleu-1 of vrefs bleu-1 is 71.96. bleu-4 of vrefs bleu-4 is 31.23. cider of vrefs cider is 95.34. bleu-1 of vrefs bleu-1 is 73.53. bleu-4 of vrefs bleu-4 is 33.09. cider of vrefs cider is 101.89. bleu-1 of v bleu-1 is 71.05. bleu-4 of v bleu-4 is 30.46. cider of v cider is 94.40. bleu-1 of v bleu-1 is 73.08. bleu-4 of v bleu-4 is 32.51. cider of v cider is 101.84. bleu-1 of vbatch bleu-1 is 71.51. bleu-4 of vbatch bleu-4 is 31.17. cider of vbatch cider is 95.78. bleu-1 of vbatch bleu-1 is 73.50. bleu-4 of vbatch bleu-4 is 33.04. cider of vbatch cider is 102.98. bleu-1 of vrefs bleu-1 is 71.93. bleu-4 of vrefs bleu-4 is 31.41. cider of vrefs cider is 96.81. bleu-1 of vrefs bleu-1 is 73.42. bleu-4 of vrefs bleu-4 is 32.91. cider of vrefs cider is 102.23. bleu-1 of v bleu-1 is 71.43. bleu-4 of v bleu-4 is 31.18. cider of v cider is 96.32. bleu-1 of v bleu-1 is 73.55. bleu-4 of v bleu-4 is 33.19. cider of v cider is 102.94. bleu-1 of vbatch bleu-1 is 71.47. bleu-4 of vbatch bleu-4 is 31.00. cider of vbatch cider is 95.56. bleu-1 of vbatch bleu-1 is 73.18. bleu-4 of vbatch bleu-4 is 32.60. cider of vbatch cider is 101.30. bleu-1 of vrefs bleu-1 is 71.82. bleu-4 of vrefs bleu-4 is 31.06. cider of vrefs cider is 95.66. bleu-1 of vrefs bleu-1 is 73.92. bleu-4 of vrefs bleu-4 is 33.10. cider of vrefs cider is 102.64. bleu-1 of v bleu-1 is 70.79. bleu-4 of v bleu-4 is 30.43. cider of v cider is 96.34. bleu-1 of v bleu-1 is 73.68. bleu-4 of v bleu-4 is 32.87. cider of v cider is 101.11. bleu-1 of vbatch bleu-1 is 72.28. bleu-4 of vbatch bleu-4 is 31.65. cider of vbatch cider is 96.73. bleu-1 of vbatch bleu-1 is 73.86. bleu-4 of vbatch bleu-4 is 33.32. cider of vbatch cider is 102.90. bleu-1 of vrefs bleu-1 is 72.69. bleu-4 of vrefs bleu-4 is 32.30. cider of vrefs cider is 98.01. bleu-1 of vrefs bleu-1 is 73.56. bleu-4 of vrefs bleu-4 is 33.00. cider of vrefs cider is 101.72. bleu-1 of v bleu-1 is 70.80. bleu-4 of v bleu-4 is 30.55. cider of v cider is 96.89. bleu-1 of v bleu-1 is 73.31. bleu-4 of v bleu-4 is 32.40. cider of v cider is 100.33. bleu-1 of vbatch bleu-1 is 72.13. bleu-4 of vbatch bleu-4 is 31.71. cider of vbatch cider is 96.92. bleu-1 of vbatch bleu-1 is 73.61. bleu-4 of vbatch bleu-4 is 32.67. cider of vbatch cider is 101.41. bleu-1 of vrefs bleu-1 is 73.08. bleu-4 of vrefs bleu-4 is 32.82. cider of vrefs cider is 99.92. bleu-1 of vrefs bleu-1 is 74.28. bleu-4 of vrefs bleu-4 is 33.34. cider of vrefs cider is 103.81.
table 3 shows test set regression evaluation for the clinical and scientific data. rmse of mean rmse is 1043.68. mae of mean mae is 294.95. mdae of mean mdae is 245.59. mape% of mean mape% is 2353.11. mdape% of mean mdape% is 409.47. mdae of mean mdae is ?10 20. mape% of mean mape% is ?10 23. mdape% of mean mdape% is ?10 22. rmse of median rmse is 1036.18. mae of median mae is 120.24. mdae of median mdae is 34.52. mape% of median mape% is 425.81. mdape% of median mdape% is 52.05. mdae of median mdae is 4.20. mape% of median mape% is 8039.15. mdape% of median mdape% is 98.65. rmse of softmax rmse is 997.84. mae of softmax mae is 80.29. mdae of softmax mdae is 12.70. mape% of softmax mape% is 621.78. mdape% of softmax mdape% is 22.41. mdae of softmax mdae is 3.00. mape% of softmax mape% is 1947.44. mdape% of softmax mdape% is 80.62. rmse of softmax+rnn rmse is 991.38. mae of softmax+rnn mae is 74.44. mdae of softmax+rnn mdae is 13.00. mape% of softmax+rnn mape% is 503.57. mdape% of softmax+rnn mdape% is 23.91. mdae of softmax+rnn mdae is 3.50. mape% of softmax+rnn mape% is 15208.37. mdape% of softmax+rnn mdape% is 80.00. rmse of h-softmax rmse is 1095.01. mae of h-softmax mae is 167.19. mdae of h-softmax mdae is 14.00. mape% of h-softmax mape% is 746.50. mdape% of h-softmax mdape% is 25.00. mdae of h-softmax mdae is 3.00. mape% of h-softmax mape% is 1652.21. mdape% of h-softmax mdape% is 80.00. rmse of h-softmax+rnn rmse is 1001.04. mae of h-softmax+rnn mae is 83.19. mdae of h-softmax+rnn mdae is 12.30. mape% of h-softmax+rnn mape% is 491.85. mdape% of h-softmax+rnn mdape% is 23.44. mdae of h-softmax+rnn mdae is 3.00. mape% of h-softmax+rnn mape% is 2703.49. mdape% of h-softmax+rnn mdape% is 80.00. rmse of d-rnn rmse is 1009.34. mae of d-rnn mae is 70.21. mdae of d-rnn mdae is 9.00. mape% of d-rnn mape% is 513.81. mdape% of d-rnn mdape% is 17.90. mdae of d-rnn mdae is 3.00. mape% of d-rnn mape% is 1287.27. mdape% of d-rnn mdape% is 52.45. rmse of mog rmse is 998.78. mae of mog mae is 57.11. mdae of mog mdae is 6.92. mape% of mog mape% is 348.10. mdape% of mog mdape% is 13.64. mdae of mog mdae is 2.10. mape% of mog mape% is 590.42. mdape% of mog mdape% is 90.00. rmse of combination rmse is 989.84. mae of combination mae is 69.47. mdae of combination mdae is 9.00. mape% of combination mape% is 552.06. mdape% of combination mdape% is 17.86. mdae of combination mdae is 3.00. mape% of combination mape% is 2332.50. mdape% of combination mdape% is 88.89.
table 1 shows results on test dataset for sick and msrpar semantic relatedness task. pearson r of illinois-lh (2014) pearson r is 0.7993. spearman rho of illinois-lh (2014) spearman rho is 0.7538. mse of illinois-lh (2014) mse is 0.3692. pearson r of unal-nlp (2014) pearson r is 0.8070. spearman rho of unal-nlp (2014) spearman rho is 0.7489. mse of unal-nlp (2014) mse is 0.3550. pearson r of meaning factory (2014) pearson r is 0.8268. spearman rho of meaning factory (2014) spearman rho is 0.7721. mse of meaning factory (2014) mse is 0.3224. pearson r of ecnu (2014) pearson r is 0.8414. pearson r of dependency tree-lstm (2015) pearson r is 0.8676 (0.0030). spearman rho of dependency tree-lstm (2015) spearman rho is 0.8083 (0.0042). mse of dependency tree-lstm (2015) mse is 0.2532 (0.0052). pearson r of decomp-attn (dependency) pearson r is 0.8239 (0.0120). spearman rho of decomp-attn (dependency) spearman rho is 0.7614 (0.0103). mse of decomp-attn (dependency) mse is 0.3326 (0.0223). pearson r of progressive-attn (dependency) pearson r is 0.8424 (0.0042). spearman rho of progressive-attn (dependency) spearman rho is 0.7733 (0.0066). mse of progressive-attn (dependency) mse is 0.2963 (0.0077). pearson r of constituency tree-lstm (2015) pearson r is 0.8582 (0.0038). spearman rho of constituency tree-lstm (2015) spearman rho is 0.7966 (0.0053). mse of constituency tree-lstm (2015) mse is 0.2734 (0.0108). pearson r of decomp-attn (constituency) pearson r is 0.7790 (0.0076). spearman rho of decomp-attn (constituency) spearman rho is 0.7074 (0.0091). mse of decomp-attn (constituency) mse is 0.4044 (0.0152). pearson r of progressive-attn (constituency) pearson r is 0.8625 (0.0032). spearman rho of progressive-attn (constituency) spearman rho is 0.7997 (0.0035). mse of progressive-attn (constituency) mse is 0.2610 (0.0057). pearson r of linear bi-lstm pearson r is 0.8398 (0.0020). spearman rho of linear bi-lstm spearman rho is 0.7782 (0.0041). mse of linear bi-lstm mse is 0.3024 (0.0044). pearson r of decomp-attn (linear) pearson r is 0.7899 (0.0055). spearman rho of decomp-attn (linear) spearman rho is 0.7173 (0.0097). mse of decomp-attn (linear) mse is 0.3897 (0.0115). pearson r of progressive-attn (linear) pearson r is 0.8550 (0.0017). spearman rho of progressive-attn (linear) spearman rho is 0.7873 (0.0020). mse of progressive-attn (linear) mse is 0.2761 (0.0038). pearson r of paragramphrase (2015) pearson r is 0.426. pearson r of projection (2015) pearson r is 0.437. pearson r of glove (2015) pearson r is 0.477. pearson r of psl (2015) pearson r is 0.416. pearson r of paragramphrase-xxl (2015) pearson r is 0.448. pearson r of dependency tree-lstm pearson r is 0.4921 (0.0112). spearman rho of dependency tree-lstm spearman rho is 0.4519 (0.0128). mse of dependency tree-lstm mse is 0.6611 (0.0219). pearson r of decomp-attn (dependency) pearson r is 0.4016 (0.0124). spearman rho of decomp-attn (dependency) spearman rho is 0.3310 (0.0118). mse of decomp-attn (dependency) mse is 0.7243 (0.0099). pearson r of progressive-attn (dependency) pearson r is 0.4727 (0.0112). spearman rho of progressive-attn (dependency) spearman rho is 0.4216 (0.0092). mse of progressive-attn (dependency) mse is 0.6823 (0.0159). pearson r of constituency tree-lstm pearson r is 0.3981 (0.0176). spearman rho of constituency tree-lstm spearman rho is 0.3150 (0.0204). mse of constituency tree-lstm mse is 0.7407 (0.0170). pearson r of decomp-attn (constituency) pearson r is 0.3991 (0.0147). spearman rho of decomp-attn (constituency) spearman rho is 0.3237 (0.0355). mse of decomp-attn (constituency) mse is 0.7220 (0.0185). pearson r of progressive-attn (constituency) pearson r is 0.5104 (0.0191). spearman rho of progressive-attn (constituency) spearman rho is 0.4764 (0.0112). mse of progressive-attn (constituency) mse is 0.6436 (0.0346). pearson r of linear bi-lstm pearson r is 0.3270 (0.0303). spearman rho of linear bi-lstm spearman rho is 0.2205 (0.0111). mse of linear bi-lstm mse is 0.8098 (0.0579). pearson r of decomp-attn (linear) pearson r is 0.3763 (0.0332). spearman rho of decomp-attn (linear) spearman rho is 0.3025 (0.0587). mse of decomp-attn (linear) mse is 0.7290 (0.0206). pearson r of progressive-attn (linear) pearson r is 0.4773 (0.0206). spearman rho of progressive-attn (linear) spearman rho is 0.4453 (0.0250). mse of progressive-attn (linear) mse is 0.6758 (0.0260).
table 2 shows results on test dataset for quora paraphrase detection task. accuracy of dependency tree-lstm accuracy is 0.7897 (0.0009). f-1 score (class=1) of dependency tree-lstm f-1 score (class=1) is 0.7060 (0.0050). precision (class=1) of dependency tree-lstm precision (class=1) is 0.7298 (0.0055). recall (class=1) of dependency tree-lstm recall (class=1) is 0.6840 (0.0139). accuracy of decomp-attn (dependency) accuracy is 0.7803 (0.0026). f-1 score (class=1) of decomp-attn (dependency) f-1 score (class=1) is 0.6977 (0.0074). precision (class=1) of decomp-attn (dependency) precision (class=1) is 0.7095 (0.0083). recall (class=1) of decomp-attn (dependency) recall (class=1) is 0.6866 (0.0199). accuracy of progressive-attn (dependency) accuracy is 0.7896 (0.0025). f-1 score (class=1) of progressive-attn (dependency) f-1 score (class=1) is 0.7113 (0.0087). precision (class=1) of progressive-attn (dependency) precision (class=1) is 0.7214 (0.0117). recall (class=1) of progressive-attn (dependency) recall (class=1) is 0.7025 (0.0266). accuracy of constituency tree-lstm accuracy is 0.7881 (0.0042). f-1 score (class=1) of constituency tree-lstm f-1 score (class=1) is 0.7065 (0.0034). precision (class=1) of constituency tree-lstm precision (class=1) is 0.7192 (0.0216). recall (class=1) of constituency tree-lstm recall (class=1) is 0.6846 (0.0380). accuracy of decomp-attn (constituency) accuracy is 0.7776 (0.0004). f-1 score (class=1) of decomp-attn (constituency) f-1 score (class=1) is 0.6942 (0.0050). precision (class=1) of decomp-attn (constituency) precision (class=1) is 0.7055 (0.0069). recall (class=1) of decomp-attn (constituency) recall (class=1) is 0.6836 (0.0164). accuracy of progressive-attn (constituency) accuracy is 0.7956 (0.0020). f-1 score (class=1) of progressive-attn (constituency) f-1 score (class=1) is 0.7192 (0.0024). precision (class=1) of progressive-attn (constituency) precision (class=1) is 0.7300 (0.0079). recall (class=1) of progressive-attn (constituency) recall (class=1) is 0.7089 (0.0104). accuracy of linear bi-lstm accuracy is 0.7859 (0.0024). f-1 score (class=1) of linear bi-lstm f-1 score (class=1) is 0.7097 (0.0047). precision (class=1) of linear bi-lstm precision (class=1) is 0.7112 (0.0129). recall (class=1) of linear bi-lstm recall (class=1) is 0.7089 (0.0219). accuracy of decomp-attn (linear) accuracy is 0.7861 (0.0034). f-1 score (class=1) of decomp-attn (linear) f-1 score (class=1) is 0.7074 (0.0109). precision (class=1) of decomp-attn (linear) precision (class=1) is 0.7151 (0.0135). recall (class=1) of decomp-attn (linear) recall (class=1) is 0.7010 (0.0315). accuracy of progressive-attn (linear) accuracy is 0.7949 (0.0031). f-1 score (class=1) of progressive-attn (linear) f-1 score (class=1) is 0.7182 (0.0162). precision (class=1) of progressive-attn (linear) precision (class=1) is 0.7298 (0.0115). recall (class=1) of progressive-attn (linear) recall (class=1) is 0.7092 (0.0469).
table 6 shows performance on various types using justice subtypes for training accuracy of sentence 1 is 68.3. accuracy of sentence 3 is 68.3. accuracy of sentence 5 is 69.5. accuracy of appeal 1 is 67.5. accuracy of appeal 3 is 97.5. accuracy of appeal 5 is 97.5. accuracy of release-parole 1 is 73.9. accuracy of release-parole 3 is 73.9. accuracy of release-parole 5 is 73.9. accuracy of attack 1 is 26.5. accuracy of attack 3 is 44.5. accuracy of attack 5 is 46.7. accuracy of transfer-money 1 is 48.4. accuracy of transfer-money 3 is 68.9. accuracy of transfer-money 5 is 79.5. accuracy of start-org 1 is 0. accuracy of start-org 3 is 33.3. accuracy of start-org 5 is 66.7. accuracy of transport 1 is 2.6. accuracy of transport 3 is 3.7. accuracy of transport 5 is 7.8. accuracy of end-position 1 is 9.1. accuracy of end-position 3 is 50.4. accuracy of end-position 5 is 53.7. accuracy of phone-write 1 is 60.8. accuracy of phone-write 3 is 88.2. accuracy of phone-write 5 is 90.2. accuracy of injure 1 is 87.6. accuracy of injure 3 is 91.0. accuracy of injure 5 is 91.0.
table 7 shows event trigger and argument extraction performance (%) on unseen ace types. p of supervised lstm p is 94.7. r of supervised lstm r is 41.8. f of supervised lstm f is 58.0. p of supervised lstm p is 89.4. r of supervised lstm r is 39.5. f of supervised lstm f is 54.8. p of supervised lstm p is 47.8. r of supervised lstm r is 22.6. f of supervised lstm f is 30.6. p of supervised lstm p is 28.9. r of supervised lstm r is 13.7. f of supervised lstm f is 18.6. p of supervised joint p is 55.8. r of supervised joint r is 67.4. f of supervised joint f is 61.1. p of supervised joint p is 50.6. r of supervised joint r is 61.2. f of supervised joint f is 55.4. p of supervised joint p is 36.4. r of supervised joint r is 28.1. f of supervised joint f is 31.7. p of supervised joint p is 33.3. r of supervised joint r is 25.7. f of supervised joint f is 29.0. p of transfer p is 85.7. r of transfer r is 41.2. f of transfer f is 55.6. p of transfer p is 75.5. r of transfer r is 36.3. f of transfer f is 49.1. p of transfer p is 28.2. r of transfer r is 27.3. f of transfer f is 27.8. p of transfer p is 16.1. r of transfer r is 15.6. f of transfer f is 15.8.
table 2 shows comparisons with different baselines. as of crosscrf as is 19.72. op of crosscrf op is 59.2. as of crosscrf as is 21.07. op of crosscrf op is 52.05. as of crosscrf as is 28.19. op of crosscrf op is 65.52. as of crosscrf as is 29.96. op of crosscrf op is 56.17. as of crosscrf as is 6.59. op of crosscrf op is 39.38. as of crosscrf as is 24.22. op of crosscrf op is 46.67. as of crosscrf as is -1.82. op of crosscrf op is -1.34. as of crosscrf as is -0.44. op of crosscrf op is -1.67. as of crosscrf as is -0.58. op of crosscrf op is -0.89. as of crosscrf as is -1.69. op of crosscrf op is -1.49. as of crosscrf as is -0.49. op of crosscrf op is -3.06. as of crosscrf as is -2.54. op of crosscrf op is -2.43. as of rap as is 25.92. op of rap op is 62.72. as of rap as is 22.63. op of rap op is 54.44. as of rap as is 46.9. op of rap op is 67.98. as of rap as is 34.54. op of rap op is 54.25. as of rap as is 45.44. op of rap op is 60.67. as of rap as is 28.22. op of rap op is 59.79. as of rap as is -2.75. op of rap op is -0.49. as of rap as is -0.52. op of rap op is -2.2. as of rap as is -1.64. op of rap op is -1.05. as of rap as is -0.64. op of rap op is -1.65. as of rap as is -1.61. op of rap op is -2.15. as of rap as is -2.42. op of rap op is -4.18. as of hier-joint as is 33.66. as of hier-joint as is 33.2. as of hier-joint as is 48.1. as of hier-joint as is 31.25. as of hier-joint as is 47.97. as of hier-joint as is 34.74. as of hier-joint as is -1.47. as of hier-joint as is -0.52. as of hier-joint as is -1.45. as of hier-joint as is -0.49. as of hier-joint as is -0.46. as of hier-joint as is -2.27. op of hier-joint op is . as of rncrf as is 24.26. op of rncrf op is 60.86. as of rncrf as is 24.31. op of rncrf op is 51.28. as of rncrf as is 40.88. op of rncrf op is 66.5. as of rncrf as is 31.52. op of rncrf op is 55.85. as of rncrf as is 34.59. op of rncrf op is 63.89. as of rncrf as is 40.59. op of rncrf op is 60.17. as of rncrf as is -3.97. op of rncrf op is -3.35. as of rncrf as is -2.57. op of rncrf op is -1.78. as of rncrf as is -2.09. op of rncrf op is -1.48. as of rncrf as is -1.4. op of rncrf op is -1.09. as of rncrf as is -1.34. op of rncrf op is -1.59. as of rncrf as is -0.8. op of rncrf op is -1.2. as of rngru as is 24.23. op of rngru op is 60.65. as of rngru as is 20.49. op of rngru op is 52.28. as of rngru as is 39.78. op of rngru op is 62.99. as of rngru as is 32.51. op of rngru op is 52.24. as of rngru as is 38.15. op of rngru op is 64.21. as of rngru as is 39.44. op of rngru op is 60.85. as of rngru as is -2.41. op of rngru op is -1.04. as of rngru as is -2.68. op of rngru op is -2.69. as of rngru as is -0.61. op of rngru op is -0.95. as of rngru as is -1.12. op of rngru op is -2.37. as of rngru as is -2.82. op of rngru op is -1.11. as of rngru as is -2.79. op of rngru op is -1.25. as of rnscn-crf as is 35.26. op of rnscn-crf op is 61.67. as of rnscn-crf as is 32. op of rnscn-crf op is 52.81. as of rnscn-crf as is 53.38. op of rnscn-crf op is 67.6. as of rnscn-crf as is 34.63. op of rnscn-crf op is 56.22. as of rnscn-crf as is 48.13. op of rnscn-crf op is 65.06. as of rnscn-crf as is 46.71. op of rnscn-crf op is 61.88. as of rnscn-crf as is -1.31. op of rnscn-crf op is -1.35. as of rnscn-crf as is -1.48. op of rnscn-crf op is -1.29. as of rnscn-crf as is -1.49. op of rnscn-crf op is -0.99. as of rnscn-crf as is -1.38. op of rnscn-crf op is -1.1. as of rnscn-crf as is -0.71. op of rnscn-crf op is -0.66. as of rnscn-crf as is -1.16. op of rnscn-crf op is -1.52. as of rnscn-gru as is 37.77. op of rnscn-gru op is 62.35. as of rnscn-gru as is 33.02. op of rnscn-gru op is 57.54. as of rnscn-gru as is 53.18. op of rnscn-gru op is 71.44. as of rnscn-gru as is 35.65. op of rnscn-gru op is 60.02. as of rnscn-gru as is 49.62. op of rnscn-gru op is 69.42. as of rnscn-gru as is 45.92. op of rnscn-gru op is 63.85. as of rnscn-gru as is -0.45. op of rnscn-gru op is -1.85. as of rnscn-gru as is -0.58. op of rnscn-gru op is -1.27. as of rnscn-gru as is -0.75. op of rnscn-gru op is -0.97. as of rnscn-gru as is -0.77. op of rnscn-gru op is -0.8. as of rnscn-gru as is -0.34. op of rnscn-gru op is -2.27. as of rnscn-gru as is -1.14. op of rnscn-gru op is -1.97. as of rnscn+-gru as is 40.43. op of rnscn+-gru op is 65.85. as of rnscn+-gru as is 35.1. op of rnscn+-gru op is 60.17. as of rnscn+-gru as is 52.91. op of rnscn+-gru op is 72.51. as of rnscn+-gru as is 40.42. op of rnscn+-gru op is 61.15. as of rnscn+-gru as is 48.36. op of rnscn+-gru op is 73.75. as of rnscn+-gru as is 51.14. op of rnscn+-gru op is 71.18. as of rnscn+-gru as is -0.96. op of rnscn+-gru op is -1.5. as of rnscn+-gru as is -0.62. op of rnscn+-gru op is -0.75. as of rnscn+-gru as is -1.82. op of rnscn+-gru op is -1.03. as of rnscn+-gru as is -0.7. op of rnscn+-gru op is -0.6. as of rnscn+-gru as is -1.14. op of rnscn+-gru op is -1.76. as of rnscn+-gru as is -1.68. op of rnscn+-gru op is -1.58.
table 4 shows human evaluation of various persona-chat models, along with a comparison to human performance, and twitter and opensubtitles based models (last 4 rows), standard deviation in parenthesis. fluency of self fluency is 4.31(1.07). engagingness of self engagingness is 4.25(1.06). consistency of self consistency is 4.36(0.92). persona detection of self persona detection is 0.95(0.22). fluency of none fluency is 3.17(1.10). engagingness of none engagingness is 3.18(1.41). consistency of none consistency is 2.98(1.45). persona detection of none persona detection is 0.51(0.50). fluency of self fluency is 3.08(1.40). engagingness of self engagingness is 3.13(1.39). consistency of self consistency is 3.14(1.26). persona detection of self persona detection is 0.72(0.45). fluency of none fluency is 3.81(1.14). engagingness of none engagingness is 3.88(0.98). consistency of none consistency is 3.36(1.37). persona detection of none persona detection is 0.59(0.49). fluency of self fluency is 3.97(0.94). engagingness of self engagingness is 3.50(1.17). consistency of self consistency is 3.44(1.30). persona detection of self persona detection is 0.81(0.39). fluency of none fluency is 3.21(1.54). engagingness of none engagingness is 1.75(1.04). consistency of none consistency is 1.95(1.22). persona detection of none persona detection is 0.57(0.50). fluency of none fluency is 2.85(1.46). engagingness of none engagingness is 2.13(1.07). consistency of none consistency is 2.15(1.08). persona detection of none persona detection is 0.35(0.48). fluency of none fluency is 2.25(1.37). engagingness of none engagingness is 2.12(1.33). consistency of none consistency is 1.96(1.22). persona detection of none persona detection is 0.38(0.49). fluency of none fluency is 2.14(1.20). engagingness of none engagingness is 2.22(1.22). consistency of none consistency is 2.06(1.29). persona detection of none persona detection is 0.42(0.49).
table 3 shows comparison of the training and testing speeds between tfn and lmf. training speed (ips) of tfn training speed (ips) is 340.74. testing speed (ips) of tfn testing speed (ips) is 1177.17. training speed (ips) of lmf training speed (ips) is 1134.82. testing speed (ips) of lmf testing speed (ips) is 2249.90.
table 1 shows performance of our approach on storycloze task from mostafazadeh et al. accuracy of sequential cg + unigram mixture accuracy is 0.602. accuracy of sequential cg + brown clustering accuracy is 0.593. accuracy of sequential cg + sentiment accuracy is 0.581. accuracy of sequential cg accuracy is 0.589. accuracy of sequential cg (unnormalized) accuracy is 0.531. accuracy of - accuracy is 0.585. accuracy of - accuracy is 0.539. accuracy of - accuracy is 0.552. accuracy of - accuracy is 0.494. accuracy of - accuracy is 0.494.
table 4 shows results of correcting lines in the rdd newspapers and tcp books with multiple witnesses when decoding with different strategies using the same supervised model. cer of none cer is 0.15149. lcer of none lcer is 0.04717. wer of none wer is 0.37111. lwer of none lwer is 0.13799. cer of none cer is 0.10590. lcer of none lcer is 0.07666. wer of none wer is 0.30549. lwer of none lwer is 0.23495. cer of single cer is 0.07199. lcer of single lcer is 0.03300. wer of single wer is 0.14906. lwer of single lwer is 0.06948. cer of single cer is 0.04508. lcer of single lcer is 0.01407. wer of single wer is 0.11283. lwer of single lwer is 0.03392. cer of flat cer is 0.07238. lcer of flat lcer is 0.02904*. wer of flat wer is 0.15818. lwer of flat lwer is 0.06241*. cer of flat cer is 0.05554. lcer of flat lcer is 0.01727. wer of flat wer is 0.13487. lwer of flat lwer is 0.04079. cer of weighted cer is 0.06882*. lcer of weighted lcer is 0.02145*. wer of weighted wer is 0.15221. lwer of weighted lwer is 0.05375. cer of weighted cer is 0.05516. lcer of weighted lcer is 0.01392*. wer of weighted wer is 0.1330. lwer of weighted lwer is 0.03669. cer of average cer is 0.04210*. lcer of average lcer is 0.01399*. wer of average wer is 0.09397. lwer of average lwer is 0.02863*. cer of average cer is 0.04072*. lcer of average lcer is 0.01021*. wer of average wer is 0.09786*. lwer of average lwer is 0.02092*.
table 5 shows results from model trained under different settings on single-input decoding and multiple-input decoding for both the rdd newspapers and tcp books. cer of none cer is 0.18133. lcer of none lcer is 0.13552. wer of none wer is 0.41780. lwer of none lwer is 0.31544. cer of none cer is 0.10670. lcer of none lcer is 0.08800. wer of none wer is 0.31734. lwer of none lwer is 0.27227. cer of seq2seq-super cer is 0.09044. lcer of seq2seq-super lcer is 0.04469. wer of seq2seq-super wer is 0.17812. lwer of seq2seq-super lwer is 0.09063. cer of seq2seq-super cer is 0.04944. lcer of seq2seq-super lcer is 0.01498. wer of seq2seq-super wer is 0.12186. lwer of seq2seq-super lwer is 0.03500. cer of seq2seq-noisy cer is 0.10524. lcer of seq2seq-noisy lcer is 0.05565. wer of seq2seq-noisy wer is 0.20600. lwer of seq2seq-noisy lwer is 0.11416. cer of seq2seq-noisy cer is 0.08704. lcer of seq2seq-noisy lcer is 0.05889. wer of seq2seq-noisy wer is 0.25994. lwer of seq2seq-noisy lwer is 0.15725. cer of seq2seq-syn cer is 0.16136. lcer of seq2seq-syn lcer is 0.11986. wer of seq2seq-syn wer is 0.35802. lwer of seq2seq-syn lwer is 0.26547. cer of seq2seq-syn cer is 0.09551. lcer of seq2seq-syn lcer is 0.06160. wer of seq2seq-syn wer is 0.27845. lwer of seq2seq-syn lwer is 0.18221. cer of seq2seq-boots cer is 0.11037. lcer of seq2seq-boots lcer is 0.06149. wer of seq2seq-boots wer is 0.22750. lwer of seq2seq-boots lwer is 0.13123. cer of seq2seq-boots cer is 0.07196. lcer of seq2seq-boots lcer is 0.03684. wer of seq2seq-boots wer is 0.21711. lwer of seq2seq-boots lwer is 0.11233. cer of lmr cer is 0.15507. lcer of lmr lcer is 0.13552. wer of lmr wer is 0.34653. lwer of lmr lwer is 0.31544. cer of lmr cer is 0.10862. lcer of lmr lcer is 0.08800. wer of lmr wer is 0.33983. lwer of lmr lwer is 0.27227. cer of majority vote cer is 0.16285. lcer of majority vote lcer is 0.13552. wer of majority vote wer is 0.40063. lwer of majority vote lwer is 0.31544. cer of majority vote cer is 0.11096. lcer of majority vote lcer is 0.08800. wer of majority vote wer is 0.34151. lwer of majority vote lwer is 0.27227. cer of seq2seq-super cer is 0.07731. lcer of seq2seq-super lcer is 0.03634. wer of seq2seq-super wer is 0.15393. lwer of seq2seq-super lwer is 0.07269. cer of seq2seq-super cer is 0.04668. lcer of seq2seq-super lcer is 0.01252. wer of seq2seq-super wer is 0.11236. lwer of seq2seq-super lwer is 0.02667. cer of seq2seq-noisy cer is 0.09203*. lcer of seq2seq-noisy lcer is 0.04554*. wer of seq2seq-noisy wer is 0.17940. lwer of seq2seq-noisy lwer is 0.09269. cer of seq2seq-noisy cer is 0.08317. lcer of seq2seq-noisy lcer is 0.05588. wer of seq2seq-noisy wer is 0.24824. lwer of seq2seq-noisy lwer is 0.14885. cer of seq2seq-syn cer is 0.12948. lcer of seq2seq-syn lcer is 0.09112. wer of seq2seq-syn wer is 0.28901. lwer of seq2seq-syn lwer is 0.19977. cer of seq2seq-syn cer is 0.08506. lcer of seq2seq-syn lcer is 0.05002. wer of seq2seq-syn wer is 0.24942. lwer of seq2seq-syn lwer is 0.15169. cer of seq2seq-boots cer is 0.09435. lcer of seq2seq-boots lcer is 0.04976. wer of seq2seq-boots wer is 0.19681. lwer of seq2seq-boots lwer is 0.10604. cer of seq2seq-boots cer is 0.06824*. lcer of seq2seq-boots lcer is 0.03343*. wer of seq2seq-boots wer is 0.20325*. lwer of seq2seq-boots lwer is 0.09995*.
table 1 shows comparing the performance of recipe generation task. vocabulary size of original vocabulary size is 52,472. perplexity of original perplexity is 20.23. vocabulary size of modified type vocabulary size is 51,675. perplexity of modified type perplexity is 17.62. vocabulary size of original vocabulary size is 52,472. perplexity of original perplexity is 18.23. vocabulary size of original vocabulary size is 52,472. perplexity of original perplexity is 9.67.
table 2 shows comparing the performance of code generation task. vocabulary size of original vocabulary size is 38,297. perplexity of original perplexity is 3.40. vocabulary size of original vocabulary size is 38,297. perplexity of original perplexity is 21.97. vocabulary size of modified type vocabulary size is 14,177. perplexity of modified type perplexity is 7.94. vocabulary size of original vocabulary size is 38,297. perplexity of original perplexity is 20.05. vocabulary size of original vocabulary size is 38,297. perplexity of original perplexity is 12.52. vocabulary size of original vocabulary size is 38,297. perplexity of original perplexity is 7.19. vocabulary size of modified type vocabulary size is 14,177. perplexity of modified type perplexity is 2.58. vocabulary size of original vocabulary size is 38,297. perplexity of original perplexity is 6.11. vocabulary size of original vocabulary size is 38,297. perplexity of original perplexity is 2.65.
table 7 shows dblp results evaluated on 63,342 citation contexts with newcomer ground-truth. rec of no rec is 3.64. map of no map is 3.23. mrr of no mrr is 3.41. ndcg of no ndcg is 2.73. rec of no rec is 1.37. map of no map is 1.13. mrr of no mrr is 1.15. ndcg of no ndcg is 0.92. rec of yes rec is 6.48. map of yes map is 3.52. mrr of yes mrr is 3.54. ndcg of yes ndcg is 3.96. rec of yes rec is 8.16. map of yes map is 5.13. mrr of yes mrr is 5.24. ndcg of yes ndcg is 5.21. rec of yes rec is 6.41. map of yes map is 4.95. mrr of yes mrr is 5.21. ndcg of yes ndcg is 4.49.
table 2 shows evaluation results. auc of semaxis auc is 92.2. ternary f1 of semaxis ternary f1 is 61. tau of semaxis tau is 0.48. auc of densifier auc is 91. ternary f1 of densifier ternary f1 is 58.2. tau of densifier tau is 0.46. auc of sentprop auc is 88.4. ternary f1 of sentprop ternary f1 is 56.1. tau of sentprop tau is 0.41. auc of wordnet auc is 89.5. ternary f1 of wordnet ternary f1 is 58.7. tau of wordnet tau is 0.34. auc of semaxis auc is 90. ternary f1 of semaxis ternary f1 is 59.2. tau of semaxis tau is 0.57. auc of densifier auc is 88.5. ternary f1 of densifier ternary f1 is 58.8. tau of densifier tau is 0.55. auc of sentprop auc is 85. ternary f1 of sentprop ternary f1 is 58.2. tau of sentprop tau is 0.5. auc of sentiment140 auc is 86.2. ternary f1 of sentiment140 ternary f1 is 57.7. tau of sentiment140 tau is 0.51.
table 1 shows results of the end-to-end taxonomy induction experiment. p a of taxi p a is 66.1. r a of taxi r a is 13.9. f1 a of taxi f1 a is 23.0. p e of taxi p e is 54.8. r e of taxi r e is 18.0. f1 e of taxi f1 e is 27.1. p a of hypenet p a is 32.8. r a of hypenet r a is 26.7. f1 a of hypenet f1 a is 29.4. p e of hypenet p e is 26.1. r e of hypenet r e is 17.2. f1 e of hypenet f1 e is 20.7. p a of hypenet+mst p a is 33.7. r a of hypenet+mst r a is 41.1. f1 a of hypenet+mst f1 a is 37.0. p e of hypenet+mst p e is 29.2. r e of hypenet+mst r e is 29.2. f1 e of hypenet+mst f1 e is 29.2. p a of taxorl (re) p a is 35.8. r a of taxorl (re) r a is 47.4. f1 a of taxorl (re) f1 a is 40.8. p e of taxorl (re) p e is 35.4. r e of taxorl (re) r e is 35.4. f1 e of taxorl (re) f1 e is 35.4. p a of taxorl (nr) p a is 41.3. r a of taxorl (nr) r a is 49.2. f1 a of taxorl (nr) f1 a is 44.9. p e of taxorl (nr) p e is 35.6. r e of taxorl (nr) r e is 35.6. f1 e of taxorl (nr) f1 e is 35.6. p a of bansal et al. (2014) p a is 48.0. r a of bansal et al. (2014) r a is 55.2. f1 a of bansal et al. (2014) f1 a is 51.4. p a of taxorl (nr) + fg p a is 52.9. r a of taxorl (nr) + fg r a is 58.6. f1 a of taxorl (nr) + fg f1 a is 55.6. p e of taxorl (nr) + fg p e is 43.8. r e of taxorl (nr) + fg r e is 43.8. f1 e of taxorl (nr) + fg f1 e is 43.8.
table 2 shows results of lexicon term sentiment classification. f1 of embeddingp hl is 0.740. f1 of embeddingp mpqa is 0.733. f1 of embeddingp hl is 0.742. f1 of embeddingp mpqa is 0.734. f1 of embeddingp hl is 0.747. f1 of embeddingp mpqa is 0.735. f1 of embeddingp hl is 0.744. f1 of embeddingp mpqa is 0.701. f1 of embeddingp hl is 0.745. f1 of embeddingp mpqa is 0.709. f1 of embeddingp hl is 0.628. f1 of embeddingp mpqa is 0.574. f1 of embeddingq hl is 0.743. f1 of embeddingq mpqa is 0.701. f1 of embeddingq hl is 0.627. f1 of embeddingq mpqa is 0.573. f1 of embeddingq hl is 0.464. f1 of embeddingq mpqa is 0.453. f1 of embeddingq hl is 0.621. f1 of embeddingq mpqa is 0.577. f1 of embeddingq hl is 0.462. f1 of embeddingq mpqa is 0.450. f1 of embeddingq hl is 0.465. f1 of embeddingq mpqa is 0.453. f1 of embeddingcat hl is 0.780. f1 of embeddingcat mpqa is 0.772. f1 of embeddingcat hl is 0.773. f1 of embeddingcat mpqa is 0.756. f1 of embeddingcat hl is 0.772. f1 of embeddingcat mpqa is 0.751. f1 of embeddingcat hl is 0.744. f1 of embeddingcat mpqa is 0.728. f1 of embeddingcat hl is 0.755. f1 of embeddingcat mpqa is 0.702. f1 of embeddingcat hl is 0.683. f1 of embeddingcat mpqa is 0.639. f1 of embeddingall hl is 0.777. f1 of embeddingall mpqa is 0.769. f1 of embeddingall hl is 0.773. f1 of embeddingall mpqa is 0.730. f1 of embeddingall hl is 0.762. f1 of embeddingall mpqa is 0.760. f1 of embeddingall hl is 0.712. f1 of embeddingall mpqa is 0.707. f1 of embeddingall hl is 0.749. f1 of embeddingall mpqa is 0.724. f1 of embeddingall hl is 0.670. f1 of embeddingall mpqa is 0.658. f1 of yang hl is 0.780. f1 of yang mpqa is 0.775. f1 of yang hl is 0.789. f1 of yang mpqa is 0.762. f1 of yang hl is 0.781. f1 of yang mpqa is 0.770. f1 of yang hl is 0.762. f1 of yang mpqa is 0.736. f1 of yang hl is 0.756. f1 of yang mpqa is 0.713. f1 of yang hl is 0.634. f1 of yang mpqa is 0.614. f1 of sswe hl is 0.816. f1 of sswe mpqa is 0.801. f1 of sswe hl is 0.831. f1 of sswe mpqa is 0.817. f1 of sswe hl is 0.822. f1 of sswe mpqa is 0.808. f1 of sswe hl is 0.826. f1 of sswe mpqa is 0.785. f1 of sswe hl is 0.784. f1 of sswe mpqa is 0.772. f1 of sswe hl is 0.707. f1 of sswe mpqa is 0.659. f1 of dse hl is 0.802. f1 of dse mpqa is 0.788. f1 of dse hl is 0.833. f1 of dse mpqa is 0.828. f1 of dse hl is 0.832. f1 of dse mpqa is 0.799. f1 of dse hl is 0.804. f1 of dse mpqa is 0.797. f1 of dse hl is 0.796. f1 of dse mpqa is 0.786. f1 of dse hl is 0.725. f1 of dse mpqa is 0.683.
table 2 shows our results are consistently better than those reported by kiela et al. mrr of # words 500 mrr is 0.658. top 1 of # words 500 top 1 is 0.567. top 5 of # words 500 top 5 is 0.692. top 20 of # words 500 top 20 is 0.774. mrr of # words 500 mrr is 0.704. top 1 of # words 500 top 1 is 0.679. top 5 of # words 500 top 5 is 0.763. top 20 of # words 500 top 20 is 0.811. mrr of # words 8500 mrr is 0.277. top 1 of # words 8500 top 1 is 0.229. top 5 of # words 8500 top 5 is 0.326. top 20 of # words 8500 top 20 is 0.385.
table 2 shows results for xpos tags. accuracy of cs_cac conll winner is 95.16. accuracy of cs_cac dqm is 95.16. accuracy of cs_cac ours is 96.91. accuracy of cs conll winner is 95.86. accuracy of cs dqm is 95.86. accuracy of cs ours is 97.28. accuracy of fi conll winner is 97.37. accuracy of fi dqm is 97.37. accuracy of fi ours is 97.81. accuracy of sl conll winner is 94.74. accuracy of sl dqm is 94.74. accuracy of sl ours is 95.54. accuracy of la_ittb conll winner is 94.79. accuracy of la_ittb dqm is 94.79. accuracy of la_ittb ours is 95.56. accuracy of grc conll winner is 84.47. accuracy of grc dqm is 84.47. accuracy of grc ours is 86.51. accuracy of bg conll winner is 96.71. accuracy of bg dqm is 96.71. accuracy of bg ours is 97.05. accuracy of ca conll winner is 98.58. accuracy of ca dqm is 98.58. accuracy of ca ours is 98.72. accuracy of grc_proiel conll winner is 97.51. accuracy of grc_proiel dqm is 97.51. accuracy of grc_proiel ours is 97.72. accuracy of pt conll winner is 83.04. accuracy of pt dqm is 83.04. accuracy of pt ours is 84.39. accuracy of cu conll winner is 96.20. accuracy of cu dqm is 96.20. accuracy of cu ours is 96.49. accuracy of it conll winner is 97.93. accuracy of it dqm is 97.93. accuracy of it ours is 98.08. accuracy of fa conll winner is 97.12. accuracy of fa dqm is 97.12. accuracy of fa ours is 97.32. accuracy of ru conll winner is 96.73. accuracy of ru dqm is 96.73. accuracy of ru ours is 96.95. accuracy of sv conll winner is 96.40. accuracy of sv dqm is 96.40. accuracy of sv ours is 96.64. accuracy of ko conll winner is 93.02. accuracy of ko dqm is 93.02. accuracy of ko ours is 93.45. accuracy of sk conll winner is 85.00. accuracy of sk dqm is 85.00. accuracy of sk ours is 85.88. accuracy of nl conll winner is 90.61. accuracy of nl dqm is 90.61. accuracy of nl ours is 91.10. accuracy of fi_ftb conll winner is 95.31. accuracy of fi_ftb dqm is 95.31. accuracy of fi_ftb ours is 95.56. accuracy of de conll winner is 97.29. accuracy of de dqm is 97.29. accuracy of de ours is 97.39. accuracy of tr conll winner is 93.11. accuracy of tr dqm is 93.11. accuracy of tr ours is 93.43. accuracy of hi conll winner is 97.01. accuracy of hi dqm is 97.01. accuracy of hi ours is 97.13. accuracy of es_ancora conll winner is 98.73. accuracy of es_ancora dqm is 98.73. accuracy of es_ancora ours is 98.78. accuracy of ro conll winner is 96.98. accuracy of ro dqm is 96.98. accuracy of ro ours is 97.08. accuracy of la_proiel conll winner is 96.93. accuracy of la_proiel dqm is 96.93. accuracy of la_proiel ours is 97.00. accuracy of pl conll winner is 91.97. accuracy of pl dqm is 91.97. accuracy of pl ours is 92.12. accuracy of ar conll winner is 87.66. accuracy of ar dqm is 87.66. accuracy of ar ours is 87.82. accuracy of gl conll winner is 97.50. accuracy of gl dqm is 97.50. accuracy of gl ours is 97.53. accuracy of sv_lines conll winner is 94.84. accuracy of sv_lines dqm is 94.84. accuracy of sv_lines ours is 94.90. accuracy of cs_clt conll winner is 89.98. accuracy of cs_clt dqm is 89.98. accuracy of cs_clt ours is 90.09. accuracy of lv conll winner is 80.05. accuracy of lv dqm is 80.05. accuracy of lv ours is 80.20. accuracy of zh conll winner is 88.40. accuracy of zh dqm is 85.07. accuracy of zh ours is 85.10. accuracy of da conll winner is 100.00. accuracy of da dqm is 99.96. accuracy of da ours is 99.96. accuracy of es conll winner is 99.81. accuracy of es dqm is 99.69. accuracy of es ours is 99.69. accuracy of eu conll winner is 99.98. accuracy of eu dqm is 99.96. accuracy of eu ours is 99.96. accuracy of fr_sequoia conll winner is 99.49. accuracy of fr_sequoia dqm is 99.06. accuracy of fr_sequoia ours is 99.06. accuracy of fr conll winner is 99.50. accuracy of fr dqm is 98.87. accuracy of fr ours is 98.87. accuracy of hr conll winner is 99.93. accuracy of hr dqm is 99.93. accuracy of hr ours is 99.93. accuracy of hu conll winner is 99.85. accuracy of hu dqm is 99.82. accuracy of hu ours is 99.82. accuracy of id conll winner is 100.00. accuracy of id dqm is 99.99. accuracy of id ours is 99.99. accuracy of ja conll winner is 98.59. accuracy of ja dqm is 89.68. accuracy of ja ours is 89.68. accuracy of nl_lassy conll winner is 99.99. accuracy of nl_lassy dqm is 99.93. accuracy of nl_lassy ours is 99.93. accuracy of no_bok. conll winner is 99.88. accuracy of no_bok. dqm is 99.75. accuracy of no_bok. ours is 99.75. accuracy of no_nyn. conll winner is 99.93. accuracy of no_nyn. dqm is 99.85. accuracy of no_nyn. ours is 99.85. accuracy of ru_syn. conll winner is 99.58. accuracy of ru_syn. dqm is 99.57. accuracy of ru_syn. ours is 99.57. accuracy of en_lines conll winner is 95.41. accuracy of en_lines dqm is 95.41. accuracy of en_lines ours is 95.39. accuracy of ur conll winner is 92.30. accuracy of ur dqm is 92.30. accuracy of ur ours is 92.21. accuracy of he conll winner is 83.24. accuracy of he dqm is 82.45. accuracy of he ours is 82.16. accuracy of vi conll winner is 75.42. accuracy of vi dqm is 73.56. accuracy of vi ours is 73.12. accuracy of gl_treegal conll winner is 91.65. accuracy of gl_treegal dqm is 91.65. accuracy of gl_treegal ours is 91.40. accuracy of en conll winner is 94.82. accuracy of en dqm is 94.82. accuracy of en ours is 94.66. accuracy of en_partut conll winner is 95.08. accuracy of en_partut dqm is 95.08. accuracy of en_partut ours is 94.81. accuracy of pt_br conll winner is 98.22. accuracy of pt_br dqm is 98.22. accuracy of pt_br ours is 98.11. accuracy of et conll winner is 95.05. accuracy of et dqm is 95.05. accuracy of et ours is 94.72. accuracy of el conll winner is 97.76. accuracy of el dqm is 97.76. accuracy of el ours is 97.53. accuracy of macro-avg conll winner is 93.18. accuracy of macro-avg dqm is 93.11. accuracy of macro-avg ours is 93.40.
table 3 shows results on wsj test set. accuracy of sogaard (2011) accuracy is 97.50. accuracy of huang et al. (2015) accuracy is 97.55. accuracy of choi (2016) accuracy is 97.64. accuracy of andor et al. (2016) accuracy is 97.44. accuracy of dozat et al. (2017) accuracy is 97.41. accuracy of ours accuracy is 97.96.
table 5 shows comparison of optimization methods: separate optimization of the word, character and meta model is more accurate on average than full back-propagation using a single loss function.the results are statistically significant with two-tailed paired t-test for xpos with p<0.001 and for morphology with p <0.0001. avg. f1 score morphology of separate avg. f1 score morphology is 94.57. avg. f1 score xpos of separate avg. f1 score xpos is 94.85. avg. f1 score morphology of jointly avg. f1 score morphology is 94.15. avg. f1 score xpos of jointly avg. f1 score xpos is 94.48.
table 8 shows f1 score of char models and their performance on the dev. f1 of el flast b1st is 96.6. f1 of el f1st blast is 96.6. f1 of el flast blast is 96.2. f1 of el f1st b1st is 96.1. f1 of el dqm is 95.9. f1 of grc flast b1st is 87.3. f1 of grc f1st blast is 87.1. f1 of grc flast blast is 87.1. f1 of grc f1st b1st is 86.8. f1 of grc dqm is 86.7. f1 of la_ittb flast b1st is 91.1. f1 of la_ittb f1st blast is 91.5. f1 of la_ittb flast blast is 91.9. f1 of la_ittb f1st b1st is 91.3. f1 of la_ittb dqm is 91.0. f1 of ru flast b1st is 95.6. f1 of ru f1st blast is 95.4. f1 of ru flast blast is 95.6. f1 of ru f1st b1st is 95.3. f1 of ru dqm is 95.8. f1 of tr flast b1st is 93.5. f1 of tr f1st blast is 93.3. f1 of tr flast blast is 93.2. f1 of tr f1st b1st is 92.5. f1 of tr dqm is 93.9.
table 4 shows experiment results (uas, %) on the ud 2.0 development set. uas of eu mh 3 is 82.07 ± 0.17. uas of eu mst is 83.61 ± 0.16. uas of eu mh 4-two is 82.94 ± 0.24. uas of eu mh 4-hybrid is 84.13 ± 0.13. uas of eu 1ec is 84.09 ± 0.19. uas of eu mh 3 is 81.27 ± 0.20. uas of eu mh 4 is 81.71 ± 0.33. uas of ur mh 3 is 86.89 ± 0.18. uas of ur mst is 86.78 ± 0.13. uas of ur mh 4-two is 86.84 ± 0.26. uas of ur mh 4-hybrid is 87.06 ± 0.24. uas of ur 1ec is 87.11 ± 0.11. uas of ur mh 3 is 86.40 ± 0.16. uas of ur mh 4 is 86.05 ± 0.18. uas of got mh 3 is 83.72 ± 0.19. uas of got mst is 84.74 ± 0.28. uas of got mh 4-two is 83.85 ± 0.19. uas of got mh 4-hybrid is 84.59 ± 0.38. uas of got 1ec is 84.77 ± 0.27. uas of got mh 3 is 82.28 ± 0.18. uas of got mh 4 is 81.40 ± 0.45. uas of hu mh 3 is 83.05 ± 0.17. uas of hu mst is 82.81 ± 0.49. uas of hu mh 4-two is 83.69 ± 0.20. uas of hu mh 4-hybrid is 84.59 ± 0.50. uas of hu 1ec is 83.48 ± 0.27. uas of hu mh 3 is 81.75 ± 0.47. uas of hu mh 4 is 80.75 ± 0.54. uas of cu mh 3 is 86.70 ± 0.30. uas of cu mst is 88.02 ± 0.25. uas of cu mh 4-two is 87.57 ± 0.14. uas of cu mh 4-hybrid is 88.09 ± 0.28. uas of cu 1ec is 88.27 ± 0.32. uas of cu mh 3 is 86.05 ± 0.23. uas of cu mh 4 is 86.01 ± 0.11. uas of da mh 3 is 85.09 ± 0.16. uas of da mst is 84.68 ± 0.36. uas of da mh 4-two is 85.45 ± 0.43. uas of da mh 4-hybrid is 85.77 ± 0.39. uas of da 1ec is 85.77 ± 0.16. uas of da mh 3 is 83.90 ± 0.24. uas of da mh 4 is 83.59 ± 0.06. uas of el mh 3 is 87.82 ± 0.24. uas of el mst is 87.27 ± 0.22. uas of el mh 4-two is 87.77 ± 0.20. uas of el mh 4-hybrid is 87.83 ± 0.36. uas of el 1ec is 87.95 ± 0.23. uas of el mh 3 is 87.14 ± 0.25. uas of el mh 4 is 86.95 ± 0.25. uas of hi mh 3 is 93.75 ± 0.14. uas of hi mst is 93.91 ± 0.26. uas of hi mh 4-two is 93.99 ± 0.15. uas of hi mh 4-hybrid is 94.27 ± 0.08. uas of hi 1ec is 94.24 ± 0.04. uas of hi mh 3 is 93.44 ± 0.09. uas of hi mh 4 is 93.02 ± 0.10. uas of de mh 3 is 86.46 ± 0.13. uas of de mst is 86.34 ± 0.24. uas of de mh 4-two is 86.53 ± 0.22. uas of de mh 4-hybrid is 86.89 ± 0.17. uas of de 1ec is 86.95 ± 0.32. uas of de mh 3 is 84.99 ± 0.26. uas of de mh 4 is 85.27 ± 0.32. uas of ro mh 3 is 89.34 ± 0.27. uas of ro mst is 88.79 ± 0.43. uas of ro mh 4-two is 89.25 ± 0.15. uas of ro mh 4-hybrid is 89.53 ± 0.20. uas of ro 1ec is 89.52 ± 0.25. uas of ro mh 3 is 88.76 ± 0.30. uas of ro mh 4 is 87.97 ± 0.31. uas of avg. mh 3 is 86.49. uas of avg. mst is 86.69. uas of avg. mh 4-two is 86.79. uas of avg. mh 4-hybrid is 87.27. uas of avg. 1ec is 87.21. uas of avg. mh 3 is 85.6. uas of avg. mh 4 is 85.27.
table 3 shows the overall performance of the two sequential models on development data. p of interspace p is 74.6. r of interspace r is 20.6. f1 of interspace f1 is 32.2. p of interspace p is 71.2. r of interspace r is 30.3. f1 of interspace f1 is 42.5. p of interspace p is 67.9. r of interspace r is 59.8. f1 of interspace f1 is 63.6. p of interspace p is 73.0. r of interspace r is 61.6. f1 of interspace f1 is 66.8. p of pre2 p is 72.4. r of pre2 r is 30.1. f1 of pre2 f1 is 42.5. p of pre2 p is 72.8. r of pre2 r is 32.4. f1 of pre2 f1 is 44.8. p of pre2 p is 71.1. r of pre2 r is 58.3. f1 of pre2 f1 is 64.1. p of pre2 p is 74.8. r of pre2 r is 57.4. f1 of pre2 f1 is 65.0. p of pre3 p is 73.1. r of pre3 r is 30.2. f1 of pre3 f1 is 42.8. p of pre3 p is 73.0. r of pre3 r is 32.5. f1 of pre3 f1 is 44.9. p of pre3 p is 71.1. r of pre3 r is 58.5. f1 of pre3 f1 is 64.2. p of pre3 p is 73.8. r of pre3 r is 57.0. f1 of pre3 f1 is 64.3. p of prepost p is 70.9. r of prepost r is 32.9. f1 of prepost f1 is 45.0. p of prepost p is 74.4. r of prepost r is 30.3. f1 of prepost f1 is 43.1. p of prepost p is 71.0. r of prepost r is 57.6. f1 of prepost f1 is 63.6. p of prepost p is 72.9. r of prepost r is 58.6. f1 of prepost f1 is 65.0.
table 6 shows the performances of the firstand second-order in-parsing models on test data. p of pro p is 52.5. r of pro r is 16.8. f1 of pro f1 is 25.5. p of pro p is 54.4. r of pro r is 19.7. f1 of pro f1 is 28.9. p of pro p is 50.5. r of pro r is 16.2. f1 of pro f1 is 24.5. p of pro p is 52.6. r of pro r is 19.1. f1 of pro f1 is 28. p of pro p is 59.7. r of pro r is 47.3. f1 of pro f1 is 52.8. p of pro p is 60.6. r of pro r is 58. f1 of pro f1 is 59.3. p of pro p is 58.4. r of pro r is 46.3. f1 of pro f1 is 51.7. p of pro p is 57.8. r of pro r is 55.3. f1 of pro f1 is 56.6. p of op p is 74.5. r of op r is 55.8. f1 of op f1 is 63.8. p of op p is 79.6. r of op r is 67.8. f1 of op f1 is 73.2. p of op p is 72.2. r of op r is 54.1. f1 of op f1 is 61.8. p of op p is 78.6. r of op r is 67. f1 of op f1 is 72.3. p of t p is 70.6. r of t r is 51.7. f1 of t f1 is 59.7. p of t p is 77.3. r of t r is 62.8. f1 of t f1 is 69.3. p of t p is 68.5. r of t r is 50.2. f1 of t f1 is 57.9. p of t p is 75.4. r of t r is 61.2. f1 of t f1 is 67.6. p of rnr p is 70.8. r of rnr r is 50. f1 of rnr f1 is 58.6. p of rnr p is 77.8. r of rnr r is 61.8. f1 of rnr f1 is 68.9. p of rnr p is 70.8. r of rnr r is 50. f1 of rnr f1 is 58.6. p of rnr p is 77.8. r of rnr r is 61.8. f1 of rnr f1 is 68.9. p of * p is 0. r of * r is 0. f1 of * f1 is 0. p of * p is 0. r of * r is 0. f1 of * f1 is 0. p of * p is 0. r of * r is 0. f1 of * f1 is 0. p of * p is 0. r of * r is 0. f1 of * f1 is 0. p of overall p is 68.2. r of overall r is 45.7. f1 of overall f1 is 54.7. p of overall p is 72.6. r of overall r is 55.5. f1 of overall f1 is 62.9. p of overall p is 66.3. r of overall r is 44.4. f1 of overall f1 is 53.2. p of overall p is 70.9. r of overall r is 54.1. f1 of overall f1 is 61.4.
table 5 shows parsing accuracy on test data. uas of train uas is 75.99. las of train las is 70.95. uas of train-hit uas is 76.20. las of train-hit las is 68.43. uas of train & train-hit uas is 79.29. las of train & train-hit las is 74.51. uas of converted train-hit uas is 80.45. las of converted train-hit las is 75.83.
table 2 shows model performances on 500 samples when evaluated against the union of the “best” annotations (b1 ∪ b2), intersection of the “valid” annotations (v 1 ∩ v 2) and the original question paired with the post in the dataset. p@1 of random p@1 is 17.5. p@3 of random p@3 is 17.5. p@5 of random p@5 is 17.5. map of random map is 35.2. p@1 of random p@1 is 26.4. p@3 of random p@3 is 26.4. p@5 of random p@5 is 26.4. map of random map is 42.1. p@1 of random p@1 is 10.0. p@1 of bag-of-ngrams p@1 is 19.4. p@3 of bag-of-ngrams p@3 is 19.4. p@5 of bag-of-ngrams p@5 is 18.7. map of bag-of-ngrams map is 34.4. p@1 of bag-of-ngrams p@1 is 25.6. p@3 of bag-of-ngrams p@3 is 27.6. p@5 of bag-of-ngrams p@5 is 27.5. map of bag-of-ngrams map is 42.7. p@1 of bag-of-ngrams p@1 is 10.7. p@1 of community qa p@1 is 23.1. p@3 of community qa p@3 is 21.2. p@5 of community qa p@5 is 20.0. map of community qa map is 40.2. p@1 of community qa p@1 is 33.6. p@3 of community qa p@3 is 30.8. p@5 of community qa p@5 is 29.1. map of community qa map is 47.0. p@1 of community qa p@1 is 18.5. p@1 of neural (p q) p@1 is 21.9. p@3 of neural (p q) p@3 is 20.9. p@5 of neural (p q) p@5 is 19.5. map of neural (p q) map is 39.2. p@1 of neural (p q) p@1 is 31.6. p@3 of neural (p q) p@3 is 30.0. p@5 of neural (p q) p@5 is 28.9. map of neural (p q) map is 45.5. p@1 of neural (p q) p@1 is 15.4. p@1 of neural (p a) p@1 is 24.1. p@3 of neural (p a) p@3 is 23.5. p@5 of neural (p a) p@5 is 20.6. map of neural (p a) map is 41.4. p@1 of neural (p a) p@1 is 32.3. p@3 of neural (p a) p@3 is 31.5. p@5 of neural (p a) p@5 is 29.0. map of neural (p a) map is 46.5. p@1 of neural (p a) p@1 is 18.8. p@1 of neural (p q a) p@1 is 25.2. p@3 of neural (p q a) p@3 is 22.7. p@5 of neural (p q a) p@5 is 21.3. map of neural (p q a) map is 42.5. p@1 of neural (p q a) p@1 is 34.4. p@3 of neural (p q a) p@3 is 31.8. p@5 of neural (p q a) p@5 is 30.1. map of neural (p q a) map is 47.7. p@1 of neural (p q a) p@1 is 20.5. p@1 of evpi p@1 is 27.7. p@3 of evpi p@3 is 23.4. p@5 of evpi p@5 is 21.5. map of evpi map is 43.6. p@1 of evpi p@1 is 36.1. p@3 of evpi p@3 is 32.2. p@5 of evpi p@5 is 30.5. map of evpi map is 49.2. p@1 of evpi p@1 is 21.4.
table 1 shows comparison of validation and test set perplexity for r-rntns with f mapping (k = 100 for ptb, k = 376 for text8) versus s-rnns and m-rnn. # params of 100 # params is 2m. test ppl of 100 test ppl is 146.7. # params of 100 # params is 7.6m. test ppl of 100 test ppl is 236.4. # params of 100 # params is 3m. test ppl of 100 test ppl is 131.2. # params of 100 # params is 11.4m. test ppl of 100 test ppl is 190.1. # params of 100 # params is 103m. test ppl of 100 test ppl is 128.8. # params of 100 # params is 388m. # params of 100 # params is 3m. test ppl of 100 test ppl is 164.2. # params of 100 # params is 11.4m. test ppl of 100 test ppl is 895. # params of 150 # params is 3m. test ppl of 150 test ppl is 133.7. # params of 150 # params is 11.4m. test ppl of 150 test ppl is 207.9. # params of 150 # params is 5.3m. test ppl of 150 test ppl is 126.4. # params of 150 # params is 19.8m. test ppl of 150 test ppl is 171.7. # params of 244 # params is 9.6m. test ppl of 244 test ppl is 92.2. # params of 650 # params is 15.5m. test ppl of 650 test ppl is 90.3. # params of 244 # params is 15.5m. test ppl of 244 test ppl is 87.5. # params of 254 # params is 10m. test ppl of 254 test ppl is 88.8. # params of 650 # params is 16.4m. test ppl of 650 test ppl is 84.6. # params of 254 # params is 16.4m. test ppl of 254 test ppl is 87.1.
table 1 shows pos prediction accuracy [%] using the trustpilot test set, stratified by sex and age (higher is better), and the absolute difference (∆) within each bias group (smaller is better). f of baseline f is 90.9. m of baseline m is 91.1. delta of baseline delta is 0.2. o45 of baseline o45 is 91.4. u35 of baseline u35 is 89.9. delta of baseline delta is 1.5. f of adv f is 92.2. m of adv m is 92.1. delta of adv delta is 0.1. o45 of adv o45 is 92.3. u35 of adv u35 is 92.0. delta of adv delta is 0.3.
table 2 shows pos predictive accuracy [%] over the aave dataset, stratified over the three domains, alongside the macro-average accuracy. accuracy of baseline lyrics is 73.7. accuracy of baseline subtitles is 81.4. accuracy of baseline tweets is 59.9. accuracy of baseline average is 71.7. accuracy of adv lyrics is 80.5. accuracy of adv subtitles is 85.8. accuracy of adv tweets is 65.4. accuracy of adv average is 77.0.
table 4 shows evaluation results on the dataset of polysemous verb classes by korhonen et al. nmpu of lda-frames nmpu is 52.60. nipu of lda-frames nipu is 45.84. f1 of lda-frames f1 is 48.98. nmpu of triframes watset nmpu is 40.05. nipu of triframes watset nipu is 62.09. f1 of triframes watset f1 is 48.69. nmpu of noac nmpu is 37.19. nipu of noac nipu is 64.09. f1 of noac f1 is 47.07. nmpu of hosg nmpu is 38.22. nipu of hosg nipu is 43.76. f1 of hosg f1 is 40.80. nmpu of triadic spectral nmpu is 35.76. nipu of triadic spectral nipu is 38.96. f1 of triadic spectral f1 is 36.86. nmpu of triadic k-means nmpu is 52.22. nipu of triadic k-means nipu is 27.43. f1 of triadic k-means f1 is 35.96. nmpu of triframes cw nmpu is 18.05. nipu of triframes cw nipu is 12.72. f1 of triframes cw f1 is 14.92. nmpu of whole nmpu is 24.14. nipu of whole nipu is 79.09. f1 of whole f1 is 36.99. nmpu of singletons nmpu is 0.00. nipu of singletons nipu is 27.21. f1 of singletons f1 is 0.00.
table 2 shows performance as a function of the number of rnn units with a fixed unit size of 64; averaged across 5 runs apart from the 16 unit (average across 10 runs). f1 of 1 f1 is 90.53 ±0.31. f1 of 2 f1 is 90.79 ±0.18. f1 of 4 f1 is 90.64 ±0.24. f1 of 8 f1 is 91.09 ±0.28. f1 of 16 f1 is 91.48 ±0.22. f1 of 32 f1 is 90.68 ±0.18.
table 3 shows kbc performance for base, typed, and related formulations. mrr of e mrr is 23.40. hits@1 of e hits@1 is 17.39. hits@10 of e hits@10 is 35.29. mrr of e mrr is 21.30. hits@1 of e hits@1 is 14.51. hits@10 of e hits@10 is 36.38. mrr of e mrr is 7.87. hits@1 of e hits@1 is 6.22. hits@10 of e hits@10 is 10.00. mrr of dm+e mrr is 60.84. hits@1 of dm+e hits@1 is 49.53. hits@10 of dm+e hits@10 is 79.70. mrr of dm+e mrr is 38.15. hits@1 of dm+e hits@1 is 28.06. hits@10 of dm+e hits@10 is 58.02. mrr of dm+e mrr is 52.48. hits@1 of dm+e hits@1 is 38.72. hits@10 of dm+e hits@10 is 77.40. mrr of dm mrr is 67.47. hits@1 of dm hits@1 is 56.52. hits@10 of dm hits@10 is 84.86. mrr of dm mrr is 37.21. hits@1 of dm hits@1 is 27.43. hits@10 of dm hits@10 is 56.12. mrr of dm mrr is 55.31. hits@1 of dm hits@1 is 46.80. hits@10 of dm hits@10 is 70.76. mrr of typedm mrr is 75.01. hits@1 of typedm hits@1 is 66.07. hits@10 of typedm hits@10 is 87.92. mrr of typedm mrr is 38.70. hits@1 of typedm hits@1 is 29.30. hits@10 of typedm hits@10 is 57.36. mrr of typedm mrr is 58.16. hits@1 of typedm hits@1 is 51.36. hits@10 of typedm hits@10 is 70.08. mrr of complex mrr is 70.50. hits@1 of complex hits@1 is 61.00. hits@10 of complex hits@10 is 86.09. mrr of complex mrr is 37.58. hits@1 of complex hits@1 is 26.97. hits@10 of complex hits@10 is 55.98. mrr of complex mrr is 54.86. hits@1 of complex hits@1 is 46.90. hits@10 of complex hits@10 is 69.08. mrr of typecomplex mrr is 75.44. hits@1 of typecomplex hits@1 is 66.32. hits@10 of typecomplex hits@10 is 88.51. mrr of typecomplex mrr is 38.93. hits@1 of typecomplex hits@1 is 29.57. hits@10 of typecomplex hits@10 is 57.50. mrr of typecomplex mrr is 58.65. hits@1 of typecomplex hits@1 is 51.62. hits@10 of typecomplex hits@10 is 70.42.
table 1 shows relation extraction performance on ace 2005 test dataset. p of sptree p is 70.1. r of sptree r is 61.2. f1 (%) of sptree f1 (%) is 65.3. p of baseline p is 72.5. r of baseline r is 53.3. f1 (%) of baseline f1 (%) is 61.4*. p of no walks l = 1 p is 71.9. r of no walks l = 1 r is 55.6. f1 (%) of no walks l = 1 f1 (%) is 62.7. p of + walks l = 2 p is 69.9. r of + walks l = 2 r is 58.4. f1 (%) of + walks l = 2 f1 (%) is 63.6◇. p of + walks l = 4 p is 69.7. r of + walks l = 4 r is 59.5. f1 (%) of + walks l = 4 f1 (%) is 64.2◇. p of + walks l = 8 p is 71.5. r of + walks l = 8 r is 55.3. f1 (%) of + walks l = 8 f1 (%) is 62.4.
table 2 shows relation extraction performance (f1 %) on ace 2005 development set for different number of entities. f1 of 2 l = 1 is 71.2. f1 of 2 l = 2 is 69.8. f1 of 2 l = 4 is 72.9. f1 of 2 l = 8 is 71.0. f1 of 3 l = 1 is 70.1. f1 of 3 l = 2 is 67.5. f1 of 3 l = 4 is 67.8. f1 of 3 l = 8 is 63.5*. f1 of [4, 6) l = 1 is 56.5. f1 of [4, 6) l = 2 is 59.7. f1 of [4, 6) l = 4 is 59.3. f1 of [4, 6) l = 8 is 59.9. f1 of [6, 12) l = 1 is 59.2. f1 of [6, 12) l = 2 is 64.2*. f1 of [6, 12) l = 4 is 62.2. f1 of [6, 12) l = 8 is 60.4. f1 of [12, 23) l = 1 is 54.7. f1 of [12, 23) l = 2 is 59.3. f1 of [12, 23) l = 4 is 62.3*. f1 of [12, 23) l = 8 is 55.0.
table 2 shows performance of seed selection methods. average p@50 of k-means average p@50 is 0.96. average p@50 of hits graph1 average p@50 is 0.90. average p@50 of hits graph2 average p@50 is 0.85. average p@50 of hits graph3 average p@50 is 0.90. average p@50 of hits+k-means graph1 average p@50 is 0.92. average p@50 of hits+k-means graph2 average p@50 is 0.85. average p@50 of hits+k-means graph3 average p@50 is 0.94. average p@50 of lsa average p@50 is 0.90. average p@50 of nmf average p@50 is 0.89. average p@50 of random average p@50 is 0.75.
table 3 shows ranking results of scoring functions. map of f0 map is 0.42. p@50 of f0 p@50 is 0.40. p@100 of f0 p@100 is 0.44. p@200 of f0 p@200 is 0.42. p@300 of f0 p@300 is 0.38. map of f1 map is 0.58. p@50 of f1 p@50 is 0.70. p@100 of f1 p@100 is 0.60. p@200 of f1 p@200 is 0.53. p@300 of f1 p@300 is 0.44. map of f2 map is 0.48. p@50 of f2 p@50 is 0.56. p@100 of f2 p@100 is 0.52. p@200 of f2 p@200 is 0.49. p@300 of f2 p@300 is 0.42. map of f3 map is 0.59. p@50 of f3 p@50 is 0.68. p@100 of f3 p@100 is 0.63. p@200 of f3 p@200 is 0.55. p@300 of f3 p@300 is 0.44. map of f4 map is 0.56. p@50 of f4 p@50 is 0.40. p@100 of f4 p@100 is 0.48. p@200 of f4 p@200 is 0.50. p@300 of f4 p@300 is 0.42.
table 1 shows evaluation on germeval data, using the official metric (metric 1) of the germeval 2014 task that combines inner and outer chunks. pr of stanfordner pr is 80.02. r of stanfordner r is 62.29. f1 of stanfordner f1 is 70.05. pr of germaner pr is 81.31. r of germaner r is 68.00. f1 of germaner f1 is 74.06. pr of ukp pr is 79.54. r of ukp r is 71.10. f1 of ukp f1 is 75.09. pr of exb pr is 78.07. r of exb r is 74.75. f1 of exb f1 is 76.38. pr of bilstm-wikiemb pr is 81.95. r of bilstm-wikiemb r is 78.13. f1 of bilstm-wikiemb f1 is 79.99*. pr of bilstm-euroemb pr is 75.50. r of bilstm-euroemb r is 70.72. f1 of bilstm-euroemb f1 is 73.03.
table 5 shows results for different test sets when using transfer learning. f1 of germeval conll is 78.55. f1 of germeval germeval is 82.93. f1 of germeval lft is 55.28. f1 of germeval onb is 64.93. f1 of germeval conll is 72.23. f1 of germeval germeval is 75.78. f1 of germeval lft is 51.98. f1 of germeval onb is 61.74. f1 of lft conll is 62.80. f1 of lft germeval is 58.89. f1 of lft lft is 72.90. f1 of lft onb is 67.96. f1 of lft conll is 56.30. f1 of lft germeval is 51.25. f1 of lft lft is 70.04. f1 of lft onb is 65.65. f1 of onb conll is 62.05. f1 of onb germeval is 57.19. f1 of onb lft is 59.43. f1 of onb onb is 76.17. f1 of onb conll is 55.82. f1 of onb germeval is 49.14. f1 of onb lft is 54.19. f1 of onb onb is 73.68. f1 of conll conll is 84.73†. f1 of conll germeval is 72.11. f1 of conll lft is 54.21. f1 of conll onb is 65.95. f1 of conll conll is 78.41. f1 of conll germeval is 63.42. f1 of conll lft is 52.02. f1 of conll onb is 59.28. f1 of lft conll is 67.77. f1 of lft germeval is 69.09. f1 of lft lft is 74.33†. f1 of lft onb is 70.57. f1 of lft conll is 55.83. f1 of lft germeval is 57.71. f1 of lft lft is 72.03. f1 of lft onb is 70.36. f1 of onb conll is 72.15. f1 of onb germeval is 73.18. f1 of onb lft is 62.52. f1 of onb onb is 76.06. f1 of onb conll is 64.05. f1 of onb germeval is 64.20. f1 of onb lft is 57.12. f1 of onb onb is 78.56†.
table 2 shows results of 10× 10−fold cross-validation. precision of # tokens precision is 0.793. recall of # tokens recall is 0.996. f1 of # tokens f1 is 0.883. auc of # tokens auc is 0.610. precision of # sentences precision is 0.792. recall of # sentences recall is 0.999. f1 of # sentences f1 is 0.884. auc of # sentences auc is 0.584. precision of all precision is 0.829. recall of all recall is 0.926. f1 of all f1 is 0.872. auc of all auc is 0.849. precision of significant precision is 0.805. recall of significant recall is 0.953. f1 of significant f1 is 0.871. auc of significant auc is 0.805. precision of relevant precision is 0.802. recall of relevant recall is 0.963. f1 of relevant f1 is 0.874. auc of relevant auc is 0.819.
table 4 shows performance of word representations learned under different configurations. accuracy of word cap. is .706. accuracy of word sta. is .966. accuracy of word fam. is .603. accuracy of word a is .117. accuracy of word ab is .162. accuracy of word pre. is .181. accuracy of word suf. is .389. accuracy of word mor. is .222. accuracy of word geo. is .414. accuracy of word his. is .345. accuracy of word nat. is .236. accuracy of word peo. is .223. accuracy of word sem. is .327. accuracy of word+ngram cap. is .715. accuracy of word+ngram sta. is .977. accuracy of word+ngram fam. is .640. accuracy of word+ngram a is .143. accuracy of word+ngram ab is .184. accuracy of word+ngram pre. is .197. accuracy of word+ngram suf. is .429. accuracy of word+ngram mor. is .250. accuracy of word+ngram geo. is .449. accuracy of word+ngram his. is .308. accuracy of word+ngram nat. is .276. accuracy of word+ngram peo. is .310. accuracy of word+ngram sem. is .368. accuracy of word+char cap. is .676. accuracy of word+char sta. is .966. accuracy of word+char fam. is .548. accuracy of word+char a is .358. accuracy of word+char ab is .540. accuracy of word+char pre. is .326. accuracy of word+char suf. is .612. accuracy of word+char mor. is .455. accuracy of word+char geo. is .468. accuracy of word+char his. is .226. accuracy of word+char nat. is .296. accuracy of word+char peo. is .305. accuracy of word+char sem. is .368. accuracy of word cap. is .925. accuracy of word sta. is .920. accuracy of word fam. is .548. accuracy of word a is .103. accuracy of word ab is .139. accuracy of word pre. is .138. accuracy of word suf. is .464. accuracy of word mor. is .226. accuracy of word geo. is .627. accuracy of word his. is .501. accuracy of word nat. is .300. accuracy of word peo. is .515. accuracy of word sem. is .522. accuracy of word+ngram cap. is .943. accuracy of word+ngram sta. is .960. accuracy of word+ngram fam. is .658. accuracy of word+ngram a is .102. accuracy of word+ngram ab is .129. accuracy of word+ngram pre. is .168. accuracy of word+ngram suf. is .456. accuracy of word+ngram mor. is .230. accuracy of word+ngram geo. is .680. accuracy of word+ngram his. is .535. accuracy of word+ngram nat. is .371. accuracy of word+ngram peo. is .626. accuracy of word+ngram sem. is .586. accuracy of word+char cap. is .913. accuracy of word+char sta. is .886. accuracy of word+char fam. is .614. accuracy of word+char a is .106. accuracy of word+char ab is .190. accuracy of word+char pre. is .173. accuracy of word+char suf. is .505. accuracy of word+char mor. is .260. accuracy of word+char geo. is .638. accuracy of word+char his. is .502. accuracy of word+char nat. is .288. accuracy of word+char peo. is .515. accuracy of word+char sem. is .524.
table 5 shows performance of word representations learned upon different training corpora by sgns with context feature of word. accuracy of wikipedia 1.2g cap. is .597. accuracy of wikipedia 1.2g sta. is .771. accuracy of wikipedia 1.2g fam. is .360. accuracy of wikipedia 1.2g a is .029. accuracy of wikipedia 1.2g ab is .018. accuracy of wikipedia 1.2g pre. is .152. accuracy of wikipedia 1.2g suf. is .266. accuracy of wikipedia 1.2g mor. is .180. accuracy of wikipedia 1.2g geo. is .339. accuracy of wikipedia 1.2g his. is .125. accuracy of wikipedia 1.2g nat. is .147. accuracy of wikipedia 1.2g peo. is .079. accuracy of wikipedia 1.2g sem. is .236. accuracy of baidubaike 4.3g cap. is .706. accuracy of baidubaike 4.3g sta. is .966. accuracy of baidubaike 4.3g fam. is .603. accuracy of baidubaike 4.3g a is .117. accuracy of baidubaike 4.3g ab is .162. accuracy of baidubaike 4.3g pre. is .181. accuracy of baidubaike 4.3g suf. is .389. accuracy of baidubaike 4.3g mor. is .222. accuracy of baidubaike 4.3g geo. is .414. accuracy of baidubaike 4.3g his. is .345. accuracy of baidubaike 4.3g nat. is .236. accuracy of baidubaike 4.3g peo. is .223. accuracy of baidubaike 4.3g sem. is .327. accuracy of people daily 4.2g cap. is .925. accuracy of people daily 4.2g sta. is .989. accuracy of people daily 4.2g fam. is .547. accuracy of people daily 4.2g a is .140. accuracy of people daily 4.2g ab is .158. accuracy of people daily 4.2g pre. is .213. accuracy of people daily 4.2g suf. is .355. accuracy of people daily 4.2g mor. is .226. accuracy of people daily 4.2g geo. is .694. accuracy of people daily 4.2g his. is .019. accuracy of people daily 4.2g nat. is .206. accuracy of people daily 4.2g peo. is .157. accuracy of people daily 4.2g sem. is .455. accuracy of sogou news 4.0g cap. is .619. accuracy of sogou news 4.0g sta. is .966. accuracy of sogou news 4.0g fam. is .496. accuracy of sogou news 4.0g a is .057. accuracy of sogou news 4.0g ab is .075. accuracy of sogou news 4.0g pre. is .131. accuracy of sogou news 4.0g suf. is .176. accuracy of sogou news 4.0g mor. is .115. accuracy of sogou news 4.0g geo. is .432. accuracy of sogou news 4.0g his. is .067. accuracy of sogou news 4.0g nat. is .150. accuracy of sogou news 4.0g peo. is .145. accuracy of sogou news 4.0g sem. is .302. accuracy of zhihu qa 2.2g cap. is .277. accuracy of zhihu qa 2.2g sta. is .491. accuracy of zhihu qa 2.2g fam. is .625. accuracy of zhihu qa 2.2g a is .175. accuracy of zhihu qa 2.2g ab is .199. accuracy of zhihu qa 2.2g pre. is .134. accuracy of zhihu qa 2.2g suf. is .251. accuracy of zhihu qa 2.2g mor. is .189. accuracy of zhihu qa 2.2g geo. is .146. accuracy of zhihu qa 2.2g his. is .147. accuracy of zhihu qa 2.2g nat. is .250. accuracy of zhihu qa 2.2g peo. is .189. accuracy of zhihu qa 2.2g sem. is .181. accuracy of combination 15.9g cap. is .872. accuracy of combination 15.9g sta. is .994. accuracy of combination 15.9g fam. is .710. accuracy of combination 15.9g a is .223. accuracy of combination 15.9g ab is .300. accuracy of combination 15.9g pre. is .234. accuracy of combination 15.9g suf. is .518. accuracy of combination 15.9g mor. is .321. accuracy of combination 15.9g geo. is .662. accuracy of combination 15.9g his. is .293. accuracy of combination 15.9g nat. is .310. accuracy of combination 15.9g peo. is .307. accuracy of combination 15.9g sem. is .467.
table 4 shows single models on ja-en. test bleu of best wat17 result (morishita et al. 2017) test bleu is 28.4. dev bleu of plain bpe dev bleu is 21.6. test bleu of plain bpe test bleu is 21.2. dev bleu of linearized derivation dev bleu is 21.9. test bleu of linearized derivation test bleu is 21.2. dev bleu of plain bpe dev bleu is 28.0. test bleu of plain bpe test bleu is 28.9. dev bleu of linearized tree dev bleu is 28.2. test bleu of linearized tree test bleu is 28.4. dev bleu of linearized derivation dev bleu is 28.5. test bleu of linearized derivation test bleu is 28.7. dev bleu of pos/bpe dev bleu is 28.5. test bleu of pos/bpe test bleu is 29.1.
table 2 shows experiment results with gold predicates. accuracy of ours wsj is 83.9. accuracy of ours brown is 73.7. accuracy of ours ontonotes is 82.1. accuracy of tan et al. (2018) wsj is 84.8. accuracy of tan et al. (2018) brown is 74.1. accuracy of tan et al. (2018) ontonotes is 82.7. accuracy of he et al. (2017) wsj is 83.1. accuracy of he et al. (2017) brown is 72.1. accuracy of he et al. (2017) ontonotes is 81.7. accuracy of yang and mitchell (2017) wsj is 81.9. accuracy of yang and mitchell (2017) brown is 72.0. accuracy of zhou and xu (2015) wsj is 82.8. accuracy of zhou and xu (2015) brown is 69.4. accuracy of zhou and xu (2015) ontonotes is 81.1.
table 2 shows evaluation results on the e-commerce data. r10@1 of rnn (lowe et al., 2015) r10@1 is 0.325. r10@2 of rnn (lowe et al., 2015) r10@2 is 0.463. r10@5 of rnn (lowe et al., 2015) r10@5 is 0.775. r10@1 of cnn (lowe et al., 2015) r10@1 is 0.328. r10@2 of cnn (lowe et al., 2015) r10@2 is 0.515. r10@5 of cnn (lowe et al., 2015) r10@5 is 0.792. r10@1 of lstm (lowe et al., 2015) r10@1 is 0.365. r10@2 of lstm (lowe et al., 2015) r10@2 is 0.536. r10@5 of lstm (lowe et al., 2015) r10@5 is 0.828. r10@1 of bilstm (kadlec et al., 2015) r10@1 is 0.355. r10@2 of bilstm (kadlec et al., 2015) r10@2 is 0.525. r10@5 of bilstm (kadlec et al., 2015) r10@5 is 0.825. r10@1 of dl2r (yan et al., 2016) r10@1 is 0.399. r10@2 of dl2r (yan et al., 2016) r10@2 is 0.571. r10@5 of dl2r (yan et al., 2016) r10@5 is 0.842. r10@1 of mv-lstm (wan et al., 2016) r10@1 is 0.412. r10@2 of mv-lstm (wan et al., 2016) r10@2 is 0.591. r10@5 of mv-lstm (wan et al., 2016) r10@5 is 0.857. r10@1 of match-lstm (wang and jiang, 2016) r10@1 is 0.41. r10@2 of match-lstm (wang and jiang, 2016) r10@2 is 0.59. r10@5 of match-lstm (wang and jiang, 2016) r10@5 is 0.858. r10@1 of multi-view (zhou et al., 2016) r10@1 is 0.421. r10@2 of multi-view (zhou et al., 2016) r10@2 is 0.601. r10@5 of multi-view (zhou et al., 2016) r10@5 is 0.861. r10@1 of smn (wu et al., 2017) r10@1 is 0.453. r10@2 of smn (wu et al., 2017) r10@2 is 0.654. r10@5 of smn (wu et al., 2017) r10@5 is 0.886. r10@1 of dua(zhang et al., 2018b) r10@1 is 0.501. r10@2 of dua(zhang et al., 2018b) r10@2 is 0.7. r10@5 of dua(zhang et al., 2018b) r10@5 is 0.921. r10@1 of dam (zhou et al., 2018b) r10@1 is 0.526. r10@2 of dam (zhou et al., 2018b) r10@2 is 0.727. r10@5 of dam (zhou et al., 2018b) r10@5 is 0.933. r10@1 of ioi-global r10@1 is 0.554. r10@2 of ioi-global r10@2 is 0.747. r10@5 of ioi-global r10@5 is 0.942. r10@1 of ioi-local r10@1 is 0.563. r10@2 of ioi-local r10@2 is 0.768. r10@5 of ioi-local r10@5 is 0.95.
table 1 shows experiment result on the ubuntu corpus. r100@1 of baseline r100@1 is 0.083. r100@10 of baseline r100@10 is 0.359. r100@1 of dam r100@1 is 0.347. r100@10 of dam r100@10 is 0.663. mrr of dam mrr is 0.356. r100@1 of dam+fine-tune r100@1 is 0.364. r100@10 of dam+fine-tune r100@10 is 0.664. mrr of dam+fine-tune mrr is 0.443. r100@1 of dme r100@1 is 0.383. r100@10 of dme r100@10 is 0.725. mrr of dme mrr is 0.498. r100@1 of dme-smn r100@1 is 0.455. r100@10 of dme-smn r100@10 is 0.761. mrr of dme-smn mrr is 0.558. r100@1 of stm(transform) r100@1 is 0.49. r100@10 of stm(transform) r100@10 is 0.764. mrr of stm(transform) mrr is 0.588. r100@1 of stm(gru) r100@1 is 0.503. r100@10 of stm(gru) r100@10 is 0.783. mrr of stm(gru) mrr is 0.597. r100@1 of stm(ensemble) r100@1 is 0.521. r100@10 of stm(ensemble) r100@10 is 0.797. mrr of stm(ensemble) mrr is 0.616. r100@1 of stm(bert) r100@1 is 0.548. r100@10 of stm(bert) r100@10 is 0.827. mrr of stm(bert) mrr is 0.614.
table 3 shows results on the biomedical domain dataset (§5.3). p of c&c p is 77.8. r of c&c r is 71.4. f1 of c&c f1 is 74.5. p of easysrl p is 81.8. r of easysrl r is 82.6. f1 of easysrl f1 is 82.2. p of depccg p is 83.11. r of depccg r is 82.63. f1 of depccg f1 is 82.87. p of #name? p is 85.87. r of #name? r is 85.34. f1 of #name? f1 is 85.61. p of #name? p is 85.45. r of #name? r is 84.49. f1 of #name? f1 is 84.97. p of #name? p is 86.9. r of #name? r is 86.14. f1 of #name? f1 is 86.52.
table 4 shows results on question sentences (§5.3). f1 of c&c f1 is 86.8. p of easysrl p is 88.2. r of easysrl r is 87.9. f1 of easysrl f1 is 88. p of depccg p is 90.42. r of depccg r is 90.15. f1 of depccg f1 is 90.29. p of depccg+elmo p is 90.55. r of depccg+elmo r is 89.86. f1 of depccg+elmo f1 is 90.21. p of depccg+proposed p is 90.27. r of depccg+proposed r is 89.97. f1 of depccg+proposed f1 is 90.12.
table 1 shows results of the proposed method in comparison to previous work (bleu). bleu of artetxe et al. (2018c) fr-en is 15.6. bleu of artetxe et al. (2018c) en-fr is 15.1. bleu of artetxe et al. (2018c) de-en is 10.2. bleu of artetxe et al. (2018c) en-de is 6.6. bleu of lample et al. (2018a) fr-en is 14.3. bleu of lample et al. (2018a) en-fr is 15.1. bleu of lample et al. (2018a) de-en is 13.3. bleu of lample et al. (2018a) en-de is 9.6. bleu of yang et al. (2018) fr-en is 15.6. bleu of yang et al. (2018) en-fr is 17. bleu of yang et al. (2018) de-en is 14.6. bleu of yang et al. (2018) en-de is 10.9. bleu of lample et al. (2018b) fr-en is 24.2. bleu of lample et al. (2018b) en-fr is 25.1. bleu of lample et al. (2018b) de-en is 21. bleu of lample et al. (2018b) en-de is 17.2. bleu of artetxe et al. (2018b) fr-en is 25.9. bleu of artetxe et al. (2018b) en-fr is 26.2. bleu of artetxe et al. (2018b) de-en is 17.4. bleu of artetxe et al. (2018b) en-de is 14.1. bleu of artetxe et al. (2018b) de-en is 23.1. bleu of artetxe et al. (2018b) en-de is 18.2. bleu of lample et al. (2018b) fr-en is 27.2. bleu of lample et al. (2018b) en-fr is 28.1. bleu of lample et al. (2018b) de-en is 22.9. bleu of lample et al. (2018b) en-de is 17.9. bleu of marie and fujita (2018) de-en is 20.2. bleu of marie and fujita (2018) en-de is 15.5. bleu of proposed system fr-en is 28.4. bleu of proposed system en-fr is 30.1. bleu of proposed system de-en is 20.1. bleu of proposed system en-de is 15.8. bleu of proposed system de-en is 25.4. bleu of proposed system en-de is 19.7. bleu of +detok. sacrebleu fr-en is 27.9. bleu of +detok. sacrebleu en-fr is 27.8. bleu of +detok. sacrebleu de-en is 19.7. bleu of +detok. sacrebleu en-de is 14.7. bleu of +detok. sacrebleu de-en is 24.8. bleu of +detok. sacrebleu en-de is 19.4. bleu of lample et al. (2018b) fr-en is 27.7. bleu of lample et al. (2018b) en-fr is 27.6. bleu of lample et al. (2018b) de-en is 25.2. bleu of lample et al. (2018b) en-de is 20.2. bleu of marie and fujita (2018) de-en is 26.7. bleu of marie and fujita (2018) en-de is 20. bleu of ren et al. (2019) fr-en is 28.9. bleu of ren et al. (2019) en-fr is 29.5. bleu of ren et al. (2019) de-en is 20.4. bleu of ren et al. (2019) en-de is 17. bleu of ren et al. (2019) de-en is 26.3. bleu of ren et al. (2019) en-de is 21.7. bleu of proposed system fr-en is 33.5. bleu of proposed system en-fr is 36.2. bleu of proposed system de-en is 27. bleu of proposed system en-de is 22.5. bleu of proposed system de-en is 34.4. bleu of proposed system en-de is 26.9. bleu of +detok. sacrebleu fr-en is 33.2. bleu of +detok. sacrebleu en-fr is 33.6. bleu of +detok. sacrebleu de-en is 26.4. bleu of +detok. sacrebleu en-de is 21.2. bleu of +detok. sacrebleu de-en is 33.8. bleu of +detok. sacrebleu en-de is 26.4.
table 3 shows results of the proposed method in comparison to different supervised systems (bleu). bleu of proposed system fr-en is 33.5. bleu of proposed system en-fr is 36.2. bleu of proposed system de-en is 27. bleu of proposed system en-de is 22.5. bleu of +detok sacrebleu* fr-en is 33.2. bleu of +detok sacrebleu* en-fr is 33.6. bleu of +detok sacrebleu* de-en is 26.4. bleu of +detok sacrebleu* en-de is 21.2. bleu of wmt best* fr-en is 35. bleu of wmt best* en-fr is 35.8. bleu of wmt best* de-en is 29. bleu of wmt best* en-de is 20.6. bleu of vaswani et al. (2017) en-fr is 41. bleu of vaswani et al. (2017) en-de is 28.4. bleu of edunov et al. (2018) en-fr is 45.6. bleu of edunov et al. (2018) en-de is 35.
table 4 shows korean→english results. bleu of (gu et al., 2018b) (supervised transformer) bleu is 5.97. bleu of phrase-based smt bleu is 6.57 ± 0.17. bleu of nmt baseline (2) bleu is 2.93 ± 0.34. bleu of nmt optimized (8) bleu is 10.37 ± 0.29.
table 3 shows experiments result. precision of minie (+aida) precision is 0.3672. recall of minie (+aida) recall is 0.4856. f1 of minie (+aida) f1 is 0.4182. precision of minie (+aida) precision is 0.3574. recall of minie (+aida) recall is 0.3901. f1 of minie (+aida) f1 is 0.373. precision of minie (+neuralel) precision is 0.3511. recall of minie (+neuralel) recall is 0.3967. f1 of minie (+neuralel) f1 is 0.3725. precision of minie (+neuralel) precision is 0.3644. recall of minie (+neuralel) recall is 0.3811. f1 of minie (+neuralel) f1 is 0.3726. precision of clausie (+aida) precision is 0.3617. recall of clausie (+aida) recall is 0.4728. f1 of clausie (+aida) f1 is 0.4099. precision of clausie (+aida) precision is 0.3531. recall of clausie (+aida) recall is 0.3951. f1 of clausie (+aida) f1 is 0.3729. precision of clausie (+neuralel) precision is 0.3445. recall of clausie (+neuralel) recall is 0.3786. f1 of clausie (+neuralel) f1 is 0.3607. precision of clausie (+neuralel) precision is 0.3563. recall of clausie (+neuralel) recall is 0.3791. f1 of clausie (+neuralel) f1 is 0.3673. precision of cnn (+aida) precision is 0.4035. recall of cnn (+aida) recall is 0.3503. f1 of cnn (+aida) f1 is 0.375. precision of cnn (+aida) precision is 0.3715. recall of cnn (+aida) recall is 0.3165. f1 of cnn (+aida) f1 is 0.3418. precision of cnn (+neuralel) precision is 0.3689. recall of cnn (+neuralel) recall is 0.3521. f1 of cnn (+neuralel) f1 is 0.3603. precision of cnn (+neuralel) precision is 0.3781. recall of cnn (+neuralel) recall is 0.3005. f1 of cnn (+neuralel) f1 is 0.3349. precision of single attention precision is 0.4591. recall of single attention recall is 0.3836. f1 of single attention f1 is 0.418. precision of single attention precision is 0.401. recall of single attention recall is 0.3912. f1 of single attention f1 is 0.396. precision of single attention (+pre-trained) precision is 0.4725. recall of single attention (+pre-trained) recall is 0.4053. f1 of single attention (+pre-trained) f1 is 0.4363. precision of single attention (+pre-trained) precision is 0.4314. recall of single attention (+pre-trained) recall is 0.4311. f1 of single attention (+pre-trained) f1 is 0.4312. precision of single attention (+beam) precision is 0.6056. recall of single attention (+beam) recall is 0.5231. f1 of single attention (+beam) f1 is 0.5613. precision of single attention (+beam) precision is 0.5869. recall of single attention (+beam) recall is 0.4851. f1 of single attention (+beam) f1 is 0.5312. precision of single attention (+triple classifier) precision is 0.7378. recall of single attention (+triple classifier) recall is 0.5013. f1 of single attention (+triple classifier) f1 is 0.597. precision of single attention (+triple classifier) precision is 0.6704. recall of single attention (+triple classifier) recall is 0.5301. f1 of single attention (+triple classifier) f1 is 0.5921. precision of transformer precision is 0.4628. recall of transformer recall is 0.3897. f1 of transformer f1 is 0.4231. precision of transformer precision is 0.4575. recall of transformer recall is 0.462. f1 of transformer f1 is 0.4597. precision of transformer (+pre-trained) precision is 0.4748. recall of transformer (+pre-trained) recall is 0.4091. f1 of transformer (+pre-trained) f1 is 0.4395. precision of transformer (+pre-trained) precision is 0.4841. recall of transformer (+pre-trained) recall is 0.4831. f1 of transformer (+pre-trained) f1 is 0.4836. precision of transformer (+beam) precision is 0.5829. recall of transformer (+beam) recall is 0.5025. f1 of transformer (+beam) f1 is 0.5397. precision of transformer (+beam) precision is 0.6181. recall of transformer (+beam) recall is 0.6161. f1 of transformer (+beam) f1 is 0.6171. precision of transformer (+triple classifier) precision is 0.7307. recall of transformer (+triple classifier) recall is 0.4866. f1 of transformer (+triple classifier) f1 is 0.5842. precision of transformer (+triple classifier) precision is 0.7124. recall of transformer (+triple classifier) recall is 0.5761. f1 of transformer (+triple classifier) f1 is 0.637. precision of n-gram attention precision is 0.7014. recall of n-gram attention recall is 0.6432. f1 of n-gram attention f1 is 0.671. precision of n-gram attention precision is 0.6029. recall of n-gram attention recall is 0.6033. f1 of n-gram attention f1 is 0.6031. precision of n-gram attention (+pre-trained) precision is 0.7157. recall of n-gram attention (+pre-trained) recall is 0.6634. f1 of n-gram attention (+pre-trained) f1 is 0.6886. precision of n-gram attention (+pre-trained) precision is 0.6581. recall of n-gram attention (+pre-trained) recall is 0.6631. f1 of n-gram attention (+pre-trained) f1 is 0.6606. precision of n-gram attention (+beam) precision is 0.7424. recall of n-gram attention (+beam) recall is 0.6845. f1 of n-gram attention (+beam) f1 is 0.7123. precision of n-gram attention (+beam) precision is 0.6816. recall of n-gram attention (+beam) recall is 0.6861. f1 of n-gram attention (+beam) f1 is 0.6838. precision of n-gram attention (+triple classifier) precision is 0.8471. recall of n-gram attention (+triple classifier) recall is 0.6762. f1 of n-gram attention (+triple classifier) f1 is 0.7521. precision of n-gram attention (+triple classifier) precision is 0.7705. recall of n-gram attention (+triple classifier) recall is 0.6771. f1 of n-gram attention (+triple classifier) f1 is 0.7208.
table 3 shows evaluation of models at early stopping points. bleu of baseline bleu is 28.28. bleu of baseline bleu is 24.84. ter of baseline ter is 62.42. bleu of full bleu is 28.93±0.02. cost of full cost is  417k. bleu of full bleu is 25.60±0.02. ter of full ter is 61.86±0.03. bleu of weak bleu is 28.65±0.01. cost of weak cost is 32k. bleu of weak bleu is 25.10±0.09. ter of weak ter is 62.12±0.12. bleu of self bleu is 28.58±0.02. bleu of self bleu is 25.33±0.06. ter of self ter is 61.96±0.05. bleu of reg4 bleu is 28.57±0.04. cost of reg4 cost is 68k. bleu of reg4 bleu is 25.23±0.05. ter of reg4 ter is 62.02±0.12. bleu of reg3 bleu is 28.61±0.03. cost of reg3 cost is 18k. bleu of reg3 bleu is 25.23±0.09. ter of reg3 ter is 62.07±0.06. bleu of reg2 bleu is 28.66±0.06. cost of reg2 cost is 88k. bleu of reg2 bleu is 25.27±0.09. ter of reg2 ter is 61.91±0.06.
table 1 shows results of the difficulty prediction approaches. rho of svm (original) rho is 0.5. rmse of svm (original) rmse is 0.23. qw k of svm (original) qw k is 0.44. rho of svm (original) rho is –. rmse of svm (original) rmse is –. qw k of svm (original) qw k is –. rho of svm (reproduced) rho is 0.49. rmse of svm (reproduced) rmse is 0.24. qw k of svm (reproduced) qw k is 0.47. rho of svm (reproduced) rho is 0.5. rmse of svm (reproduced) rmse is 0.21. qw k of svm (reproduced) qw k is 0.39. rho of mlp rho is 0.42. rmse of mlp rmse is 0.25. qw k of mlp qw k is 0.31. rho of mlp rho is 0.41. rmse of mlp rmse is 0.22. qw k of mlp qw k is 0.25. rho of bilstm rho is 0.49. rmse of bilstm rmse is 0.24. qw k of bilstm qw k is 0.35. rho of bilstm rho is 0.39. rmse of bilstm rmse is 0.24. qw k of bilstm qw k is 0.27.
table 4 shows performance of our method on the operational risk text classification task precision of level 1 precision is 91.8. recall of level 1 recall is 89.37. f1-score of level 1 f1-score is 90.45. precision of level 2 precision is 86.08. recall of level 2 recall is 74.8. f1-score of level 2 f1-score is 78.1. precision of level 3 precision is 34.98. recall of level 3 recall is 19.88. f1-score of level 3 f1-score is 22.95.
table 3 shows manual evaluation on the yelp dataset. ts of fu et al. (2018) ts is 1.67. cp of fu et al. (2018) cp is 3.84. lq of fu et al. (2018) lq is 3.66. gm of fu et al. (2018) gm is 2.86. ts of shen et al. (2017) ts is 3.63. cp of shen et al. (2017) cp is 3.07. lq of shen et al. (2017) lq is 3.08. gm of shen et al. (2017) gm is 3.25. ts of zhao et al. (2018) ts is 3.55. cp of zhao et al. (2018) cp is 3.09. lq of zhao et al. (2018) lq is 3.77. gm of zhao et al. (2018) gm is 3.46. ts of ours (dae) ts is 3.67. cp of ours (dae) cp is 3.64. lq of ours (dae) lq is 4.19. gm of ours (dae) gm is 3.83. ts of ours (vae) ts is 4.32. cp of ours (vae) cp is 3.73. lq of ours (vae) lq is 4.48. gm of ours (vae) gm is 4.16.
table 4 shows average single model results comparing different strategies to model cross-sentence context. f 0.5 of baseline f 0.5 is 33.21. p of baseline p is 54.51. r of baseline r is 15.16. f 0.5 of baseline f 0.5 is 35.88. f 0.5 of concat f 0.5 is 33.41. p of concat p is 55.14. r of concat r is 15.28. f 0.5 of concat f 0.5 is 36.23. f 0.5 of aux (no gate) f 0.5 is 32.99. p of aux (no gate) p is 55.1. r of aux (no gate) r is 14.83. f 0.5 of aux (no gate) f 0.5 is 35.69. f 0.5 of aux (+gate) f 0.5 is 35.68. p of aux (+gate) p is 55.65. r of aux (+gate) r is 16.93. f 0.5 of aux (+gate) f 0.5 is 38.17.
table 5 shows comparison of efficiency. flops of bc-lstm flops is 1322024. number of parameters of bc-lstm number of parameters is 1383902. flops of tfn flops is 8491845. number of parameters of tfn number of parameters is 4245986. flops of hffn flops is 16665. number of parameters of hffn number of parameters is 8301.
table 4 shows f1-i scores of different model variants. f1-i of vanilla model d1 is 66.66. f1-i of vanilla model d2 is 55.63. f1-i of vanilla model d3 is 56.24. f1-i of +opinion transmission d1 is 66.98. f1-i of +opinion transmission d2 is 56.03. f1-i of +opinion transmission d3 is 56.65. f1-i of +message passing-a (imn -d) d1 is 68.32. f1-i of +message passing-a (imn -d) d2 is 57.66. f1-i of +message passing-a (imn -d) d3 is 57.91. f1-i of +ds d1 is 68.48. f1-i of +ds d2 is 57.86. f1-i of +ds d3 is 58.03. f1-i of +dd d1 is 68.65. f1-i of +dd d2 is 57.5. f1-i of +dd d3 is 58.26. f1-i of +message passing-d (imn) d1 is 69.54. f1-i of +message passing-d (imn) d2 is 58.37. f1-i of +message passing-d (imn) d3 is 59.18.
table 7 shows model comparison in a setting without opinion term labels. f1-a of decnn-alstm f1-a is 83.33. acc-s of decnn-alstm acc-s is 77.63. f1-s of decnn-alstm f1-s is 70.09. f1-i of decnn-alstm f1-i is 64.32. f1-a of decnn-alstm f1-a is 80.28. acc-s of decnn-alstm acc-s is 69.98. f1-s of decnn-alstm f1-s is 66.2. f1-i of decnn-alstm f1-i is 55.92. f1-a of decnn-alstm f1-a is 68.72. acc-s of decnn-alstm acc-s is 79.22. f1-s of decnn-alstm f1-s is 54.4. f1-i of decnn-alstm f1-i is 54.22. f1-a of decnn-dtrans f1-a is 83.33. acc-s of decnn-dtrans acc-s is 79.45. f1-s of decnn-dtrans f1-s is 73.08. f1-i of decnn-dtrans f1-i is 66.15. f1-a of decnn-dtrans f1-a is 80.28. acc-s of decnn-dtrans acc-s is 71.51. f1-s of decnn-dtrans f1-s is 68.03. f1-i of decnn-dtrans f1-i is 57.28. f1-a of decnn-dtrans f1-a is 68.72. acc-s of decnn-dtrans acc-s is 82.09. f1-s of decnn-dtrans f1-s is 68.35. f1-i of decnn-dtrans f1-i is 56.08. f1-a of pipeline f1-a is 83.33. acc-s of pipeline acc-s is 79.39. f1-s of pipeline f1-s is 69.45. f1-i of pipeline f1-i is 65.96. f1-a of pipeline f1-a is 80.28. acc-s of pipeline acc-s is 72.12. f1-s of pipeline f1-s is 68.56. f1-i of pipeline f1-i is 57.29. f1-a of pipeline f1-a is 68.72. acc-s of pipeline acc-s is 81.85. f1-s of pipeline f1-s is 58.74. f1-i of pipeline f1-i is 56.04. f1-a of mnn f1-a is 83.2. acc-s of mnn acc-s is 77.57. f1-s of mnn f1-s is 68.19. f1-i of mnn f1-i is 64.26. f1-a of mnn f1-a is 76.33. acc-s of mnn acc-s is 70.62. f1-s of mnn f1-s is 65.44. f1-i of mnn f1-i is 53.77. f1-a of mnn f1-a is 69.29. acc-s of mnn acc-s is 80.86. f1-s of mnn f1-s is 55.45. f1-i of mnn f1-i is 55.93. f1-a of inabsa f1-a is 83.12. acc-s of inabsa acc-s is 79.06. f1-s of inabsa f1-s is 68.77. f1-i of inabsa f1-i is 65.94. f1-a of inabsa f1-a is 77.67. acc-s of inabsa acc-s is 71.72. f1-s of inabsa f1-s is 68.36. f1-i of inabsa f1-i is 55.95. f1-a of inabsa f1-a is 68.79. acc-s of inabsa acc-s is 80.96. f1-s of inabsa f1-s is 57.1. f1-i of inabsa f1-i is 55.45. f1-a of imn -d f1-a is 83.89. acc-s of imn -d acc-s is 80.69. f1-s of imn -d f1-s is 72.09. f1-i of imn -d f1-i is 67.27. f1-a of imn -d f1-a is 78.43. acc-s of imn -d acc-s is 72.49. f1-s of imn -d f1-s is 69.71. f1-i of imn -d f1-i is 57.13. f1-a of imn -d f1-a is 70.35. acc-s of imn -d acc-s is 81.86. f1-s of imn -d f1-s is 56.88. f1-i of imn -d f1-i is 57.86. f1-a of imn f1-a is 83.04. acc-s of imn acc-s is 83.05. f1-s of imn f1-s is 73.3. f1-i of imn f1-i is 68.71. f1-a of imn f1-a is 77.69. acc-s of imn acc-s is 75.12. f1-s of imn f1-s is 71.35. f1-i of imn f1-i is 58.04. f1-a of imn f1-a is 69.25. acc-s of imn acc-s is 84.53. f1-s of imn f1-s is 70.85. f1-i of imn f1-i is 58.18.
table 3 shows f1 score (%) comparison only for aspect term extraction. f1 of de-cnn sl is 81.26. f1 of de-cnn sr is 78.98. f1 of de-cnn st is 63.23. f1 of doer* sl is 82.11. f1 of doer* sr is 79.98. f1 of doer* st is 68.99. f1 of doer sl is 82.61. f1 of doer sr is 81.06. f1 of doer st is 71.35.
table 5 shows results of ims trained on different corpora on the english all-words wsd tasks. accuracy of senseval-2 onesec is 73.2. accuracy of senseval-2 tom is 70.5. accuracy of senseval-2 omsti is 74.1. accuracy of senseval-2 semcor is 76.8. accuracy of senseval-2 mfs is 72.1. accuracy of senseval-3 onesec is 68.2. accuracy of senseval-3 tom is 67.4. accuracy of senseval-3 omsti is 67.2. accuracy of senseval-3 semcor is 73.8. accuracy of senseval-3 mfs is 72. accuracy of semeval-07 onesec is 63.5. accuracy of semeval-07 tom is 59.8. accuracy of semeval-07 omsti is 62.3. accuracy of semeval-07 semcor is 67.3. accuracy of semeval-07 mfs is 65.4. accuracy of semeval-13 onesec is 66.5. accuracy of semeval-13 tom is 65.5. accuracy of semeval-13 omsti is 62.8. accuracy of semeval-13 semcor is 65.5. accuracy of semeval-13 mfs is 63. accuracy of semeval-15 onesec is 70.8. accuracy of semeval-15 tom is 68.6. accuracy of semeval-15 omsti is 63.1. accuracy of semeval-15 semcor is 66.1. accuracy of semeval-15 mfs is 66.3. accuracy of all onesec is 69. accuracy of all tom is 67.3. accuracy of all omsti is 66.4. accuracy of all semcor is 70.4. accuracy of all mfs is 67.6.
table 7 shows performance of joint relation and supporting evidence prediction in f1 measurement (%). f1 of heuristic predictor dev is 36.21. f1 of heuristic predictor test is 36.76. f1 of neural predictor dev is 44.07. f1 of neural predictor test is 43.85.
table 5 shows results on benchmark dataset (atis and subsets of snips). intent acc. of joint-sf-ic intent acc. is 96.1. slot f1 of joint-sf-ic slot f1 is 95.4. intent acc. of joint-sf-ic intent acc. is 99.7. slot f1 of joint-sf-ic slot f1 is 96.3. intent acc. of joint-sf-ic intent acc. is 100. slot f1 of joint-sf-ic slot f1 is 93.1. intent acc. of joint-sf-ic intent acc. is 100. slot f1 of joint-sf-ic slot f1 is 96.6. intent acc. of parallel[univ] intent acc. is 96.4. slot f1 of parallel[univ] slot f1 is 95.4. intent acc. of parallel[univ] intent acc. is 99.7. slot f1 of parallel[univ] slot f1 is 95.8. intent acc. of parallel[univ] intent acc. is 100. slot f1 of parallel[univ] slot f1 is 92.1. intent acc. of parallel[univ] intent acc. is 100. slot f1 of parallel[univ] slot f1 is 95.8. intent acc. of parallel[univ+task] intent acc. is 96.2. slot f1 of parallel[univ+task] slot f1 is 95.5. intent acc. of parallel[univ+task] intent acc. is 99.7. slot f1 of parallel[univ+task] slot f1 is 96. intent acc. of parallel[univ+task] intent acc. is 100. slot f1 of parallel[univ+task] slot f1 is 93.4. intent acc. of parallel[univ+task] intent acc. is 100. slot f1 of parallel[univ+task] slot f1 is 97.2. intent acc. of parallel[univ+group+task] intent acc. is 96.9. slot f1 of parallel[univ+group+task] slot f1 is 95.4. intent acc. of parallel[univ+group+task] intent acc. is 99.7. slot f1 of parallel[univ+group+task] slot f1 is 96.5. intent acc. of parallel[univ+group+task] intent acc. is 99.5. slot f1 of parallel[univ+group+task] slot f1 is 94.4. intent acc. of parallel[univ+group+task] intent acc. is 100. slot f1 of parallel[univ+group+task] slot f1 is 97.3. intent acc. of serial intent acc. is 97.2. slot f1 of serial slot f1 is 95.8. intent acc. of serial intent acc. is 100. slot f1 of serial slot f1 is 96.5. intent acc. of serial intent acc. is 100. slot f1 of serial slot f1 is 93.8. intent acc. of serial intent acc. is 100. slot f1 of serial slot f1 is 97.2. intent acc. of serial+highway intent acc. is 96.9. slot f1 of serial+highway slot f1 is 95.7. intent acc. of serial+highway intent acc. is 100. slot f1 of serial+highway slot f1 is 97.2. intent acc. of serial+highway intent acc. is 99.5. slot f1 of serial+highway slot f1 is 94.8. intent acc. of serial+highway intent acc. is 100. slot f1 of serial+highway slot f1 is 97.2. intent acc. of serial+highway+swap intent acc. is 97.5. slot f1 of serial+highway+swap slot f1 is 95.6. intent acc. of serial+highway+swap intent acc. is 99.7. slot f1 of serial+highway+swap slot f1 is 96. intent acc. of serial+highway+swap intent acc. is 100. slot f1 of serial+highway+swap slot f1 is 93.9. intent acc. of serial+highway+swap intent acc. is 100. slot f1 of serial+highway+swap slot f1 is 97.8.
table 6 shows results on the alexa dataset. intent acc. of joint-sf-ic mean is 93.36. intent acc. of joint-sf-ic median is 95.9. slot f1 of joint-sf-ic mean is 79.97. slot f1 of joint-sf-ic median is 85.23. intent acc. of parallel[univ] mean is 93.44. intent acc. of parallel[univ] median is 95.5. slot f1 of parallel[univ] mean is 80.76. slot f1 of parallel[univ] median is 86.18. intent acc. of parallel[univ+task] mean is 93.78. intent acc. of parallel[univ+task] median is 96.35. slot f1 of parallel[univ+task] mean is 80.49. slot f1 of parallel[univ+task] median is 85.81. intent acc. of parallel[univ+group+task] mean is 93.87. intent acc. of parallel[univ+group+task] median is 96.31. slot f1 of parallel[univ+group+task] mean is 80.84. slot f1 of parallel[univ+group+task] median is 86.21. intent acc. of serial mean is 93.83. intent acc. of serial median is 96.24. slot f1 of serial mean is 80.84. slot f1 of serial median is 86.14. intent acc. of serial+highway mean is 93.81. intent acc. of serial+highway median is 96.28. slot f1 of serial+highway mean is 80.73. slot f1 of serial+highway median is 85.71. intent acc. of serial+highway+swap mean is 94.02. intent acc. of serial+highway+swap median is 96.42. slot f1 of serial+highway+swap mean is 80.8. slot f1 of serial+highway+swap median is 86.44.
table 3 shows cross-domain (train/test on the different domain) response generation performance on the opendialkg dataset (metric: recall@k). r@1 of seq2seq (sutskever et al.,2014) r@1 is 2.9. r@3 of seq2seq (sutskever et al.,2014) r@3 is 21.3. r@5 of seq2seq (sutskever et al.,2014) r@5 is 35.1. r@10 of seq2seq (sutskever et al.,2014) r@10 is 50.6. r@25 of seq2seq (sutskever et al.,2014) r@25 is 64.2. r@1 of seq2seq (sutskever et al.,2014) r@1 is 1.5. r@3 of seq2seq (sutskever et al.,2014) r@3 is 12.1. r@5 of seq2seq (sutskever et al.,2014) r@5 is 19.7. r@10 of seq2seq (sutskever et al.,2014) r@10 is 34.9. r@25 of seq2seq (sutskever et al.,2014) r@25 is 49.4. r@1 of tri-lstm (young et al.,2018) r@1 is 2.3. r@3 of tri-lstm (young et al.,2018) r@3 is 17.9. r@5 of tri-lstm (young et al.,2018) r@5 is 29.7. r@10 of tri-lstm (young et al.,2018) r@10 is 44.9. r@25 of tri-lstm (young et al.,2018) r@25 is 61. r@1 of tri-lstm (young et al.,2018) r@1 is 1.9. r@3 of tri-lstm (young et al.,2018) r@3 is 8.7. r@5 of tri-lstm (young et al.,2018) r@5 is 12.9. r@10 of tri-lstm (young et al.,2018) r@10 is 25.8. r@25 of tri-lstm (young et al.,2018) r@25 is 44.4. r@1 of ext-ed (parthasarathi and pineau,2018) r@1 is 2. r@3 of ext-ed (parthasarathi and pineau,2018) r@3 is 7.9. r@5 of ext-ed (parthasarathi and pineau,2018) r@5 is 11.2. r@10 of ext-ed (parthasarathi and pineau,2018) r@10 is 16.4. r@25 of ext-ed (parthasarathi and pineau,2018) r@25 is 22.4. r@1 of ext-ed (parthasarathi and pineau,2018) r@1 is 1.3. r@3 of ext-ed (parthasarathi and pineau,2018) r@3 is 2.6. r@5 of ext-ed (parthasarathi and pineau,2018) r@5 is 3.8. r@10 of ext-ed (parthasarathi and pineau,2018) r@10 is 4.1. r@25 of ext-ed (parthasarathi and pineau,2018) r@25 is 8.3. r@1 of dialkg walker (ablation) r@1 is 8.2. r@3 of dialkg walker (ablation) r@3 is 15.7. r@5 of dialkg walker (ablation) r@5 is 22.8. r@10 of dialkg walker (ablation) r@10 is 31.8. r@25 of dialkg walker (ablation) r@25 is 48.9. r@1 of dialkg walker (ablation) r@1 is 4.5. r@3 of dialkg walker (ablation) r@3 is 16.7. r@5 of dialkg walker (ablation) r@5 is 21.6. r@10 of dialkg walker (ablation) r@10 is 25.8. r@25 of dialkg walker (ablation) r@25 is 33. r@1 of dialkg walker (ablation) r@1 is 12.6. r@3 of dialkg walker (ablation) r@3 is 28.6. r@5 of dialkg walker (ablation) r@5 is 38.6. r@10 of dialkg walker (ablation) r@10 is 54.1. r@25 of dialkg walker (ablation) r@25 is 65.6. r@1 of dialkg walker (ablation) r@1 is 6. r@3 of dialkg walker (ablation) r@3 is 15.9. r@5 of dialkg walker (ablation) r@5 is 22.8. r@10 of dialkg walker (ablation) r@10 is 33. r@25 of dialkg walker (ablation) r@25 is 47.5. r@1 of dialkg walker (proposed) r@1 is 13.5. r@3 of dialkg walker (proposed) r@3 is 28.8. r@5 of dialkg walker (proposed) r@5 is 39.5. r@10 of dialkg walker (proposed) r@10 is 52.6. r@25 of dialkg walker (proposed) r@25 is 64.8. r@1 of dialkg walker (proposed) r@1 is 5.3. r@3 of dialkg walker (proposed) r@3 is 13.3. r@5 of dialkg walker (proposed) r@5 is 19.7. r@10 of dialkg walker (proposed) r@10 is 28.8. r@25 of dialkg walker (proposed) r@25 is 38.
table 4 shows sentence selection evaluation and average label accuracy of gear with different thresholds on dev set (%). ofever of 0 ofever is 91.1. precision of 0 precision is 24.08. recall of 0 recall is 86.72. f1 of 0 f1 is 37.69. gear la of 0 gear la is 74.84. ofever of 10^-4 ofever is 91.04. precision of 10^-4 precision is 30.88. recall of 10^-4 recall is 86.63. f1 of 10^-4 f1 is 45.53. gear la of 10^-4 gear la is 74.86. ofever of 10^-3 ofever is 90.86. precision of 10^-3 precision is 40.6. recall of 10^-3 recall is 86.36. f1 of 10^-3 f1 is 55.23. gear la of 10^-3 gear la is 74.91. ofever of 10^-2 ofever is 90.27. precision of 10^-2 precision is 53.12. recall of 10^-2 recall is 85.47. f1 of 10^-2 f1 is 65.52. gear la of 10^-2 gear la is 74.89. ofever of 10^-1 ofever is 87.7. precision of 10^-1 precision is 70.61. recall of 10^-1 recall is 81.64. f1 of 10^-1 f1 is 75.72. gear la of 10^-1 gear la is 74.81.
table 7 shows evaluations of the full pipeline. la of athene la is 68.49. fever of athene fever is 64.74. la of athene la is 65.46. fever of athene fever is 61.58. la of ucl mrg la is 69.66. fever of ucl mrg fever is 65.41. la of ucl mrg la is 67.62. fever of ucl mrg fever is 62.52. la of unc nlp la is 69.72. fever of unc nlp fever is 66.49. la of unc nlp la is 68.21. fever of unc nlp fever is 64.21. la of bert pair la is 73.3. fever of bert pair fever is 68.9. la of bert pair la is 69.75. fever of bert pair fever is 65.18. la of bert concat la is 73.67. fever of bert concat fever is 68.89. la of bert concat la is 71.01. fever of bert concat fever is 65.64. la of our pipeline la is 74.84. fever of our pipeline fever is 70.69. la of our pipeline la is 71.6. fever of our pipeline fever is 67.1.
table 4 shows the comparison of seq2seq model performance using transformer (xformer) and lstm encoders. unweighted f1 of sx xformer is 0.67. unweighted f1 of sx lstm is 0.70. weighted f1 of sx xformer is 0.76. weighted f1 of sx lstm is 0.79. unweighted f1 of sx + status xformer is 0.51. unweighted f1 of sx + status lstm is 0.55. weighted f1 of sx + status xformer is 0.61. weighted f1 of sx + status lstm is 0.64.
table 5 shows overall prediction results and f-scores for counseling quality using linguistic feature sets acc. of baseline - is 59.85%. f-score of baseline  low is . f-score of baseline  high is . acc. of n-grams - is 87.26%. f-score of n-grams  low is 0.849. f-score of n-grams  high is 0.89. acc. of semantic - is 80.31%. f-score of semantic  low is 0.763. f-score of semantic  high is 0.832. acc. of metafeatures - is 72.59%. f-score of metafeatures  low is 0.297. f-score of metafeatures  high is 0.83. acc. of sentiment - is 74.52%. f-score of sentiment  low is 0.298. f-score of sentiment  high is 0.844. acc. of alignment - is 72.59%. f-score of alignment  low is 0.64. f-score of alignment  high is 0.779. acc. of topics - is 81.08%. f-score of topics  low is 0.768. f-score of topics  high is 0.84. acc. of miti behav - is 79.54%. f-score of miti behav  low is 0.787. f-score of miti behav  high is 0.808. acc. of all features - is 88.03%. f-score of all features  low is 0.857. f-score of all features  high is 0.897.
table 1 shows seq vs. macro f-scores of news camb is 0.8633. macro f-scores of news seq is 0.8763 (+1.30). macro f-scores of wikinews camb is 0.8317. macro f-scores of wikinews seq is 0.8540 (+2.23). macro f-scores of wikipedia camb is 0.7780. macro f-scores of wikipedia seq is 0.8140 (+3.60). macro f-scores of news camb is 0.8736. macro f-scores of news seq is 0.8763 (+0.27). macro f-scores of wikinews camb is 0.8400. macro f-scores of wikinews seq is 0.8505 (+1.05). macro f-scores of wikipedia camb is 0.8115. macro f-scores of wikipedia seq is 0.8158 (+0.43).
table 3 shows experimental results in semeval setting match m of dl-bilstm+glove m=1 is 54.6. match m of dl-bilstm+glove m=2 is 69.2. match m of dl-bilstm+glove m=3 is 76.5. match m of dl-bilstm+glove m=4 is 81.9. match m of dl-bilstm+glove+att m=1 is 57.5. match m of dl-bilstm+glove+att m=2 is 69.7. match m of dl-bilstm+glove+att m=3 is 76.7. match m of dl-bilstm+glove+att m=4 is 80.7. match m of dl-bilstm+elmo m=1 is 0.6. match m of dl-bilstm+elmo m=2 is 71.7. match m of dl-bilstm+elmo m=3 is 78.7. match m of dl-bilstm+elmo m=4 is 84.1. match m of dl-bilstm+elmo+att m=1 is 59.6. match m of dl-bilstm+elmo+att m=2 is 72.7. match m of dl-bilstm+elmo+att m=3 is 77.7. match m of dl-bilstm+elmo+att m=4 is 84.6. match m of sl-bilstm+glove m=1 is 51.7. match m of sl-bilstm+glove m=2 is 66.7. match m of sl-bilstm+glove m=3 is 75.0. match m of sl-bilstm+glove m=4 is 81.1. match m of sl-bilstm+glove+att m=1 is 52.9. match m of sl-bilstm+glove+att m=2 is 66.5. match m of sl-bilstm+glove+att m=3 is 73.6. match m of sl-bilstm+glove+att m=4 is 0.8. match m of sl-bilstm+elmo m=1 is 54.2. match m of sl-bilstm+elmo m=2 is 69.0. match m of sl-bilstm+elmo m=3 is 77.9. match m of sl-bilstm+elmo m=4 is 83.0. match m of sl-bilstm+elmo+att m=1 is 54.2. match m of sl-bilstm+elmo+att m=2 is 70.7. match m of sl-bilstm+elmo+att m=3 is 78.5. match m of sl-bilstm+elmo+att m=4 is 82.8. match m of crf m=1 is 45.4. match m of crf m=2 is 66.0. match m of crf m=3 is 72.8. match m of crf m=4 is 80.2.
table 1 shows results of unmt accuracy of baseline fr-en is 24.5. accuracy of baseline en-fr is 25.37. accuracy of baseline ja-en is 14.09. accuracy of baseline en-ja is 21.63. accuracy of baseline-fix fr-en is 24.22. accuracy of baseline-fix en-fr is 25.26. accuracy of baseline-fix ja-en is 13.88. accuracy of baseline-fix en-ja is 21.93.
table 2 shows translation results of different transfer learning setups. bleu (%) of baseline eu-en is 1.7. bleu (%) of baseline sl-en is 10.1. bleu (%) of baseline be-en is 3.2. bleu (%) of baseline az-en is 3.1. bleu (%) of baseline tr-en is 0.8. bleu (%) of multilingual (johnson et al. 2017) eu-en is 5.1. bleu (%) of multilingual (johnson et al. 2017) sl-en is 16.7. bleu (%) of multilingual (johnson et al. 2017) be-en is 4.2. bleu (%) of multilingual (johnson et al. 2017) az-en is 4.5. bleu (%) of multilingual (johnson et al. 2017) tr-en is 8.7. bleu (%) of transfer (zoph et al. 2016) eu-en is 4.9. bleu (%) of transfer (zoph et al. 2016) sl-en is 19.2. bleu (%) of transfer (zoph et al. 2016) be-en is 8.9. bleu (%) of transfer (zoph et al. 2016) az-en is 5.3. bleu (%) of transfer (zoph et al. 2016) tr-en is 7.4. bleu (%) of  + cross-lingual word embedding eu-en is 7.4. bleu (%) of  + cross-lingual word embedding sl-en is 20.6. bleu (%) of  + cross-lingual word embedding be-en is 12.2. bleu (%) of  + cross-lingual word embedding az-en is 7.4. bleu (%) of  + cross-lingual word embedding tr-en is 9.4. bleu (%) of  + artificial noises eu-en is 8.2. bleu (%) of  + artificial noises sl-en is 21.3. bleu (%) of  + artificial noises be-en is 12.8. bleu (%) of  + artificial noises az-en is 8.1. bleu (%) of  + artificial noises tr-en is 10.1. bleu (%) of  + synthetic data eu-en is 9.7. bleu (%) of  + synthetic data sl-en is 22.1. bleu (%) of  + synthetic data be-en is 14. bleu (%) of  + synthetic data az-en is 9. bleu (%) of  + synthetic data tr-en is 11.3.
table 5 shows translation results with different sizes of the source vocabulary. bleu (%) of 10k sl-en is 21. bleu (%) of 10k be-en is 11.2. bleu (%) of 20k sl-en is 20.6. bleu (%) of 20k be-en is 12.2. bleu (%) of 50k sl-en is 20.2. bleu (%) of 50k be-en is 10.9. bleu (%) of 70k sl-en is 20. bleu (%) of 70k be-en is 10.9.
table 1 shows relation extraction manual evaluation results: precision of top 1000 predictions. precision of pcnn+att 100 is 97. precision of pcnn+att 300 is 93.7. precision of pcnn+att 500 is 92.8. precision of pcnn+att 700 is 89.1. precision of pcnn+att 900 is 85.2. precision of pcnn+att 1000 is 83.9. precision of pcnn+att+glore 100 is 97. precision of pcnn+att+glore 300 is 97.3. precision of pcnn+att+glore 500 is 94.6. precision of pcnn+att+glore 700 is 93.3. precision of pcnn+att+glore 900 is 90.1. precision of pcnn+att+glore 1000 is 89.3. precision of pcnn+att+glore+ 100 is 98. precision of pcnn+att+glore+ 300 is 98.7. precision of pcnn+att+glore+ 500 is 96.6. precision of pcnn+att+glore+ 700 is 93.1. precision of pcnn+att+glore+ 900 is 89.9. precision of pcnn+att+glore+ 1000 is 88.8. precision of pcnn+att+glore++ 100 is 98. precision of pcnn+att+glore++ 300 is 97.3. precision of pcnn+att+glore++ 500 is 96. precision of pcnn+att+glore++ 700 is 93.6. precision of pcnn+att+glore++ 900 is 91. precision of pcnn+att+glore++ 1000 is 89.8.
table 1 shows comparison between our model and the stateof-the-art models using ace 2005 english corpus. p% of sptree p% is 70.1. r% of sptree r% is 61.2. f1% of sptree f1% is 65.3. p% of walk-based p% is 69.7. r% of walk-based r% is 59.5. f1% of walk-based f1% is 64.2. p% of baseline p% is 58.8. r% of baseline r% is 57.3. f1% of baseline f1% is 57.2. p% of baseline+tag p% is 61.3. r% of baseline+tag r% is 76.7. f1% of baseline+tag f1% is 67.4. p% of baseline+mtl p% is 63.8. r% of baseline+mtl r% is 56.1. f1% of baseline+mtl f1% is 59.5. p% of baseline+mtl+tag p% is 66.5. r% of baseline+mtl+tag r% is 71.8. f1% of baseline+mtl+tag f1% is 68.9.
table 1 shows results for both nyt and webnlg datasets. precision of noveltagging precision is 62.40%. recall of noveltagging recall is 31.70%. f1 of noveltagging f1 is 42.00%. precision of noveltagging precision is 52.50%. recall of noveltagging recall is 19.30%. f1 of noveltagging f1 is 28.30%. precision of onedecoder precision is 59.40%. recall of onedecoder recall is 53.10%. f1 of onedecoder f1 is 56.00%. precision of onedecoder precision is 32.20%. recall of onedecoder recall is 28.90%. f1 of onedecoder f1 is 30.50%. precision of multidecoder precision is 61.00%. recall of multidecoder recall is 56.60%. f1 of multidecoder f1 is 58.70%. precision of multidecoder precision is 37.70%. recall of multidecoder recall is 36.40%. f1 of multidecoder f1 is 37.10%. precision of graphrel1p precision is 62.90%. recall of graphrel1p recall is 57.30%. f1 of graphrel1p f1 is 60.00%. precision of graphrel1p precision is 42.30%. recall of graphrel1p recall is 39.20%. f1 of graphrel1p f1 is 40.70%. precision of graphrel2p precision is 63.90%. recall of graphrel2p recall is 60.00%. f1 of graphrel2p f1 is 61.90%. precision of graphrel2p precision is 44.70%. recall of graphrel2p recall is 41.10%. f1 of graphrel2p f1 is 42.90%.
table 3 shows total diagnostic results, where columns contain the precision, recall and accuracy of ds-generated labels evaluated on 200 human-annotated labels as well as the number of positive and negative patterns preserved after the pattern-refinement stage, and we underline some cases in which ds performs poorly. prec. of r0 prec. is 100. recall of r0 recall is 81.8. acc. of r0 acc. is 82. #pos. of r0 #pos. is 20. #neg. of r0 #neg. is 0. prec. of r1 prec. is 93.9. recall of r1 recall is 33.5. acc. of r1 acc. is 36.2. #pos. of r1 #pos. is 18. #neg. of r1 #neg. is 0. prec. of r2 prec. is 75.7. recall of r2 recall is 88. acc. of r2 acc. is 76.5. #pos. of r2 #pos. is 9. #neg. of r2 #neg. is 5. prec. of r3 prec. is 100. recall of r3 recall is 91.4. acc. of r3 acc. is 92. #pos. of r3 #pos. is 20. #neg. of r3 #neg. is 0. prec. of r4 prec. is 93.3. recall of r4 recall is 72.4. acc. of r4 acc. is 80.9. #pos. of r4 #pos. is 10. #neg. of r4 #neg. is 2. prec. of r5 prec. is 93.8. recall of r5 recall is 77.3. acc. of r5 acc. is 86.5. #pos. of r5 #pos. is 15. #neg. of r5 #neg. is 0. prec. of r6 prec. is 88.3. recall of r6 recall is 76.9. acc. of r6 acc. is 75.1. #pos. of r6 #pos. is 14. #neg. of r6 #neg. is 0. prec. of r7 prec. is 91.9. recall of r7 recall is 64.6. acc. of r7 acc. is 64. #pos. of r7 #pos. is 20. #neg. of r7 #neg. is 0. prec. of r8 prec. is 29.3. recall of r8 recall is 30.4. acc. of r8 acc. is 60. #pos. of r8 #pos. is 4. #neg. of r8 #neg. is 10. prec. of r9 prec. is 66.7. recall of r9 recall is 38.1. acc. of r9 acc. is 74.4. #pos. of r9 #pos. is 6. #neg. of r9 #neg. is 11. prec. of r6u prec. is 81.8. recall of r6u recall is 90.7. acc. of r6u acc. is 81. #pos. of r6u #pos. is 7. #neg. of r6u #neg. is 0. prec. of r7u prec. is 93.5. recall of r7u recall is 70.7. acc. of r7u acc. is 68.3. #pos. of r7u #pos. is 17. #neg. of r7u #neg. is 1. prec. of r8u prec. is 35. recall of r8u recall is 70. acc. of r8u acc. is 60. #pos. of r8u #pos. is 4. #neg. of r8u #neg. is 15. prec. of r9u prec. is 87.5. recall of r9u recall is 59.2. acc. of r9u acc. is 67.7. #pos. of r9u #pos. is 12. #neg. of r9u #neg. is 5.
table 3 shows overall performance. h@1 of mtranse h@1 is 0.308. h@10 of mtranse h@10 is 0.614. mrr of mtranse mrr is 0.364. h@1 of mtranse h@1 is 0.279. h@10 of mtranse h@10 is 0.575. mrr of mtranse mrr is 0.349. h@1 of mtranse h@1 is 0.244. h@10 of mtranse h@10 is 0.556. mrr of mtranse mrr is 0.335. h@1 of mtranse h@1 is 0.281. h@10 of mtranse h@10 is 0.52. mrr of mtranse mrr is 0.363. h@1 of mtranse h@1 is 0.252. h@10 of mtranse h@10 is 0.493. mrr of mtranse mrr is 0.334. h@1 of jape h@1 is 0.412. h@10 of jape h@10 is 0.745. mrr of jape mrr is 0.49. h@1 of jape h@1 is 0.363. h@10 of jape h@10 is 0.685. mrr of jape mrr is 0.476. h@1 of jape h@1 is 0.324. h@10 of jape h@10 is 0.667. mrr of jape mrr is 0.43. h@1 of jape h@1 is 0.318. h@10 of jape h@10 is 0.589. mrr of jape mrr is 0.411. h@1 of jape h@1 is 0.236. h@10 of jape h@10 is 0.484. mrr of jape mrr is 0.32. h@1 of alignea h@1 is 0.472. h@10 of alignea h@10 is 0.792. mrr of alignea mrr is 0.581. h@1 of alignea h@1 is 0.448. h@10 of alignea h@10 is 0.789. mrr of alignea mrr is 0.563. h@1 of alignea h@1 is 0.481. h@10 of alignea h@10 is 0.824. mrr of alignea mrr is 0.599. h@1 of alignea h@1 is 0.566. h@10 of alignea h@10 is 0.827. mrr of alignea mrr is 0.655. h@1 of alignea h@1 is 0.633. h@10 of alignea h@10 is 0.848. mrr of alignea mrr is 0.707. h@1 of gcn-align h@1 is 0.413. h@10 of gcn-align h@10 is 0.744. mrr of gcn-align mrr is 0.549. h@1 of gcn-align h@1 is 0.399. h@10 of gcn-align h@10 is 0.745. mrr of gcn-align mrr is 0.546. h@1 of gcn-align h@1 is 0.373. h@10 of gcn-align h@10 is 0.745. mrr of gcn-align mrr is 0.532. h@1 of gcn-align h@1 is 0.506. h@10 of gcn-align h@10 is 0.772. mrr of gcn-align mrr is 0.6. h@1 of gcn-align h@1 is 0.597. h@10 of gcn-align h@10 is 0.838. mrr of gcn-align mrr is 0.682. h@1 of mugnn w/o asr h@1 is 0.479. h@10 of mugnn w/o asr h@10 is 0.833. mrr of mugnn w/o asr mrr is 0.597. h@1 of mugnn w/o asr h@1 is 0.487. h@10 of mugnn w/o asr h@10 is 0.851. mrr of mugnn w/o asr mrr is 0.604. h@1 of mugnn w/o asr h@1 is 0.496. h@10 of mugnn w/o asr h@10 is 0.869. mrr of mugnn w/o asr mrr is 0.621. h@1 of mugnn w/o asr h@1 is 0.59. h@10 of mugnn w/o asr h@10 is 0.887. mrr of mugnn w/o asr mrr is 0.693. h@1 of mugnn w/o asr h@1 is 0.73. h@10 of mugnn w/o asr h@10 is 0.934. mrr of mugnn w/o asr mrr is 0.801. h@1 of mugnn h@1 is 0.494. h@10 of mugnn h@10 is 0.844. mrr of mugnn mrr is 0.611. h@1 of mugnn h@1 is 0.501. h@10 of mugnn h@10 is 0.857. mrr of mugnn mrr is 0.621. h@1 of mugnn h@1 is 0.495. h@10 of mugnn h@10 is 0.87. mrr of mugnn mrr is 0.621. h@1 of mugnn h@1 is 0.616. h@10 of mugnn h@10 is 0.897. mrr of mugnn mrr is 0.714. h@1 of mugnn h@1 is 0.741. h@10 of mugnn h@10 is 0.937. mrr of mugnn mrr is 0.81.
table 3 shows comparison of lstm and bert models under human evaluations against gs-ec attack. readability of lstm readability is 0.6. human accuracy of lstm human accuracy is 52.10%. readability of bert readability is 1. human accuracy of bert human accuracy is 68.80%.
table 2 shows sst-5 and sst-2 performance on all and root nodes respectively. accuracy of nb sst-5(all) is 67.2. accuracy of nb sst-2(root) is 81.8. accuracy of svm sst-5(all) is 64.3. accuracy of svm sst-2(root) is 79.4. accuracy of binb sst-5(all) is 71. accuracy of binb sst-2(root) is 83.1. accuracy of vecavg sst-5(all) is 73.3. accuracy of vecavg sst-2(root) is 80.1. accuracy of rnn sst-5(all) is 79. accuracy of rnn sst-2(root) is 82.4. accuracy of mv-rnn sst-5(all) is 78.7. accuracy of mv-rnn sst-2(root) is 82.9. accuracy of rntn sst-5(all) is 80.7. accuracy of rntn sst-2(root) is 85.4. accuracy of rae sst-5(all) is 81.07. accuracy of rae sst-2(root) is 83. accuracy of gensen sst-2(root) is 84.5. accuracy of rae + gensen sst-2(root) is 86.43. accuracy of bertbase sst-2(root) is 93.5.
table 3 shows the results (feats) of the learning curve over the egy training dataset, for the egy dataset alone, multitask learning (mtl), and the adversarial training (adv). accuracy of 2k (1.5%) none is 29.7. accuracy of 2k (1.5%) mtl is 61.9. accuracy of 2k (1.5%) adv is 71.1. accuracy of 8k (6%) none is 62.5. accuracy of 8k (6%) mtl is 73.5. accuracy of 8k (6%) adv is 78.3. accuracy of 16k (12%) none is 74.7. accuracy of 16k (12%) mtl is 78.1. accuracy of 16k (12%) adv is 81.5. accuracy of 33k (25%) none is 80.7. accuracy of 33k (25%) mtl is 81.6. accuracy of 33k (25%) adv is 83.5. accuracy of 67k (50%) none is 83.3. accuracy of 67k (50%) mtl is 82. accuracy of 67k (50%) adv is 84. accuracy of 134k (100%) none is 84.5. accuracy of 134k (100%) mtl is 85.4. accuracy of 134k (100%) adv is 85.6.
table 6 shows test results en-nl (all sentences). bleu of baseline nmt bleu is 51.45. ter of baseline nmt ter is 36.21. met. of baseline nmt met. is 69.83. bleu of baseline smt bleu is 54.21. ter of baseline smt ter is 35.99. met. of baseline smt met. is 71.28. bleu of baseline tm-smt bleu is 55.72. ter of baseline tm-smt ter is 34.96. met. of baseline tm-smt met. is 72.25. bleu of google translate bleu is 44.37. ter of google translate ter is 41.51. met. of google translate met. is 65.07. bleu of best nfr + nmt backoff bleu is 58.91. ter of best nfr + nmt backoff ter is 31.36. met. of best nfr + nmt backoff met. is 74.12. bleu of best nfr unified bleu is 58.6. ter of best nfr unified ter is 31.57. met. of best nfr unified met. is 73.96.
table 4 shows performance comparisons on the personsentence dataset (yamaguchi et al., 2017). accuracy of random 0.4 is 15.1. accuracy of random 0.5 is 7.2. accuracy of random 0.6 is 3.5. accuracy of random average is 8.6. accuracy of proposal upper bound 0.4 is 89.8. accuracy of proposal upper bound 0.5 is 79.9. accuracy of proposal upper bound 0.6 is 64.1. accuracy of proposal upper bound average is 77.9. accuracy of dvsa+avg 0.4 is 39.8. accuracy of dvsa+avg 0.5 is 30.3. accuracy of dvsa+avg 0.6 is 19.7. accuracy of dvsa+avg average is 29.9. accuracy of dvsa+netvlad 0.4 is 34.1. accuracy of dvsa+netvlad 0.5 is 25. accuracy of dvsa+netvlad 0.6 is 18.3. accuracy of dvsa+netvlad average is 25.8. accuracy of dvsa+lstm 0.4 is 42.7. accuracy of dvsa+lstm 0.5 is 30.2. accuracy of dvsa+lstm 0.6 is 20. accuracy of dvsa+lstm average is 31. accuracy of grounder+avg 0.4 is 45.5. accuracy of grounder+avg 0.5 is 32.2. accuracy of grounder+avg 0.6 is 21.7. accuracy of grounder+avg average is 33.1. accuracy of grounder+netvlad 0.4 is 22.1. accuracy of grounder+netvlad 0.5 is 16.1. accuracy of grounder+netvlad 0.6 is 8.6. accuracy of grounder+netvlad average is 15.6. accuracy of grounder+lstm 0.4 is 39.9. accuracy of grounder+lstm 0.5 is 28.2. accuracy of grounder+lstm 0.6 is 17.7. accuracy of grounder+lstm average is 28.6. accuracy of ours w/o l div 0.4 is 57.9. accuracy of ours w/o l div 0.5 is 47.7. accuracy of ours w/o l div 0.6 is 35.6. accuracy of ours w/o l div average is 47.1. accuracy of ours 0.4 is 62.5. accuracy of ours 0.5 is 52. accuracy of ours 0.6 is 38.4. accuracy of ours average is 51.
table 1 shows test results on glue tasks for various models: baseline, enas, and cas (continual architecture search). accuracy of bilstm+elmo (2018) qnli is 69.4. accuracy of bilstm+elmo (2018) rte is 50.1. accuracy of bilstm+elmo (2018) wnli is 65.1. accuracy of bilstm+elmo+attn (2018) qnli is 61.1. accuracy of bilstm+elmo+attn (2018) rte is 50.3. accuracy of bilstm+elmo+attn (2018) wnli is 65.1. accuracy of baseline (with elmo) qnli is 73.2. accuracy of baseline (with elmo) rte is 52.3. accuracy of baseline (with elmo) wnli is 65.1. accuracy of enas (architecture search) qnli is 74.5. accuracy of enas (architecture search) rte is 52.9. accuracy of enas (architecture search) wnli is 65.1. accuracy of cas step-1 (qnli training) qnli is 73.8. accuracy of cas step-1 (qnli training) rte is n/a. accuracy of cas step-1 (qnli training) wnli is n/a. accuracy of cas step-2 (rte training) qnli is 73.6. accuracy of cas step-2 (rte training) rte is 54.1. accuracy of cas step-2 (rte training) wnli is n/a. accuracy of cas step-3 (wnli training) qnli is 73.3. accuracy of cas step-3 (wnli training) rte is 54.0. accuracy of cas step-3 (wnli training) wnli is 64.4.
table 3 shows accuracy [%] of csda w. accuracy of f b is 77.9. accuracy of f d is 80.6. accuracy of f e is 84.4. accuracy of f k is 86.5. accuracy of f average is 82.3. accuracy of f+y b is 80.0. accuracy of f+y d is 84.3. accuracy of f+y e is 86.2. accuracy of f+y k is 87.0. accuracy of f+y average is 84.4. accuracy of y b is 77.6. accuracy of y d is 81.5. accuracy of y e is 83.7. accuracy of y k is 85.2. accuracy of y average is 82.0.
table 2 shows results of human evaluation. consistency of sc-lstm consistency is 1.67. novelty of sc-lstm novelty is 2.04. diversity of sc-lstm diversity is 1.39. coherence of sc-lstm coherence is 1.16. consistency of pnn consistency is 2.52. novelty of pnn novelty is 1.96. diversity of pnn diversity is 1.95. coherence of pnn coherence is 2.84. consistency of mta consistency is 3.17. novelty of mta novelty is 2.56. diversity of mta diversity is 2.43. coherence of mta coherence is 3.28. consistency of cvae consistency is 3.42*. novelty of cvae novelty is 2.87*. diversity of cvae diversity is 2.74*. coherence of cvae coherence is 2.63. consistency of plan&write consistency is 3.27. novelty of plan&write novelty is 2.81. diversity of plan&write diversity is 2.56. coherence of plan&write coherence is 3.36*. consistency of proposal consistency is 3.84. novelty of proposal novelty is 3.24. diversity of proposal diversity is 3.16. coherence of proposal coherence is 3.61.
table 3 shows automatic evaluations of ablation study. bleu of full model bleu is 9.72. consistency of full model consistency is 39.42. novelty of full model novelty is 75.71. dist-1 of full model dist-1 is 5.19. dist-2 of full model dist-2 is 20.49. bleu of w/o adversarial training bleu is 7.74. consistency of w/o adversarial training consistency is 31.74. novelty of w/o adversarial training novelty is 74.13. dist-1 of w/o adversarial training dist-1 is 5.22. dist-2 of w/o adversarial training dist-2 is 20.43. bleu of w/o memory bleu is 8.4. consistency of w/o memory consistency is 33.95. novelty of w/o memory novelty is 71.86. dist-1 of w/o memory dist-1 is 4.16. dist-2 of w/o memory dist-2 is 17.59. bleu of w/o dynamic bleu is 8.46. consistency of w/o dynamic consistency is 36.18. novelty of w/o dynamic novelty is 73.62. dist-1 of w/o dynamic dist-1 is 4.18. dist-2 of w/o dynamic dist-2 is 18.49.
table 7 shows results on rotowire (rw) and mlb development sets using relation generation (rg) count (#) and precision (p%), content selection (cs) precision (p%) and recall (r%), content ordering (co) in normalized damerau-levenshtein distance (dld%), and bleu. # of templ # is 54.29. p% of templ p% is 99.92. p% of templ p% is 26.61. r% of templ r% is 59.16. dld% of templ dld% is 14.42. bleu of templ bleu is 8.51. # of ws-2017 # is 23.95. p% of ws-2017 p% is 75.1. p% of ws-2017 p% is 28.11. r% of ws-2017 r% is 35.86. dld% of ws-2017 dld% is 15.33. bleu of ws-2017 bleu is 14.57. # of ed+cc # is 22.68. p% of ed+cc p% is 79.4. p% of ed+cc p% is 29.96. r% of ed+cc r% is 34.11. dld% of ed+cc dld% is 16. bleu of ed+cc bleu is 14. # of ncp+cc # is 33.88. p% of ncp+cc p% is 87.51. p% of ncp+cc p% is 33.52. r% of ncp+cc r% is 51.21. dld% of ncp+cc dld% is 18.57. bleu of ncp+cc bleu is 16.19. # of ent # is 31.84. p% of ent p% is 91.97. p% of ent p% is 36.65. r% of ent r% is 48.18. dld% of ent dld% is 19.68. bleu of ent bleu is 15.97.
table 1 shows results of our model and the baselines. bleu of vanilla seq2seq bleu is 2.14. nist of vanilla seq2seq nist is 0.2809. rouge of vanilla seq2seq rouge is 0.47. bleu of structure-s2s bleu is 3.27. nist of structure-s2s nist is 0.9612. rouge of structure-s2s rouge is 0.71. bleu of pretrainedmt bleu is 4.35. nist of pretrainedmt nist is 1.9937. rouge of pretrainedmt rouge is 0.91. bleu of semimt bleu is 6.76. nist of semimt nist is 3.5017. rouge of semimt rouge is 2.04. bleu of pivot-vanilla bleu is 20.09. nist of pivot-vanilla nist is 6.5130. rouge of pivot-vanilla rouge is 18.31. bleu of transformer bleu is 5.48. nist of transformer nist is 1.9873. rouge of transformer rouge is 1.26. bleu of pretrainedmt bleu is 6.43. nist of pretrainedmt nist is 2.1019. rouge of pretrainedmt rouge is 1.77. bleu of semimt bleu is 9.71. nist of semimt nist is 2.7019. rouge of semimt rouge is 3.31. bleu of pivot-trans bleu is 27.34. nist of pivot-trans nist is 6.8763. rouge of pivot-trans rouge is 19.3.
table 2 shows rouge scores on the cl-scisumm 2016 test benchmark. 2-r of talksumm-hybrid 2-r is 35.05. 2-f of talksumm-hybrid 2-f is 34.11. 3-f of talksumm-hybrid 3-f is 27.19. su4-f of talksumm-hybrid su4-f is 24.13. 2-r of talksumm-only 2-r is 22.77. 2-f of talksumm-only 2-f is 21.94. 3-f of talksumm-only 3-f is 15.94. su4-f of talksumm-only su4-f is 12.55. 2-r of gcn hybrid2* 2-r is 32.44. 2-f of gcn hybrid2* 2-f is 30.08. 3-f of gcn hybrid2* 3-f is 23.43. su4-f of gcn hybrid2* su4-f is 23.77. 2-r of gcn cited text spans* 2-r is 25.16. 2-f of gcn cited text spans* 2-f is 24.26. 3-f of gcn cited text spans* 3-f is 18.79. su4-f of gcn cited text spans* su4-f is 17.67. 2-r of abstract* 2-r is 29.52. 2-f of abstract* 2-f is 29.4. 3-f of abstract* 3-f is 23.16. su4-f of abstract* su4-f is 23.34.
table 2 shows rouge f1 score of the evaluation set (%). r-1 of textrank r-1 is 8.63. r-2 of textrank r-2 is 1.24. r-l of textrank r-l is 7.26. r-1 of textrank r-1 is 7.16. r-2 of textrank r-2 is 0.89. r-l of textrank r-l is 6.39. r-1 of textrank r-1 is 8.27. r-2 of textrank r-2 is 1.44. r-l of textrank r-l is 7.35. r-1 of opinosis r-1 is 8.25. r-2 of opinosis r-2 is 1.51. r-l of opinosis r-l is 7.52. r-1 of opinosis r-1 is 7.04. r-2 of opinosis r-2 is 1.42. r-l of opinosis r-l is 6.45. r-1 of opinosis r-1 is 7.8. r-2 of opinosis r-2 is 1.2. r-l of opinosis r-l is 7.11. r-1 of meansum-single r-1 is 8.12. r-2 of meansum-single r-2 is 0.58. r-l of meansum-single r-l is 7.3. r-1 of meansum-single r-1 is 5.42. r-2 of meansum-single r-2 is 0.47. r-l of meansum-single r-l is 4.97. r-1 of meansum-single r-1 is 6.96. r-2 of meansum-single r-2 is 0.35. r-l of meansum-single r-l is 6.08. r-1 of strsum r-1 is 11.61. r-2 of strsum r-2 is 1.56. r-l of strsum r-l is 11.04. r-1 of strsum r-1 is 9.15. r-2 of strsum r-2 is 1.38. r-l of strsum r-l is 8.79. r-1 of strsum r-1 is 7.38. r-2 of strsum r-2 is 1.03. r-l of strsum r-l is 6.94. r-1 of strsum+discourserank r-1 is 11.87. r-2 of strsum+discourserank r-2 is 1.63. r-l of strsum+discourserank r-l is 11.4. r-1 of strsum+discourserank r-1 is 9.62. r-2 of strsum+discourserank r-2 is 1.58. r-l of strsum+discourserank r-l is 9.28. r-1 of strsum+discourserank r-1 is 8.15. r-2 of strsum+discourserank r-2 is 1.33. r-l of strsum+discourserank r-l is 7.62. r-1 of seq-seq r-1 is 13.5. r-2 of seq-seq r-2 is 2.1. r-l of seq-seq r-l is 13.31. r-1 of seq-seq r-1 is 10.69. r-2 of seq-seq r-2 is 2.02. r-l of seq-seq r-l is 10.61. r-1 of seq-seq r-1 is 7.71. r-2 of seq-seq r-2 is 2.18. r-l of seq-seq r-l is 7.08. r-1 of seq-seq-att r-1 is 16.28. r-2 of seq-seq-att r-2 is 3.13. r-l of seq-seq-att r-l is 16.13. r-1 of seq-seq-att r-1 is 11.49. r-2 of seq-seq-att r-2 is 2.39. r-l of seq-seq-att r-l is 11.47. r-1 of seq-seq-att r-1 is 9.05. r-2 of seq-seq-att r-2 is 2.99. r-l of seq-seq-att r-l is 8.46.
table 2 shows instance selection results; evaluated for primary, secondary, and all ground-truth sentences. p of lead-baseline p is 31.9. r of lead-baseline r is 38.4. f of lead-baseline f is 34.9. p of lead-baseline p is 10.7. r of lead-baseline r is 34.3. f of lead-baseline f is 16.3. p of lead-baseline p is 39.9. r of lead-baseline r is 37.3. f of lead-baseline f is 38.6. p of sumbasic (vanderwende et al., 2007) p is 15.2. r of sumbasic (vanderwende et al., 2007) r is 17.3. f of sumbasic (vanderwende et al., 2007) f is 16.2. p of sumbasic (vanderwende et al., 2007) p is 5.3. r of sumbasic (vanderwende et al., 2007) r is 15.8. f of sumbasic (vanderwende et al., 2007) f is 8. p of sumbasic (vanderwende et al., 2007) p is 19.6. r of sumbasic (vanderwende et al., 2007) r is 16.9. f of sumbasic (vanderwende et al., 2007) f is 18.1. p of kl-summ (haghighi et al., 2009) p is 15.7. r of kl-summ (haghighi et al., 2009) r is 17.9. f of kl-summ (haghighi et al., 2009) f is 16.7. p of kl-summ (haghighi et al., 2009) p is 5.4. r of kl-summ (haghighi et al., 2009) r is 15.9. f of kl-summ (haghighi et al., 2009) f is 8. p of kl-summ (haghighi et al., 2009) p is 20. r of kl-summ (haghighi et al., 2009) r is 17.4. f of kl-summ (haghighi et al., 2009) f is 18.6. p of lexrank (erkan and radev, 2004) p is 22. r of lexrank (erkan and radev, 2004) r is 25.9. f of lexrank (erkan and radev, 2004) f is 23.8. p of lexrank (erkan and radev, 2004) p is 7.2. r of lexrank (erkan and radev, 2004) r is 21.4. f of lexrank (erkan and radev, 2004) f is 10.7. p of lexrank (erkan and radev, 2004) p is 27.5. r of lexrank (erkan and radev, 2004) r is 24.7. f of lexrank (erkan and radev, 2004) f is 26. p of vsm-singonly (this work) p is 30.8. r of vsm-singonly (this work) r is 36.9. f of vsm-singonly (this work) f is 33.6. p of vsm-singonly (this work) p is 9.8. r of vsm-singonly (this work) r is 34.4. f of vsm-singonly (this work) f is 15.2. p of vsm-singonly (this work) p is 39.5. r of vsm-singonly (this work) r is 35.7. f of vsm-singonly (this work) f is 37.5. p of vsm-singpairmix (this work) p is 27. r of vsm-singpairmix (this work) r is 46.5. f of vsm-singpairmix (this work) f is 34.2. p of vsm-singpairmix (this work) p is 9. r of vsm-singpairmix (this work) r is 42.1. f of vsm-singpairmix (this work) f is 14.9. p of vsm-singpairmix (this work) p is 34. r of vsm-singpairmix (this work) r is 45.4. f of vsm-singpairmix (this work) f is 38.9. p of bert-singonly (this work) p is 35.3. r of bert-singonly (this work) r is 41.9. f of bert-singonly (this work) f is 38.3. p of bert-singonly (this work) p is 9.8. r of bert-singonly (this work) r is 32.5. f of bert-singonly (this work) f is 15.1. p of bert-singonly (this work) p is 44. r of bert-singonly (this work) r is 38.6. f of bert-singonly (this work) f is 41.1. p of bert-singpairmix (this work) p is 33.6. r of bert-singpairmix (this work) r is 67.1. f of bert-singpairmix (this work) f is 44.8. p of bert-singpairmix (this work) p is 13.6. r of bert-singpairmix (this work) r is 70.2. f of bert-singpairmix (this work) f is 22.8. p of bert-singpairmix (this work) p is 44.7. r of bert-singpairmix (this work) r is 68. f of bert-singpairmix (this work) f is 53.9. p of lead-baseline p is 8.5. r of lead-baseline r is 9.4. f of lead-baseline f is 8.9. p of lead-baseline p is 5.3. r of lead-baseline r is 9.5. f of lead-baseline f is 6.8. p of lead-baseline p is 13.8. r of lead-baseline r is 9.4. f of lead-baseline f is 11.2. p of sumbasic (vanderwende et al., 2007) p is 8.7. r of sumbasic (vanderwende et al., 2007) r is 9.7. f of sumbasic (vanderwende et al., 2007) f is 9.2. p of sumbasic (vanderwende et al., 2007) p is 5. r of sumbasic (vanderwende et al., 2007) r is 8.9. f of sumbasic (vanderwende et al., 2007) f is 6.4. p of sumbasic (vanderwende et al., 2007) p is 13.7. r of sumbasic (vanderwende et al., 2007) r is 9.4. f of sumbasic (vanderwende et al., 2007) f is 11.1. p of kl-summ (haghighi et al., 2009) p is 9.2. r of kl-summ (haghighi et al., 2009) r is 10.2. f of kl-summ (haghighi et al., 2009) f is 9.7. p of kl-summ (haghighi et al., 2009) p is 5. r of kl-summ (haghighi et al., 2009) r is 8.9. f of kl-summ (haghighi et al., 2009) f is 6.4. p of kl-summ (haghighi et al., 2009) p is 14.2. r of kl-summ (haghighi et al., 2009) r is 9.7. f of kl-summ (haghighi et al., 2009) f is 11.5. p of lexrank (erkan and radev, 2004) p is 9.7. r of lexrank (erkan and radev, 2004) r is 10.8. f of lexrank (erkan and radev, 2004) f is 10.2. p of lexrank (erkan and radev, 2004) p is 5.5. r of lexrank (erkan and radev, 2004) r is 9.8. f of lexrank (erkan and radev, 2004) f is 7. p of lexrank (erkan and radev, 2004) p is 15.2. r of lexrank (erkan and radev, 2004) r is 10.4. f of lexrank (erkan and radev, 2004) f is 12.4. p of vsm-singonly (this work) p is 12.3. r of vsm-singonly (this work) r is 14.1. f of vsm-singonly (this work) f is 13.1. p of vsm-singonly (this work) p is 3.8. r of vsm-singonly (this work) r is 11. f of vsm-singonly (this work) f is 5.6. p of vsm-singonly (this work) p is 17.9. r of vsm-singonly (this work) r is 12. f of vsm-singonly (this work) f is 14.4. p of vsm-singpairmix (this work) p is 10.1. r of vsm-singpairmix (this work) r is 22.6. f of vsm-singpairmix (this work) f is 13.9. p of vsm-singpairmix (this work) p is 4.2. r of vsm-singpairmix (this work) r is 17.4. f of vsm-singpairmix (this work) f is 6.8. p of vsm-singpairmix (this work) p is 14.3. r of vsm-singpairmix (this work) r is 20.8. f of vsm-singpairmix (this work) f is 17. p of bert-singonly (this work) p is 24.2. r of bert-singonly (this work) r is 26.1. f of bert-singonly (this work) f is 25.1. p of bert-singonly (this work) p is 6.6. r of bert-singonly (this work) r is 16.7. f of bert-singonly (this work) f is 9.5. p of bert-singonly (this work) p is 35.3. r of bert-singonly (this work) r is 20.8. f of bert-singonly (this work) f is 26.2. p of bert-singpairmix (this work) p is 33.2. r of bert-singpairmix (this work) r is 56. f of bert-singpairmix (this work) f is 41.7. p of bert-singpairmix (this work) p is 24.1. r of bert-singpairmix (this work) r is 65.5. f of bert-singpairmix (this work) f is 35.2. p of bert-singpairmix (this work) p is 57.3. r of bert-singpairmix (this work) r is 59.6. f of bert-singpairmix (this work) f is 58.5. p of lead-baseline p is 6. r of lead-baseline r is 4.8. f of lead-baseline f is 5.3. p of lead-baseline p is 2.8. r of lead-baseline r is 3.8. f of lead-baseline f is 3.2. p of lead-baseline p is 8.8. r of lead-baseline r is 4.4. f of lead-baseline f is 5.9. p of sumbasic (vanderwende et al., 2007) p is 4.2. r of sumbasic (vanderwende et al., 2007) r is 3.2. f of sumbasic (vanderwende et al., 2007) f is 3.6. p of sumbasic (vanderwende et al., 2007) p is 3. r of sumbasic (vanderwende et al., 2007) r is 3.8. f of sumbasic (vanderwende et al., 2007) f is 3.3. p of sumbasic (vanderwende et al., 2007) p is 7.2. r of sumbasic (vanderwende et al., 2007) r is 3.4. f of sumbasic (vanderwende et al., 2007) f is 4.6. p of kl-summ (haghighi et al., 2009) p is 5.6. r of kl-summ (haghighi et al., 2009) r is 4.5. f of kl-summ (haghighi et al., 2009) f is 5. p of kl-summ (haghighi et al., 2009) p is 2.8. r of kl-summ (haghighi et al., 2009) r is 3.8. f of kl-summ (haghighi et al., 2009) f is 3.2. p of kl-summ (haghighi et al., 2009) p is 8. r of kl-summ (haghighi et al., 2009) r is 4.2. f of kl-summ (haghighi et al., 2009) f is 5.5. p of lexrank (erkan and radev, 2004) p is 8.5. r of lexrank (erkan and radev, 2004) r is 6.7. f of lexrank (erkan and radev, 2004) f is 7.5. p of lexrank (erkan and radev, 2004) p is 4.8. r of lexrank (erkan and radev, 2004) r is 6.5. f of lexrank (erkan and radev, 2004) f is 5.5. p of lexrank (erkan and radev, 2004) p is 12.1. r of lexrank (erkan and radev, 2004) r is 6.6. f of lexrank (erkan and radev, 2004) f is 8.6. p of vsm-singonly (this work) p is 18. r of vsm-singonly (this work) r is 14.7. f of vsm-singonly (this work) f is 16.2. p of vsm-singonly (this work) p is 3.6. r of vsm-singonly (this work) r is 8.4. f of vsm-singonly (this work) f is 5. p of vsm-singonly (this work) p is 23.6. r of vsm-singonly (this work) r is 11.8. f of vsm-singonly (this work) f is 15.7. p of vsm-singpairmix (this work) p is 3.8. r of vsm-singpairmix (this work) r is 6.2. f of vsm-singpairmix (this work) f is 4.7. p of vsm-singpairmix (this work) p is 3.6. r of vsm-singpairmix (this work) r is 11.4. f of vsm-singpairmix (this work) f is 5.5. p of vsm-singpairmix (this work) p is 7.4. r of vsm-singpairmix (this work) r is 8. f of vsm-singpairmix (this work) f is 7.7. p of bert-singonly (this work) p is 8.4. r of bert-singonly (this work) r is 6.5. f of bert-singonly (this work) f is 7.4. p of bert-singonly (this work) p is 2.8. r of bert-singonly (this work) r is 5.3. f of bert-singonly (this work) f is 3.7. p of bert-singonly (this work) p is 15.6. r of bert-singonly (this work) r is 6.6. f of bert-singonly (this work) f is 9.2. p of bert-singpairmix (this work) p is 4.8. r of bert-singpairmix (this work) r is 9.1. f of bert-singpairmix (this work) f is 6.3. p of bert-singpairmix (this work) p is 4.2. r of bert-singpairmix (this work) r is 14.2. f of bert-singpairmix (this work) f is 6.5. p of bert-singpairmix (this work) p is 9. r of bert-singpairmix (this work) r is 10.9. f of bert-singpairmix (this work) f is 9.9.
table 4 shows rouge scores on three large datasets. r-1 of lead-3 r-1 is 40.23. r-2 of lead-3 r-2 is 17.52. r-l of lead-3 r-l is 36.34. r-1 of lead-3 r-1 is 32.93. r-2 of lead-3 r-2 is 17.69. r-l of lead-3 r-l is 29.58. r-1 of lead-3 r-1 is 31.27. r-2 of lead-3 r-2 is 8.75. r-l of lead-3 r-l is 26.18. r-1 of oraclefrag (grusky et al. 2018) r-1 is 93.36. r-2 of oraclefrag (grusky et al. 2018) r-2 is 83.19. r-l of oraclefrag (grusky et al. 2018) r-l is 93.36. r-1 of oraclefrag (grusky et al. 2018) r-1 is 88.15. r-2 of oraclefrag (grusky et al. 2018) r-2 is 74.74. r-l of oraclefrag (grusky et al. 2018) r-l is 88.15. r-1 of oraclefrag (grusky et al. 2018) r-1 is 91.85. r-2 of oraclefrag (grusky et al. 2018) r-2 is 78.66. r-l of oraclefrag (grusky et al. 2018) r-l is 91.85. r-1 of oracleext r-1 is 49.35. r-2 of oracleext r-2 is 27.96. r-l of oracleext r-l is 46.24. r-1 of oracleext r-1 is 42.62. r-2 of oracleext r-2 is 26.39. r-l of oracleext r-l is 39.5. r-1 of oracleext r-1 is 43.56. r-2 of oracleext r-2 is 16.91. r-l of oracleext r-l is 36.52. r-1 of textrank (mihalcea and tarau 2004) r-1 is 37.72. r-2 of textrank (mihalcea and tarau 2004) r-2 is 15.59. r-l of textrank (mihalcea and tarau 2004) r-l is 33.81. r-1 of textrank (mihalcea and tarau 2004) r-1 is 28.57. r-2 of textrank (mihalcea and tarau 2004) r-2 is 14.29. r-l of textrank (mihalcea and tarau 2004) r-l is 23.79. r-1 of textrank (mihalcea and tarau 2004) r-1 is 35.99. r-2 of textrank (mihalcea and tarau 2004) r-2 is 11.14. r-l of textrank (mihalcea and tarau 2004) r-l is 29.6. r-1 of lexrank (erkan and radev 2004) r-1 is 33.96. r-2 of lexrank (erkan and radev 2004) r-2 is 11.79. r-l of lexrank (erkan and radev 2004) r-l is 30.17. r-1 of lexrank (erkan and radev 2004) r-1 is 27.32. r-2 of lexrank (erkan and radev 2004) r-2 is 11.93. r-l of lexrank (erkan and radev 2004) r-l is 23.75. r-1 of lexrank (erkan and radev 2004) r-1 is 35.57. r-2 of lexrank (erkan and radev 2004) r-2 is 10.47. r-l of lexrank (erkan and radev 2004) r-l is 29.03. r-1 of sumbasic (nenkova and vanderwende 2005) r-1 is 31.72. r-2 of sumbasic (nenkova and vanderwende 2005) r-2 is 9.6. r-l of sumbasic (nenkova and vanderwende 2005) r-l is 28.58. r-1 of sumbasic (nenkova and vanderwende 2005) r-1 is 23.16. r-2 of sumbasic (nenkova and vanderwende 2005) r-2 is 7.18. r-l of sumbasic (nenkova and vanderwende 2005) r-l is 20.06. r-1 of sumbasic (nenkova and vanderwende 2005) r-1 is 27.44. r-2 of sumbasic (nenkova and vanderwende 2005) r-2 is 7.08. r-l of sumbasic (nenkova and vanderwende 2005) r-l is 23.66. r-1 of rnn-ext rl (chen and bansal 2018) r-1 is 41.47. r-2 of rnn-ext rl (chen and bansal 2018) r-2 is 18.72. r-l of rnn-ext rl (chen and bansal 2018) r-l is 37.76. r-1 of rnn-ext rl (chen and bansal 2018) r-1 is 39.15. r-2 of rnn-ext rl (chen and bansal 2018) r-2 is 22.6. r-l of rnn-ext rl (chen and bansal 2018) r-l is 34.99. r-1 of rnn-ext rl (chen and bansal 2018) r-1 is 34.63. r-2 of rnn-ext rl (chen and bansal 2018) r-2 is 10.62. r-l of rnn-ext rl (chen and bansal 2018) r-l is 29.43. r-1 of seq2seq (sutskever et al. 2014) r-1 is 31.1. r-2 of seq2seq (sutskever et al. 2014) r-2 is 11.54. r-l of seq2seq (sutskever et al. 2014) r-l is 28.56. r-1 of seq2seq (sutskever et al. 2014) r-1 is 41.57. r-2 of seq2seq (sutskever et al. 2014) r-2 is 26.89. r-l of seq2seq (sutskever et al. 2014) r-l is 38.17. r-1 of seq2seq (sutskever et al. 2014) r-1 is 28.74. r-2 of seq2seq (sutskever et al. 2014) r-2 is 7.87. r-l of seq2seq (sutskever et al. 2014) r-l is 24.66. r-1 of pointgen (see et al. 2017) r-1 is 36.15. r-2 of pointgen (see et al. 2017) r-2 is 15.11. r-l of pointgen (see et al. 2017) r-l is 33.22. r-1 of pointgen (see et al. 2017) r-1 is 43.49. r-2 of pointgen (see et al. 2017) r-2 is 28.7. r-l of pointgen (see et al. 2017) r-l is 39.66. r-1 of pointgen (see et al. 2017) r-1 is 30.59. r-2 of pointgen (see et al. 2017) r-2 is 10.01. r-l of pointgen (see et al. 2017) r-l is 25.65. r-1 of pointgen+cov (see et al. 2017) r-1 is 39.23. r-2 of pointgen+cov (see et al. 2017) r-2 is 17.09. r-l of pointgen+cov (see et al. 2017) r-l is 36.03. r-1 of pointgen+cov (see et al. 2017) r-1 is 45.13. r-2 of pointgen+cov (see et al. 2017) r-2 is 30.13. r-l of pointgen+cov (see et al. 2017) r-l is 39.67. r-1 of pointgen+cov (see et al. 2017) r-1 is 33.14. r-2 of pointgen+cov (see et al. 2017) r-2 is 11.63. r-l of pointgen+cov (see et al. 2017) r-l is 28.55. r-1 of sentrewriting (chen and bansal 2018) r-1 is 40.04. r-2 of sentrewriting (chen and bansal 2018) r-2 is 17.61. r-l of sentrewriting (chen and bansal 2018) r-l is 37.59. r-1 of sentrewriting (chen and bansal 2018) r-1 is 44.77. r-2 of sentrewriting (chen and bansal 2018) r-2 is 29.1. r-l of sentrewriting (chen and bansal 2018) r-l is 41.55. r-1 of sentrewriting (chen and bansal 2018) r-1 is 37.12. r-2 of sentrewriting (chen and bansal 2018) r-2 is 11.87. r-l of sentrewriting (chen and bansal 2018) r-l is 32.45.
table 2 shows results for the giga-msc dataset. nn-1 of ground truth nn-1 is 8.6. nn-2 of ground truth nn-2 is 28. nn-3 of ground truth nn-3 is 40. nn-4 of ground truth nn-4 is 49.1. comp. rate of ground truth comp. rate is 0.5. meteor of #1 wg (filippova, 10) meteor is 0.29. nn-1 of #1 wg (filippova, 10) nn-1 is 0. nn-2 of #1 wg (filippova, 10) nn-2 is 0. nn-3 of #1 wg (filippova, 10) nn-3 is 2.8. nn-4 of #1 wg (filippova, 10) nn-4 is 6.8. comp. rate of #1 wg (filippova, 10) comp. rate is 0.34. meteor of #2 kwg (boudin+, 13) meteor is 0.36. nn-1 of #2 kwg (boudin+, 13) nn-1 is 0. nn-2 of #2 kwg (boudin+, 13) nn-2 is 0. nn-3 of #2 kwg (boudin+, 13) nn-3 is 1.1. nn-4 of #2 kwg (boudin+, 13) nn-4 is 3.1. comp. rate of #2 kwg (boudin+, 13) comp. rate is 0.52. meteor of #3 hard para. meteor is 0.35. nn-1 of #3 hard para. nn-1 is 10.1. nn-2 of #3 hard para. nn-2 is 19.7. nn-3 of #3 hard para. nn-3 is 29.1. nn-4 of #3 hard para. nn-4 is 38. comp. rate of #3 hard para. comp. rate is 0.51. meteor of #4 seq2seq with attention meteor is 0.33. nn-1 of #4 seq2seq with attention nn-1 is 12.7. nn-2 of #4 seq2seq with attention nn-2 is 24. nn-3 of #4 seq2seq with attention nn-3 is 34.7. nn-4 of #4 seq2seq with attention nn-4 is 44.4. comp. rate of #4 seq2seq with attention comp. rate is 0.49. meteor of #5 our rewriter (rwt) meteor is 0.36. nn-1 of #5 our rewriter (rwt) nn-1 is 9. nn-2 of #5 our rewriter (rwt) nn-2 is 17.4. nn-3 of #5 our rewriter (rwt) nn-3 is 25.7. nn-4 of #5 our rewriter (rwt) nn-4 is 33.8. comp. rate of #5 our rewriter (rwt) comp. rate is 0.5.
table 4 shows human evaluation for informativeness and grammaticality. informativeness of kwg informativeness is 1.06. grammaticality of kwg grammaticality is 1.19. informativeness of rwt informativeness is 1.02. grammaticality of rwt grammaticality is 1.40†.
table 2 shows performance of our and competing models on the ms marco v2 leaderboard (4 march 2019). r-l of bidafa r-l is 16.91. b-1 of bidafa b-1 is 9.3. r-l of bidafa r-l is 23.96. b-1 of bidafa b-1 is 10.64. r-l of deep cascade qab r-l is 35.14. b-1 of deep cascade qab b-1 is 37.35. r-l of deep cascade qab r-l is 52.01. b-1 of deep cascade qab b-1 is 54.64. r-l of s-net+ces2sc r-l is 45.04. b-1 of s-net+ces2sc b-1 is 40.62. r-l of s-net+ces2sc r-l is 44.96. b-1 of s-net+ces2sc b-1 is 46.36. r-l of bert+multi-pgnetd r-l is 47.37. b-1 of bert+multi-pgnetd b-1 is 45.09. r-l of bert+multi-pgnetd r-l is 48.14. b-1 of bert+multi-pgnetd b-1 is 52.03. r-l of selector+ccge r-l is 47.39. b-1 of selector+ccge b-1 is 45.26. r-l of selector+ccge r-l is 50.63. b-1 of selector+ccge b-1 is 52.03. r-l of vnetf r-l is 48.37. b-1 of vnetf b-1 is 46.75. r-l of vnetf r-l is 51.63. b-1 of vnetf b-1 is 54.37. r-l of masque (nlg single) r-l is 49.19. b-1 of masque (nlg single) b-1 is 49.63. r-l of masque (nlg single) r-l is 48.42. b-1 of masque (nlg single) b-1 is 48.68. r-l of masque (nlg ensemble) r-l is 49.61. b-1 of masque (nlg ensemble) b-1 is 50.13. r-l of masque (nlg ensemble) r-l is 48.92. b-1 of masque (nlg ensemble) b-1 is 48.75. r-l of masque (q&a single) r-l is 25.66. b-1 of masque (q&a single) b-1 is 36.62. r-l of masque (q&a single) r-l is 50.93. b-1 of masque (q&a single) b-1 is 42.37. r-l of masque (q&a ensemble) r-l is 28.53. b-1 of masque (q&a ensemble) b-1 is 39.87. r-l of masque (q&a ensemble) r-l is 52.2. b-1 of masque (q&a ensemble) b-1 is 43.77. r-l of human performance r-l is 63.21. b-1 of human performance b-1 is 53.03. r-l of human performance r-l is 53.87. b-1 of human performance b-1 is 48.5.
table 5 shows performance of our and competing models on the narrativeqa test set. b-1 of bidafa b-1 is 33.72. b-4 of bidafa b-4 is 15.53. m of bidafa m is 15.38. r-l of bidafa r-l is 36.3. b-1 of decaprop b-1 is 42. b-4 of decaprop b-4 is 23.42. m of decaprop m is 23.42. r-l of decaprop r-l is 40.07. b-1 of mhpgm+noic b-1 is 43.63. b-4 of mhpgm+noic b-4 is 21.07. m of mhpgm+noic m is 19.03. r-l of mhpgm+noic r-l is 44.16. b-1 of conznet b-1 is 42.76. b-4 of conznet b-4 is 22.49. m of conznet m is 19.24. r-l of conznet r-l is 46.67. b-1 of ermr+a2d b-1 is 50.4. b-4 of ermr+a2d b-4 is 26.5. m of ermr+a2d m is n/a. r-l of ermr+a2d r-l is 53.3. b-1 of masque (nqa) b-1 is 54.11. b-4 of masque (nqa) b-4 is 30.43. m of masque (nqa) m is 26.13. r-l of masque (nqa) r-l is 59.87. b-1 of w/o multi-style learning b-1 is 48.7. b-4 of w/o multi-style learning b-4 is 20.98. m of w/o multi-style learning m is 21.95. r-l of w/o multi-style learning r-l is 54.74. b-1 of masque (nlg) b-1 is 39.14. b-4 of masque (nlg) b-4 is 18.11. m of masque (nlg) m is 24.62. r-l of masque (nlg) r-l is 50.09. b-1 of fmasque (nqa; valid.) b-1 is 52.78. b-4 of fmasque (nqa; valid.) b-4 is 28.72. m of fmasque (nqa; valid.) m is 25.38. r-l of fmasque (nqa; valid.) r-l is 58.94.
table 4 shows results on the squad-document dev set. em of s-norm (clark and gardner, 2018) em is 64.08. f1 of s-norm (clark and gardner, 2018) f1 is 72.37. em of re3qabase em is 77.9. f1 of re3qabase f1 is 84.81. em of re3qalarge em is 80.71. f1 of re3qalarge f1 is 87.2.
table 5 shows performance of our model and the baseline in evidence extraction on the development set in the distractor setting. precision of baseline precision is 79. recall of baseline recall is 82.4. correlation of baseline correlation is 0.259. precision of qfe precision is 88.4. recall of qfe recall is 83.2. correlation of qfe correlation is 0.375.
table 6 shows overall results on the xqa dataset. em of english em is 32.32. f1 of english f1 is 38.29. em of english em is 33.72. f1 of english f1 is 40.51. em of english em is 32.32. f1 of english f1 is 38.29. em of english em is 33.72. f1 of english f1 is 40.51. em of english em is 30.85. f1 of english f1 is 38.11. em of chinese em is 7.17. f1 of chinese f1 is 17.2. em of chinese em is 9.81. f1 of chinese f1 is 23.05. em of chinese em is 7.45. f1 of chinese f1 is 18.73. em of chinese em is 18.93. f1 of chinese f1 is 31.5. em of chinese em is 25.88. f1 of chinese f1 is 39.53. em of french em is 11.19. f1 of french f1 is 18.97. em of french em is 15.42. f1 of french f1 is 26.13. em of french em is 23.34. f1 of french f1 is 31.08. em of german em is 12.98. f1 of german f1 is 19.15. em of german em is 16.84. f1 of german f1 is 23.65. em of german em is 11.23. f1 of german f1 is 15.08. em of german em is 19.06. f1 of german f1 is 24.33. em of german em is 21.42. f1 of german f1 is 26.87. em of polish em is 9.73. f1 of polish f1 is 16.51. em of polish em is 13.62. f1 of polish f1 is 22.18. em of polish em is 16.27. f1 of polish f1 is 21.87. em of portuguese em is 10.03. f1 of portuguese f1 is 15.86. em of portuguese em is 13.75. f1 of portuguese f1 is 21.27. em of portuguese em is 18.97. f1 of portuguese f1 is 23.95. em of russian em is 5.01. f1 of russian f1 is 9.62. em of russian em is 7.34. f1 of russian f1 is 13.61. em of russian em is 10.38. f1 of russian f1 is 13.44. em of tamil em is 2.2. f1 of tamil f1 is 6.41. em of tamil em is 4.58. f1 of tamil f1 is 10.15. em of tamil em is 10.07. f1 of tamil f1 is 14.25. em of ukrainian em is 7.94. f1 of ukrainian f1 is 14.07. em of ukrainian em is 10.53. f1 of ukrainian f1 is 17.72. em of ukrainian em is 15.12. f1 of ukrainian f1 is 20.82.
table 9 shows performance with respect to language distance and percentage of “easy” questions. genetic dist. of german genetic dist. is 30.8. pct. of easy of german pct. of easy is 19.09. em of german em is 36.67. genetic dist. of chinese genetic dist. is 82.4. pct. of easy of chinese pct. of easy is 33.24. em of chinese em is 35.93. genetic dist. of portuguese genetic dist. is 59.8. pct. of easy of portuguese pct. of easy is 29.03. em of portuguese em is 33.68. genetic dist. of french genetic dist. is 48.7. pct. of easy of french pct. of easy is 23.37. em of french em is 31.21. genetic dist. of polish genetic dist. is 66.9. pct. of easy of polish pct. of easy is 17.7. em of polish em is 31.17. genetic dist. of ukrainian genetic dist. is 60.3. pct. of easy of ukrainian pct. of easy is 21.18. em of ukrainian em is 24.26. genetic dist. of russian genetic dist. is 60.3. pct. of easy of russian pct. of easy is 18.56. em of russian em is 21.11. genetic dist. of tamil genetic dist. is 96.5. pct. of easy of tamil pct. of easy is 17.63. em of tamil em is 16.95.
table 3 shows performance on dev data of models trained on a single-domain training data. uas of bc uas is 82.77. las of bc las is 77.66. uas of bc uas is 68.73. las of bc las is 61.93. uas of bc uas is 69.34. las of bc las is 61.32. uas of pb uas is 62.1. las of pb las is 55.2. uas of pb uas is 75.85. las of pb las is 70.12. uas of pb uas is 51.5. las of pb las is 41.92. uas of zx uas is 56.15. las of zx las is 48.34. uas of zx uas is 52.56. las of zx las is 43.76. uas of zx uas is 69.54. las of zx las is 63.65.
table 5 shows final results on the test data. uas of bc-train uas is 67.55. las of bc-train las is 61.01. uas of bc-train uas is 68.44. las of bc-train las is 59.55. uas of pb-train uas is 74.52. las of pb-train las is 69.02. uas of pb-train uas is 51.62. las of pb-train las is 40.36. uas of zx-train uas is 52.24. las of zx-train las is 42.76. uas of zx-train uas is 68.14. las of zx-train las is 61.71. uas of mtl uas is 75.39. las of mtl las is 69.69. uas of mtl uas is 72.11. las of mtl las is 65.66. uas of concat uas is 77.49. las of concat las is 72.16. uas of concat uas is 76.8. las of concat las is 70.85. uas of doemb uas is 78.24. las of doemb las is 72.81. uas of doemb uas is 77.96. las of doemb las is 72.04. uas of + elmo uas is 77.62. las of + elmo las is 72.35. uas of + elmo uas is 78.5. las of + elmo las is 72.49. uas of + fine-tuning uas is 82.05. las of + fine-tuning las is 77.16. uas of + fine-tuning uas is 80.44. las of + fine-tuning las is 75.11.
table 3 shows model performance by f1 on the testing set of each dataset. f1 of per memm is 91.61. f1 of per crf is 93.12. f1 of per bilstm is 94.21. f1 of per bilstm+crf is 95.71. f1 of per matching is 6.7. f1 of per upu is 74.22. f1 of per bupu is 85.01. f1 of per bnpu is 87.2f11. f1 of per adapu is 90.17. f1 of loc memm is 89.72. f1 of loc crf is 91.15. f1 of loc bilstm is 91.76. f1 of loc bilstm+crf is 93.02. f1 of loc matching is 67.16. f1 of loc upu is 69.88. f1 of loc bupu is 81.27. f1 of loc bnpu is 83.37. f1 of loc adapu is 85.62. f1 of org memm is 80.6. f1 of org crf is 81.91. f1 of org bilstm is 83.21. f1 of org bilstm+crf is 88.45. f1 of org matching is 46.65. f1 of org upu is 73.64. f1 of org bupu is 74.72. f1 of org bnpu is 75.29. f1 of org adapu is 76.03. f1 of misc memm is 77.45. f1 of misc crf is 79.35. f1 of misc bilstm is 76. f1 of misc bilstm+crf is 79.86. f1 of misc matching is 53.98. f1 of misc upu is 68.9. f1 of misc bupu is 68.9. f1 of misc bnpu is 66.88. f1 of misc adapu is 69.3. f1 of overall memm is 86.13. f1 of overall crf is 87.94. f1 of overall bilstm is 88.3. f1 of overall bilstm+crf is 90.01. f1 of overall matching is 44.9. f1 of overall upu is 72.32. f1 of overall bupu is 79.2. f1 of overall bnpu is 80.74. f1 of overall adapu is 82.94. f1 of per memm is 86.18. f1 of per crf is 86.77. f1 of per bilstm is 88.93. f1 of per bilstm+crf is 90.41. f1 of per matching is 32.4. f1 of per upu is 82.28. f1 of per bupu is 83.76. f1 of per bnpu is 84.3. f1 of per adapu is 85.1. f1 of loc memm is 78.48. f1 of loc crf is 80.3. f1 of loc bilstm is 75.43. f1 of loc bilstm+crf is 80.55. f1 of loc matching is 28.53. f1 of loc upu is 70.44. f1 of loc bupu is 72.55. f1 of loc bnpu is 73.68. f1 of loc adapu is 75.23. f1 of org memm is 79.23. f1 of org crf is 80.83. f1 of org bilstm is 79.27. f1 of org bilstm+crf is 83.26. f1 of org matching is 55.76. f1 of org upu is 69.82. f1 of org bupu is 71.22. f1 of org bnpu is 69.82. f1 of org adapu is 72.28. f1 of overall memm is 81.14. f1 of overall crf is 82.63. f1 of overall bilstm is 80.28. f1 of overall bilstm+crf is 84.74. f1 of overall matching is 42.23. f1 of overall upu is 73.84. f1 of overall bupu is 74.5. f1 of overall bnpu is 74.43. f1 of overall adapu is 75.85. f1 of per memm is 86.32. f1 of per crf is 87.5. f1 of per bilstm is 85.71. f1 of per bilstm+crf is 84.55. f1 of per matching is 27.84. f1 of per upu is 77.98. f1 of per bupu is 84.94. f1 of per bnpu is 84.21. f1 of per adapu is 85.26. f1 of loc memm is 81.7. f1 of loc crf is 83.83. f1 of loc bilstm is 79.48. f1 of loc bilstm+crf is 83.43. f1 of loc matching is 62.82. f1 of loc upu is 64.56. f1 of loc bupu is 72.62. f1 of loc bnpu is 75.61. f1 of loc adapu is 77.35. f1 of org memm is 68.48. f1 of org crf is 72.33. f1 of org bilstm is 66.17. f1 of org bilstm+crf is 67.66. f1 of org matching is 51.6. f1 of org upu is 45.3. f1 of org bupu is 58.39. f1 of org bnpu is 58.75. f1 of org adapu is 60.15. f1 of overall memm is 74.66. f1 of overall crf is 76.47. f1 of overall bilstm is 73.12. f1 of overall bilstm+crf is 75.08. f1 of overall matching is 50.12. f1 of overall upu is 63.87. f1 of overall bupu is 69.89. f1 of overall bnpu is 70.06. f1 of overall adapu is 71.6. f1 of per memm is 73.85. f1 of per crf is 80.86. f1 of per bilstm is 80.61. f1 of per bilstm+crf is 80.77. f1 of per matching is 41.33. f1 of per upu is 67.3. f1 of per bupu is 72.72. f1 of per bnpu is 72.68. f1 of per adapu is 74.66. f1 of loc memm is 69.35. f1 of loc crf is 75.39. f1 of loc bilstm is 73.52. f1 of loc bilstm+crf is 72.56. f1 of loc matching is 49.74. f1 of loc upu is 59.28. f1 of loc bupu is 61.41. f1 of loc bnpu is 63.44. f1 of loc adapu is 65.18. f1 of org memm is 41.81. f1 of org crf is 47.77. f1 of org bilstm is 41.39. f1 of org bilstm+crf is 41.33. f1 of org matching is 32.38. f1 of org upu is 31.51. f1 of org bupu is 36.78. f1 of org bnpu is 35.77. f1 of org adapu is 36.62. f1 of overall memm is 61.48. f1 of overall crf is 67.15. f1 of overall bilstm is 65.6. f1 of overall bilstm+crf is 65.32. f1 of overall matching is 37.9. f1 of overall upu is 53.63. f1 of overall bupu is 57.16. f1 of overall bnpu is 57.54. f1 of overall adapu is 59.36.
table 3 shows f1-scores on 13pc and 13cg. f1-scores of crichton et al. (2017) 13pc is 81.92. f1-scores of crichton et al. (2017) 13cg is 78.9. f1-scores of stm-target 13pc is 82.59. f1-scores of stm-target 13cg is 76.55. f1-scores of multitask(ner+lm) 13pc is 81.33. f1-scores of multitask(ner+lm) 13cg is 75.27. f1-scores of multitask(ner) 13pc is 83.09. f1-scores of multitask(ner) 13cg is 77.73. f1-scores of finetune 13pc is 82.55. f1-scores of finetune 13cg is 76.73. f1-scores of stm+elmo 13pc is 82.76. f1-scores of stm+elmo 13cg is 78.24. f1-scores of co-lm 13pc is 84.43. f1-scores of co-lm 13cg is 78.6. f1-scores of co-ner 13pc is 83.87. f1-scores of co-ner 13cg is 78.43. f1-scores of mix-data 13pc is 83.88. f1-scores of mix-data 13cg is 78.7. f1-scores of final 13pc is 85.54. f1-scores of final 13cg is 79.86.
table 6 shows comparison results of our ablation models on three datasets (se: stackexchange) — separate train: our model with pre-trained latent topics; w/o topic-attn: decoder attention without topics (eq. f1 of seq2seq-copy twitter is 36.6. f1 of seq2seq-copy weibo is 32.01. f1 of seq2seq-copy se is 31.53. f1 of our model (separate train) twitter is 36.75. f1 of our model (separate train) weibo is 32.75. f1 of our model (separate train) se is 31.78. f1 of our model (w/o topic-attn) twitter is 37.24. f1 of our model (w/o topic-attn) weibo is 32.42. f1 of our model (w/o topic-attn) se is 32.34. f1 of our model (w/o topic-state) twitter is 37.44. f1 of our model (w/o topic-state) weibo is 33.48. f1 of our model (w/o topic-state) se is 31.98. f1 of our full model twitter is 38.49. f1 of our full model weibo is 34.99. f1 of our full model se is 33.41.
table 3 shows performance comparisons on the shr dataset accuracy of dad model accuracy is 0.91. precision of dad model precision is 0.9. recall of dad model recall is 0.91. f1 of dad model f1 is 0.9. accuracy of sda model accuracy is 0.9. precision of sda model precision is 0.87. recall of sda model recall is 0.9. f1 of sda model f1 is 0.88. accuracy of word-cnn accuracy is 0.92. precision of word-cnn precision is 0.68. recall of word-cnn recall is 0.95. f1 of word-cnn f1 is 0.79. accuracy of lstm accuracy is 0.92. precision of lstm precision is 0.7. recall of lstm recall is 0.98. f1 of lstm f1 is 0.81. accuracy of rnn accuracy is 0.93. precision of rnn precision is 0.86. recall of rnn recall is 0.95. f1 of rnn f1 is 0.9. accuracy of cl-cnn accuracy is 0.92. precision of cl-cnn precision is 0.7. recall of cl-cnn recall is 0.91. f1 of cl-cnn f1 is 0.79. accuracy of fasttext-bot accuracy is 0.87. precision of fasttext-bot precision is 0.7. recall of fasttext-bot recall is 0.8. f1 of fasttext-bot f1 is 0.74. accuracy of hatt accuracy is 0.93. precision of hatt precision is 0.93. recall of hatt recall is 0.95. f1 of hatt f1 is 0.93. accuracy of bi-lstm accuracy is 0.93. precision of bi-lstm precision is 0.86. recall of bi-lstm recall is 0.98. f1 of bi-lstm f1 is 0.91. accuracy of rcnn accuracy is 0.9. precision of rcnn precision is 0.86. recall of rcnn recall is 0.9. f1 of rcnn f1 is 0.87. accuracy of cnn-lstm accuracy is 0.94. precision of cnn-lstm precision is 0.93. recall of cnn-lstm recall is 0.94. f1 of cnn-lstm f1 is 0.94. accuracy of attentional bi-lstm accuracy is 0.93. precision of attentional bi-lstm precision is 0.9. recall of attentional bi-lstm recall is 0.98. f1 of attentional bi-lstm f1 is 0.93. accuracy of a-cnn-lstm accuracy is 0.94. precision of a-cnn-lstm precision is 0.92. recall of a-cnn-lstm recall is 0.98. f1 of a-cnn-lstm f1 is 0.94. accuracy of openai-transformer accuracy is 0.95. precision of openai-transformer precision is 0.94. recall of openai-transformer recall is 0.96. f1 of openai-transformer f1 is 0.94. accuracy of smlm accuracy is 0.96. precision of smlm precision is 0.95. recall of smlm recall is 0.97. f1 of smlm f1 is 0.96.
table 5 shows results of different claim verification models on fever dataset (dev set). acc. of fever-base acc. is 0.521. fever of fever-base fever is 0.326. acc. of nsmn acc. is 0.697. prec. of nsmn prec. is 0.286. rec. of nsmn rec. is 0.87. f1 of nsmn f1 is 0.431. fever of nsmn fever is 0.665. acc. of han-nli acc. is 0.642. prec. of han-nli prec. is 0.34. rec. of han-nli rec. is 0.484. f1 of han-nli f1 is 0.4. fever of han-nli fever is 0.464. acc. of han-nli* acc. is 0.72. prec. of han-nli* prec. is 0.447. rec. of han-nli* rec. is 0.536. f1 of han-nli* f1 is 0.488. fever of han-nli* fever is 0.571. acc. of han* acc. is 0.475. prec. of han* prec. is 0.356. rec. of han* rec. is 0.471. f1 of han* f1 is 0.406. fever of han* fever is 0.365.
table 5 shows accuracy of (top) the state of the art gender prediction approaches on their respective datasets and transfer performance to celebrities, and (bottom) our baseline deep learning approach, with and without retraining on the pan datasets. accuracy of alvarezcamona15 (2015) pan15 is 0.859. accuracy of alvarezcamona15 (2015) celeb is 0.723. accuracy of nissim16 (2016) pan16 is 0.641. accuracy of nissim16 (2016) celeb is 0.74. accuracy of nissim17 (2017) pan17 is 0.823. accuracy of nissim17 (2017) celeb is 0.855. accuracy of danehsvar18 (2018) pan18 is 0.822. accuracy of danehsvar18 (2018) celeb is 0.817. accuracy of cnn (celeb) pan15 is 0.747. accuracy of cnn (celeb) pan16 is 0.59. accuracy of cnn (celeb) pan17 is 0.747. accuracy of cnn (celeb) pan18 is 0.756. accuracy of cnn (celeb) celeb is 0.861. accuracy of cnn (celeb + pan15) pan15 is 0.793. accuracy of cnn (celeb + pan16) pan16 is 0.69. accuracy of cnn (celeb + pan17) pan17 is 0.768. accuracy of cnn (celeb + pan18) pan18 is 0.759.
table 3 shows the overall classification performance of the baseline methods and our approach. prec. of bow prec. is 0.807. rec. of bow rec. is 0.829. f1 of bow f1 is 0.809. prec of bow prec is 0.687. rec. of bow rec. is 0.619. f1 of bow f1 is 0.631. prec. of paqi prec. is 0.816. rec. of paqi rec. is 0.728. f1 of paqi f1 is 0.757. prec of paqi prec is 0.611. rec. of paqi rec. is 0.677. f1 of paqi f1 is 0.617. prec. of ours prec. is 0.863. rec. of ours rec. is 0.811. f1 of ours f1 is 0.828. prec of ours prec is 0.691. rec. of ours rec. is 0.776. f1 of ours f1 is 0.714. prec. of bow prec. is 0.792. rec. of bow rec. is 0.786. f1 of bow f1 is 0.786. prec of bow prec is 0.508. rec. of bow rec. is 0.508. f1 of bow f1 is 0.501. prec. of paqi prec. is 0.847. rec. of paqi rec. is 0.682. f1 of paqi f1 is 0.737. prec of paqi prec is 0.567. rec. of paqi rec. is 0.649. f1 of paqi f1 is 0.548. prec. of ours prec. is 0.855. rec. of ours rec. is 0.849. f1 of ours f1 is 0.852. prec of ours prec is 0.64. rec. of ours rec. is 0.652. f1 of ours f1 is 0.645. prec. of bow prec. is 0.775. rec. of bow rec. is 0.802. f1 of bow f1 is 0.791. prec of bow prec is 0.506. rec. of bow rec. is 0.499. f1 of bow f1 is 0.484. prec. of paqi prec. is 0.834. rec. of paqi rec. is 0.686. f1 of paqi f1 is 0.737. prec of paqi prec is 0.58. rec. of paqi rec. is 0.666. f1 of paqi f1 is 0.566. prec. of ours prec. is 0.844. rec. of ours rec. is 0.847. f1 of ours f1 is 0.845. prec of ours prec is 0.646. rec. of ours rec. is 0.638. f1 of ours f1 is 0.64. prec. of bow prec. is 0.744. rec. of bow rec. is 0.78. f1 of bow f1 is 0.76. prec of bow prec is 0.515. rec. of bow rec. is 0.512. f1 of bow f1 is 0.51. prec. of paqi prec. is 0.8. rec. of paqi rec. is 0.683. f1 of paqi f1 is 0.724. prec of paqi prec is 0.569. rec. of paqi rec. is 0.622. f1 of paqi f1 is 0.562. prec. of ours prec. is 0.813. rec. of ours rec. is 0.813. f1 of ours f1 is 0.815. prec of ours prec is 0.629. rec. of ours rec. is 0.627. f1 of ours f1 is 0.627. prec. of bow prec. is 0.647. rec. of bow rec. is 0.683. f1 of bow f1 is 0.66. prec of bow prec is 0.495. rec. of bow rec. is 0.488. f1 of bow f1 is 0.485. prec. of paqi prec. is 0.826. rec. of paqi rec. is 0.725. f1 of paqi f1 is 0.745. prec of paqi prec is 0.7. rec. of paqi rec. is 0.772. f1 of paqi f1 is 0.694. prec. of ours prec. is 0.83. rec. of ours rec. is 0.786. f1 of ours f1 is 0.798. prec of ours prec is 0.728. rec. of ours rec. is 0.786. f1 of ours f1 is 0.742.
table 3 shows accuracy at choosing the correct reference string for a mention, discriminating against 10, 50 and 100 random distractors. accuracy of word-based rank 10 is 42.3. accuracy of word-based rank 50 is 25.4. accuracy of word-based rank 100 is 17.2. accuracy of word-based rank 10 is 48.1. accuracy of word-based rank 50 is 38.4. accuracy of word-based rank 100 is 28.8. accuracy of bpe rank 10 is 48.1. accuracy of bpe rank 50 is 20.3. accuracy of bpe rank 100 is 25.5. accuracy of bpe rank 10 is 52.5. accuracy of bpe rank 50 is 50.7. accuracy of bpe rank 100 is 48.8. accuracy of character-level rank 10 is 64.2. accuracy of character-level rank 50 is 51. accuracy of character-level rank 100 is 35.6. accuracy of character-level rank 10 is 66.1. accuracy of character-level rank 50 is 55. accuracy of character-level rank 100 is 51.2. accuracy of no story rank 10 is 50.3. accuracy of no story rank 50 is 40. accuracy of no story rank 100 is 26.7. accuracy of no story rank 10 is 54.7. accuracy of no story rank 50 is 51.3. accuracy of no story rank 100 is 30.4. accuracy of left story context rank 10 is 59.1. accuracy of left story context rank 50 is 49.6. accuracy of left story context rank 100 is 33.3. accuracy of left story context rank 10 is 62.9. accuracy of left story context rank 50 is 53.2. accuracy of left story context rank 100 is 49.4. accuracy of full story rank 10 is 64.2. accuracy of full story rank 50 is 51. accuracy of full story rank 100 is 35.6. accuracy of full story rank 10 is 66.1. accuracy of full story rank 50 is 55. accuracy of full story rank 100 is 51.2.
table 2 shows automatic evaluation results of different nlu models on both training and test sets train err (%) of original data train err (%) is 35.5. test err (%) of original data test err (%) is 37.59. train err (%) of nlu refined data train err (%) is 16.31. test err (%) of nlu refined data test err (%) is 14.26. train err (%) of w/o self-training train err (%) is 25.14. test err (%) of w/o self-training test err (%) is 22.69.
table 3 shows human evaluation results for nlu on test set (inter-annotator agreement: fleiss’ kappa = 0.855) e (%) of original data e (%) is 71.93. m (%) of original data m (%) is 0. a (%) of original data a (%) is 24.13. c (%) of original data c (%) is 3.95. e (%) of nlu refined data e (%) is 88.62. m (%) of nlu refined data m (%) is 5.45. a (%) of nlu refined data a (%) is 2.48. c (%) of nlu refined data c (%) is 3.47. e (%) of w/o self-training e (%) is 73.53. m (%) of w/o self-training m (%) is 13.23. a (%) of w/o self-training a (%) is 8.33. c (%) of w/o self-training c (%) is 4.91.
table 2 shows quality evaluation results of the testing set. flue. of score flue. is 9.2. rele. of score rele. is 6.7. info. of score info. is 6.4. overall of score overall is 7.6. flue. of pearson flue. is 0.74. rele. of pearson rele. is 0.76. info. of pearson info. is 0.66. overall of pearson overall is 0.68.
table 4 shows results summary over the 510 comparisons of reimers and gurevych (2017a). % of comparisons of case a % of comparisons is 0.98%. avg. e of case a avg. e is 0. e std of case a e std is 0. % of comparisons of case b % of comparisons is 48.04%. avg. e of case b avg. e is 0.072. e std of case b e std is 0.108. % of comparisons of case c % of comparisons is 50.98%. avg. e of case c avg. e is 0.202. e std of case c e std is 0.143.
table 4 shows overall performance of schema matching. p of nguyen et al. (2015) p is 41.5. r of nguyen et al. (2015) r is 53.4. f1 of nguyen et al. (2015) f1 is 46.7. p of clustering p is 41.2. r of clustering r is 50.6. f1 of clustering f1 is 45.4. p of odee-f p is 41.7. r of odee-f r is 53.2. f1 of odee-f f1 is 46.8. p of odee-fe p is 42.4. r of odee-fe r is 56.1. f1 of odee-fe f1 is 48.3. p of odee-fer p is 43.4. r of odee-fer r is 58.3. f1 of odee-fer f1 is 49.8.
table 3 shows snli results (accuracy). accuracy of  lstm (bowman et al. 2016)  dev is  –. accuracy of  lstm (bowman et al. 2016)  test is 80.6. accuracy of  da (parikh et al.  2016)  dev is  –. accuracy of  da (parikh et al.  2016)  test is 86.3. accuracy of  da (reimplementation)  dev is 86.9. accuracy of  da (reimplementation)  test is 86.5. accuracy of  da with hardkuma attention  dev is 86. accuracy of  da with hardkuma attention  test is 85.5.
table 1 shows translation results for chinese-english and english-german translation task. bleu of edr (tu et al., 2017) 3 is n/a. bleu of edr (tu et al., 2017) 4 is n/a. bleu of edr (tu et al., 2017) 5 is 33.73. bleu of edr (tu et al., 2017) 6 is 34.15. bleu of edr (tu et al., 2017) avg is n/a. bleu of edr (tu et al., 2017) 14 is n/a. bleu of (kuang et al., 2018) 3 is 38.02. bleu of (kuang et al., 2018) 4 is 40.83. bleu of (kuang et al., 2018) 5 is n/a. bleu of (kuang et al., 2018) 6 is n/a. bleu of (kuang et al., 2018) avg is n/a. bleu of (kuang et al., 2018) 14 is n/a. bleu of transformer(base) 3 is 45.57. bleu of transformer(base) 4 is 46.4. bleu of transformer(base) 5 is 46.11. bleu of transformer(base) 6 is 44.92. bleu of transformer(base) avg is 45.75. bleu of transformer(base) 14 is 27.28. bleu of +lossmse 3 is 46.71†. bleu of +lossmse 4 is 47.23†. bleu of +lossmse 5 is 47.12†. bleu of +lossmse 6 is 45.78†. bleu of +lossmse avg is 46.71. bleu of +lossmse 14 is 28.11†. bleu of +lossmse + enhanced 3 is 46.94†. bleu of +lossmse + enhanced 4 is 47.52†. bleu of +lossmse + enhanced 5 is 47.43†. bleu of +lossmse + enhanced 6 is 46.04†. bleu of +lossmse + enhanced avg is 46.98. bleu of +lossmse + enhanced 14 is 28.38†. bleu of transformer(big) 3 is 46.73. bleu of transformer(big) 4 is 47.36. bleu of transformer(big) 5 is 47.15. bleu of transformer(big) 6 is 46.82. bleu of transformer(big) avg is 47.01. bleu of transformer(big) 14 is 28.36. bleu of +lossmse 3 is 47.43†. bleu of +lossmse 4 is 47.96. bleu of +lossmse 5 is 47.78. bleu of +lossmse 6 is 47.39. bleu of +lossmse avg is 47.74. bleu of +lossmse 14 is 28.71. bleu of +lossmse + enhanced 3 is 47.68†. bleu of +lossmse + enhanced 4 is 48.13†. bleu of +lossmse + enhanced 5 is 47.96†. bleu of +lossmse + enhanced 6 is 47.56†. bleu of +lossmse + enhanced avg is 47.83. bleu of +lossmse + enhanced 14 is 28.92†.
table 3 shows comparison on the cross-lingual assum performances. rouge-1 of transformerbpe rouge-1 is 38.1. rouge-2 of transformerbpe rouge-2 is 19.1. rouge-l of transformerbpe rouge-l is 35.2. rouge-1 of transformerbpe rouge-1 is 31.2. rouge-2 of transformerbpe rouge-2 is 10.7. rouge-l of transformerbpe rouge-l is 27.1. rouge-1 of pipeline-ts rouge-1 is 25.8. rouge-2 of pipeline-ts rouge-2 is 9.7. rouge-l of pipeline-ts rouge-l is 23.6. rouge-1 of pipeline-ts rouge-1 is 23.7. rouge-2 of pipeline-ts rouge-2 is 6.8. rouge-l of pipeline-ts rouge-l is 20.9. rouge-1 of pipeline-st rouge-1 is 22. rouge-2 of pipeline-st rouge-2 is 7. rouge-l of pipeline-st rouge-l is 20.9. rouge-1 of pipeline-st rouge-1 is 20.9. rouge-2 of pipeline-st rouge-2 is 5.3. rouge-l of pipeline-st rouge-l is 18.3. rouge-1 of pseudo-summary (ayana et al., 2018) rouge-1 is 21.5. rouge-2 of pseudo-summary (ayana et al., 2018) rouge-2 is 6.6. rouge-l of pseudo-summary (ayana et al., 2018) rouge-l is 19.6. rouge-1 of pseudo-summary (ayana et al., 2018) rouge-1 is 19.3. rouge-2 of pseudo-summary (ayana et al., 2018) rouge-2 is 4.3. rouge-l of pseudo-summary (ayana et al., 2018) rouge-l is 17. rouge-1 of pivot-based (cheng et al., 2017) rouge-1 is 26.7. rouge-2 of pivot-based (cheng et al., 2017) rouge-2 is 10.2. rouge-l of pivot-based (cheng et al., 2017) rouge-l is 24.3. rouge-1 of pivot-based (cheng et al., 2017) rouge-1 is 24. rouge-2 of pivot-based (cheng et al., 2017) rouge-2 is 7. rouge-l of pivot-based (cheng et al., 2017) rouge-l is 21.3. rouge-1 of pseudo-chinese rouge-1 is 27.9. rouge-2 of pseudo-chinese rouge-2 is 10.9. rouge-l of pseudo-chinese rouge-l is 25.6. rouge-1 of pseudo-chinese rouge-1 is 24.4. rouge-2 of pseudo-chinese rouge-2 is 6.6. rouge-l of pseudo-chinese rouge-l is 21.4. rouge-1 of teaching generation rouge-1 is 29.6. rouge-2 of teaching generation rouge-2 is 12.1. rouge-l of teaching generation rouge-l is 27.3. rouge-1 of teaching generation rouge-1 is 25.6. rouge-2 of teaching generation rouge-2 is 7.9. rouge-l of teaching generation rouge-l is 22.7. rouge-1 of teaching attention rouge-1 is 28.1. rouge-2 of teaching attention rouge-2 is 11.4. rouge-l of teaching attention rouge-l is 26. rouge-1 of teaching attention rouge-1 is 24.3. rouge-2 of teaching attention rouge-2 is 7.4. rouge-l of teaching attention rouge-l is 21.7. rouge-1 of teaching generation+attention rouge-1 is 30.1. rouge-2 of teaching generation+attention rouge-2 is 12.2. rouge-l of teaching generation+attention rouge-l is 27.7. rouge-1 of teaching generation+attention rouge-1 is 26. rouge-2 of teaching generation+attention rouge-2 is 8. rouge-l of teaching generation+attention rouge-l is 23.1.
table 2 shows test set result on english to swahili and english to tagalog. map of dictionary-based query translation (dbqt) map is 20.93. p@20 of dictionary-based query translation (dbqt) p@20 is 4.86. ndcg@20 of dictionary-based query translation (dbqt) ndcg@20 is 28.65. aqwv of dictionary-based query translation (dbqt) aqwv is 6.5. map of dictionary-based query translation (dbqt) map is 20.01. p@20 of dictionary-based query translation (dbqt) p@20 is 5.42. ndcg@20 of dictionary-based query translation (dbqt) ndcg@20 is 27.01. aqwv of dictionary-based query translation (dbqt) aqwv is 5.93. map of probabilistic structured query (psq) map is 27.16. p@20 of probabilistic structured query (psq) p@20 is 5.81. ndcg@20 of probabilistic structured query (psq) ndcg@20 is 36.03. aqwv of probabilistic structured query (psq) aqwv is 12.56. map of probabilistic structured query (psq) map is 35.2. p@20 of probabilistic structured query (psq) p@20 is 8.18. ndcg@20 of probabilistic structured query (psq) ndcg@20 is 44.04. aqwv of probabilistic structured query (psq) aqwv is 19.81. map of statistical mt (smt) map is 26.3. p@20 of statistical mt (smt) p@20 is 5.28. ndcg@20 of statistical mt (smt) ndcg@20 is 34.6. aqwv of statistical mt (smt) aqwv is 13.77. map of statistical mt (smt) map is 37.31. p@20 of statistical mt (smt) p@20 is 8.77. ndcg@20 of statistical mt (smt) ndcg@20 is 46.77. aqwv of statistical mt (smt) aqwv is 21.9. map of neural mt (nmt) map is 26.54. p@20 of neural mt (nmt) p@20 is 5.26. ndcg@20 of neural mt (nmt) ndcg@20 is 34.83. aqwv of neural mt (nmt) aqwv is 15.7. map of neural mt (nmt) map is 33.83. p@20 of neural mt (nmt) p@20 is 8.2. ndcg@20 of neural mt (nmt) ndcg@20 is 43.17. aqwv of neural mt (nmt) aqwv is 18.56. map of pacrr map is 24.69. p@20 of pacrr p@20 is 5.24. ndcg@20 of pacrr ndcg@20 is 32.85. aqwv of pacrr aqwv is 11.73. map of pacrr map is 32.53. p@20 of pacrr p@20 is 8.42. ndcg@20 of pacrr ndcg@20 is 41.75. aqwv of pacrr aqwv is 17.48. map of pacrr-drmm map is 22.15. p@20 of pacrr-drmm p@20 is 5.14. ndcg@20 of pacrr-drmm ndcg@20 is 30.28. aqwv of pacrr-drmm aqwv is 8.5. map of pacrr-drmm map is 32.59. p@20 of pacrr-drmm p@20 is 8.6. ndcg@20 of pacrr-drmm ndcg@20 is 42.17. aqwv of pacrr-drmm aqwv is 16.59. map of posit-drmm map is 23.91. p@20 of posit-drmm p@20 is 6.04. ndcg@20 of posit-drmm ndcg@20 is 33.83. aqwv of posit-drmm aqwv is 12.06. map of posit-drmm map is 25.16. p@20 of posit-drmm p@20 is 8.15. ndcg@20 of posit-drmm ndcg@20 is 34.8. aqwv of posit-drmm aqwv is 9.28. map of pacrr map is 27.03. p@20 of pacrr p@20 is 5.34. ndcg@20 of pacrr ndcg@20 is 35.36. aqwv of pacrr aqwv is 14.18. map of pacrr map is 41.43. p@20 of pacrr p@20 is 8.98. ndcg@20 of pacrr ndcg@20 is 49.96. aqwv of pacrr aqwv is 27.46. map of pacrr-drmm map is 25.46. p@20 of pacrr-drmm p@20 is 5.5. ndcg@20 of pacrr-drmm ndcg@20 is 34.15. aqwv of pacrr-drmm aqwv is 12.18. map of pacrr-drmm map is 35.61. p@20 of pacrr-drmm p@20 is 8.69. ndcg@20 of pacrr-drmm ndcg@20 is 45.34. aqwv of pacrr-drmm aqwv is 22.7. map of posit-drmm map is 26.1. p@20 of posit-drmm p@20 is 5.26. ndcg@20 of posit-drmm ndcg@20 is 34.27. aqwv of posit-drmm aqwv is 14.11. map of posit-drmm map is 39.35. p@20 of posit-drmm p@20 is 9.24. ndcg@20 of posit-drmm ndcg@20 is 48.41. aqwv of posit-drmm aqwv is 25.01. map of bilingual pacrr map is 29.64. p@20 of bilingual pacrr p@20 is 5.75. ndcg@20 of bilingual pacrr ndcg@20 is 38.27. aqwv of bilingual pacrr aqwv is 17.87. map of bilingual pacrr map is 43.02. p@20 of bilingual pacrr p@20 is 9.63. ndcg@20 of bilingual pacrr ndcg@20 is 52.27. aqwv of bilingual pacrr aqwv is 29.12. map of bilingual pacrr-drmm map is 26.15. p@20 of bilingual pacrr-drmm p@20 is 5.84. ndcg@20 of bilingual pacrr-drmm ndcg@20 is 35.54. aqwv of bilingual pacrr-drmm aqwv is 12.92. map of bilingual pacrr-drmm map is 38.29. p@20 of bilingual pacrr-drmm p@20 is 9.21. ndcg@20 of bilingual pacrr-drmm ndcg@20 is 47.6. aqwv of bilingual pacrr-drmm aqwv is 22.94. map of bilingual posit-drmm map is 30.13. p@20 of bilingual posit-drmm p@20 is 6.28. ndcg@20 of bilingual posit-drmm ndcg@20 is 39.68. aqwv of bilingual posit-drmm aqwv is 18.69. map of bilingual posit-drmm map is 43.67. p@20 of bilingual posit-drmm p@20 is 9.73. ndcg@20 of bilingual posit-drmm ndcg@20 is 52.8. aqwv of bilingual posit-drmm aqwv is 29.12. map of bilingual posit-drmm (3-model ensemble) map is 31.6. p@20 of bilingual posit-drmm (3-model ensemble) p@20 is 6.37. ndcg@20 of bilingual posit-drmm (3-model ensemble) ndcg@20 is 41.25. aqwv of bilingual posit-drmm (3-model ensemble) aqwv is 20.19. map of bilingual posit-drmm (3-model ensemble) map is 45.35. p@20 of bilingual posit-drmm (3-model ensemble) p@20 is 9.84. ndcg@20 of bilingual posit-drmm (3-model ensemble) ndcg@20 is 54.26. aqwv of bilingual posit-drmm (3-model ensemble) aqwv is 31.
table 2 shows the accuracy of different methods in various language pairs. accuracy of mikolov et al. (2013a) de-en is 61.93. accuracy of mikolov et al. (2013a) en-de is 73.07. accuracy of mikolov et al. (2013a) es-en is 74. accuracy of mikolov et al. (2013a) en-es is 80.73. accuracy of mikolov et al. (2013a) fr-en is 71.33. accuracy of mikolov et al. (2013a) en-fr is 82.2. accuracy of mikolov et al. (2013a) it-en is 68.93. accuracy of mikolov et al. (2013a) en-it is 77.6. accuracy of xing et al. (2015) de-en is 67.73. accuracy of xing et al. (2015) en-de is 69.53. accuracy of xing et al. (2015) es-en is 77.2. accuracy of xing et al. (2015) en-es is 78.6. accuracy of xing et al. (2015) fr-en is 76.33. accuracy of xing et al. (2015) en-fr is 78.67. accuracy of xing et al. (2015) it-en is 72. accuracy of xing et al. (2015) en-it is 73.33. accuracy of shigeto et al. (2015) de-en is 71.07. accuracy of shigeto et al. (2015) en-de is 63.73. accuracy of shigeto et al. (2015) es-en is 81.07. accuracy of shigeto et al. (2015) en-es is 74.53. accuracy of shigeto et al. (2015) fr-en is 79.93. accuracy of shigeto et al. (2015) en-fr is 73.13. accuracy of shigeto et al. (2015) it-en is 76.47. accuracy of shigeto et al. (2015) en-it is 68.13. accuracy of artetxe et al. (2016) de-en is 69.13. accuracy of artetxe et al. (2016) en-de is 72.13. accuracy of artetxe et al. (2016) es-en is 78.27. accuracy of artetxe et al. (2016) en-es is 80.07. accuracy of artetxe et al. (2016) fr-en is 77.73. accuracy of artetxe et al. (2016) en-fr is 79.2. accuracy of artetxe et al. (2016) it-en is 73.6. accuracy of artetxe et al. (2016) en-it is 74.47. accuracy of artetxe et al. (2017) de-en is 68.07. accuracy of artetxe et al. (2017) en-de is 69.2. accuracy of artetxe et al. (2017) es-en is 75.6. accuracy of artetxe et al. (2017) en-es is 78.2. accuracy of artetxe et al. (2017) fr-en is 74.47. accuracy of artetxe et al. (2017) en-fr is 77.67. accuracy of artetxe et al. (2017) it-en is 70.53. accuracy of artetxe et al. (2017) en-it is 71.67. accuracy of zhang et al. (2017a) de-en is 40.13. accuracy of zhang et al. (2017a) en-de is 41.27. accuracy of zhang et al. (2017a) es-en is 58.8. accuracy of zhang et al. (2017a) en-es is 60.93. accuracy of zhang et al. (2017a) en-fr is 57.6. accuracy of zhang et al. (2017a) it-en is 43.6. accuracy of zhang et al. (2017a) en-it is 44.53. accuracy of zhang et al. (2017b) en-de is 55.2. accuracy of zhang et al. (2017b) es-en is 70.87. accuracy of zhang et al. (2017b) en-es is 71.4. accuracy of zhang et al. (2017b) it-en is 64.87. accuracy of zhang et al. (2017b) en-it is 65.27. accuracy of lample et al. (2018) de-en is 69.73. accuracy of lample et al. (2018) en-de is 71.33. accuracy of lample et al. (2018) es-en is 79.07. accuracy of lample et al. (2018) en-es is 78.8. accuracy of lample et al. (2018) fr-en is 77.87. accuracy of lample et al. (2018) en-fr is 78.13. accuracy of lample et al. (2018) it-en is 74.47. accuracy of lample et al. (2018) en-it is 75.33. accuracy of xu et al. (2018) de-en is 67. accuracy of xu et al. (2018) en-de is 69.33. accuracy of xu et al. (2018) es-en is 77.8. accuracy of xu et al. (2018) en-es is 79.53. accuracy of xu et al. (2018) fr-en is 75.47. accuracy of xu et al. (2018) en-fr is 77.93. accuracy of xu et al. (2018) it-en is 72.6. accuracy of xu et al. (2018) en-it is 73.47. accuracy of artetxe et al. (2018a) de-en is 72.27. accuracy of artetxe et al. (2018a) en-de is 73.6. accuracy of artetxe et al. (2018a) es-en is 81.6. accuracy of artetxe et al. (2018a) en-es is 80.67. accuracy of artetxe et al. (2018a) fr-en is 80.2. accuracy of artetxe et al. (2018a) en-fr is 80.4. accuracy of artetxe et al. (2018a) it-en is 76.33. accuracy of artetxe et al. (2018a) en-it is 77.13. accuracy of ours de-en is 73.13. accuracy of ours en-de is 74.47. accuracy of ours es-en is 82.13. accuracy of ours en-es is 81.87. accuracy of ours fr-en is 81.53. accuracy of ours en-fr is 81.27. accuracy of ours it-en is 77.6. accuracy of ours en-it is 78.33.
table 2 shows bucc results (precision, recall and f1) on the training set, used to optimize the filtering threshold. p of forward p is 78.9. r of forward r is 75.1. f1 of forward f1 is 77. p of forward p is 82.1. r of forward r is 74.2. f1 of forward f1 is 77.9. p of backward p is 79. r of backward r is 73.1. f1 of backward f1 is 75.9. p of backward p is 77.2. r of backward r is 72.2. f1 of backward f1 is 74.7. p of intersection p is 84.9. r of intersection r is 80.8. f1 of intersection f1 is 82.8. p of intersection p is 83.6. r of intersection r is 78.3. f1 of intersection f1 is 80.9. p of max. score p is 83.1. r of max. score r is 77.2. f1 of max. score f1 is 80.1. p of max. score p is 80.9. r of max. score r is 77.5. f1 of max. score f1 is 79.2. p of forward p is 94.8. r of forward r is 94.1. f1 of forward f1 is 94.4. p of forward p is 91.1. r of forward r is 91.8. f1 of forward f1 is 91.4. p of backward p is 94.8. r of backward r is 94.1. f1 of backward f1 is 94.4. p of backward p is 91.5. r of backward r is 91.4. f1 of backward f1 is 91.4. p of intersection p is 94.9. r of intersection r is 94.1. f1 of intersection f1 is 94.5. p of intersection p is 91.2. r of intersection r is 91.8. f1 of intersection f1 is 91.5. p of max. score p is 94.9. r of max. score r is 94.1. f1 of max. score f1 is 94.5. p of max. score p is 91.2. r of max. score r is 91.8. f1 of max. score f1 is 91.5. p of forward p is 95.2. r of forward r is 94.4. f1 of forward f1 is 94.8. p of forward p is 92.4. r of forward r is 91.3. f1 of forward f1 is 91.8. p of backward p is 95.2. r of backward r is 94.4. f1 of backward f1 is 94.8. p of backward p is 92.3. r of backward r is 91.3. f1 of backward f1 is 91.8. p of intersection p is 95.3. r of intersection r is 94.4. f1 of intersection f1 is 94.8. p of intersection p is 92.4. r of intersection r is 91.3. f1 of intersection f1 is 91.9. p of max. score p is 95.3. r of max. score r is 94.4. f1 of max. score f1 is 94.8. p of max. score p is 92.4. r of max. score r is 91.3. f1 of max. score f1 is 91.9.
table 3 shows name normalization accuracy on disease (di) and chemical (ch) datasets. accuracy of jaccard (di) is 0.843. accuracy of jaccard (di) is 0.772. accuracy of jaccard (ch) is 0.935. accuracy of sg w (di) is 0.8. accuracy of sg w (di) is 0.725. accuracy of sg w (ch) is 0.771. accuracy of sg w + wmd (di) is 0.779. accuracy of sg w + wmd (di) is 0.731. accuracy of sg w + wmd (ch) is 0.919. accuracy of sg s (di) is 0.815. accuracy of sg s (di) is 0.79. accuracy of sg s (ch) is 0.929. accuracy of sg s.c (di) is 0.838. accuracy of sg s.c (di) is 0.811. accuracy of sg s.c (ch) is 0.929. accuracy of bne + sg w (di) is 0.854. accuracy of bne + sg w (di) is 0.829. accuracy of bne + sg w (ch) is 0.93. accuracy of bne + sg s.c (di) is 0.857. accuracy of bne + sg s.c (di) is 0.829. accuracy of bne + sg s.c (ch) is 0.934. accuracy of wieting et al. (2015) (di) is 0.822. accuracy of wieting et al. (2015) (di) is 0.813. accuracy of wieting et al. (2015) (ch) is 0.93. accuracy of dsouza and ng (2015) (di) is 0.847. accuracy of dsouza and ng (2015) (di) is 0.841. accuracy of leaman and lu (2016) (di) is 0.877*. accuracy of leaman and lu (2016) (di) is 0.889*. accuracy of leaman and lu (2016) (ch) is 0.941. accuracy of wright et al. (2019) (di) is 0.878*. accuracy of wright et al. (2019) (di) is 0.880*. accuracy of bne + sg w + xm (di) is 0.873. accuracy of bne + sg w + xm (di) is 0.905. accuracy of bne + sg w + xm (ch) is 0.954. accuracy of bne + sg s.c + xm (di) is 0.877. accuracy of bne + sg s.c + xm (di) is 0.906. accuracy of bne + sg s.c + xm (ch) is 0.958.
table 1 shows accuracy and macro-averaged f-measure, precision and recall on bless and diffvec. acc. of (this paper) acc. is 85.3. f1 of (this paper) f1 is 64.2. prec. of (this paper) prec. is 65.1. rec. of (this paper) rec. is 64.5. acc. of (this paper) acc. is 94.3. f1 of (this paper) f1 is 92.8. prec. of (this paper) prec. is 93.0. rec. of (this paper) rec. is 92.6. acc. of (joshi et al., 2019) acc. is 85. f1 of (joshi et al., 2019) f1 is 64.0. prec. of (joshi et al., 2019) prec. is 65.0. rec. of (joshi et al., 2019) rec. is 64.5. acc. of (joshi et al., 2019) acc. is 91.2. f1 of (joshi et al., 2019) f1 is 89.3. prec. of (joshi et al., 2019) prec. is 88.9. rec. of (joshi et al., 2019) rec. is 89.7. acc. of (bojanowski et al., 2017) acc. is 84.2. f1 of (bojanowski et al., 2017) f1 is 61.4. prec. of (bojanowski et al., 2017) prec. is 62.6. rec. of (bojanowski et al., 2017) rec. is 61.9. acc. of (bojanowski et al., 2017) acc. is 92.8. f1 of (bojanowski et al., 2017) f1 is 90.4. prec. of (bojanowski et al., 2017) prec. is 90.7. rec. of (bojanowski et al., 2017) rec. is 90.2. acc. of (faruqui et al., 2015) acc. is 86.1*. f1 of (faruqui et al., 2015) f1 is 64.6*. prec. of (faruqui et al., 2015) prec. is 66.6*. rec. of (faruqui et al., 2015) rec. is 64.5*. acc. of (faruqui et al., 2015) acc. is 90.6. f1 of (faruqui et al., 2015) f1 is 88.3. prec. of (faruqui et al., 2015) prec. is 88.1. rec. of (faruqui et al., 2015) rec. is 88.6. acc. of (mrkšić et al., 2017) acc. is 86.0*. f1 of (mrkšić et al., 2017) f1 is 64.6*. prec. of (mrkšić et al., 2017) prec. is 66.0*. rec. of (mrkšić et al., 2017) rec. is 65.2*. acc. of (mrkšić et al., 2017) acc. is 91.2. f1 of (mrkšić et al., 2017) f1 is 89.0. prec. of (mrkšić et al., 2017) prec. is 88.8. rec. of (mrkšić et al., 2017) rec. is 89.3. acc. of (joshi et al., 2019) acc. is 84.8. f1 of (joshi et al., 2019) f1 is 64.1. prec. of (joshi et al., 2019) prec. is 65.7. rec. of (joshi et al., 2019) rec. is 64.4. acc. of (joshi et al., 2019) acc. is 90.9. f1 of (joshi et al., 2019) f1 is 88.8. prec. of (joshi et al., 2019) prec. is 88.6. rec. of (joshi et al., 2019) rec. is 89.1. acc. of (bojanowski et al., 2017) acc. is 84.3. f1 of (bojanowski et al., 2017) f1 is 61.3. prec. of (bojanowski et al., 2017) prec. is 62.4. rec. of (bojanowski et al., 2017) rec. is 61.8. acc. of (bojanowski et al., 2017) acc. is 92.9. f1 of (bojanowski et al., 2017) f1 is 90.6. prec. of (bojanowski et al., 2017) prec. is 90.8. rec. of (bojanowski et al., 2017) rec. is 90.4. acc. of (bojanowski et al., 2017) acc. is 81.9. f1 of (bojanowski et al., 2017) f1 is 57.3. prec. of (bojanowski et al., 2017) prec. is 59.3. rec. of (bojanowski et al., 2017) rec. is 57.8. acc. of (bojanowski et al., 2017) acc. is 88.5. f1 of (bojanowski et al., 2017) f1 is 85.4. prec. of (bojanowski et al., 2017) prec. is 85.7. rec. of (bojanowski et al., 2017) rec. is 85.4.
table 2 shows results on the mcrae feature norms dataset (macro f-score) and qvec (correlation score). f-score of rwe overall is 55.2. f-score of rwe metal is 73.6. f-score of rwe is_small is 46.7. f-score of rwe is_large is 45.9. f-score of rwe animal is 89.2. f-score of rwe is_edible is 61.5. f-score of rwe wood is 38.5. f-score of rwe is_round is 39. f-score of rwe is_long is 46.8. correlation of rwe - is 55.4. f-score of pair2vec overall is 55. f-score of pair2vec metal is 71.9. f-score of pair2vec is_small is 49.2. f-score of pair2vec is_large is 43.3. f-score of pair2vec animal is 88.9. f-score of pair2vec is_edible is 68.3. f-score of pair2vec wood is 37.7. f-score of pair2vec is_round is 35. f-score of pair2vec is_long is 45.5. correlation of pair2vec - is 52.7. f-score of retrofitting overall is 50.6. f-score of retrofitting metal is 72.3. f-score of retrofitting is_small is 44. f-score of retrofitting is_large is 39.1. f-score of retrofitting animal is 90.6. f-score of retrofitting is_edible is 75.7. f-score of retrofitting wood is 15.4. f-score of retrofitting is_round is 22.9. f-score of retrofitting is_long is 44.4. correlation of retrofitting - is 56.8*. f-score of attract-repel overall is 50.4. f-score of attract-repel metal is 73.2. f-score of attract-repel is_small is 44.4. f-score of attract-repel is_large is 33.3. f-score of attract-repel animal is 88.9. f-score of attract-repel is_edible is 71.8. f-score of attract-repel wood is 31.1. f-score of attract-repel is_round is 24.2. f-score of attract-repel is_long is 35.9. correlation of attract-repel - is 55.9*. f-score of fasttext overall is 54.6. f-score of fasttext metal is 72.7. f-score of fasttext is_small is 48.4. f-score of fasttext is_large is 45.2. f-score of fasttext animal is 87.5. f-score of fasttext is_edible is 63.2. f-score of fasttext wood is 33.3. f-score of fasttext is_round is 39. f-score of fasttext is_long is 47.8. correlation of fasttext - is 54.6.
table 1 shows word analogy accuracy results on different datasets. accuracy of glove gsem is 78.85. accuracy of glove gsyn is 62.81. accuracy of glove msr is 53.04. accuracy of glove im is 55.21. accuracy of glove dm is 14.82. accuracy of glove es is 10.56. accuracy of glove ls is 0.881. accuracy of sg gsem is 71.58. accuracy of sg gsyn is 60.50. accuracy of sg msr is 51.71. accuracy of sg im is 55.45. accuracy of sg dm is 13.48. accuracy of sg es is 08.78. accuracy of sg ls is 0.671. accuracy of cbow gsem is 64.81. accuracy of cbow gsyn is 47.39. accuracy of cbow msr is 45.33. accuracy of cbow im is 50.58. accuracy of cbow dm is 10.11. accuracy of cbow es is 07.02. accuracy of cbow ls is 0.764. accuracy of wemap gsem is 83.52. accuracy of wemap gsyn is 63.08. accuracy of wemap msr is 55.08. accuracy of wemap im is 56.03. accuracy of wemap dm is 14.95. accuracy of wemap es is 10.62. accuracy of wemap ls is 0.903. accuracy of cvmf gsem is 63.22. accuracy of cvmf gsyn is 67.41. accuracy of cvmf msr is 63.21. accuracy of cvmf im is 65.94. accuracy of cvmf dm is 17.46. accuracy of cvmf es is 9.380. accuracy of cvmf ls is 1.100. accuracy of cvmf(nig) gsem is 64.14. accuracy of cvmf(nig) gsyn is 67.55. accuracy of cvmf(nig) msr is 63.55. accuracy of cvmf(nig) im is 65.95. accuracy of cvmf(nig) dm is 17.49. accuracy of cvmf(nig) es is 9.410. accuracy of cvmf(nig) ls is 1.210.
table 7 shows document classification results (f1). f1 of tf-idf 20ng is 0.852. f1 of tf-idf ohs is 0.632. f1 of tf-idf techtc is 0.306. f1 of tf-idf reu is 0.319. f1 of lda 20ng is 0.859. f1 of lda ohs is 0.629. f1 of lda techtc is 0.305. f1 of lda reu is 0.323. f1 of hdp 20ng is 0.862. f1 of hdp ohs is 0.627. f1 of hdp techtc is 0.304. f1 of hdp reu is 0.339. f1 of movmf 20ng is 0.809. f1 of movmf ohs is 0.610. f1 of movmf techtc is 0.302. f1 of movmf reu is 0.336. f1 of glda 20ng is 0.862. f1 of glda ohs is 0.629. f1 of glda techtc is 0.305. f1 of glda reu is 0.352. f1 of shdp 20ng is 0.863. f1 of shdp ohs is 0.631. f1 of shdp techtc is 0.304. f1 of shdp reu is 0.353. f1 of glove 20ng is 0.852. f1 of glove ohs is 0.629. f1 of glove techtc is 0.301. f1 of glove reu is 0.315. f1 of wemap 20ng is 0.855. f1 of wemap ohs is 0.630. f1 of wemap techtc is 0.306. f1 of wemap reu is 0.345. f1 of sg 20ng is 0.853. f1 of sg ohs is 0.631. f1 of sg techtc is 0.304. f1 of sg reu is 0.341. f1 of cbow 20ng is 0.823. f1 of cbow ohs is 0.629. f1 of cbow techtc is 0.297. f1 of cbow reu is 0.339. f1 of cvmf 20ng is 0.871. f1 of cvmf ohs is 0.633. f1 of cvmf techtc is 0.305. f1 of cvmf reu is 0.362. f1 of cvmf(nig) 20ng is 0.871. f1 of cvmf(nig) ohs is 0.633. f1 of cvmf(nig) techtc is 0.305. f1 of cvmf(nig) reu is 0.363.
table 2 shows ablation study results of our approach. best of our approach best is 20.3. best-m of our approach best-m is 34.2. oot of our approach oot is 55.4. oot-m of our approach oot-m is 68.4. p@1 of our approach p@1 is 51.1. best of  - w/o sp (keep) best is 18.9. best-m of  - w/o sp (keep) best-m is 32.6. oot of  - w/o sp (keep) oot is 51.7. oot-m of  - w/o sp (keep) oot-m is 63.5. p@1 of  - w/o sp (keep) p@1 is 48.6. best of  - w/o sp (mask) best is 16.2. best-m of  - w/o sp (mask) best-m is 27.5. oot of  - w/o sp (mask) oot is 46.4. oot-m of  - w/o sp (mask) oot-m is 57.9. p@1 of  - w/o sp (mask) p@1 is 43.3. best of  - w/o sp (wordnet) best is 15.9. best-m of  - w/o sp (wordnet) best-m is 27.1. oot of  - w/o sp (wordnet) oot is 45.9. oot-m of  - w/o sp (wordnet) oot-m is 57.1. p@1 of  - w/o sp (wordnet) p@1 is 42.8. best of  - w/o sv best is 12.1. best-m of  - w/o sv best-m is 20.2. oot of  - w/o sv oot is 40.8. oot-m of  - w/o sv oot-m is 56.9. p@1 of  - w/o sv p@1 is 13.1. best of bert (keep) best is 9.2. best-m of bert (keep) best-m is 16.3. oot of bert (keep) oot is 37.3. oot-m of bert (keep) oot-m is 52.2. p@1 of bert (keep) p@1 is 9.2. best of bert (mask) best is 8.6. best-m of bert (mask) best-m is 14.2. oot of bert (mask) oot is 33.2. oot-m of bert (mask) oot-m is 48.9. p@1 of bert (mask) p@1 is 5.7. best of our approach best is 14.5. best-m of our approach best-m is 33.9. oot of our approach oot is 45.9. oot-m of our approach oot-m is 69.9. p@1 of our approach p@1 is 56.3. best of  - w/o sp (keep) best is 13.7. best-m of  - w/o sp (keep) best-m is 31.4. oot of  - w/o sp (keep) oot is 41.3. oot-m of  - w/o sp (keep) oot-m is 63.5. p@1 of  - w/o sp (keep) p@1 is 53.1. best of  - w/o sp (mask) best is 11.3. best-m of  - w/o sp (mask) best-m is 26.7. oot of  - w/o sp (mask) oot is 36.2. oot-m of  - w/o sp (mask) oot-m is 59.1. p@1 of  - w/o sp (mask) p@1 is 47.1. best of  - w/o sp (wordnet) best is 11. best-m of  - w/o sp (wordnet) best-m is 26.3. oot of  - w/o sp (wordnet) oot is 35.9. oot-m of  - w/o sp (wordnet) oot-m is 58.7. p@1 of  - w/o sp (wordnet) p@1 is 46.3. best of  - w/o sv best is 9.1. best-m of  - w/o sv best-m is 19.7. oot of  - w/o sv oot is 33.5. oot-m of  - w/o sv oot-m is 56.9. p@1 of  - w/o sv p@1 is 14.3. best of bert (keep) best is 8.3. best-m of bert (keep) best-m is 17.2. oot of bert (keep) oot is 31.1. oot-m of bert (keep) oot-m is 54.4. p@1 of bert (keep) p@1 is 11.2. best of bert (mask) best is 7.6. best-m of bert (mask) best-m is 15.4. oot of bert (mask) oot is 38.5. oot-m of bert (mask) oot-m is 51.3. p@1 of bert (mask) p@1 is 7.6.
table 5 shows performance of the full-transformer (uwb) model evaluated on seen and unseen entities from the training and validation worlds. accuracy of training worlds seen accuracy is 87.74. accuracy of training worlds unseen accuracy is 82.96. accuracy of validation worlds unseen accuracy is 76.
table 1 shows parsing performance with and without punctuation. mean f of left-branching mean f is 20.7. mean f of left-branching mean f is 18.9. mean f of right-branching mean f is 58.5. mean f of right-branching mean f is 18.5. mean f of balanced-tree mean f is 39.5. mean f of balanced-tree mean f is 22. mean f of st-gumbel mean f is 36.4. self-agreement of st-gumbel self-agreement is 57. rb-agreement of st-gumbel rb-agreement is 33.8. mean f of st-gumbel mean f is 21.9. self-agreement of st-gumbel self-agreement is 56.8. rb-agreement of st-gumbel rb-agreement is 38.1. mean f of prpn mean f is 46. self-agreement of prpn self-agreement is 48.9. rb-agreement of prpn rb-agreement is 51.2. mean f of prpn mean f is 51.6. self-agreement of prpn self-agreement is 65. rb-agreement of prpn rb-agreement is 27.4. mean f of imitation (sbs only) mean f is 45.9. self-agreement of imitation (sbs only) self-agreement is 49.5. rb-agreement of imitation (sbs only) rb-agreement is 62.2. mean f of imitation (sbs only) mean f is 52. self-agreement of imitation (sbs only) self-agreement is 70.8. rb-agreement of imitation (sbs only) rb-agreement is 20.6. mean f of imitation (sbs + refine) mean f is 53.3. self-agreement of imitation (sbs + refine) self-agreement is 58.2. rb-agreement of imitation (sbs + refine) rb-agreement is 64.9. mean f of imitation (sbs + refine) mean f is 53.7. self-agreement of imitation (sbs + refine) self-agreement is 67.4. rb-agreement of imitation (sbs + refine) rb-agreement is 21.1.
table 6 shows correlation (τ) of generic da lexicons with gold standard lexicons. correlation of cz web gen is 0.58. correlation of cz web gen is 0.576. correlation of cz web pbc+/t is 0.529. correlation of cz web pbc+/nt is 0.524. correlation of de web gen is 0.654. correlation of de web gen is 0.654. correlation of de web pbc+/t is 0.634. correlation of de web pbc+/nt is 0.634. correlation of es web gen is 0.563. correlation of es web gen is 0.568. correlation of es web pbc+/t is 0.524. correlation of es web pbc+/nt is 0.514. correlation of fr web gen is 0.544. correlation of fr web gen is 0.54. correlation of fr web pbc+/t is 0.514. correlation of fr web pbc+/nt is 0.474. correlation of en tw. gen is 0.654. correlation of en tw. gen is 0.629. correlation of en tw. pbc+/t is 0.583. correlation of en tw. pbc+/nt is 0.583. correlation of en ne. gen is 0.622. correlation of en ne. gen is 0.582. correlation of en ne. pbc+/t is 0.562. correlation of en ne. pbc+/nt is 0.557. correlation of ja wiki gen is 0.628. correlation of ja wiki pbc+/t is 0.571. correlation of ja wiki pbc+/nt is 0.558.
table 6 shows sentence-level phrase accuracy (spacc) and phrase error deviation (pedev) comparison on sst-5 between bi-tree-lstm and tcm. spacc alpha=1 of btl spacc alpha=1 is 3.2. spacc alpha=0.9 of btl spacc alpha=0.9 is 20. spacc alpha=0.8 of btl spacc alpha=0.8 is 70.7. pedev-mean of btl pedev-mean is 36.4. pedev-median of btl pedev-median is 37.6. spacc alpha=1 of tcm spacc alpha=1 is 3.7. spacc alpha=0.9 of tcm spacc alpha=0.9 is 21.2. spacc alpha=0.8 of tcm spacc alpha=0.8 is 71.4. pedev-mean of tcm pedev-mean is 35.7. pedev-median of tcm pedev-median is 37. spacc alpha=1 of diff. spacc alpha=1 is 0.5. spacc alpha=0.9 of diff. spacc alpha=0.9 is 1.2. spacc alpha=0.8 of diff. spacc alpha=0.8 is 0.7. pedev-mean of diff. pedev-mean is -0.7. pedev-median of diff. pedev-median is -0.6.
table 3 shows comparison of oracles, baselines, retrieval, extractive, and abstractive models on the full proposed answers. rouge-1 of support document rouge-1 is 16.8. rouge-2 of support document rouge-2 is 2.3. rouge-l of support document rouge-l is 10.2. rouge-1 of nearest neighbor rouge-1 is 16.7. rouge-2 of nearest neighbor rouge-2 is 2.3. rouge-l of nearest neighbor rouge-l is 12.5. rouge-1 of extractive (tfidf) rouge-1 is 20.6. rouge-2 of extractive (tfidf) rouge-2 is 2.9. rouge-l of extractive (tfidf) rouge-l is 17. rouge-1 of extractive (bidaf) rouge-1 is 23.5. rouge-2 of extractive (bidaf) rouge-2 is 3.1. rouge-l of extractive (bidaf) rouge-l is 17.5. rouge-1 of oracle support doc rouge-1 is 27.4. rouge-2 of oracle support doc rouge-2 is 2.8. rouge-l of oracle support doc rouge-l is 19.9. rouge-1 of oracle web sources rouge-1 is 54.8. rouge-2 of oracle web sources rouge-2 is 8.6. rouge-l of oracle web sources rouge-l is 40.3. ppl of lm q + a ppl is 42.2. rouge-1 of lm q + a rouge-1 is 27.8. rouge-2 of lm q + a rouge-2 is 4.7. rouge-l of lm q + a rouge-l is 23.1. ppl of lm q + d + a ppl is 33.9. rouge-1 of lm q + d + a rouge-1 is 26.4. rouge-2 of lm q + d + a rouge-2 is 4. rouge-l of lm q + d + a rouge-l is 20.5. ppl of seq2seq q to a ppl is 52.9. rouge-1 of seq2seq q to a rouge-1 is 28.3. rouge-2 of seq2seq q to a rouge-2 is 5.1. rouge-l of seq2seq q to a rouge-l is 22.7. ppl of seq2seq q + d to a ppl is 55.1. rouge-1 of seq2seq q + d to a rouge-1 is 28.3. rouge-2 of seq2seq q + d to a rouge-2 is 5.1. rouge-l of seq2seq q + d to a rouge-l is 22.8. ppl of seq2seq multi-task ppl is 32.7. rouge-1 of seq2seq multi-task rouge-1 is 28.9. rouge-2 of seq2seq multi-task rouge-2 is 5.4. rouge-l of seq2seq multi-task rouge-l is 23.1.
table 5 shows results reported in the embedding scores, bleu, diversity, and the quality of emotional expression. average of seq2seq average is 0.523. greedy of seq2seq greedy is 0.376. extreme of seq2seq extreme is 0.35. bleu of seq2seq bleu is 1.5. distinct-1 of seq2seq distinct-1 is 0.0038. distinct-2 of seq2seq distinct-2 is 0.012. emotion-a of seq2seq emotion-a is 0.335. emotion-w of seq2seq emotion-w is 0.371. average of emoemb average is 0.524. greedy of emoemb greedy is 0.381. extreme of emoemb extreme is 0.355. bleu of emoemb bleu is 1.69. distinct-1 of emoemb distinct-1 is 0.0054. distinct-2 of emoemb distinct-2 is 0.0484. emotion-a of emoemb emotion-a is 0.72. emotion-w of emoemb emotion-w is 0.512. average of ecm average is 0.624. greedy of ecm greedy is 0.434. extreme of ecm extreme is 0.409. bleu of ecm bleu is 1.68. distinct-1 of ecm distinct-1 is 0.009. distinct-2 of ecm distinct-2 is 0.0735. emotion-a of ecm emotion-a is 0.765. emotion-w of ecm emotion-w is 0.58. average of emods-mle average is 0.548. greedy of emods-mle greedy is 0.367. extreme of emods-mle extreme is 0.374. bleu of emods-mle bleu is 1.6. distinct-1 of emods-mle distinct-1 is 0.0053. distinct-2 of emods-mle distinct-2 is 0.067. emotion-a of emods-mle emotion-a is 0.721. emotion-w of emods-mle emotion-w is 0.556. average of emods-ev average is 0.571. greedy of emods-ev greedy is 0.39. extreme of emods-ev extreme is 0.384. bleu of emods-ev bleu is 1.64. distinct-1 of emods-ev distinct-1 is 0.0053. distinct-2 of emods-ev distinct-2 is 0.0659. emotion-a of emods-ev emotion-a is 0.746. emotion-w of emods-ev emotion-w is 0.47. average of emods-bs average is 0.614. greedy of emods-bs greedy is 0.442. extreme of emods-bs extreme is 0.409. bleu of emods-bs bleu is 1.73. distinct-1 of emods-bs distinct-1 is 0.0051. distinct-2 of emods-bs distinct-2 is 0.0467. emotion-a of emods-bs emotion-a is 0.773. emotion-w of emods-bs emotion-w is 0.658. average of emods average is 0.634. greedy of emods greedy is 0.451. extreme of emods extreme is 0.435. bleu of emods bleu is 1.73. distinct-1 of emods distinct-1 is 0.0113. distinct-2 of emods distinct-2 is 0.0867. emotion-a of emods emotion-a is 0.81. emotion-w of emods emotion-w is 0.687.
table 6 shows the results of human evaluation. joy of seq2seq cont. is 1.35. joy of seq2seq emot. is 0.455. contentment of seq2seq cont. is 1.445. contentment of seq2seq emot. is 0.325. disguss of seq2seq cont. is 1.18. disguss of seq2seq emot. is 0.095. anger of seq2seq cont. is 1.15. anger of seq2seq emot. is 0.115. sadness of seq2seq cont. is 1.09. sadness of seq2seq emot. is 0.1. overall of seq2seq cont. is 1.243. overall of seq2seq emot. is 0.216. joy of emoemb cont. is 1.285. joy of emoemb emot. is 0.655. contentment of emoemb cont. is 1.32. contentment of emoemb emot. is 0.565. disguss of emoemb cont. is 1.015. disguss of emoemb emot. is 0.225. anger of emoemb cont. is 1.16. anger of emoemb emot. is 0.4. sadness of emoemb cont. is 0.995. sadness of emoemb emot. is 0.19. overall of emoemb cont. is 1.155. overall of emoemb emot. is 0.407. joy of ecm cont. is 1.395. joy of ecm emot. is 0.69. contentment of ecm cont. is 1.4. contentment of ecm emot. is 0.615. disguss of ecm cont. is 1.13. disguss of ecm emot. is 0.425. anger of ecm cont. is 1.19. anger of ecm emot. is 0.33. sadness of ecm cont. is 1.195. sadness of ecm emot. is 0.335. overall of ecm cont. is 1.262. overall of ecm emot. is 0.479. joy of emods cont. is 1.265. joy of emods emot. is 0.695. contentment of emods cont. is 1.26. contentment of emods emot. is 0.685. disguss of emods cont. is 1.37. disguss of emods emot. is 0.53. anger of emods cont. is 1.185. anger of emods emot. is 0.505. sadness of emods cont. is 1.265. sadness of emods emot. is 0.625. overall of emods cont. is 1.269. overall of emods emot. is 0.608.
table 2 shows performances on whether using multi-level vocabularies or not, where “sv” represents single vocabulary (from raw words), and “mvs” means multilevel vocabularies obtained from hierarchical clustering. bleu of enc3-dec1 (sv) bleu is 6.27. rouge of enc3-dec1 (sv) rouge is 6.29. bleu of enc3-dec1 (sv) bleu is 6.61. rouge of enc3-dec1 (sv) rouge is 7.08. bleu of enc3-dec1 (mvs) bleu is 7.16. rouge of enc3-dec1 (mvs) rouge is 8.01. bleu of enc3-dec1 (mvs) bleu is 9.15. rouge of enc3-dec1 (mvs) rouge is 10.63. bleu of enc1-dec3 (sv) bleu is 7.43. rouge of enc1-dec3 (sv) rouge is 7.54. bleu of enc1-dec3 (sv) bleu is 9.92. rouge of enc1-dec3 (sv) rouge is 10.24. bleu of enc1-dec3 (mvs) bleu is 6.75. rouge of enc1-dec3 (mvs) rouge is 7.78. bleu of enc1-dec3 (mvs) bleu is 12.01. rouge of enc1-dec3 (mvs) rouge is 10.86. bleu of enc3-dec3 (sv) bleu is 7.44. rouge of enc3-dec3 (sv) rouge is 7.56. bleu of enc3-dec3 (sv) bleu is 9.95. rouge of enc3-dec3 (sv) rouge is 9.7. bleu of enc3-dec3 (mvs) bleu is 8.58. rouge of enc3-dec3 (mvs) rouge is 7.88. bleu of enc3-dec3 (mvs) bleu is 12.51. rouge of enc3-dec3 (mvs) rouge is 11.76.
table 2 shows on-device results and comparison on multiple datasets and languages accuracy of sgnn++ (our on-device) mrda is 87.3. accuracy of sgnn++ (our on-device) swda is 88.43. accuracy of sgnn++ (our on-device) atis is 93.73. accuracy of sgnn++ (our on-device) cf-en is 65. accuracy of sgnn++ (our on-device) cf-jp is 74.33. accuracy of sgnn++ (our on-device) cf-fr is 70.93. accuracy of sgnn++ (our on-device) cf-sp is 83.95. accuracy of sgnn(ravi and kozareva, 2018)(sota on-device) mrda is 86.7. accuracy of sgnn(ravi and kozareva, 2018)(sota on-device) swda is 83.1. accuracy of rnn(khanpour et al., 2016) mrda is 86.8. accuracy of rnn(khanpour et al., 2016) swda is 80.1. accuracy of rnn+attention(ortega and vu, 2017) mrda is 84.3. accuracy of rnn+attention(ortega and vu, 2017) swda is 73.9. accuracy of cnn(lee and dernoncourt, 2016) mrda is 84.6. accuracy of cnn(lee and dernoncourt, 2016) swda is 73.1. accuracy of gatedatten.(goo et al., 2018) atis is 93.6. accuracy of jointbilstm(hakkani-tur et al., 2016) atis is 92.6. accuracy of jointbilstm(hakkani-tur et al., 2016) cf-sp is . accuracy of atten.rnn(liu and lane, 2016) atis is 91.1. accuracy of adapt-run1(dzendzik et al., 2017) cf-en is 63.4. accuracy of adapt-run1(dzendzik et al., 2017) cf-jp is 67.67. accuracy of adapt-run1(dzendzik et al., 2017) cf-fr is 69.5. accuracy of adapt-run1(dzendzik et al., 2017) cf-sp is 83.61. accuracy of bingo-logistic-reg(elfardy et al., 2017) cf-en is 55.8. accuracy of bingo-logistic-reg(elfardy et al., 2017) cf-jp is 60.67. accuracy of bingo-logistic-reg(elfardy et al., 2017) cf-fr is 59. accuracy of bingo-logistic-reg(elfardy et al., 2017) cf-sp is 72.91. accuracy of baseline mrda is 74.6. accuracy of baseline swda is 47.3. accuracy of baseline atis is 72.22. accuracy of baseline cf-en is 48.8. accuracy of baseline cf-jp is 56.67. accuracy of baseline cf-fr is 54.75. accuracy of baseline cf-sp is 77.26.
table 1 shows automatic evaluation results of different models where the best results are bold. bleu-1 of s2s bleu-1 is 21.49. bleu-2 of s2s bleu-2 is 9.498. g of s2s g is 0.567. a of s2s a is 0.677. e of s2s e is 0.415. dist-1 of s2s dist-1 is 0.311. dist-2 of s2s dist-2 is 0.447. dist-1 of s2s dist-1 is 0.027. dist-2 of s2s dist-2 is 0.127. bleu-1 of s2s+db bleu-1 is 20.2. bleu-2 of s2s+db bleu-2 is 9.445. g of s2s+db g is 0.561. a of s2s+db a is 0.682. e of s2s+db e is 0.422. dist-1 of s2s+db dist-1 is 0.324. dist-2 of s2s+db dist-2 is 0.457. dist-1 of s2s+db dist-1 is 0.028. dist-2 of s2s+db dist-2 is 0.13. bleu-1 of mms bleu-1 is 21.4. bleu-2 of mms bleu-2 is 9.398. g of mms g is 0.569. a of mms a is 0.691. e of mms e is 0.427. dist-1 of mms dist-1 is 0.561. dist-2 of mms dist-2 is 0.697. dist-1 of mms dist-1 is 0.033. dist-2 of mms dist-2 is 0.158. bleu-1 of cvae bleu-1 is 22.71. bleu-2 of cvae bleu-2 is 8.923. g of cvae g is 0.601. a of cvae a is 0.73. e of cvae e is 0.452. dist-1 of cvae dist-1 is 0.628. dist-2 of cvae dist-2 is 0.801. dist-1 of cvae dist-1 is 0.035. dist-2 of cvae dist-2 is 0.179. bleu-1 of cvae+bow bleu-1 is 23.12. bleu-2 of cvae+bow bleu-2 is 8.42. g of cvae+bow g is 0.605. a of cvae+bow a is 0.741. e of cvae+bow e is 0.456. dist-1 of cvae+bow dist-1 is 0.687. dist-2 of cvae+bow dist-2 is 0.873. dist-1 of cvae+bow dist-1 is 0.038. dist-2 of cvae+bow dist-2 is 0.194. bleu-1 of wae bleu-1 is 24.02. bleu-2 of wae bleu-2 is 9.281. g of wae g is 0.611. a of wae a is 0.754. e of wae e is 0.46. dist-1 of wae dist-1 is 0.734. dist-2 of wae dist-2 is 0.885. dist-1 of wae dist-1 is 0.044. dist-2 of wae dist-2 is 0.196. bleu-1 of ours-first bleu-1 is 23.68. bleu-2 of ours-first bleu-2 is 9.24. g of ours-first g is 0.619. a of ours-first a is 0.762. e of ours-first e is 0.471. dist-1 of ours-first dist-1 is 0.725. dist-2 of ours-first dist-2 is 0.891. dist-1 of ours-first dist-1 is 0.045. dist-2 of ours-first dist-2 is 0.199. bleu-1 of ours-disc bleu-1 is 24.22. bleu-2 of ours-disc bleu-2 is 9.101. g of ours-disc g is 0.617. a of ours-disc a is 0.754. e of ours-disc e is 0.465. dist-1 of ours-disc dist-1 is 0.67. dist-2 of ours-disc dist-2 is 0.863. dist-1 of ours-disc dist-1 is 0.036. dist-2 of ours-disc dist-2 is 0.184. bleu-1 of ours-mbow bleu-1 is 23.88. bleu-2 of ours-mbow bleu-2 is 9.582. g of ours-mbow g is 0.622. a of ours-mbow a is 0.778. e of ours-mbow e is 0.477. dist-1 of ours-mbow dist-1 is 0.681. dist-2 of ours-mbow dist-2 is 0.877. dist-1 of ours-mbow dist-1 is 0.04. dist-2 of ours-mbow dist-2 is 0.19. bleu-1 of ours bleu-1 is 24.04. bleu-2 of ours bleu-2 is 9.362. g of ours g is 0.625. a of ours a is 0.771. e of ours e is 0.48. dist-1 of ours dist-1 is 0.699. dist-2 of ours dist-2 is 0.876. dist-1 of ours dist-1 is 0.042. dist-2 of ours dist-2 is 0.19. bleu-1 of ours+gmp bleu-1 is 24.2. bleu-2 of ours+gmp bleu-2 is 9.417. g of ours+gmp g is 0.618. a of ours+gmp a is 0.769. e of ours+gmp e is 0.482. dist-1 of ours+gmp dist-1 is 0.728. dist-2 of ours+gmp dist-2 is 0.889. dist-1 of ours+gmp dist-1 is 0.044. dist-2 of ours+gmp dist-2 is 0.198.
table 4 shows conversation results on the ubuntu test set. vi of previous vi is 66.1. 1-1 of previous 1-1 is 27.6. p of previous p is 0. r of previous r is 0. f of previous f is 0. vi of linear vi is 88.9. 1-1 of linear 1-1 is 69.5. p of linear p is 19.3. r of linear r is 24.9. f of linear f is 21.8. vi of feedforward vi is 91.3. 1-1 of feedforward 1-1 is 75.6. p of feedforward p is 34.6. r of feedforward r is 38. f of feedforward f is 36.2. vi of x10 union vi is 86.2. 1-1 of x10 union 1-1 is 62.5. p of x10 union p is 40.4. r of x10 union r is 28.5. f of x10 union f is 33.4. vi of x10 vote vi is 91.5. 1-1 of x10 vote 1-1 is 76. p of x10 vote p is 36.3. r of x10 vote r is 39.7. f of x10 vote f is 38. vi of x10 intersect vi is 69.3. 1-1 of x10 intersect 1-1 is 26.6. p of x10 intersect p is 67. r of x10 intersect r is 21.1. f of x10 intersect f is 32.1. vi of lowe (2017) vi is 80.6. 1-1 of lowe (2017) 1-1 is 53.7. p of lowe (2017) p is 10.8. r of lowe (2017) r is 7.6. f of lowe (2017) f is 8.9. vi of elsner (2008) vi is 82.1. 1-1 of elsner (2008) 1-1 is 51.4. p of elsner (2008) p is 12.1. r of elsner (2008) r is 21.5. f of elsner (2008) f is 15.5.
table 5 shows performance with different training conditions on the ubuntu test set. accuracy of standard graph-f is 72.3 (0.4). accuracy of standard conv-f is 36.2 (1.7). accuracy of no context graph-f is 72.3 (0.2). accuracy of no context conv-f is 37.6 (1.6). accuracy of 1k random msg graph-f is 63.0* (0.4). accuracy of 1k random msg conv-f is 21 (2.3). accuracy of 2x 500 msg samples graph-f is 61.4* (1.8). accuracy of 2x 500 msg samples conv-f is 20.4 (3.2).
table 2 shows results (%) on 10,000 test query segments on the classification-for-modeling task. accuracy of cnn-encoder (separated) accuracy is 97.5. macro-f1 of cnn-encoder (separated) macro-f1 is 87.6. micro-f1 of cnn-encoder (separated) micro-f1 is 97.5. accuracy of cnn-encoder (separated) accuracy is 86.2. macro-f1 of cnn-encoder (separated) macro-f1 is 52. micro-f1 of cnn-encoder (separated) micro-f1 is 86.2. accuracy of rnn-encoder (separated) accuracy is 97.6. macro-f1 of rnn-encoder (separated) macro-f1 is 90.9. micro-f1 of rnn-encoder (separated) micro-f1 is 97.6. accuracy of rnn-encoder (separated) accuracy is 87.2. macro-f1 of rnn-encoder (separated) macro-f1 is 65.8. micro-f1 of rnn-encoder (separated) micro-f1 is 87.1. accuracy of cnn-encoder (joint) accuracy is 97.4. macro-f1 of cnn-encoder (joint) macro-f1 is 87.3. micro-f1 of cnn-encoder (joint) micro-f1 is 97.3. accuracy of cnn-encoder (joint) accuracy is 86.5. macro-f1 of cnn-encoder (joint) macro-f1 is 51.8. micro-f1 of cnn-encoder (joint) micro-f1 is 86.4. accuracy of rnn-encoder (joint) accuracy is 97.6. macro-f1 of rnn-encoder (joint) macro-f1 is 91.2. micro-f1 of rnn-encoder (joint) micro-f1 is 97.5. accuracy of rnn-encoder (joint) accuracy is 87.6. macro-f1 of rnn-encoder (joint) macro-f1 is 64.2. micro-f1 of rnn-encoder (joint) micro-f1 is 87.6.
table 4 shows results(%) on 5,000 test queries on the classification-for-testing task. accuracy of cnn-encoder (without query sefun) accuracy is 81.2. macro-f1 of cnn-encoder (without query sefun) macro-f1 is 15.1. micro-f1 of cnn-encoder (without query sefun) micro-f1 is 81.1. accuracy of cnn-encoder (without query sefun) accuracy is 55.7. macro-f1 of cnn-encoder (without query sefun) macro-f1 is 23.5. micro-f1 of cnn-encoder (without query sefun) micro-f1 is 55.7. accuracy of rnn-encoder (without query sefun) accuracy is 77.9. macro-f1 of rnn-encoder (without query sefun) macro-f1 is 30.3. micro-f1 of rnn-encoder (without query sefun) micro-f1 is 77.9. accuracy of rnn-encoder (without query sefun) accuracy is 65.6. macro-f1 of rnn-encoder (without query sefun) macro-f1 is 25.8. micro-f1 of rnn-encoder (without query sefun) micro-f1 is 65.5. accuracy of cnn-encoder (with query sefun) accuracy is 81.2. macro-f1 of cnn-encoder (with query sefun) macro-f1 is 17.4. micro-f1 of cnn-encoder (with query sefun) micro-f1 is 81.1. accuracy of cnn-encoder (with query sefun) accuracy is 65.6. macro-f1 of cnn-encoder (with query sefun) macro-f1 is 21.1. micro-f1 of cnn-encoder (with query sefun) micro-f1 is 65.6. accuracy of rnn-encoder (with query sefun) accuracy is 81.3. macro-f1 of rnn-encoder (with query sefun) macro-f1 is 28.5. micro-f1 of rnn-encoder (with query sefun) micro-f1 is 81.5. accuracy of rnn-encoder (with query sefun) accuracy is 65.5. macro-f1 of rnn-encoder (with query sefun) macro-f1 is 25.7. micro-f1 of rnn-encoder (with query sefun) micro-f1 is 65.7.
table 2 shows performance on named entity recognition and part-of-speech tagging tasks. f1-score of word2vec rare-ner is 0.1862. f1-score of word2vec bio-ner is 0.7205. acc of word2vec twitter pos is 0.7649. f1-score of fasttext rare-ner is 0.1981. f1-score of fasttext bio-ner is 0.7241. acc of fasttext twitter pos is 0.8116. f1-score of additive rare-ner is 0.2021. f1-score of additive bio-ner is 0.7034. acc of additive twitter pos is 0.7576. f1-score of nonce2vec rare-ner is 0.2096. f1-score of nonce2vec bio-ner is 0.7289. acc of nonce2vec twitter pos is 0.7734. f1-score of ã  la carte rare-ner is 0.2153. f1-score of ã  la carte bio-ner is 0.7423. acc of ã  la carte twitter pos is 0.7883. f1-score of hice w/o morph rare-ner is 0.2394. f1-score of hice w/o morph bio-ner is 0.7486. acc of hice w/o morph twitter pos is 0.8194. f1-score of hice + morph rare-ner is 0.2375. f1-score of hice + morph bio-ner is 0.7522. acc of hice + morph twitter pos is 0.8227. f1-score of hice + morph + maml rare-ner is 0.2419. f1-score of hice + morph + maml bio-ner is 0.7636. acc of hice + morph + maml twitter pos is 0.8286.
table 5 shows performance gains of two neural temporality adaptation models when they are initialized by diachronic word embeddings as compared to initialization with standard non-diachronic word embeddings. f1 of twitter incre is -0.7. f1 of twitter linear is 1.4. f1 of twitter procrustes is -0.2. f1 of twitter subword is -0.8. f1 of twitter incre is 1.4. f1 of twitter linear is -0.3. f1 of twitter procrutes is 1.7. f1 of twitter subword is 3.5. f1 of economy incre is 0.5. f1 of economy linear is 0. f1 of economy procrustes is -0.7. f1 of economy subword is 0.4. f1 of economy incre is -0.3. f1 of economy linear is -1. f1 of economy procrutes is -0.5. f1 of economy subword is 0.3. f1 of yelp-rest incre is 1.4. f1 of yelp-rest linear is 0.1. f1 of yelp-rest procrustes is -1.9. f1 of yelp-rest subword is 2.3. f1 of yelp-rest incre is 1.9. f1 of yelp-rest linear is 1.6. f1 of yelp-rest procrutes is 1.4. f1 of yelp-rest subword is 4.3. f1 of yelp-hotel incre is -1.5. f1 of yelp-hotel linear is -1.2. f1 of yelp-hotel procrustes is -0.5. f1 of yelp-hotel subword is -0.2. f1 of yelp-hotel incre is -0.7. f1 of yelp-hotel linear is -2. f1 of yelp-hotel procrutes is -1.8. f1 of yelp-hotel subword is 0.8. f1 of amazon incre is 0.2. f1 of amazon linear is 0.2. f1 of amazon procrustes is -2. f1 of amazon subword is 0.5. f1 of amazon incre is -0.8. f1 of amazon linear is -0.7. f1 of amazon procrutes is -0.8. f1 of amazon subword is 2.1. f1 of dianping incre is 0.4. f1 of dianping linear is 1.6. f1 of dianping procrustes is 0.7. f1 of dianping subword is 1. f1 of dianping incre is 0.8. f1 of dianping linear is 1.8. f1 of dianping procrutes is 3.4. f1 of dianping subword is 4.2. f1 of average incre is 0.05. f1 of average linear is 0.35. f1 of average procrustes is -0.47. f1 of average subword is 0.53. f1 of average incre is 0.38. f1 of average linear is -0.1. f1 of average procrutes is 0.57. f1 of average subword is 2.53. f1 of median incre is 0.3. f1 of median linear is 0.15. f1 of median procrustes is -0.6. f1 of median subword is 0.45. f1 of median incre is 0.25. f1 of median linear is -0.5. f1 of median procrutes is 0.45. f1 of median subword is 2.8.
table 1 shows performance for non-scratchpad models are taken from he et al. bleu of mixer de-en is 21.83. bleu of ac + ll de-en is 28.53. bleu of npmt de-en is 29.96. bleu of npmt en-vi is 28.07. bleu of stanford nmt en-vi is 26.1. bleu of transformer (6 layer) de-en is 32.86. bleu of transformer (6 layer) es-en is 38.57. bleu of layer-coord (14 layer) de-en is 35.07. bleu of layer-coord (14 layer) es-en is 40.5. bleu of scratchpad (3 layer) de-en is 35.08. bleu of scratchpad (3 layer) es-en is 40.92. bleu of scratchpad (3 layer) en-vi is 29.59.
table 5 shows conll-2012 shared task systems evaluations based on maximum spans, mina spans, and head words. conll of fernandes max is 60.6 (1). conll of fernandes mina is  62.2 (1). conll of fernandes head is  63.9. lea of fernandes max is  53.3. lea of fernandes mina is  55.1. lea of fernandes head is  57.0. conll of martschat max is 57.7 (2). conll of martschat mina is  59.7 (2). conll of martschat head is  61.0. lea of martschat max is  50.0. lea of martschat mina is  52.4. lea of martschat head is  53.9. conll of bjorkelund max is 57.4 (3). conll of bjorkelund mina is  58.9 (3). conll of bjorkelund head is  60.7. lea of bjorkelund max is  50.0. lea of bjorkelund mina is  51.6. lea of bjorkelund head is  53.6. conll of chang max is 56.1 (4). conll of chang mina is  58.0 (4). conll of chang head is  59.6. lea of chang max is  48.5. lea of chang mina is  50.7. lea of chang head is  52.5. conll of chen max is 54.5 (5). conll of chen mina is  56.5 (5). conll of chen head is  58.2. lea of chen max is  46.2. lea of chen mina is  48.6. lea of chen head is  50.4. conll of chunyuang max is 54.2 (6). conll of chunyuang mina is  56.1 (6). conll of chunyuang head is  57.9. lea of chunyuang max is  45.8. lea of chunyuang mina is  48.1. lea of chunyuang head is  50.2. conll of shou max is 53.0 (7). conll of shou mina is  54.8 (8). conll of shou head is  56.5. lea of shou max is  44.0. lea of shou mina is  46.1. lea of shou head is  48.1. conll of yuan max is 52.9 (8). conll of yuan mina is  54.9 (7). conll of yuan head is  56.7. lea of yuan max is  44.8. lea of yuan mina is  47.0. lea of yuan head is  48.9. conll of xu max is 52.6 (9). conll of xu mina is  53.9 (9). conll of xu head is  55.2. lea of xu max is  46.8. lea of xu mina is  48.4. lea of xu head is  50.0. conll of uryupina max is 50.0 (10). conll of uryupina mina is  51.0 (11). conll of uryupina head is  52.4. lea of uryupina max is  41.2. lea of uryupina mina is  42.3. lea of uryupina head is  43.7. conll of songyang max is  49.4 (11). conll of songyang mina is  51.3 (10). conll of songyang head is  52.9. lea of songyang max is  41.3. lea of songyang mina is  43.5. lea of songyang head is  45.3.
table 3 shows combined withinand cross-document event coreference results on the ecb+ test set. r of cluster+lemma r is 76.5. p of cluster+lemma p is 79.9. f1 of cluster+lemma f1 is 78.1. r of cluster+lemma r is 71.7. p of cluster+lemma p is 85. f1 of cluster+lemma f1 is 77.8. r of cluster+lemma r is 75.5. p of cluster+lemma p is 71.7. f1 of cluster+lemma f1 is 73.6. f1 of cluster+lemma f1 is 76.5. r of cv (cybulska and vossen 2015a),71,75,73,71,78,74,-,-,64,73
table 2 shows system performance for the multi-class classification settings (i.e., f1 for 4-way and accuracy for pdtb-lin and pdtb-ji as in the prior work). accuracy of (lin et al. 2009) accuracy is 40.2. accuracy of (lin et al. 2009) accuracy is -0.1. accuracy of (ji and eisenstein 2015b) accuracy is 44.59. accuracy of (qin et al. 2016) accuracy is 43.81. accuracy of (qin et al. 2016) accuracy is 45.04. f1 of (liu and li 2016b) f1 is 46.29. accuracy of (qin et al. 2017) accuracy is 44.65. accuracy of (qin et al. 2017) accuracy is 46.23. f1 of (lan et al. 2017) f1 is 47.8. f1 of (dai and huang 2018) f1 is 51.84. f1 of (lei et al. 2018) f1 is 47.15. f1 of (guo et al. 2018) f1 is 47.59. f1 of (bai and zhao 2018) f1 is 51.06. accuracy of (bai and zhao 2018) accuracy is 45.73. accuracy of (bai and zhao 2018) accuracy is 48.22. f1 of this work f1 is 53. accuracy of this work accuracy is 46.48. accuracy of this work accuracy is 49.95.
table 3 shows system performance with different combinations of l1, l2 and l3 (i.e., f1 for 4-way and accuracy for pdtb-lin and pdtb-ji as in prior work). f1 of l1 + l2 + l3 f1 is 53. accuracy of l1 + l2 + l3 accuracy is 46.48. accuracy of l1 + l2 + l3 accuracy is 49.95. f1 of l1 + l2 f1 is 52.18. accuracy of l1 + l2 accuracy is 46.08. accuracy of l1 + l2 accuracy is 49.28. f1 of l1 + l3 f1 is 52.31. accuracy of l1 + l3 accuracy is 45.3. accuracy of l1 + l3 accuracy is 49.57. f1 of l2 + l3 f1 is 52.57. accuracy of l2 + l3 accuracy is 44.91. accuracy of l2 + l3 accuracy is 49.86. f1 of l1 f1 is 51.11. accuracy of l1 accuracy is 46.21. accuracy of l1 accuracy is 49.09. f1 of l2 f1 is 50.38. accuracy of l2 accuracy is 45.56. accuracy of l2 accuracy is 47.83. f1 of l3 f1 is 52.52. accuracy of l3 accuracy is 45.69. accuracy of l3 accuracy is 49.09. f1 of none f1 is 51.62. accuracy of none accuracy is 45.82. accuracy of none accuracy is 48.6.
table 3 shows performance and number of items per feature. r of cond. rule is . r of cond. hybr. is 0.02. mae of cond. rule is 2.08. mae of cond. hybr. is 1.50. r of modal rule is -0.01. r of modal hybr. is 0.21. mae of modal rule is 1.37. mae of modal hybr. is 1.08. r of negation rule is 0.45. r of negation hybr. is 0.22. mae of negation rule is 2.26. mae of negation hybr. is 2.40. r of question rule is -0.22. r of question hybr. is 0.29. mae of question rule is 2.35. mae of question hybr. is 1.25.
table 3 shows why-qa performances p@1 of oh et al.(2013) p@1 is 41.8. map of oh et al.(2013) map is 41.0. p@1 of sharp et al.(2016) p@1 is 33.2. map of sharp et al.(2016) map is 32.2. p@1 of tan et al.(2016) p@1 is 34.0. map of tan et al.(2016) map is 33.4. p@1 of oh et al.(2017) p@1 is 47.6. map of oh et al.(2017) map is 45.0. p@1 of base p@1 is 51.4. map of base map is 50.4. p@1 of base+addtr p@1 is 52.0. map of base+addtr map is 49.3. p@1 of base+cans p@1 is 51.8. map of base+cans map is 50.3. p@1 of base+cenc p@1 is 52.4. map of base+cenc map is 51.5. p@1 of base+enc p@1 is 52.2. map of base+enc map is 50.6. p@1 of bert p@1 is 51.2. map of bert map is 50.8. p@1 of bert+addtr p@1 is 51.8. map of bert+addtr map is 51.0. p@1 of bert+fop p@1 is 53.4. map of bert+fop map is 51.2. p@1 of bert+frv p@1 is 53.2. map of bert+frv map is 50.9. p@1 of ours (op) p@1 is 54.8. map of ours (op) map is 52.4. p@1 of ours (rp) p@1 is 53.4. map of ours (rp) map is 51.5. p@1 of ours (rv) p@1 is 54.6. map of ours (rv) map is 51.8. p@1 of oracle p@1 is 60.4. map of oracle map is 60.4.
table 2 shows experimental results of applying data augmentation to reading comprehension models on the squad 2.0 dataset. em of bna em is 59.7. f1 of bna f1 is 62.7. em of bna + unansq em is 61.0. f1 of bna + unansq f1 is 63.5. em of docqa em is 61.9. f1 of docqa f1 is 64.5. em of docqa + unansq em is 62.4. f1 of docqa + unansq f1 is 65.3. em of bertbase em is 74.3. f1 of bertbase f1 is 77.4. em of bertbase + unansq em is 76.4. f1 of bertbase + unansq f1 is 79.3. em of bert large em is 78.2. f1 of bert large f1 is 81.3. em of bert large+ unansq em is 80.0. f1 of bert large+ unansq f1 is 83.0.
table 3 shows human evaluation results. unans of tfidf unans is 0.96. rela of tfidf rela is  1.52. read of tfidf read is  2.98. unans of seq2seq unans is 0.62. rela of seq2seq rela is 2.88. read of seq2seq read is 2.39. unans of pair2seq unans is 0.65. rela of pair2seq rela is 2.95. read of pair2seq read is 2.61. unans of human unans is 0.95. rela of human rela is 2.96. read of human read is 3.
table 2 shows comparison with baseline methods trained on different backbone models (second column). bleu of trans.-base mt06 is 44.59. bleu of trans.-base mt02 is 44.82. bleu of trans.-base mt03 is 43.68. bleu of trans.-base mt04 is 45.6. bleu of trans.-base mt05 is 44.57. bleu of trans.-base mt08 is 35.07. bleu of trans.-base mt06 is 45.11. bleu of trans.-base mt02 is 45.95. bleu of trans.-base mt03 is 44.68. bleu of trans.-base mt04 is 45.99. bleu of trans.-base mt05 is 45.32. bleu of trans.-base mt08 is 35.84. bleu of trans.-base mt06 is 44.96. bleu of trans.-base mt02 is 46.03. bleu of trans.-base mt03 is 44.81. bleu of trans.-base mt04 is 46.01. bleu of trans.-base mt05 is 45.69. bleu of trans.-base mt08 is 35.32. bleu of trans.-base mt06 is 45.47. bleu of trans.-base mt02 is 46.31. bleu of trans.-base mt03 is 45.3. bleu of trans.-base mt04 is 46.45. bleu of trans.-base mt05 is 45.62. bleu of trans.-base mt08 is 35.66. bleu of rnmt lex. mt06 is 43.57. bleu of rnmt lex. mt02 is 44.82. bleu of rnmt lex. mt03 is 42.95. bleu of rnmt lex. mt04 is 45.05. bleu of rnmt lex. mt05 is 43.45. bleu of rnmt lex. mt08 is 34.85. bleu of rnmt feat. mt06 is 44.44. bleu of rnmt feat. mt02 is 46.1. bleu of rnmt feat. mt03 is 44.07. bleu of rnmt feat. mt04 is 45.61. bleu of rnmt feat. mt05 is 44.06. bleu of rnmt feat. mt08 is 34.94. bleu of trans.-base feat. mt06 is 45.37. bleu of trans.-base feat. mt02 is 46.16. bleu of trans.-base feat. mt03 is 44.41. bleu of trans.-base feat. mt04 is 46.32. bleu of trans.-base feat. mt05 is 45.3. bleu of trans.-base feat. mt08 is 35.85. bleu of trans.-base lex mt06 is 45.78. bleu of trans.-base lex mt02 is 45.96. bleu of trans.-base lex mt03 is 45.51. bleu of trans.-base lex mt04 is 46.49. bleu of trans.-base lex mt05 is 45.73. bleu of trans.-base lex mt08 is 36.08. bleu of trans.-base mt06 is 46.39. bleu of trans.-base mt02 is 47.31. bleu of trans.-base mt03 is 47.1. bleu of trans.-base mt04 is 47.81. bleu of trans.-base mt05 is 45.69. bleu of trans.-base mt08 is 36.43. bleu of trans.-base mt06 is 46.95. bleu of trans.-base mt02 is 47.06. bleu of trans.-base mt03 is 46.48. bleu of trans.-base mt04 is 47.39. bleu of trans.-base mt05 is 46.58. bleu of trans.-base mt08 is 37.38. bleu of trans.-base mt06 is 47.74. bleu of trans.-base mt02 is 48.13. bleu of trans.-base mt03 is 47.83. bleu of trans.-base mt04 is 49.13. bleu of trans.-base mt05 is 49.04. bleu of trans.-base mt08 is 38.61.
table 3 shows results on nist chinese-english translation. bleu of trans.-base mt06 is 44.59. bleu of trans.-base mt02 is 44.82. bleu of trans.-base mt03 is 43.68. bleu of trans.-base mt04 is 45.60. bleu of trans.-base mt05 is 44.57. bleu of trans.-base mt08 is 35.07. bleu of  trans.-base mt06 is 46.95. bleu of  trans.-base mt02 is 47.06. bleu of  trans.-base mt03 is 46.48. bleu of  trans.-base mt04 is 47.39. bleu of  trans.-base mt05 is 46.58. bleu of  trans.-base mt08 is 37.38.
table 2 shows experiment results on ace 2005. p of maxent p is 74.5. r of maxent  r is 59.1. f1 of maxent  f1 is 65.9. p of combined-psl p is 75.3. r of combined-psl  r is 64.4. f1 of combined-psl  f1 is 69.4. p of dmcnn p is 75.6. r of dmcnn  r is 63.6. f1 of dmcnn  f1 is 69.1. p of bi-rnn p is 66. r of bi-rnn  r is 73. f1 of bi-rnn  f1 is 69.3. p of nc-cnn p is  -. r of nc-cnn  r is  -. f1 of nc-cnn  f1 is 71.3. p of sa-ann-arg (+arguments) p is 78. r of sa-ann-arg (+arguments)  r is 66.3. f1 of sa-ann-arg (+arguments)  f1 is 71.7. p of gmlatt (+multi-lingual) p is 78.9. r of gmlatt (+multi-lingual)  r is 66.9. f1 of gmlatt (+multi-lingual)  f1 is 72.4. p of gcn-ed (+syntactic) p is 77.9. r of gcn-ed (+syntactic)  r is 68.8. f1 of gcn-ed (+syntactic)  f1 is 73.1. p of hbtngma (+document) p is 77.9. r of hbtngma (+document)  r is 69.1. f1 of hbtngma (+document)  f1 is 73.3. p of elmo p is 75.6. r of elmo  r is 62.3. f1 of elmo  f1 is 68.3. p of ∆concat w2v p is 71.8. r of ∆concat w2v  r is 70.8. f1 of ∆concat w2v  f1 is 71.3. p of ∆concat elm o p is 73.7. r of ∆concat elm o  r is 71.9. f1 of ∆concat elm o  f1 is 72.8. p of ∆w2v p is 74. r of ∆w2v  r is 70.5. f1 of ∆w2v  f1 is 72.2. p of ∆elm o p is 76.3. r of ∆elm o  r is 71.9. f1 of ∆elm o  f1 is 74.
table 1 shows the accuracy scores of predicting the label with unlexicalized features, leakage features, and advanced graph-based features and the relative improvements. accuracy of majority  snli is 33.7. accuracy of majority  multinli matched is 35.6. accuracy of majority multinli mismatched is 36.5. accuracy of majority  quoraqp is 50. accuracy of majority  msrp is 66.5. accuracy of majority sick nli is 56.7. accuracy of majority sick sts is 50.3. accuracy of majority  bytedance is 68.59. accuracy of unlexicalized  snli is 47.7. accuracy of unlexicalized  multinli matched is 44.9. accuracy of unlexicalized multinli mismatched is 45.5. accuracy of unlexicalized  quoraqp is 68.2. accuracy of unlexicalized  msrp is 73.9. accuracy of unlexicalized sick nli is 70.1. accuracy of unlexicalized sick sts is 70.2. accuracy of unlexicalized  bytedance is 75.23. accuracy of lstm  snli is 77.6. accuracy of lstm  multinli matched is 66.9. accuracy of lstm multinli mismatched is 66.9. accuracy of lstm  quoraqp is 82.58. accuracy of lstm  msrp is 70.6. accuracy of lstm sick nli is 71.3. accuracy of lstm sick sts is 70.2. accuracy of lstm  bytedance is 86.45. accuracy of leakage  snli is 36.6. accuracy of leakage  multinli matched is 32.1. accuracy of leakage multinli mismatched is 31.1. accuracy of leakage  quoraqp is 79.63. accuracy of leakage  msrp is 66.7. accuracy of leakage sick nli is 56.7. accuracy of leakage sick sts is 55.5. accuracy of leakage  bytedance is 78.24. accuracy of advanced  snli is 39.1. accuracy of advanced  multinli matched is 32.7. accuracy of advanced multinli mismatched is 33.8. accuracy of advanced  quoraqp is 80.47. accuracy of advanced  msrp is 67.9. accuracy of advanced sick nli is 57.5. accuracy of advanced sick sts is 56.3. accuracy of advanced  bytedance is 85.73. accuracy of leakage vs majority  snli is 8.61. accuracy of leakage vs majority  multinli matched is -9.83. accuracy of leakage vs majority multinli mismatched is -14.79. accuracy of leakage vs majority  quoraqp is 59.26. accuracy of leakage vs majority  msrp is 0.3. accuracy of leakage vs majority sick nli is 0. accuracy of leakage vs majority sick sts is 10.34. accuracy of leakage vs majority  bytedance is 14.07. accuracy of advanced vs majority  snli is 16.02. accuracy of advanced vs majority  multinli matched is -8.15. accuracy of advanced vs majority multinli mismatched is -7.4. accuracy of advanced vs majority  quoraqp is 60.94. accuracy of advanced vs majority  msrp is 2.11. accuracy of advanced vs majority sick nli is 1.41. accuracy of advanced vs majority sick sts is 11.93. accuracy of advanced vs majority  bytedance is 24.99.
table 4 shows evaluation results with the synthetic dataset, msrp and sicksts dataset. accuracy of biased model synthetic is 89.46. accuracy of biased model msrp is 51.94. accuracy of biased model sick sts is 64.95. accuracy of debiased model synthetic is 92.62. accuracy of debiased model msrp is 56.77. accuracy of debiased model sick sts is 66.05.
table 1 shows results on squad v1.1. em of drqa em is 69.5. f1 of drqa f1 is 78.8. w/s of drqa w/s is 4.8k. em of bert-large em is 84.1. f1 of bert-large f1 is 90.9. w/s of bert-large w/s is 51. em of lstm+sa em is 49. f1 of lstm+sa f1 is 59.8. em of lstm+sa+elmo em is 52.7. f1 of lstm+sa+elmo f1 is 62.7. em of denspi (dense only) em is 73.6. f1 of denspi (dense only) f1 is 81.7. w/s of denspi (dense only) w/s is 28.7m. em of + linear layer em is 66.9. f1 of + linear layer f1 is 76.4. em of + indep. encoders em is 65.4. f1 of + indep. encoders f1 is 75.1. em of - coherency scalar em is 71.5. f1 of - coherency scalar f1 is 81.5.
table 2 shows glue test set results scored using the glue evaluation server. accuracy of bilstm+elmo+attn 1 8.5k is 36. accuracy of bilstm+elmo+attn 1 67k is 90.4. accuracy of bilstm+elmo+attn 1 3.7k is  84.9/77.9. accuracy of bilstm+elmo+attn 1 7k is  75.1/73.3. accuracy of bilstm+elmo+attn 1 364k is  64.8/84.7. accuracy of bilstm+elmo+attn 1 393k is  76.4/76.1. accuracy of bilstm+elmo+attn 1 108k is  -. accuracy of bilstm+elmo+attn 1 2.5k is 56.8. accuracy of bilstm+elmo+attn 1 634 is 65.1. accuracy of bilstm+elmo+attn 1 - is 26.5. accuracy of bilstm+elmo+attn 1 - is 70.5. accuracy of singletask pretrain transformer 2 8.5k is 45.4. accuracy of singletask pretrain transformer 2 67k is 91.3. accuracy of singletask pretrain transformer 2 3.7k is  82.3/75.7. accuracy of singletask pretrain transformer 2 7k is  82.0/80.0. accuracy of singletask pretrain transformer 2 364k is  70.3/88.5. accuracy of singletask pretrain transformer 2 393k is  82.1/81.4. accuracy of singletask pretrain transformer 2 108k is  -. accuracy of singletask pretrain transformer 2 2.5k is 56. accuracy of singletask pretrain transformer 2 634 is 53.4. accuracy of singletask pretrain transformer 2 - is 29.8. accuracy of singletask pretrain transformer 2 - is 72.8. accuracy of gpt on stilts 3 8.5k is 47.2. accuracy of gpt on stilts 3 67k is 93.1. accuracy of gpt on stilts 3 3.7k is  87.7/83.7. accuracy of gpt on stilts 3 7k is  85.3/84.8. accuracy of gpt on stilts 3 364k is  70.1/88.1. accuracy of gpt on stilts 3 393k is  80.8/80.6. accuracy of gpt on stilts 3 108k is  -. accuracy of gpt on stilts 3 2.5k is 69.1. accuracy of gpt on stilts 3 634 is 65.1. accuracy of gpt on stilts 3 - is 29.4. accuracy of gpt on stilts 3 - is 76.9. accuracy of bert large 4 8.5k is 60.5. accuracy of bert large 4 67k is 94.9. accuracy of bert large 4 3.7k is  89.3/85.4. accuracy of bert large 4 7k is  87.6/86.5. accuracy of bert large 4 364k is  72.1/89.3. accuracy of bert large 4 393k is  86.7/85.9. accuracy of bert large 4 108k is 92.7. accuracy of bert large 4 2.5k is 70.1. accuracy of bert large 4 634 is 65.1. accuracy of bert large 4 - is 39.6. accuracy of bert large 4 - is 80.5. accuracy of mt-dnnno-fine-tune 8.5k is 58.9. accuracy of mt-dnnno-fine-tune 67k is 94.6. accuracy of mt-dnnno-fine-tune 3.7k is  90.1/86.4. accuracy of mt-dnnno-fine-tune 7k is  89.5/88.8. accuracy of mt-dnnno-fine-tune 364k is  72.7/89.6. accuracy of mt-dnnno-fine-tune 393k is  86.5/85.8. accuracy of mt-dnnno-fine-tune 108k is 93.1. accuracy of mt-dnnno-fine-tune 2.5k is 79.1. accuracy of mt-dnnno-fine-tune 634 is 65.1. accuracy of mt-dnnno-fine-tune - is 39.4. accuracy of mt-dnnno-fine-tune - is 81.7. accuracy of mt-dnn 8.5k is 62.5. accuracy of mt-dnn 67k is 95.6. accuracy of mt-dnn 3.7k is  91.1/88.2. accuracy of mt-dnn 7k is  89.5/88.8. accuracy of mt-dnn 364k is  72.7/89.6. accuracy of mt-dnn 393k is  86.7/86.0. accuracy of mt-dnn 108k is 93.1. accuracy of mt-dnn 2.5k is 81.4. accuracy of mt-dnn 634 is 65.1. accuracy of mt-dnn - is 40.3. accuracy of mt-dnn - is 82.7. accuracy of human performance 8.5k is 66.4. accuracy of human performance 67k is 97.8. accuracy of human performance 3.7k is  86.3/80.8. accuracy of human performance 7k is  92.7/92.6. accuracy of human performance 364k is  59.5/80.4. accuracy of human performance 393k is  92.0/92.8. accuracy of human performance 108k is 91.2. accuracy of human performance 2.5k is 93.6. accuracy of human performance 634 is 95.9. accuracy of human performance - is  -. accuracy of human performance - is 87.1.
table 8 shows performance stratified by question difficulty on the development set. accuracy of easy (483)  cd-seq2seq is 35.1. accuracy of easy (483)  syntaxsql-con is 38.9. accuracy of medium (441)  cd-seq2seq is 7. accuracy of medium (441)  syntaxsql-con is 7.3. accuracy of hard (145)  cd-seq2seq is 2.8. accuracy of hard (145)  syntaxsql-con is 1.4. accuracy of extra hard (134)  cd-seq2seq is 0.8. accuracy of extra hard (134)  syntaxsql-con is 0.7.
table 4 shows sembleu and smatch scores for several recent models. sembleu of  lyu  sembleu is 52.7. smatch of  lyu  smatch is  73.7†. sembleu of  guo  sembleu is 50.1. smatch of  guo  smatch is  68.7†. sembleu of  gros  sembleu is 50. smatch of  gros  smatch is  70.2†. sembleu of  jamr  sembleu is 46.8. smatch of  jamr  smatch is 67. sembleu of  camr  sembleu is 37.2. smatch of  camr  smatch is 62. sembleu of  lyu  sembleu is 54.3. smatch of  lyu  smatch is  74.4†. sembleu of  van nood  sembleu is 49.2. smatch of  van nood  smatch is  71.0†. sembleu of  guo  sembleu is 52. smatch of  guo  smatch is  69.8†. sembleu of  gros  sembleu is 50.7. smatch of  gros  smatch is  71.0†. sembleu of  jamr  sembleu is 47. smatch of  jamr  smatch is 66. sembleu of  camr  sembleu is 36.6. smatch of  camr  smatch is 61.
table 1 shows comparison between training on 1 million examples from a backtranslated english-english corpus (en-en) and the original bitext corpus (en-cs) sampling 1 million and 2 million sentence pairs (the latter equalizes the amount of english text with the en-en setting). r of lstm-sp (20k) en-en is 66.7. r of lstm-sp (20k) en-cs (1m) is 65.7. r of lstm-sp (20k) en-cs (2m) is 66.6. r of sp (20k) en-en is 68.3. r of sp (20k) en-cs (1m) is 68.6. r of sp (20k) en-cs (2m) is 70. r of word en-en is 66. r of word en-cs (1m) is 63.8. r of word en-cs (2m) is 65.9. r of trigram en-en is 69.2. r of trigram en-cs (1m) is 68.6. r of trigram en-cs (2m) is 69.9.
table 2 shows experimental results with constituent tree-lstms. accuracy of contree (le and zuidema, 2015)  sst-5 root is  49.9. accuracy of contree (le and zuidema, 2015)  sst-5 phrase is  -. accuracy of contree (le and zuidema, 2015)  sst-2 root is  88.0. accuracy of contree (le and zuidema, 2015)  sst-2 phrase is  -. accuracy of contree (tai et al., 2015)  sst-5 root is  51.0. accuracy of contree (tai et al., 2015)  sst-5 phrase is  -. accuracy of contree (tai et al., 2015)  sst-2 root is  88.0. accuracy of contree (tai et al., 2015)  sst-2 phrase is  -. accuracy of contree (zhu et al., 2015)  sst-5 root is  50.1. accuracy of contree (zhu et al., 2015)  sst-5 phrase is  -. accuracy of contree (zhu et al., 2015)  sst-2 root is  -. accuracy of contree (zhu et al., 2015)  sst-2 phrase is  -. accuracy of contree (li et al., 2015)  sst-5 root is  50.4. accuracy of contree (li et al., 2015)  sst-5 phrase is  83.4. accuracy of contree (li et al., 2015)  sst-2 root is  86.7. accuracy of contree (li et al., 2015)  sst-2 phrase is  -. accuracy of contree (our implementation)  sst-5 root is  51.5. accuracy of contree (our implementation)  sst-5 phrase is  82.8. accuracy of contree (our implementation)  sst-2 root is  89.4. accuracy of contree (our implementation)  sst-2 phrase is  86.9. accuracy of contree + wg  sst-5 root is  51.7. accuracy of contree + wg  sst-5 phrase is  83.0. accuracy of contree + wg  sst-2 root is  89.7. accuracy of contree + wg  sst-2 phrase is  88.9. accuracy of contree + lvg4  sst-5 root is  52.2. accuracy of contree + lvg4  sst-5 phrase is  83.2. accuracy of contree + lvg4  sst-2 root is  89.8. accuracy of contree + lvg4  sst-2 phrase is  89.1. accuracy of contree + lveg  sst-5 root is  52.9. accuracy of contree + lveg  sst-5 phrase is  83.4. accuracy of contree + lveg  sst-2 root is  89.8. accuracy of contree + lveg  sst-2 phrase is  89.5.
table 3 shows experimental results with elmo. accuracy of bcn(p) root is 54.7. accuracy of bcn(p)  phrase is  -. accuracy of bcn(p)  root is  -. accuracy of bcn(p)  phrase is  -. accuracy of bcn(o) root is 54.6. accuracy of bcn(o)  phrase is 83.3. accuracy of bcn(o)  root is 91.4. accuracy of bcn(o)  phrase is 88.8. accuracy of bcn+wg root is 55.1. accuracy of bcn+wg  phrase is 83.5. accuracy of bcn+wg  root is 91.5. accuracy of bcn+wg  phrase is 90.5. accuracy of bcn+lvg4 root is 55.5. accuracy of bcn+lvg4  phrase is 83.5. accuracy of bcn+lvg4  root is 91.7. accuracy of bcn+lvg4  phrase is 91.3. accuracy of bcn+lveg root is 56. accuracy of bcn+lveg  phrase is 83.5. accuracy of bcn+lveg  root is 92.1. accuracy of bcn+lveg  phrase is 91.6.
table 6 shows comparison of results using large and small corpora. error of bcn+elmo  yahoo binary is 10.24. error of ulmfit  yahoo binary is 12.2. error of ulmfit adapted  yahoo binary is 8.52. error of bertbase  yahoo binary is 8.42. error of bcn+elmo [100mb]  yahoo binary is 10.32. error of ulmfit adapted [100mb]  yahoo binary is 8.57. error of bertbase [100mb]  yahoo binary is 14.26.
table 3 shows experimental results on quora test set. acc.(%) of bimpm (wang et al., 2017) acc.(%) is 88.2. acc.(%) of pt-decattn-word (tomar et al., 2017) acc.(%) is 87.5. acc.(%) of pt-decattn-char (tomar et al., 2017) acc.(%) is 88.4. acc.(%) of diin (gong et al., 2018) acc.(%) is 89.1. acc.(%) of mwan (tan et al., 2018) acc.(%) is 89.1. acc.(%) of csran (tay et al., 2018a) acc.(%) is 89.2. acc.(%) of san (liu et al., 2018) acc.(%) is 89.4. acc.(%) of re2 (ours) acc.(%) is  89.2±0.2.
table 1 shows semi-supervised classification results on the snli dataset. accuracy of lstm(a) 28k is 57.9. accuracy of lstm(a) 59k is 62.5. accuracy of lstm(a) 120k is 65.9. accuracy of cnn(b) 28k is 58.7. accuracy of cnn(b) 59k is 62.7. accuracy of cnn(b) 120k is 65.6. accuracy of lstm-ae(a) 28k is 59.9. accuracy of lstm-ae(a) 59k is 64.6. accuracy of lstm-ae(a) 120k is 68.5. accuracy of lstm-adae(a) 28k is 62.5. accuracy of lstm-adae(a) 59k is 66.8. accuracy of lstm-adae(a) 120k is 70.9. accuracy of deconv-ae(b) 28k is 62.1. accuracy of deconv-ae(b) 59k is 65.5. accuracy of deconv-ae(b) 120k is 68.7. accuracy of lstm-vae(b) 28k is 64.7. accuracy of lstm-vae(b) 59k is 67.5. accuracy of lstm-vae(b) 120k is 71.1. accuracy of deconv-vae(b) 28k is 67.2. accuracy of deconv-vae(b) 59k is 69.3. accuracy of deconv-vae(b) 120k is 72.2. accuracy of lstm-vmf-vae (ours) 28k is 65.6. accuracy of lstm-vmf-vae (ours) 59k is 68.7. accuracy of lstm-vmf-vae (ours) 120k is 71.1. accuracy of cs-lvm (ours) 28k is 68.4. accuracy of cs-lvm (ours) 59k is 73.5. accuracy of cs-lvm (ours) 120k is 76.9.
table 1 shows automatic evaluations of quality and novelty for generations of atomic commonsense. bleu-2 of 9enc9dec (sap et al., 2019) bleu-2 is 10.01. n/t sro6 of 9enc9dec (sap et al., 2019) n/t sro6 is 100. n/t o of 9enc9dec (sap et al., 2019) n/t o is 8.61. n/u o of 9enc9dec (sap et al., 2019) n/u o is 40.77. bleu-2 of nearestneighbor (sap et al., 2019) bleu-2 is 6.61. bleu-2 of event2(in)volun (sap et al., 2019) bleu-2 is 9.67. n/t sro6 of event2(in)volun (sap et al., 2019) n/t sro6 is 100. n/t o of event2(in)volun (sap et al., 2019) n/t o is 9.52. n/u o of event2(in)volun (sap et al., 2019) n/u o is 45.06. bleu-2 of event2personx/y (sap et al., 2019) bleu-2 is 9.24. n/t sro6 of event2personx/y (sap et al., 2019) n/t sro6 is 100. n/t o of event2personx/y (sap et al., 2019) n/t o is 8.22. n/u o of event2personx/y (sap et al., 2019) n/u o is 41.66. bleu-2 of event2pre/post (sap et al., 2019) bleu-2 is 9.93. n/t sro6 of event2pre/post (sap et al., 2019) n/t sro6 is 100. n/t o of event2pre/post (sap et al., 2019) n/t o is 7.38. n/u o of event2pre/post (sap et al., 2019) n/u o is 41.99. ppl5 of comet (- pretrain) ppl5 is 15.42. bleu-2 of comet (- pretrain) bleu-2 is 13.88. n/t sro6 of comet (- pretrain) n/t sro6 is 100. n/t o of comet (- pretrain) n/t o is 7.25. n/u o of comet (- pretrain) n/u o is 45.71. ppl5 of comet ppl5 is 11.14. bleu-2 of comet bleu-2 is 15.1. n/t sro6 of comet n/t sro6 is 100. n/t o of comet n/t o is 9.71. n/u o of comet n/u o is 51.2.
table 4 shows effect of amount of training data on automatic evaluation of commonsense generations ppl of 1% train ppl is 23.81. bleu-2 of 1% train bleu-2 is 5.08. n/t o of 1% train n/t o is 7.24. n/u o of 1% train n/u o is 49.36. ppl of 10% train ppl is 13.74. bleu-2 of 10% train bleu-2 is 12.72. n/t o of 10% train n/t o is 9.54. n/u o of 10% train n/u o is 58.34. ppl of 50% train ppl is 11.82. bleu-2 of 50% train bleu-2 is 13.97. n/t o of 50% train n/t o is 9.32. n/u o of 50% train n/u o is 50.37. ppl of full (- pretrain) ppl is 15.18. bleu-2 of full (- pretrain) bleu-2 is 13.22. n/t o of full (- pretrain) n/t o is 7.14. n/u o of full (- pretrain) n/u o is 44.55. ppl of full train ppl is 11.13. bleu-2 of full train bleu-2 is 14.34. n/t o of full train n/t o is 9.51. n/u o of full train n/u o is 50.05.
table 6 shows conceptnet generation results score of lstm - s score is 60.83. n/t sro of lstm - s n/t sro is 86.25. n/t o of lstm - s n/t o is 7.83. human of lstm - s human is 63.86. score of ckbg (saito et al., 2018) score is 57.17. n/t sro of ckbg (saito et al., 2018) n/t sro is 86.25. n/t o of ckbg (saito et al., 2018) n/t o is 8.67. human of ckbg (saito et al., 2018) human is 53.95. ppl of comet (- pretrain) ppl is 8.05. score of comet (- pretrain) score is 89.25. n/t sro of comet (- pretrain) n/t sro is 36.17. n/t o of comet (- pretrain) n/t o is 6. human of comet (- pretrain) human is 83.49. ppl of comet - reltok ppl is 4.39. score of comet - reltok score is 95.17. n/t sro of comet - reltok n/t sro is 56.42. n/t o of comet - reltok n/t o is 2.62. human of comet - reltok human is 92.11. ppl of comet ppl is 4.32. score of comet score is 95.25. n/t sro of comet n/t sro is 59.25. n/t o of comet n/t o is 3.75. human of comet human is 91.69.
table 1 shows results on wsc273 and its subsets. accuracy of bert_wiki wsc273 is 0.619. accuracy of bert_wiki non-assoc. is 0.597. accuracy of bert_wiki assoc. is 0.757. accuracy of bert_wiki unswitched is 0.573. accuracy of bert_wiki switched is 0.603. accuracy of bert_wiki consist. is 0.389. accuracy of bert_wiki wnli is 0.712. accuracy of bert_wiki_wscr wsc273 is 0.725. accuracy of bert_wiki_wscr non-assoc. is 0.72. accuracy of bert_wiki_wscr assoc. is 0.757. accuracy of bert_wiki_wscr unswitched is 0.732. accuracy of bert_wiki_wscr switched is 0.71. accuracy of bert_wiki_wscr consist. is 0.55. accuracy of bert_wiki_wscr wnli is 0.747. accuracy of bert wsc273 is 0.619. accuracy of bert non-assoc. is 0.602. accuracy of bert assoc. is 0.73. accuracy of bert unswitched is 0.595. accuracy of bert switched is 0.573. accuracy of bert consist. is 0.458. accuracy of bert wnli is 0.658. accuracy of bert_wscr wsc273 is 0.714. accuracy of bert_wscr non-assoc. is 0.699. accuracy of bert_wscr assoc. is 0.811. accuracy of bert_wscr unswitched is 0.695. accuracy of bert_wscr switched is 0.702. accuracy of bert_wscr consist. is 0.55. accuracy of bert_wscr wnli is 0.719. accuracy of bert-base wsc273 is 0.564. accuracy of bert-base non-assoc. is 0.551. accuracy of bert-base assoc. is 0.649. accuracy of bert-base unswitched is 0.527. accuracy of bert-base switched is 0.565. accuracy of bert-base consist. is 0.443. accuracy of bert-base wnli is 0.63. accuracy of bert-base_wscr wsc273 is 0.623. accuracy of bert-base_wscr non-assoc. is 0.606. accuracy of bert-base_wscr assoc. is 0.73. accuracy of bert-base_wscr unswitched is 0.611. accuracy of bert-base_wscr switched is 0.634. accuracy of bert-base_wscr consist. is 0.443. accuracy of bert-base_wscr wnli is 0.705. accuracy of gpt wsc273 is 0.553. accuracy of gpt non-assoc. is 0.525. accuracy of gpt assoc. is 0.73. accuracy of gpt unswitched is 0.595. accuracy of gpt switched is 0.519. accuracy of gpt consist. is 0.466. accuracy of gpt wnli is –. accuracy of gpt_wscr wsc273 is 0.674. accuracy of gpt_wscr non-assoc. is 0.653. accuracy of gpt_wscr assoc. is 0.811. accuracy of gpt_wscr unswitched is 0.664. accuracy of gpt_wscr switched is 0.58. accuracy of gpt_wscr consist. is 0.641. accuracy of gpt_wscr wnli is –. accuracy of bert_wiki_wscr_no_pairs wsc273 is 0.663. accuracy of bert_wiki_wscr_no_pairs non-assoc. is 0.669. accuracy of bert_wiki_wscr_no_pairs assoc. is 0.622. accuracy of bert_wiki_wscr_no_pairs unswitched is 0.672. accuracy of bert_wiki_wscr_no_pairs switched is 0.641. accuracy of bert_wiki_wscr_no_pairs consist. is 0.511. accuracy of bert_wiki_wscr_no_pairs wnli is –. accuracy of bert_wiki_wscr_pairs wsc273 is 0.703. accuracy of bert_wiki_wscr_pairs non-assoc. is 0.695. accuracy of bert_wiki_wscr_pairs assoc. is 0.757. accuracy of bert_wiki_wscr_pairs unswitched is 0.718. accuracy of bert_wiki_wscr_pairs switched is 0.71. accuracy of bert_wiki_wscr_pairs consist. is 0.565. accuracy of bert_wiki_wscr_pairs wnli is –. accuracy of lm ensemble wsc273 is 0.637. accuracy of lm ensemble non-assoc. is 0.606. accuracy of lm ensemble assoc. is 0.838. accuracy of lm ensemble unswitched is 0.634. accuracy of lm ensemble switched is 0.534. accuracy of lm ensemble consist. is 0.443. accuracy of lm ensemble wnli is –. accuracy of knowledge hunter wsc273 is 0.571. accuracy of knowledge hunter non-assoc. is 0.583. accuracy of knowledge hunter assoc. is 0.5. accuracy of knowledge hunter unswitched is 0.588. accuracy of knowledge hunter switched is 0.588. accuracy of knowledge hunter consist. is 0.901. accuracy of knowledge hunter wnli is –.
table 4 shows comparison between our graph2seq model and baseline models for the topic of entertainment. coherence of seq2seq-t (qin et al., 2018) coherence is 5.38. informativeness of seq2seq-t (qin et al., 2018) informativeness is 3.7. fluency of seq2seq-t (qin et al., 2018) fluency is 8.22. total of seq2seq-t (qin et al., 2018) total is 5.77. coherence of seq2seq-c (qin et al., 2018) coherence is 4.87. informativeness of seq2seq-c (qin et al., 2018) informativeness is 3.72. fluency of seq2seq-c (qin et al., 2018) fluency is 8.53. total of seq2seq-c (qin et al., 2018) total is 5.71. coherence of seq2seq-tc (qin et al., 2018) coherence is 3.28. informativeness of seq2seq-tc (qin et al., 2018) informativeness is 4.02. fluency of seq2seq-tc (qin et al., 2018) fluency is 8.68. total of seq2seq-tc (qin et al., 2018) total is 5.33. coherence of self-attention-b (chen et al., 2018) coherence is 6.72. informativeness of self-attention-b (chen et al., 2018) informativeness is 5.05. fluency of self-attention-b (chen et al., 2018) fluency is 8.27. total of self-attention-b (chen et al., 2018) total is 6.68. coherence of self-attention-k (chen et al., 2018) coherence is 6.62. informativeness of self-attention-k (chen et al., 2018) informativeness is 4.73. fluency of self-attention-k (chen et al., 2018) fluency is 8.28. total of self-attention-k (chen et al., 2018) total is 6.54. coherence of hierarchical-attention (yang et al., 2016) coherence is 1.38. informativeness of hierarchical-attention (yang et al., 2016) informativeness is 2.97. fluency of hierarchical-attention (yang et al., 2016) fluency is 8.65. total of hierarchical-attention (yang et al., 2016) total is 4.33. coherence of graph2seq (proposed) coherence is 8.23. informativeness of graph2seq (proposed) informativeness is 5.27. fluency of graph2seq (proposed) fluency is 8.08. total of graph2seq (proposed) total is 7.19.
table 2 shows bleu, meteor and rouge-l scores on the test set for hindi and chinese question generation. bleu-1 of transformer bleu-1 is 28.414. bleu-2 of transformer bleu-2 is 18.493. bleu-3 of transformer bleu-3 is 12.356. bleu-4 of transformer bleu-4 is 8.644. meteor of transformer meteor is 23.803. rouge-l of transformer rouge-l is 29.893. bleu-1 of transformer+pretraining bleu-1 is 41.059. bleu-2 of transformer+pretraining bleu-2 is 29.294. bleu-3 of transformer+pretraining bleu-3 is 21.403. bleu-4 of transformer+pretraining bleu-4 is 16.047. meteor of transformer+pretraining meteor is 28.159. rouge-l of transformer+pretraining rouge-l is 39.395. bleu-1 of clqg bleu-1 is 41.034. bleu-2 of clqg bleu-2 is 29.792. bleu-3 of clqg bleu-3 is 22.038. bleu-4 of clqg bleu-4 is 16.598. meteor of clqg meteor is 27.581. rouge-l of clqg rouge-l is 39.852. bleu-1 of clqg+parallel bleu-1 is 42.281. bleu-2 of clqg+parallel bleu-2 is 32.074. bleu-3 of clqg+parallel bleu-3 is 25.182. bleu-4 of clqg+parallel bleu-4 is 20.242. meteor of clqg+parallel meteor is 29.143. rouge-l of clqg+parallel rouge-l is 40.643. bleu-1 of transformer bleu-1 is 25.52. bleu-2 of transformer bleu-2 is 9.22. bleu-3 of transformer bleu-3 is 5.14. bleu-4 of transformer bleu-4 is 3.25. meteor of transformer meteor is 7.64. rouge-l of transformer rouge-l is 27.4. bleu-1 of transformer+pretraining bleu-1 is 30.38. bleu-2 of transformer+pretraining bleu-2 is 14.01. bleu-3 of transformer+pretraining bleu-3 is 8.37. bleu-4 of transformer+pretraining bleu-4 is 5.18. meteor of transformer+pretraining meteor is 10.46. rouge-l of transformer+pretraining rouge-l is 32.71. bleu-1 of clqg bleu-1 is 30.69. bleu-2 of clqg bleu-2 is 14.51. bleu-3 of clqg bleu-3 is 8.82. bleu-4 of clqg bleu-4 is 5.39. meteor of clqg meteor is 10.44. rouge-l of clqg rouge-l is 31.82. bleu-1 of clqg+parallel bleu-1 is 30.3. bleu-2 of clqg+parallel bleu-2 is 13.93. bleu-3 of clqg+parallel bleu-3 is 8.43. bleu-4 of clqg+parallel bleu-4 is 5.51. meteor of clqg+parallel meteor is 10.26. rouge-l of clqg+parallel rouge-l is 31.58.
table 4 shows automatic evaluation results for classification accuracy and bleu with human reference. acc of crossaligned acc is 74.7. bleu of crossaligned bleu is 9.06. acc of crossaligned acc is 75.1. bleu of crossaligned bleu is 1.9. acc of multidecoder acc is 50.6. bleu of multidecoder bleu is 14.54. acc of multidecoder acc is 69.9. bleu of multidecoder bleu is 9.07. acc of styleembedding acc is 8.4. bleu of styleembedding bleu is 21.06. acc of styleembedding acc is 38.2. bleu of styleembedding bleu is 15.07. acc of templatebased acc is 81.2. bleu of templatebased bleu is 22.57. acc of templatebased acc is 64.3. bleu of templatebased bleu is 34.79. acc of deleteonly acc is 86. bleu of deleteonly bleu is 14.64. acc of deleteonly acc is 47. bleu of deleteonly bleu is 33. acc of del-ret-gen acc is 88.6. bleu of del-ret-gen bleu is 15.96. acc of del-ret-gen acc is 51. bleu of del-ret-gen bleu is 30.09. acc of backtranslate acc is 94.6. bleu of backtranslate bleu is 2.46. acc of backtranslate acc is 76.7. bleu of backtranslate bleu is 1.04. acc of unpairedrl acc is 57.5. bleu of unpairedrl bleu is 18.81. acc of unpairedrl acc is 56.3. bleu of unpairedrl bleu is 15.93. acc of unsupermt acc is 97.8. bleu of unsupermt bleu is 22.75. acc of unsupermt acc is 72.4. bleu of unsupermt bleu is 33.95. acc of human acc is 74.7. acc of human acc is 43.2. acc of point-then-operate acc is 91.5. bleu of point-then-operate bleu is 29.86. acc of point-then-operate acc is 40.2. bleu of point-then-operate bleu is 41.86.
table 3 shows test accuracy on cqa v1.0. accuracy (%) of rc (talmor et al., 2019) accuracy (%) is 47.7. accuracy (%) of gpt (talmor et al., 2019) accuracy (%) is 54.8. accuracy (%) of cos-e-open-ended accuracy (%) is 60.2. accuracy (%) of cage-reasoning accuracy (%) is 64.7. accuracy (%) of human (talmor et al., 2019) accuracy (%) is 95.3.
table 4 shows oracle results on cqa dev-random-split using different variants of cos-e for both training and validation. accuracy (%) of cos-e-selected w/o ques accuracy (%) is 53. accuracy (%) of cos-e-limited-open-ended accuracy (%) is 67.6. accuracy (%) of cos-e-selected accuracy (%) is 70. accuracy (%) of cos-e-open-ended w/o ques accuracy (%) is 84.5. accuracy (%) of cos-e-open-ended* accuracy (%) is 89.8.
table 6 shows m-bert’s pos accuracy on the code-switched hindi/english dataset from bhat et al. accuracy of m-bert corrected is 86.59. accuracy of m-bert transliterated is 50.41. accuracy of ball and garrette (2018) transliterated is 77.4. accuracy of m-bert corrected is 90.56. accuracy of m-bert transliterated is 85.64. accuracy of bhat et al. (2018) transliterated is 90.53.
table 7 shows complaint prediction results using the original data set and distantly supervised data. acc of most frequent class acc is 64.2. f1 of most frequent class f1 is 39.1. auc of most frequent class auc is 0.5. acc of lr-all features – original data acc is 80.5. f1 of lr-all features – original data f1 is 78. auc of lr-all features – original data auc is 0.873. acc of dist. supervision + pooling acc is 77.2. f1 of dist. supervision + pooling f1 is 75.7. auc of dist. supervision + pooling auc is 0.853. acc of dist. supervision + easyadapt acc is 81.2. f1 of dist. supervision + easyadapt f1 is 79. auc of dist. supervision + easyadapt auc is 0.885.
table 1 shows experimental results of abstractive summarization on gigaword test set with rouge metric. r1 of lead-75c r1 is 23.69. r2 of lead-75c r2 is 7.93. rl of lead-75c rl is 21.5. r1 of lead-8 r1 is 21.3. r2 of lead-8 r2 is 7.34. rl of lead-8 rl is 19.94. r1 of schumann (2018) r1 is 22.19. r2 of schumann (2018) r2 is 4.56. rl of schumann (2018) rl is 19.88. r1 of wang and lee (2018) r1 is 27.09. r2 of wang and lee (2018) r2 is 9.86. rl of wang and lee (2018) rl is 24.97. r1 of contextual match r1 is 26.48. r2 of contextual match r2 is 10.05. rl of contextual match rl is 24.41. r1 of cao et al. (2018) r1 is 37.04. r2 of cao et al. (2018) r2 is 19.03. rl of cao et al. (2018) rl is 34.46. r1 of seq2seq r1 is 33.5. r2 of seq2seq r2 is 15.85. rl of seq2seq rl is 31.44. r1 of contextual oracle r1 is 37.03. r2 of contextual oracle r2 is 15.46. rl of contextual oracle rl is 33.23.
table 3 shows performance comparison between our model and three baselines on four frequent attributes. p (%) of bilstm p (%) is 95.08. r (%) of bilstm r (%) is 96.81. f1 (%) of bilstm f1 (%) is 95.94. p (%) of bilstm-crf p (%) is 95.45. r (%) of bilstm-crf r (%) is 97.17. f1 (%) of bilstm-crf f1 (%) is 96.3. p (%) of opentag p (%) is 95.18. r (%) of opentag r (%) is 97.55. f1 (%) of opentag f1 (%) is 96.35. p (%) of our model-110k p (%) is 97.21. r (%) of our model-110k r (%) is 96.68. f1 (%) of our model-110k f1 (%) is 96.94. p (%) of our model-650k p (%) is 96.94. r (%) of our model-650k r (%) is 97.14. f1 (%) of our model-650k f1 (%) is 97.04. p (%) of bilstm p (%) is 78.26. r (%) of bilstm r (%) is 78.54. f1 (%) of bilstm f1 (%) is 78.4. p (%) of bilstm-crf p (%) is 77.15. r (%) of bilstm-crf r (%) is 78.12. f1 (%) of bilstm-crf f1 (%) is 77.63. p (%) of opentag p (%) is 78.69. r (%) of opentag r (%) is 78.62. f1 (%) of opentag f1 (%) is 78.65. p (%) of our model-110k p (%) is 82.76. r (%) of our model-110k r (%) is 83.57. f1 (%) of our model-110k f1 (%) is 83.16. p (%) of ourmodel-650k p (%) is 83.3. r (%) of ourmodel-650k r (%) is 82.94. f1 (%) of ourmodel-650k f1 (%) is 83.12. p (%) of bilstm p (%) is 68.08. r (%) of bilstm r (%) is 68. f1 (%) of bilstm f1 (%) is 68.04. p (%) of bilstm-crf p (%) is 68.13. r (%) of bilstm-crf r (%) is 67.46. f1 (%) of bilstm-crf f1 (%) is 67.79. p (%) of opentag p (%) is 71.19. r (%) of opentag r (%) is 70.5. f1 (%) of opentag f1 (%) is 70.84. p (%) of our model-110k p (%) is 75.11. r (%) of our model-110k r (%) is 72.61. f1 (%) of our model-110k f1 (%) is 73.84. p (%) of our model-650k p (%) is 77.55. r (%) of our model-650k r (%) is 72.8. f1 (%) of our model-650k f1 (%) is 75.1. p (%) of bilstm p (%) is 82.74. r (%) of bilstm r (%) is 78.4. f1 (%) of bilstm f1 (%) is 80.51. p (%) of bilstm-crf p (%) is 81.57. r (%) of bilstm-crf r (%) is 79.94. f1 (%) of bilstm-crf f1 (%) is 80.75. p (%) of opentag p (%) is 82.74. r (%) of opentag r (%) is 80.63. f1 (%) of opentag f1 (%) is 81.67. p (%) of our model-110k p (%) is 84.11. r (%) of our model-110k r (%) is 80.8. f1 (%) of our model-110k f1 (%) is 82.42. p (%) of our model-650k p (%) is 88.11. r (%) of our model-650k r (%) is 81.79. f1 (%) of our model-650k f1 (%) is 84.83.
table 1 shows result comparison of the proposed method with the state-of-art baseline methods. p of st-blstm p is 57.7. r of st-blstm r is 56.8. f1 of st-blstm f1 is 57.3. p of st-blstm p is 52.9. r of st-blstm r is 49.4. f1 of st-blstm f1 is 51.1. p of st-blstm p is 71.65. r of st-blstm r is 72.19. f1 of st-blstm f1 is 71.91. p of st-cnn p is 63.8. r of st-cnn r is 65.8. f1 of st-cnn f1 is 67.1. p of st-cnn p is 39.7. r of st-cnn r is 42.7. f1 of st-cnn f1 is 42. p of st-cnn p is 66.88. r of st-cnn r is 73.81. f1 of st-cnn f1 is 70.17. p of crnn (huynh et al., 2016) p is 61.1. r of crnn (huynh et al., 2016) r is 62.4. f1 of crnn (huynh et al., 2016) f1 is 64.9. p of crnn (huynh et al., 2016) p is 49.5. r of crnn (huynh et al., 2016) r is 46.9. f1 of crnn (huynh et al., 2016) f1 is 48.2. p of crnn (huynh et al., 2016) p is 71. r of crnn (huynh et al., 2016) r is 77.3. f1 of crnn (huynh et al., 2016) f1 is 75.5. p of rcnn (huynh et al., 2016) p is 57.6. r of rcnn (huynh et al., 2016) r is 58.7. f1 of rcnn (huynh et al., 2016) f1 is 63.6. p of rcnn (huynh et al., 2016) p is 42.4. r of rcnn (huynh et al., 2016) r is 44.9. f1 of rcnn (huynh et al., 2016) f1 is 43.6. p of rcnn (huynh et al., 2016) p is 73.5. r of rcnn (huynh et al., 2016) r is 72. f1 of rcnn (huynh et al., 2016) f1 is 74. p of mt-blstm (chowdhury et al., 2018) p is 65.57. r of mt-blstm (chowdhury et al., 2018) r is 61.02. f1 of mt-blstm (chowdhury et al., 2018) f1 is 63.19. p of mt-blstm (chowdhury et al., 2018) p is 60.5. r of mt-blstm (chowdhury et al., 2018) r is 55.16. f1 of mt-blstm (chowdhury et al., 2018) f1 is 57.62. p of mt-blstm (chowdhury et al., 2018) p is 72.72. r of mt-blstm (chowdhury et al., 2018) r is 75.49. f1 of mt-blstm (chowdhury et al., 2018) f1 is 74. p of mt-atten-blstm (chowdhury et al., 2018) p is 62.26. r of mt-atten-blstm (chowdhury et al., 2018) r is 69.62. f1 of mt-atten-blstm (chowdhury et al., 2018) f1 is 65.73. p of mt-atten-blstm (chowdhury et al., 2018) p is 56.63. r of mt-atten-blstm (chowdhury et al., 2018) r is 60. f1 of mt-atten-blstm (chowdhury et al., 2018) f1 is 58.27. p of mt-atten-blstm (chowdhury et al., 2018) p is 75.08. r of mt-atten-blstm (chowdhury et al., 2018) r is 81.06. f1 of mt-atten-blstm (chowdhury et al., 2018) f1 is 77.95. p of proposed model p is 68.78. r of proposed model r is 70.81. f1 of proposed model f1 is 69.69. p of proposed model p is 64.33. r of proposed model r is 67.03. f1 of proposed model f1 is 65.58. p of proposed model p is 81.97. r of proposed model r is 82.61. f1 of proposed model f1 is 82.18.
table 2 shows aspect and opinion term extraction performance of different approaches. f1 of dp (qiu et al. 2011) aspect is 38.72. f1 of dp (qiu et al. 2011) opinion is 65.94. f1 of dp (qiu et al. 2011) aspect is 19.19. f1 of dp (qiu et al. 2011) opinion is 55.29. f1 of dp (qiu et al. 2011) aspect is 27.32. f1 of dp (qiu et al. 2011) opinion is 46.31. f1 of ihs rd (chernyshevich 2014) aspect is 79.62. f1 of ihs rd (chernyshevich 2014) opinion is  -. f1 of ihs rd (chernyshevich 2014) aspect is 74.55. f1 of ihs rd (chernyshevich 2014) opinion is  -. f1 of ihs rd (chernyshevich 2014) aspect is  -. f1 of ihs rd (chernyshevich 2014) opinion is  -. f1 of dlirec (toh and wang 2014) aspect is 84.01. f1 of dlirec (toh and wang 2014) opinion is  -. f1 of dlirec (toh and wang 2014) aspect is 73.78. f1 of dlirec (toh and wang 2014) opinion is  -. f1 of dlirec (toh and wang 2014) aspect is  -. f1 of dlirec (toh and wang 2014) opinion is  -. f1 of elixa (vicente et al. 2017) aspect is  -. f1 of elixa (vicente et al. 2017) opinion is  -. f1 of elixa (vicente et al. 2017) aspect is  -. f1 of elixa (vicente et al. 2017) opinion is  -. f1 of elixa (vicente et al. 2017) aspect is 70.04. f1 of elixa (vicente et al. 2017) opinion is  -. f1 of wdemb (yin et al. 2016) aspect is 84.31. f1 of wdemb (yin et al. 2016) opinion is  -. f1 of wdemb (yin et al. 2016) aspect is 74.68. f1 of wdemb (yin et al. 2016) opinion is  -. f1 of wdemb (yin et al. 2016) aspect is 69.12. f1 of wdemb (yin et al. 2016) opinion is  -. f1 of wdemb* (yin et al. 2016) aspect is 84.97. f1 of wdemb* (yin et al. 2016) opinion is  -. f1 of wdemb* (yin et al. 2016) aspect is 75.16. f1 of wdemb* (yin et al. 2016) opinion is  -. f1 of wdemb* (yin et al. 2016) aspect is 69.73. f1 of wdemb* (yin et al. 2016) opinion is  -. f1 of rncrf (wang et al. 2016) aspect is 82.23. f1 of rncrf (wang et al. 2016) opinion is 83.93. f1 of rncrf (wang et al. 2016) aspect is 75.28. f1 of rncrf (wang et al. 2016) opinion is 77.03. f1 of rncrf (wang et al. 2016) aspect is 65.39. f1 of rncrf (wang et al. 2016) opinion is 63.75. f1 of cmla (wang et al. 2017) aspect is 82.46. f1 of cmla (wang et al. 2017) opinion is 84.67. f1 of cmla (wang et al. 2017) aspect is 73.63. f1 of cmla (wang et al. 2017) opinion is 79.16. f1 of cmla (wang et al. 2017) aspect is 68.22. f1 of cmla (wang et al. 2017) opinion is 70.5. f1 of ncrf-ae (zhang et al. 2017) aspect is 83.28. f1 of ncrf-ae (zhang et al. 2017) opinion is 85.23. f1 of ncrf-ae (zhang et al. 2017) aspect is 74.32. f1 of ncrf-ae (zhang et al. 2017) opinion is 75.44. f1 of ncrf-ae (zhang et al. 2017) aspect is 65.33. f1 of ncrf-ae (zhang et al. 2017) opinion is 70.16. f1 of hast (li et al. 2018) aspect is 85.61. f1 of hast (li et al. 2018) opinion is  -. f1 of hast (li et al. 2018) aspect is 79.52. f1 of hast (li et al. 2018) opinion is  -. f1 of hast (li et al. 2018) aspect is 69.77. f1 of hast (li et al. 2018) opinion is  -. f1 of de-cnn (xu et al. 2018) aspect is 85.2. f1 of de-cnn (xu et al. 2018) opinion is  -. f1 of de-cnn (xu et al. 2018) aspect is 81.59. f1 of de-cnn (xu et al. 2018) opinion is  -. f1 of de-cnn (xu et al. 2018) aspect is 68.28. f1 of de-cnn (xu et al. 2018) opinion is  -. f1 of mined rules aspect is 70.82. f1 of mined rules opinion is 79.6. f1 of mined rules aspect is 67.67. f1 of mined rules opinion is 76.1. f1 of mined rules aspect is 57.67. f1 of mined rules opinion is 64.29. f1 of rinante (no rule) aspect is 84.06. f1 of rinante (no rule) opinion is 84.59. f1 of rinante (no rule) aspect is 73.47. f1 of rinante (no rule) opinion is 75.41. f1 of rinante (no rule) aspect is 66.17. f1 of rinante (no rule) opinion is 68.16. f1 of rinante-shared-alt aspect is 86.76. f1 of rinante-shared-alt opinion is 86.05. f1 of rinante-shared-alt aspect is 77.92. f1 of rinante-shared-alt opinion is 79.2. f1 of rinante-shared-alt aspect is 67.47. f1 of rinante-shared-alt opinion is 71.41. f1 of rinante-shared-pre aspect is 85.09. f1 of rinante-shared-pre opinion is 85.63. f1 of rinante-shared-pre aspect is 79.16. f1 of rinante-shared-pre opinion is 79.03. f1 of rinante-shared-pre aspect is 68.15. f1 of rinante-shared-pre opinion is 70.44. f1 of rinante-double-alt aspect is 85.8. f1 of rinante-double-alt opinion is 86.34. f1 of rinante-double-alt aspect is 78.59. f1 of rinante-double-alt opinion is 78.94. f1 of rinante-double-alt aspect is 67.42. f1 of rinante-double-alt opinion is 70.53. f1 of rinante-double-pre aspect is 86.45. f1 of rinante-double-pre opinion is 85.67. f1 of rinante-double-pre aspect is 80.16. f1 of rinante-double-pre opinion is 81.96. f1 of rinante-double-pre aspect is 69.9. f1 of rinante-double-pre opinion is 72.09.
table 1 shows results on conll 2003 and ontonotes 5.0 f1-score of ma and hovy (2016) conll is 91.21. f1-score of ma and hovy (2016) ontonotes is  -. f1-score of lample et al. (2016) conll is 90.94. f1-score of lample et al. (2016) ontonotes is  -. f1-score of liu et al. (2018) conll is 91.24±0.12. f1-score of liu et al. (2018) ontonotes is  -. f1-score of devlin et al. (2018) conll is 92.8. f1-score of devlin et al. (2018) ontonotes is  -. f1-score of chiu and nichols (2016) conll is 91.62±0.33. f1-score of chiu and nichols (2016) ontonotes is  86.28±0.26. f1-score of ghaddar and langlais ’18 conll is 91.73±0.10. f1-score of ghaddar and langlais ’18 ontonotes is  87.95±0.13. f1-score of peters et al. (2018) conll is 92.22±0.10. f1-score of peters et al. (2018) ontonotes is  89.04±0.27. f1-score of clark et al. (2018) conll is 92.6 ±0.1. f1-score of clark et al. (2018) ontonotes is  88.8±0.1. f1-score of akbik et al. (2018) conll is 93.09±0.12. f1-score of akbik et al. (2018) ontonotes is 89.71. f1-score of hscrf conll is 92.54±0.11. f1-score of hscrf ontonotes is  89.38±0.11. f1-score of hscrf + concat conll is 92.52±0.09. f1-score of hscrf + concat ontonotes is  89.73±0.19. f1-score of hscrf + gazemb conll is 92.63±0.08. f1-score of hscrf + gazemb ontonotes is  89.77±0.20. f1-score of hscrf + softdict conll is 92.75±0.18. f1-score of hscrf + softdict ontonotes is  89.94±0.16.
table 6 shows comparison of different sentence encoders in d-ndmv. dda of bag-of-tags method  dda is 74.1. dda of anchored words method  dda is 75.1. dda of lstm  dda is 75.9. dda of attention-based lstm  dda is 75.5. dda of bi-lstm  dda is 74.2.
table 1 shows nested ner results (f1) for ace-2004, ace-2005, genia and cnec 1.0 (czech) corpora. f1 of (finkel and manning, 2009)** genia is 70.3. f1 of (lu and roth, 2015)** ace-2004 is 62.8. f1 of (lu and roth, 2015)** ace-2005 is 62.5. f1 of (lu and roth, 2015)** genia is 70.3. f1 of (muis and lu, 2017)** ace-2004 is 64.5. f1 of (muis and lu, 2017)** ace-2005 is 63.1. f1 of (muis and lu, 2017)** genia is 70.8. f1 of (katiyar and cardie, 2018) ace-2004 is 72.7. f1 of (katiyar and cardie, 2018) ace-2005 is 70.5. f1 of (katiyar and cardie, 2018) genia is 73.6. f1 of (ju et al., 2018)* ace-2005 is 72.2. f1 of (ju et al., 2018)* genia is 74.7. f1 of (wang and lu, 2018) ace-2004 is 75.1. f1 of (wang and lu, 2018) ace-2005 is 74.5. f1 of (wang and lu, 2018) genia is 75.1. f1 of (strakovã¡ et al., 2016) cnec 1.0 is 81.2. f1 of lstm-crf ace-2004 is 72.26. f1 of lstm-crf ace-2005 is 71.62. f1 of lstm-crf genia is 76.23. f1 of lstm-crf cnec 1.0 is 80.28. f1 of lstm-crf+elmo ace-2004 is 78.72. f1 of lstm-crf+elmo ace-2005 is 78.36. f1 of lstm-crf+elmo genia is 75.94. f1 of lstm-crf+bert ace-2004 is 81.48. f1 of lstm-crf+bert ace-2005 is 79.95. f1 of lstm-crf+bert genia is 77.8. f1 of lstm-crf+bert cnec 1.0 is 85.67. f1 of lstm-crf+flair ace-2004 is 77.65. f1 of lstm-crf+flair ace-2005 is 77.25. f1 of lstm-crf+flair genia is 76.65. f1 of lstm-crf+flair cnec 1.0 is 81.74. f1 of lstm-crf+bert+elmo ace-2004 is 80.07. f1 of lstm-crf+bert+elmo ace-2005 is 80.04. f1 of lstm-crf+bert+elmo genia is 76.29. f1 of lstm-crf+bert+flair ace-2004 is 81.22. f1 of lstm-crf+bert+flair ace-2005 is 80.82. f1 of lstm-crf+bert+flair genia is 77.91. f1 of lstm-crf+bert+flair cnec 1.0 is 85.7. f1 of lstm-crf+elmo+bert+flair ace-2004 is 80.19. f1 of lstm-crf+elmo+bert+flair ace-2005 is 79.85. f1 of lstm-crf+elmo+bert+flair genia is 76.56. f1 of seq2seq ace-2004 is 77.08. f1 of seq2seq ace-2005 is 75.36. f1 of seq2seq genia is 76.44. f1 of seq2seq cnec 1.0 is 82.96. f1 of seq2seq+elmo ace-2004 is 81.94. f1 of seq2seq+elmo ace-2005 is 81.95. f1 of seq2seq+elmo genia is 77.33. f1 of seq2seq+bert ace-2004 is 84.33. f1 of seq2seq+bert ace-2005 is 83.42. f1 of seq2seq+bert genia is 78.2. f1 of seq2seq+bert cnec 1.0 is 86.73. f1 of seq2seq+flair ace-2004 is 81.38. f1 of seq2seq+flair ace-2005 is 79.83. f1 of seq2seq+flair genia is 76.63. f1 of seq2seq+flair cnec 1.0 is 83.55. f1 of seq2seq+bert+elmo ace-2004 is 84.32. f1 of seq2seq+bert+elmo ace-2005 is 82.15. f1 of seq2seq+bert+elmo genia is 77.77. f1 of seq2seq+bert+flair ace-2004 is 84.4. f1 of seq2seq+bert+flair ace-2005 is 84.33. f1 of seq2seq+bert+flair genia is 78.31. f1 of seq2seq+bert+flair cnec 1.0 is 86.88. f1 of seq2seq+elmo+bert+flair ace-2004 is 84.07. f1 of seq2seq+elmo+bert+flair ace-2005 is 83.41. f1 of seq2seq+elmo+bert+flair genia is 78.01.
table 2 shows results on the ptb and spmrl test sets. uas of s-s uas is 93.6. las of s-s las is 91.74. f1 of s-s f1 is 90.14. uas of s-mtl uas is 93.84. las of s-mtl las is 91.83. f1 of s-mtl f1 is 90.32. uas of d-mtl-aux uas is 94.05. las of d-mtl-aux las is 92.01. f1 of d-mtl-aux f1 is 90.39. uas of d-mtl uas is 93.96. las of d-mtl las is 91.9. f1 of d-mtl f1 is 89.81. uas of s-s uas is 86.2. las of s-s las is 81.7. f1 of s-s f1 is 89.54. uas of s-mtl uas is 87.42. las of s-mtl las is 81.71. f1 of s-mtl f1 is 90.86. uas of d-mtl-aux uas is 87.19. las of d-mtl-aux las is 81.73. f1 of d-mtl-aux f1 is 91.12. uas of d-mtl uas is 87.09. las of d-mtl las is 81.77. f1 of d-mtl f1 is 90.76. uas of s-s uas is 89.13. las of s-s las is 85.03. f1 of s-s f1 is 80.68. uas of s-mtl uas is 89.54. las of s-mtl las is 84.89. f1 of s-mtl f1 is 81.34. uas of d-mtl-aux uas is 89.52. las of d-mtl-aux las is 84.97. f1 of d-mtl-aux f1 is 81.33. uas of d-mtl uas is 89.45. las of d-mtl las is 85.07. f1 of d-mtl f1 is 81.19. uas of s-s uas is 91.24. las of s-s las is 88.76. f1 of s-s f1 is 84.19. uas of s-mtl uas is 91.54. las of s-mtl las is 88.75. f1 of s-mtl f1 is 84.46. uas of d-mtl-aux uas is 91.58. las of d-mtl-aux las is 88.8. f1 of d-mtl-aux f1 is 84.38. uas of d-mtl uas is 91.45. las of d-mtl las is 88.67. f1 of d-mtl f1 is 84.28. uas of s-s uas is 82.74. las of s-s las is 75.08. f1 of s-s f1 is 88.85. uas of s-mtl uas is 83.42. las of s-mtl las is 74.91. f1 of s-mtl f1 is 91.91. uas of d-mtl-aux uas is 83.9. las of d-mtl-aux las is 75.89. f1 of d-mtl-aux f1 is 91.83. uas of d-mtl uas is 82.6. las of d-mtl las is 73.73. f1 of d-mtl f1 is 91.1. uas of s-s uas is 88.24. las of s-s las is 84.54. f1 of s-s f1 is 90.42. uas of s-mtl uas is 88.69. las of s-mtl las is 84.54. f1 of s-mtl f1 is 90.76. uas of d-mtl-aux uas is 88.99. las of d-mtl-aux las is 84.95. f1 of d-mtl-aux f1 is 90.69. uas of d-mtl uas is 88.89. las of d-mtl las is 84.89. f1 of d-mtl f1 is 90.93. uas of s-s uas is 86.47. las of s-s las is 84.12. f1 of s-s f1 is 83.33. uas of s-mtl uas is 86.78. las of s-mtl las is 84.39. f1 of s-mtl f1 is 83.51. uas of d - mtl - aux uas is 87. las of d - mtl - aux las is 84.6. f1 of d - mtl - aux f1 is 83.39. uas of d-mtl uas is 86.64. las of d-mtl las is 84.34. f1 of d-mtl f1 is 83.08. uas of s-s uas is 91.17. las of s-s las is 85.64. f1 of s-s f1 is 92.59. uas of s-mtl uas is 91.58. las of s-mtl las is 85.04. f1 of s-mtl f1 is 93.17. uas of d-mtl-aux uas is 91.37. las of d-mtl-aux las is 85.2. f1 of d-mtl-aux f1 is 93.36. uas of d-mtl uas is 92. las of d-mtl las is 85.92. f1 of d-mtl f1 is 93.52. uas of s-s uas is 86.49. las of s-s las is 80.6. f1 of s-s f1 is 83.81. uas of s-mtl uas is 87.22. las of s-mtl las is 80.61. f1 of s-mtl f1 is 86.23. uas of d-mtl-aux uas is 87.24. las of d-mtl-aux las is 80.34. f1 of d-mtl-aux f1 is 86.53. uas of d-mtl uas is 87.15. las of d-mtl las is 80.71. f1 of d-mtl f1 is 86.44. uas of s-s uas is 88.36. las of s-s las is 84.13. f1 of s-s f1 is 87.06. uas of s-mtl uas is 88.89. las of s-mtl las is 84.07. f1 of s-mtl f1 is 88.06. uas of d-mtl-aux uas is 88.98. las of d-mtl-aux las is 84.28. f1 of d-mtl-aux f1 is 88.11. uas of d-mtl uas is 88.8. las of d-mtl las is 84.11. f1 of d-mtl f1 is 87.9.
table 1 shows results of automatic and human evaluation: paml vs dialogue+persona shows the our approach can achieve good consistency by using few dialogues instead of conditioning on the persona description, paml vs dialogue+fine-tuning shows the effectiveness of meta-learning approach in personalizing dialogue model. c of human c is 0.33. fluency of human fluency is 3.434. consistency of human consistency is 0.234. ppl of dialogue+persona ppl is 30.42. bleu of dialogue+persona bleu is 1. c of dialogue+persona c is 0.07. fluency of dialogue+persona fluency is 3.053. consistency of dialogue+persona consistency is 0.011. ppl of dialogue ppl is 36.75. bleu of dialogue bleu is 0.64. c of dialogue c is -0.03. ppl of dialogue+fine-tuning ppl is 32.96. bleu of dialogue+fine-tuning bleu is 0.9. c of dialogue+fine-tuning c is 0. fluency of dialogue+fine-tuning fluency is 3.103. consistency of dialogue+fine-tuning consistency is 0.038. ppl of paml ppl is 41.64. bleu of paml bleu is 0.74. c of paml c is 0.2. fluency of paml fluency is 3.185. consistency of paml consistency is 0.197.
table 1 shows comparison with baseline models. em score of pointer lstm em score is 77.85. f1 score of pointer lstm f1 score is 82.73. em score of bi-daf em score is 87.24. f1 score of bi-daf f1 score is 88.67. em score of r-net em score is 88.93. f1 score of r-net f1 score is 90.41. em score of utterance-based ha em score is 88.59. f1 score of utterance-based ha f1 score is 90.12. em score of turn-based ha (proposed) em score is 91.07. f1 score of turn-based ha (proposed) f1 score is 92.39.
table 4 shows competitive results on dbpedia and ag news reported in accuracy (%) without any hyper-parameter tuning. accuracy of bi-blosan(shen et al., 2018) dbpedia(%) is 98.77. accuracy of bi-blosan(shen et al., 2018) ag news (%) is 93.32. accuracy of leam(wang et al., 2018a) dbpedia(%) is 99.02. accuracy of leam(wang et al., 2018a) ag news (%) is 92.45. accuracy of this work dbpedia(%) is 98.9. accuracy of this work ag news (%) is 92.05.
table 2 shows comparison of mtn (base) to state-of-the-art visual dialogue models on the test-std v1.0. ndcg of mtn (base) ndcg is 55.33. ndcg of corefnmn (kottur et al., 2018) ndcg is 54.7. ndcg of mn (das et al., 2017a) ndcg is 47.5. ndcg of hre (das et al., 2017a) ndcg is 45.46. ndcg of lf (das et al., 2017a) ndcg is 45.31.
table 3 shows results of turn-level evaluation. r20@1 of retrieval r20@1 is 0.5196. r20@3 of retrieval r20@3 is 0.7636. r20@5 of retrieval r20@5 is 0.8622. mrr of retrieval mrr is 0.6661. rw@1 of ours-random rw@1 is 0.0005. rw@3 of ours-random rw@3 is 0.0015. rw@5 of ours-random rw@5 is 0.0025. p@1 of ours-random p@1 is 0.0009. cor. of ours-random cor. is 0.4995. r20@1 of ours-random r20@1 is 0.5187. r20@3 of ours-random r20@3 is 0.7619. r20@5 of ours-random r20@5 is 0.8631. mrr of ours-random mrr is 0.665. rw@1 of ours-pmi rw@1 is 0.0585. rw@3 of ours-pmi rw@3 is 0.1351. rw@5 of ours-pmi rw@5 is 0.1872. p@1 of ours-pmi p@1 is 0.0871. cor. of ours-pmi cor. is 0.7974. r20@1 of ours-pmi r20@1 is 0.5441. r20@3 of ours-pmi r20@3 is 0.7839. r20@5 of ours-pmi r20@5 is 0.8716. mrr of ours-pmi mrr is 0.6847. rw@1 of ours-neural rw@1 is 0.0609. rw@3 of ours-neural rw@3 is 0.1324. rw@5 of ours-neural rw@5 is 0.1825. p@1 of ours-neural p@1 is 0.1006. cor. of ours-neural cor. is 0.8075. r20@1 of ours-neural r20@1 is 0.5395. r20@3 of ours-neural r20@3 is 0.7801. r20@5 of ours-neural r20@5 is 0.8799. mrr of ours-neural mrr is 0.6816. rw@1 of ours-kernel rw@1 is 0.0642. rw@3 of ours-kernel rw@3 is 0.1431. rw@5 of ours-kernel rw@5 is 0.1928. p@1 of ours-kernel p@1 is 0.1191. cor. of ours-kernel cor. is 0.8164. r20@1 of ours-kernel r20@1 is 0.5486. r20@3 of ours-kernel r20@3 is 0.7827. r20@5 of ours-kernel r20@5 is 0.8845. mrr of ours-kernel mrr is 0.6914.
table 3 shows comparison with other works on the test sets of raganato et al. f1 of mfs† (most frequent sense) senseval2 is 65.6. f1 of mfs† (most frequent sense) senseval3 is 66. f1 of mfs† (most frequent sense) semeval2007 is 54.5. f1 of mfs† (most frequent sense) semeval2013 is 63.8. f1 of mfs† (most frequent sense) semeval2015 is 67.1. f1 of mfs† (most frequent sense) all is 64.8. f1 of ims† (2010) senseval2 is 70.90. f1 of ims† (2010) senseval3 is 69.30. f1 of ims† (2010) semeval2007 is 61.30. f1 of ims† (2010) semeval2013 is 65.3. f1 of ims† (2010) semeval2015 is 69.5. f1 of ims† (2010) all is 68.4. f1 of ims + embeddings† (2016) senseval2 is 72.2. f1 of ims + embeddings† (2016) senseval3 is 70.4. f1 of ims + embeddings† (2016) semeval2007 is 62.6. f1 of ims + embeddings† (2016) semeval2013 is 65.9. f1 of ims + embeddings† (2016) semeval2015 is 71.5. f1 of ims + embeddings† (2016) all is 69.6. f1 of context2vec k-nn† (2016) senseval2 is 71.80. f1 of context2vec k-nn† (2016) senseval3 is 69.10. f1 of context2vec k-nn† (2016) semeval2007 is 61.30. f1 of context2vec k-nn† (2016) semeval2013 is 65.60. f1 of context2vec k-nn† (2016) semeval2015 is 71.9. f1 of context2vec k-nn† (2016) all is 69. f1 of word2vec k-nn (2016) senseval2 is 67.80. f1 of word2vec k-nn (2016) senseval3 is 62.10. f1 of word2vec k-nn (2016) semeval2007 is 58.50. f1 of word2vec k-nn (2016) semeval2013 is 66.10. f1 of word2vec k-nn (2016) semeval2015 is 66.7. f1 of lstm-lp (label prop.) (2016) senseval2 is 73.80. f1 of lstm-lp (label prop.) (2016) senseval3 is 71.80. f1 of lstm-lp (label prop.) (2016) semeval2007 is 63.50. f1 of lstm-lp (label prop.) (2016) semeval2013 is 69.50. f1 of lstm-lp (label prop.) (2016) semeval2015 is 72.6. f1 of seq2seq (task modelling) (2017b) senseval2 is 70.10. f1 of seq2seq (task modelling) (2017b) senseval3 is 68.50. f1 of seq2seq (task modelling) (2017b) semeval2007 is 63.1*. f1 of seq2seq (task modelling) (2017b) semeval2013 is 66.50. f1 of seq2seq (task modelling) (2017b) semeval2015 is 69.2. f1 of seq2seq (task modelling) (2017b) all is 68.6*. f1 of bilstm (task modelling) (2017b) senseval2 is 72.00. f1 of bilstm (task modelling) (2017b) senseval3 is 69.10. f1 of bilstm (task modelling) (2017b) semeval2007 is 64.8*. f1 of bilstm (task modelling) (2017b) semeval2013 is 66.90. f1 of bilstm (task modelling) (2017b) semeval2015 is 71.5. f1 of bilstm (task modelling) (2017b) all is 69.9*. f1 of elmo k-nn (2018) senseval2 is 71.50. f1 of elmo k-nn (2018) senseval3 is 67.50. f1 of elmo k-nn (2018) semeval2007 is 57.10. f1 of elmo k-nn (2018) semeval2013 is 65.30. f1 of elmo k-nn (2018) semeval2015 is 69.9. f1 of elmo k-nn (2018) all is 67.9. f1 of hcan (hier. co-attention) (2018a) senseval2 is 72.80. f1 of hcan (hier. co-attention) (2018a) senseval3 is 70.30. f1 of hcan (hier. co-attention) (2018a) semeval2007 is -*. f1 of hcan (hier. co-attention) (2018a) semeval2013 is 68.50. f1 of hcan (hier. co-attention) (2018a) semeval2015 is 72.8. f1 of hcan (hier. co-attention) (2018a) all is -*. f1 of bilstm w/vocab. reduction (2018) senseval2 is 72.60. f1 of bilstm w/vocab. reduction (2018) senseval3 is 70.40. f1 of bilstm w/vocab. reduction (2018) semeval2007 is 61.50. f1 of bilstm w/vocab. reduction (2018) semeval2013 is 70.80. f1 of bilstm w/vocab. reduction (2018) semeval2015 is 71.3. f1 of bilstm w/vocab. reduction (2018) all is 70.8. f1 of bert k-nn senseval2 is 76.30. f1 of bert k-nn senseval3 is 73.20. f1 of bert k-nn semeval2007 is 66.20. f1 of bert k-nn semeval2013 is 71.70. f1 of bert k-nn semeval2015 is 74.1. f1 of bert k-nn all is 73.5. f1 of lmms2348 (elmo) senseval2 is 68.10. f1 of lmms2348 (elmo) senseval3 is 64.70. f1 of lmms2348 (elmo) semeval2007 is 53.80. f1 of lmms2348 (elmo) semeval2013 is 66.90. f1 of lmms2348 (elmo) semeval2015 is 66.2. f1 of lmms2348 (elmo) all is . f1 of lmms2348 (bert) senseval2 is 76.30. f1 of lmms2348 (bert) senseval3 is 75.60. f1 of lmms2348 (bert) semeval2007 is 68.10. f1 of lmms2348 (bert) semeval2013 is 75.10. f1 of lmms2348 (bert) semeval2015 is 77. f1 of lmms2348 (bert) all is 75.4.
table 6 shows comparison of w ordctx2sense with the state-of-the-art methods for word sense induction on makesense-2016 and semeval-2010 dataset. f-scr of - f-scr is 47.4. v-msr of - v-msr is 15.5. f-scr of - f-scr is 38.05. v-msr of - v-msr is 10.6. f-scr of - f-scr is 54.49. v-msr of - v-msr is 19.4. f-scr of - f-scr is 47.26. v-msr of - v-msr is 9. f-scr of - f-scr is 57.91. v-msr of - v-msr is 14.4. f-scr of - f-scr is 48.43. v-msr of - v-msr is 6.9. f-scr of 2 f-scr is 64.66. v-msr of 2 v-msr is 28.8. f-scr of 2 f-scr is 57.14. v-msr of 2 v-msr is 7.1. f-scr of 5 f-scr is 58.25. v-msr of 5 v-msr is 34.3. f-scr of 5 f-scr is 44.07. v-msr of 5 v-msr is 14.5. f-scr of 2 f-scr is 58.55. v-msr of 2 v-msr is 6.1. f-scr of 5 f-scr is 46.38. v-msr of 5 v-msr is 11.5. f-scr of 2 f-scr is 63.71. v-msr of 2 v-msr is 22.2. f-scr of 2 f-scr is 59.38. v-msr of 2 v-msr is 6.8. f-scr of 5 f-scr is 59.75. v-msr of 5 v-msr is 32.9. f-scr of 5 f-scr is 46.47. v-msr of 5 v-msr is 13.2. f-scr of 6 f-scr is 59.13. v-msr of 6 v-msr is 34.2. f-scr of 6 f-scr is 44.04. v-msr of 6 v-msr is 14.3. f-scr of 2 f-scr is 65.27. v-msr of 2 v-msr is 24.4. f-scr of 2 f-scr is 59.15. v-msr of 2 v-msr is 6.7. f-scr of 5 f-scr is 62.88. v-msr of 5 v-msr is 35. f-scr of 5 f-scr is 47.34. v-msr of 5 v-msr is 13.7. f-scr of 6 f-scr is 61.43. v-msr of 6 v-msr is 35.3. f-scr of 6 f-scr is 44.7. v-msr of 6 v-msr is 15.
table 2 shows binary hipaa f1 scores of our non-private (top) and private (bottom) de-identification approaches on the i2b2 2014 test set in comparison to non-private the state of the art. f1 (%) of our non-private fasttext f1 (%) is 97.67. f1 (%) of our non-private glove f1 (%) is 97.24. f1 (%) of our non-private glove + casing f1 (%) is 97.62. f1 (%) of dernoncourt et al. (lstm-crf) f1 (%) is 97.85. f1 (%) of liu et al. (ensemble + rules) f1 (%) is 98.27.
table 2 shows comparison of test set results. glue score of bert-base (devlin et al., 2019) glue score is 78.5. glue score of bert-large (devlin et al., 2019) glue score is 80.5. glue score of bert on stilts (phang et al., 2018) glue score is 82. glue score of mt-dnn (liu et al., 2019b) glue score is 82.2. glue score of span-extractive bert on stilts (keskar et al., 2019) glue score is 82.3. glue score of snorkel metal ensemble (hancock et al., 2019) glue score is 83.2. glue score of mt-dnnkd* (liu et al., 2019a) glue score is 83.7. glue score of bert-large + bam (ours) glue score is 82.3.
table 3 shows test results with wpl at different positions. bl r-1 of vgvae w/o wpl bl r-1 is 3.5 24.8. r-2 of vgvae w/o wpl r-2 is 7.3. r-l of vgvae w/o wpl r-l is 29.7. met of vgvae w/o wpl met is 12.6. st of vgvae w/o wpl st is 10.6. bl r-1 of dec. hidden state bl r-1 is 3.6 24.9. r-2 of dec. hidden state r-2 is 7.3. r-l of dec. hidden state r-l is 29.7. met of dec. hidden state met is 12.6. st of dec. hidden state st is 10.5. bl r-1 of enc. emb. bl r-1 is 3.9 26.1. r-2 of enc. emb. r-2 is 7.8. r-l of enc. emb. r-l is 31. met of enc. emb. met is 12.9. st of enc. emb. st is 10.2. bl r-1 of dec. emb. bl r-1 is 4.1 26.3. r-2 of dec. emb. r-2 is 8.1. r-l of dec. emb. r-l is 31.3. met of dec. emb. met is 13.1. st of dec. emb. st is 10.1. bl r-1 of enc. & dec. emb. bl r-1 is 4.5 26.5. r-2 of enc. & dec. emb. r-2 is 8.2. r-l of enc. & dec. emb. r-l is 31.5. met of enc. & dec. emb. met is 13.3. st of enc. & dec. emb. st is 10.
table 7 shows test results when using a single code. bl of lc bl is 13.6. r-1 of lc r-1 is 44.7. r-2 of lc r-2 is 21. r-l of lc r-l is 48.3. met of lc met is 24.8. st of lc st is 6.7. bl of single lc bl is 12.9. r-1 of single lc r-1 is 44.2. r-2 of single lc r-2 is 20.3. r-l of single lc r-l is 47.4. met of single lc met is 24.1. st of single lc st is 6.9.
table 3 shows performance of paraphrase generation. bleu-ref of origin sentence bleu-ref is 30.49. bleu-ori of origin sentence bleu-ori is 100. bleu-ref of vae-svg-eq (supervised) bleu-ref is 22.9. bleu-ori of vae-svg-eq (supervised) bleu-ori is –. bleu-ref of vae (unsupervised) bleu-ref is 9.25. bleu-ori of vae (unsupervised) bleu-ori is 27.23. bleu-ref of cgmh bleu-ref is 18.85. bleu-ori of cgmh bleu-ori is 50.18. bleu-ref of dss-vae bleu-ref is 20.54. bleu-ori of dss-vae bleu-ori is 52.77.
table 2 shows automatic evaluation of generation models. bleu-1 of seq2seq + sentimod bleu-1 is 10.7. bleu-2 of seq2seq + sentimod bleu-2 is 3.2. i-o senticons of seq2seq + sentimod i-o senticons is 0.788. bleu-1 of sic-seq2seq + rb bleu-1 is 19.3. bleu-2 of sic-seq2seq + rb bleu-2 is 6.3. i-o senticons of sic-seq2seq + rb i-o senticons is 0.879. bleu-1 of sic-seq2seq + rm bleu-1 is 19.5. bleu-2 of sic-seq2seq + rm bleu-2 is 6.2. i-o senticons of sic-seq2seq + rm i-o senticons is 0.83. bleu-1 of sic-seq2seq + da bleu-1 is 19.8. bleu-2 of sic-seq2seq + da bleu-2 is 6.7. i-o senticons of sic-seq2seq + da i-o senticons is 0.794.
table 3 shows human evaluation of generation models. coherency of seq2seq + sentimod coherency is 1.5. fluency of seq2seq + sentimod fluency is 2.5. sentiment of seq2seq + sentimod sentiment is 3.68. coherency of sic-seq2seq + rb coherency is 2.65. fluency of sic-seq2seq + rb fluency is 4.75. sentiment of sic-seq2seq + rb sentiment is 4.09. coherency of sic-seq2seq + rm coherency is 2.15. fluency of sic-seq2seq + rm fluency is 4.6. sentiment of sic-seq2seq + rm sentiment is 3.65. coherency of sic-seq2seq + da coherency is 2.2. fluency of sic-seq2seq + da fluency is 4.5. sentiment of sic-seq2seq + da sentiment is 3.71.
table 3 shows comparison with previous models on text simplification in newsela dataset and formality transfer in gyafc dataset. add of source add is 0. keep of source keep is 60.3. del of source del is 0. bleu of source bleu is 21.4. sari of source sari is 2.8. add of source add is 0. keep of source keep is 85.4. del of source del is 0. bleu of source bleu is 49.1. add of source add is 0. keep of source keep is 85.8. del of source del is 0. bleu of source bleu is 51. add of reference add is 100. keep of reference keep is 100. del of reference del is 100. bleu of reference bleu is 100. sari of reference sari is 70.3. add of reference add is 57.2. keep of reference keep is 82.9. del of reference del is 61.2. bleu of reference bleu is 100. add of reference add is 56.5. keep of reference keep is 82.7. del of reference del is 60.6. bleu of reference bleu is 100. add of dress-ls add is 2.4. keep of dress-ls keep is 60.7. del of dress-ls del is 44.9. bleu of dress-ls bleu is 24.3. sari of dress-ls sari is 26.6. add of dress-ls add is . keep of dress-ls keep is . del of dress-ls del is . bleu of dress-ls bleu is . add of dress-ls add is . keep of dress-ls keep is . del of dress-ls del is . bleu of dress-ls bleu is . add of bift-ens add is . keep of bift-ens keep is . del of bift-ens del is . bleu of bift-ens bleu is . sari of bift-ens sari is . add of bift-ens add is 32.1. keep of bift-ens keep is 90. del of bift-ens del is 58.2. bleu of bift-ens bleu is 71.4. add of bift-ens add is 32.6. keep of bift-ens keep is 90.6. del of bift-ens del is 60.9. bleu of bift-ens bleu is 74.5. add of ours (rnn) add is 2.8. keep of ours (rnn) keep is 61.1. del of ours (rnn) del is 36.5. bleu of ours (rnn) bleu is 24.7. sari of ours (rnn) sari is 22.8. add of ours (rnn) add is 33.5. keep of ours (rnn) keep is 90. del of ours (rnn) del is 59.9. bleu of ours (rnn) bleu is 71.7. add of ours (rnn) add is 34.3. keep of ours (rnn) keep is 90.9. del of ours (rnn) del is 63.1. bleu of ours (rnn) bleu is 75.9. add of ours (san) add is 2.5. keep of ours (san) keep is 61.3. del of ours (san) del is 38. bleu of ours (san) bleu is 24.6. sari of ours (san) sari is 23.3. add of ours (san) add is 35.2. keep of ours (san) keep is 90. del of ours (san) del is 61.2. bleu of ours (san) bleu is 72.1. add of ours (san) add is 35.3. keep of ours (san) keep is 91.1. del of ours (san) del is 64. bleu of ours (san) bleu is 77.
table 2 shows human evaluation results on the chinese-toenglish task. flu. of mle flu. is 4.31. ade. of mle ade. is 4.25. flu. of mle + cp flu. is 4.31. ade. of mle + cp ade. is 4.31. flu. of worddropout flu. is 4.29. ade. of worddropout ade. is 4.25. flu. of clone flu. is 4.32. ade. of clone ade. is 4.58. flu. of mle flu. is 4.27. ade. of mle ade. is 4.22. flu. of mle + cp flu. is 4.26. ade. of mle + cp ade. is 4.25. flu. of worddropout flu. is 4.25. ade. of worddropout ade. is 4.23. flu. of clone flu. is 4.27. ade. of clone ade. is 4.53.
table 2 shows test set results on the nyt and cnndailymail datasets using rouge f1 (r-1 and r-2 are shorthands for unigram and bigram overlap, r-l is the longest common subsequence). r-1 of oracle r-1 is 61.9. r-2 of oracle r-2 is 41.7. r-l of oracle r-l is 58.3. r-1 of oracle r-1 is 54.7. r-2 of oracle r-2 is 30.4. r-l of oracle r-l is 50.8. r-1 of refresh 4 (narayan et al., 2018b) r-1 is 41.3. r-2 of refresh 4 (narayan et al., 2018b) r-2 is 22. r-l of refresh 4 (narayan et al., 2018b) r-l is 37.8. r-1 of refresh 4 (narayan et al., 2018b) r-1 is 41.3. r-2 of refresh 4 (narayan et al., 2018b) r-2 is 18.4. r-l of refresh 4 (narayan et al., 2018b) r-l is 37.5. r-1 of pointer-generator (see et al., 2017) r-1 is 42.7. r-2 of pointer-generator (see et al., 2017) r-2 is 22.1. r-l of pointer-generator (see et al., 2017) r-l is 38. r-1 of pointer-generator (see et al., 2017) r-1 is 39.5. r-2 of pointer-generator (see et al., 2017) r-2 is 17.3. r-l of pointer-generator (see et al., 2017) r-l is 36.4. r-1 of lead-3 r-1 is 35.5. r-2 of lead-3 r-2 is 17.2. r-l of lead-3 r-l is 32. r-1 of lead-3 r-1 is 40.5. r-2 of lead-3 r-2 is 17.7. r-l of lead-3 r-l is 36.7. r-1 of degree (tf-idf) r-1 is 33.2. r-2 of degree (tf-idf) r-2 is 13.1. r-l of degree (tf-idf) r-l is 29. r-1 of degree (tf-idf) r-1 is 33.0. r-2 of degree (tf-idf) r-2 is 11.7. r-l of degree (tf-idf) r-l is 29.5. r-1 of textrank (tf-idf) r-1 is 33.2. r-2 of textrank (tf-idf) r-2 is 13.1. r-l of textrank (tf-idf) r-l is 29. r-1 of textrank (tf-idf) r-1 is 33.2. r-2 of textrank (tf-idf) r-2 is 11.8. r-l of textrank (tf-idf) r-l is 29.6. r-1 of textrank (skip-thought vectors) r-1 is 30.1. r-2 of textrank (skip-thought vectors) r-2 is 9.6. r-l of textrank (skip-thought vectors) r-l is 26.1. r-1 of textrank (skip-thought vectors) r-1 is 31.4. r-2 of textrank (skip-thought vectors) r-2 is 10.2. r-l of textrank (skip-thought vectors) r-l is 28.2. r-1 of textrank (bert) r-1 is 29.7. r-2 of textrank (bert) r-2 is 9. r-l of textrank (bert) r-l is 25.3. r-1 of textrank (bert) r-1 is 30.8. r-2 of textrank (bert) r-2 is 9.6. r-l of textrank (bert) r-l is 27.4. r-1 of pacsum (tf-idf) r-1 is 40.4. r-2 of pacsum (tf-idf) r-2 is 20.6. r-l of pacsum (tf-idf) r-l is 36.4. r-1 of pacsum (tf-idf) r-1 is 39.2. r-2 of pacsum (tf-idf) r-2 is 16.3. r-l of pacsum (tf-idf) r-l is 35.3. r-1 of pacsum (skip-thought vectors) r-1 is 38.3. r-2 of pacsum (skip-thought vectors) r-2 is 18.8. r-l of pacsum (skip-thought vectors) r-l is 34.5. r-1 of pacsum (skip-thought vectors) r-1 is 38.6. r-2 of pacsum (skip-thought vectors) r-2 is 16.1. r-l of pacsum (skip-thought vectors) r-l is 34.9. r-1 of pacsum (bert) r-1 is 41.4. r-2 of pacsum (bert) r-2 is 21.7. r-l of pacsum (bert) r-l is 37.5. r-1 of pacsum (bert) r-1 is 40.7. r-2 of pacsum (bert) r-2 is 17.8. r-l of pacsum (bert) r-l is 36.9.
table 3 shows results (dev set) on document-level gmb benchmark. par-f1 of shallow par-f1 is 66.63. exa-f1 of shallow exa-f1 is 61.74. par-f1 of deep par-f1 is 71.01. exa-f1 of deep exa-f1 is 65.42. par-f1 of deepfeat par-f1 is 71.44. exa-f1 of deepfeat exa-f1 is 66.43. par-f1 of deepcopy par-f1 is 75.89. exa-f1 of deepcopy exa-f1 is 69.45.
table 4 shows results (test set) on document-level gmb benchmark. par-f1 of docsent par-f1 is 57.1. exa-f1 of docsent exa-f1 is 53.27. par-f1 of doctree par-f1 is 62.83. exa-f1 of doctree exa-f1 is 58.22. par-f1 of deepcopy par-f1 is 70.83. exa-f1 of deepcopy exa-f1 is 66.56.
table 5 shows performance statistics of all approaches on the wikipedia dataset filtered on samples including identity terms. acc of baseline acc is 0.931. f1 of baseline f1 is 0.692. auc of baseline auc is 0.91. fp of baseline fp is 0.011. fn of baseline fn is 0.057. acc of importance acc is 0.933. f1 of importance f1 is 0.704. auc of importance auc is 0.945. fp of importance fp is 0.012. fn of importance fn is 0.055. acc of tok replace acc is 0.91. f1 of tok replace f1 is 0.528. auc of tok replace auc is 0.882. fp of tok replace fp is 0.008. fn of tok replace fn is 0.081. acc of our method acc is 0.934. f1 of our method f1 is 0.697. auc of our method auc is 0.949. fp of our method fp is 0.008. fn of our method fn is 0.058. acc of finetuned acc is 0.928. f1 of finetuned f1 is 0.66. auc of finetuned auc is 0.94. fp of finetuned fp is 0.007. fn of finetuned fn is 0.064.
table 7 shows results for exaggerated numeral detection. micro-f1 of ±10% micro-f1 is 58.54%. macro-f1 of ±10% macro-f1 is 57.87%. micro-f1 of ±30% micro-f1 is 56.94%. macro-f1 of ±30% macro-f1 is 56.11%. micro-f1 of ±50% micro-f1 is 57.69%. macro-f1 of ±50% macro-f1 is 56.85%. micro-f1 of ±70% micro-f1 is 70.92%. macro-f1 of ±70% macro-f1 is 70.85%. micro-f1 of ±90% micro-f1 is 76.91%. macro-f1 of ±90% macro-f1 is 76.94%.
table 6 shows results from baselines and our best multimodal method on validation and test data. accuracy of action e + inception accuracy is 0.722. precision of action e + inception precision is 0.765. recall of action e + inception recall is 0.863. f1 of action e + inception f1 is 0.811. accuracy of action e + inception + c3d accuracy is 0.725. precision of action e + inception + c3d precision is 0.769. recall of action e + inception + c3d recall is 0.869. f1 of action e + inception + c3d f1 is 0.814. accuracy of action e + pos + inception + c3d accuracy is 0.731. precision of action e + pos + inception + c3d precision is 0.763. recall of action e + pos + inception + c3d recall is 0.885. f1 of action e + pos + inception + c3d f1 is 0.82. accuracy of action e + context s + inception + c3d accuracy is 0.725. precision of action e + context s + inception + c3d precision is 0.77. recall of action e + context s + inception + c3d recall is 0.859. f1 of action e + context s + inception + c3d f1 is 0.812. accuracy of action e + context a + inception + c3d accuracy is 0.729. precision of action e + context a + inception + c3d precision is 0.757. recall of action e + context a + inception + c3d recall is 0.895. f1 of action e + context a + inception + c3d f1 is 0.82. accuracy of action e + concreteness + inception + c3d accuracy is 0.723. precision of action e + concreteness + inception + c3d precision is 0.768. recall of action e + concreteness + inception + c3d recall is 0.86. f1 of action e + concreteness + inception + c3d f1 is 0.811. accuracy of action e + pos + context s + concreteness + inception + c3d accuracy is 0.737. precision of action e + pos + context s + concreteness + inception + c3d precision is 0.758. recall of action e + pos + context s + concreteness + inception + c3d recall is 0.911. f1 of action e + pos + context s + concreteness + inception + c3d f1 is 0.827.
table 1 shows summary of segmentation performance on phoneme version of the brent corpus (br-phono). p of lstm suprisal (elman, 1990) p is 54.5. r of lstm suprisal (elman, 1990) r is 55.5. f1 of lstm suprisal (elman, 1990) f1 is 55. p of hmlstm (chung et al. 2017) p is 8.1. r of hmlstm (chung et al. 2017) r is 13.3. f1 of hmlstm (chung et al. 2017) f1 is 10.1. p of unigram dp p is 63.3. r of unigram dp r is 50.4. f1 of unigram dp f1 is 56.1. p of bigram hdp p is 53. r of bigram hdp r is 61.4. f1 of bigram hdp f1 is 56.9. p of snlm (- memory, - length) p is 54.3. r of snlm (- memory, - length) r is 34.9. f1 of snlm (- memory, - length) f1 is 42.5. p of snlm (+ memory, - length) p is 52.4. r of snlm (+ memory, - length) r is 36.8. f1 of snlm (+ memory, - length) f1 is 43.3. p of snlm (- memory, + length) p is 57.6. r of snlm (- memory, + length) r is 43.4. f1 of snlm (- memory, + length) f1 is 49.5. p of snlm (+ memory, + length) p is 81.3. r of snlm (+ memory, + length) r is 77.5. f1 of snlm (+ memory, + length) f1 is 79.3.
table 2 shows summary of segmentation performance on other corpora. p of lstm surprisal p is 36.4. r of lstm surprisal r is 49. f1 of lstm surprisal f1 is 41.7. p of unigram dp p is 64.9. r of unigram dp r is 55.7. f1 of unigram dp f1 is 60. p of bigram hdp p is 52.5. r of bigram hdp r is 63.1. f1 of bigram hdp f1 is 57.3. p of snlm p is 68.7. r of snlm r is 78.9. f1 of snlm f1 is 73.5. p of lstm surprisal p is 27.3. r of lstm surprisal r is 36.5. f1 of lstm surprisal f1 is 31.2. p of unigram dp p is 51. r of unigram dp r is 49.1. f1 of unigram dp f1 is 50. p of bigram hdp p is 34.8. r of bigram hdp r is 47.3. f1 of bigram hdp f1 is 40.1. p of snlm p is 54.1. r of snlm r is 60.1. f1 of snlm f1 is 56.9. p of lstm surprisal p is 41.6. r of lstm surprisal r is 25.6. f1 of lstm surprisal f1 is 31.7. p of unigram dp p is 61.8. r of unigram dp r is 49.6. f1 of unigram dp f1 is 55. p of bigram hdp p is 67.3. r of bigram hdp r is 67.7. f1 of bigram hdp f1 is 67.5. p of snlm p is 78.1. r of snlm r is 81.5. f1 of snlm f1 is 79.8. p of lstm surprisal p is 38.1. r of lstm surprisal r is 23. f1 of lstm surprisal f1 is 28.7. p of unigram dp p is 60.2. r of unigram dp r is 48.2. f1 of unigram dp f1 is 53.6. p of bigram hdp p is 66.8. r of bigram hdp r is 67.1. f1 of bigram hdp f1 is 66.9. p of snlm p is 75. r of snlm r is 71.2. f1 of snlm f1 is 73.1.
table 4 shows test language modeling performance (bpc). bpc of unigram dp br-text is 2.33. bpc of unigram dp br-phono is 2.93. bpc of unigram dp ptb is 2.25. bpc of unigram dp ctb is 6.16. bpc of unigram dp pku is 6.88. bpc of bigram hdp br-text is 1.96. bpc of bigram hdp br-phono is 2.55. bpc of bigram hdp ptb is 1.8. bpc of bigram hdp ctb is 5.4. bpc of bigram hdp pku is 6.42. bpc of lstm br-text is 2.03. bpc of lstm br-phono is 2.62. bpc of lstm ptb is 1.65. bpc of lstm ctb is 4.94. bpc of lstm pku is 6.2. bpc of snlm br-text is 1.94. bpc of snlm br-phono is 2.54. bpc of snlm ptb is 1.56. bpc of snlm ctb is 4.84. bpc of snlm pku is 5.89.
table 1 shows comparison of redan with a discriminative decoder to state-of-the-art methods on visdial v1.0 validation set. ndcg of mn-g (das et al., 2017a) ndcg is 56.99. mrr of mn-g (das et al., 2017a) mrr is 47.83. r@1 of mn-g (das et al., 2017a) r@1 is 38.01. r@5 of mn-g (das et al., 2017a) r@5 is 57.49. r@10 of mn-g (das et al., 2017a) r@10 is 64.08. mean of mn-g (das et al., 2017a) mean is 18.76. ndcg of hciae-g (lu et al., 2017) ndcg is 59.7. mrr of hciae-g (lu et al., 2017) mrr is 49.07. r@1 of hciae-g (lu et al., 2017) r@1 is 39.72. r@5 of hciae-g (lu et al., 2017) r@5 is 58.23. r@10 of hciae-g (lu et al., 2017) r@10 is 64.73. mean of hciae-g (lu et al., 2017) mean is 18.43. ndcg of coatt-g (wu et al., 2018) ndcg is 59.24. mrr of coatt-g (wu et al., 2018) mrr is 49.64. r@1 of coatt-g (wu et al., 2018) r@1 is 40.09. r@5 of coatt-g (wu et al., 2018) r@5 is 59.37. r@10 of coatt-g (wu et al., 2018) r@10 is 65.92. mean of coatt-g (wu et al., 2018) mean is 17.86. ndcg of redan-g (t=1) ndcg is 59.41. mrr of redan-g (t=1) mrr is 49.6. r@1 of redan-g (t=1) r@1 is 39.95. r@5 of redan-g (t=1) r@5 is 59.32. r@10 of redan-g (t=1) r@10 is 65.97. mean of redan-g (t=1) mean is 17.79. ndcg of redan-g (t=2) ndcg is 60.11. mrr of redan-g (t=2) mrr is 49.96. r@1 of redan-g (t=2) r@1 is 40.36. r@5 of redan-g (t=2) r@5 is 59.72. r@10 of redan-g (t=2) r@10 is 66.57. mean of redan-g (t=2) mean is 17.53. ndcg of redan-g (t=3) ndcg is 60.47. mrr of redan-g (t=3) mrr is 50.02. r@1 of redan-g (t=3) r@1 is 40.27. r@5 of redan-g (t=3) r@5 is 59.93. r@10 of redan-g (t=3) r@10 is 66.78. mean of redan-g (t=3) mean is 17.4. ndcg of ensemble of 4 ndcg is 61.43. mrr of ensemble of 4 mrr is 50.41. r@1 of ensemble of 4 r@1 is 40.85. r@5 of ensemble of 4 r@5 is 60.08. r@10 of ensemble of 4 r@10 is 67.17. mean of ensemble of 4 mean is 17.38.
table 1 shows baseline model results, using either image or entity labels (2nd column). cider of n|y  cider is 62.08. covr we of n|y  covr we is 21.01. covp obj of n|y  covp obj is 6.19. cider of  y|n  cider is 51.09. covr we of  y|n  covr we is 7.3. covp obj of  y|n  covp obj is 4.95. cider of  y|n  cider is 62.35. covr we of  y|n  covr we is 10.52. covp obj of  y|n  covp obj is 6.74. cider of y|y  cider is 69.46. covr we of y|y  covr we is 36.8. covp obj of y|y  covp obj is 6.93.
table 3 shows human ranking results: normalised rank (micro-averaged). rank of de base+att is 0.35. rank of de del is 0.62. rank of de del+obj is 0.59. rank of fr base+att is 0.41. rank of fr del is 0.6. rank of fr del+obj is 0.67.
table 1 shows main results for findings generation on the cx-chr (upper) and iu-xray (lower) datasets. bleu-1 of cnn-rnn (vinyals et al.,2015) bleu-1 is 0.59. bleu-2 of cnn-rnn (vinyals et al.,2015) bleu-2 is 0.506. bleu-3 of cnn-rnn (vinyals et al.,2015) bleu-3 is 0.45. bleu-4 of cnn-rnn (vinyals et al.,2015) bleu-4 is 0.411. rouge of cnn-rnn (vinyals et al.,2015) rouge is 0.577. cider of cnn-rnn (vinyals et al.,2015) cider is 1.58. bleu-1 of lrcn (donahue et al., 2015) bleu-1 is 0.593. bleu-2 of lrcn (donahue et al., 2015) bleu-2 is 0.508. bleu-3 of lrcn (donahue et al., 2015) bleu-3 is 0.452. bleu-4 of lrcn (donahue et al., 2015) bleu-4 is 0.413. rouge of lrcn (donahue et al., 2015) rouge is 0.577. cider of lrcn (donahue et al., 2015) cider is 1.588. bleu-1 of adaatt (lu et al., 2017) bleu-1 is 0.588. bleu-2 of adaatt (lu et al., 2017) bleu-2 is 0.503. bleu-3 of adaatt (lu et al., 2017) bleu-3 is 0.446. bleu-4 of adaatt (lu et al., 2017) bleu-4 is 0.409. rouge of adaatt (lu et al., 2017) rouge is 0.575. cider of adaatt (lu et al., 2017) cider is 1.568. bleu-1 of att2in (rennie et al., 2017) bleu-1 is 0.587. bleu-2 of att2in (rennie et al., 2017) bleu-2 is 0.503. bleu-3 of att2in (rennie et al., 2017) bleu-3 is 0.446. bleu-4 of att2in (rennie et al., 2017) bleu-4 is 0.408. rouge of att2in (rennie et al., 2017) rouge is 0.576. cider of att2in (rennie et al., 2017) cider is 1.566. bleu-1 of coatt (jing et al., 2018) bleu-1 is 0.651. bleu-2 of coatt (jing et al., 2018) bleu-2 is 0.568. bleu-3 of coatt (jing et al., 2018) bleu-3 is 0.521. bleu-4 of coatt (jing et al., 2018) bleu-4 is 0.469. rouge of coatt (jing et al., 2018) rouge is 0.602. cider of coatt (jing et al., 2018) cider is 2.532. bleu-1 of hgrg-agent (li et al., 2018) bleu-1 is 0.673. bleu-2 of hgrg-agent (li et al., 2018) bleu-2 is 0.587. bleu-3 of hgrg-agent (li et al., 2018) bleu-3 is 0.53. bleu-4 of hgrg-agent (li et al., 2018) bleu-4 is 0.486. rouge of hgrg-agent (li et al., 2018) rouge is 0.612. cider of hgrg-agent (li et al., 2018) cider is 2.895. bleu-1 of cmasw bleu-1 is 0.659. bleu-2 of cmasw bleu-2 is 0.585. bleu-3 of cmasw bleu-3 is 0.534. bleu-4 of cmasw bleu-4 is 0.497. rouge of cmasw rouge is 0.627. cider of cmasw cider is 2.564. bleu-1 of cmasnwaw bleu-1 is 0.657. bleu-2 of cmasnwaw bleu-2 is 0.579. bleu-3 of cmasnwaw bleu-3 is 0.522. bleu-4 of cmasnwaw bleu-4 is 0.479. rouge of cmasnwaw rouge is 0.585. cider of cmasnwaw cider is 1.532. bleu-1 of cmas-il bleu-1 is 0.663. bleu-2 of cmas-il bleu-2 is 0.592. bleu-3 of cmas-il bleu-3 is 0.543. bleu-4 of cmas-il bleu-4 is 0.507. rouge of cmas-il rouge is 0.628. cider of cmas-il cider is 2.475. bleu-1 of cmas-rl bleu-1 is 0.693. bleu-2 of cmas-rl bleu-2 is 0.626. bleu-3 of cmas-rl bleu-3 is 0.58. bleu-4 of cmas-rl bleu-4 is 0.545. rouge of cmas-rl rouge is 0.661. cider of cmas-rl cider is 2.9. bleu-1 of cnn-rnn (vinyals et al., 2015) bleu-1 is 0.216. bleu-2 of cnn-rnn (vinyals et al., 2015) bleu-2 is 0.124. bleu-3 of cnn-rnn (vinyals et al., 2015) bleu-3 is 0.087. bleu-4 of cnn-rnn (vinyals et al., 2015) bleu-4 is 0.066. rouge of cnn-rnn (vinyals et al., 2015) rouge is 0.306. cider of cnn-rnn (vinyals et al., 2015) cider is 0.294. bleu-1 of lrcn (donahue et al., 2015) bleu-1 is 0.223. bleu-2 of lrcn (donahue et al., 2015) bleu-2 is 0.128. bleu-3 of lrcn (donahue et al., 2015) bleu-3 is 0.089. bleu-4 of lrcn (donahue et al., 2015) bleu-4 is 0.067. rouge of lrcn (donahue et al., 2015) rouge is 0.305. cider of lrcn (donahue et al., 2015) cider is 0.284. bleu-1 of adaatt (lu et al., 2017) bleu-1 is 0.22. bleu-2 of adaatt (lu et al., 2017) bleu-2 is 0.127. bleu-3 of adaatt (lu et al., 2017) bleu-3 is 0.089. bleu-4 of adaatt (lu et al., 2017) bleu-4 is 0.068. rouge of adaatt (lu et al., 2017) rouge is 0.308. cider of adaatt (lu et al., 2017) cider is 0.295. bleu-1 of att2in (rennie et al., 2017) bleu-1 is 0.224. bleu-2 of att2in (rennie et al., 2017) bleu-2 is 0.129. bleu-3 of att2in (rennie et al., 2017) bleu-3 is 0.089. bleu-4 of att2in (rennie et al., 2017) bleu-4 is 0.068. rouge of att2in (rennie et al., 2017) rouge is 0.308. cider of att2in (rennie et al., 2017) cider is 0.297. bleu-1 of coatt (jing et al., 2018) bleu-1 is 0.455. bleu-2 of coatt (jing et al., 2018) bleu-2 is 0.288. bleu-3 of coatt (jing et al., 2018) bleu-3 is 0.205. bleu-4 of coatt (jing et al., 2018) bleu-4 is 0.154. rouge of coatt (jing et al., 2018) rouge is 0.369. cider of coatt (jing et al., 2018) cider is 0.277. bleu-1 of hgrg-agent (li et al., 2018) bleu-1 is 0.438. bleu-2 of hgrg-agent (li et al., 2018) bleu-2 is 0.298. bleu-3 of hgrg-agent (li et al., 2018) bleu-3 is 0.208. bleu-4 of hgrg-agent (li et al., 2018) bleu-4 is 0.151. rouge of hgrg-agent (li et al., 2018) rouge is 0.322. cider of hgrg-agent (li et al., 2018) cider is 0.343. bleu-1 of cmasw bleu-1 is 0.44. bleu-2 of cmasw bleu-2 is 0.292. bleu-3 of cmasw bleu-3 is 0.204. bleu-4 of cmasw bleu-4 is 0.147. rouge of cmasw rouge is 0.365. cider of cmasw cider is 0.252. bleu-1 of cmasnw aw bleu-1 is 0.451. bleu-2 of cmasnw aw bleu-2 is 0.286. bleu-3 of cmasnw aw bleu-3 is 0.199. bleu-4 of cmasnw aw bleu-4 is 0.146. rouge of cmasnw aw rouge is 0.366. cider of cmasnw aw cider is 0.269. bleu-1 of cmas-il bleu-1 is 0.454. bleu-2 of cmas-il bleu-2 is 0.283. bleu-3 of cmas-il bleu-3 is 0.195. bleu-4 of cmas-il bleu-4 is 0.143. rouge of cmas-il rouge is 0.353. cider of cmas-il cider is 0.266. bleu-1 of cmas-rl bleu-1 is 0.464. bleu-2 of cmas-rl bleu-2 is 0.301. bleu-3 of cmas-rl bleu-3 is 0.21. bleu-4 of cmas-rl bleu-4 is 0.154. rouge of cmas-rl rouge is 0.362. cider of cmas-rl cider is 0.275.
table 2 shows human evaluation results. focus of n/a focus is 3.487. coherence of n/a coherence is 3.751. share of n/a share is 3.763. human of n/a human is 3.746. grounded of n/a grounded is 3.602. detailed of n/a detailed is 3.761. focus of n/a focus is 3.878. coherence of n/a coherence is 3.908. share of n/a share is 3.93. human of n/a human is 3.817. grounded of n/a grounded is 3.864. detailed of n/a detailed is 3.938. focus of tf (t) focus is 3.433. coherence of tf (t) coherence is 3.705. share of tf (t) share is 3.641. human of tf (t) human is 3.656. grounded of tf (t) grounded is 3.619. detailed of tf (t) detailed is 3.631. focus of tf (t) focus is 3.717. coherence of tf (t) coherence is 3.773. share of tf (t) share is 3.863. human of tf (t) human is 3.672. grounded of tf (t) grounded is 3.765. detailed of tf (t) detailed is 3.795. focus of tf (t+i) focus is 3.542. coherence of tf (t+i) coherence is 3.693. share of tf (t+i) share is 3.676. human of tf (t+i) human is 3.643. grounded of tf (t+i) grounded is 3.548. detailed of tf (t+i) detailed is 3.672. focus of tf (t+i) focus is 3.734. coherence of tf (t+i) coherence is 3.759. share of tf (t+i) share is 3.786. human of tf (t+i) human is 3.622. grounded of tf (t+i) grounded is 3.758. detailed of tf (t+i) detailed is 3.744. focus of lstm (t) focus is 3.551. coherence of lstm (t) coherence is 3.8. share of lstm (t) share is 3.771. human of lstm (t) human is 3.751. grounded of lstm (t) grounded is 3.631. detailed of lstm (t) detailed is 3.81. focus of lstm (t) focus is 3.894. coherence of lstm (t) coherence is 3.896. share of lstm (t) share is 3.864. human of lstm (t) human is 3.848. grounded of lstm (t) grounded is 3.751. detailed of lstm (t) detailed is 3.897. focus of lstm (t+i) focus is 3.497. coherence of lstm (t+i) coherence is 3.734. share of lstm (t+i) share is 3.746. human of lstm (t+i) human is 3.742. grounded of lstm (t+i) grounded is 3.573. detailed of lstm (t+i) detailed is 3.755. focus of lstm (t+i) focus is 3.815. coherence of lstm (t+i) coherence is 3.872. share of lstm (t+i) share is 3.847. human of lstm (t+i) human is 3.813. grounded of lstm (t+i) grounded is 3.75. detailed of lstm (t+i) detailed is 3.869. focus of human focus is 3.592. coherence of human coherence is 3.87. share of human share is 3.856. human of human human is 3.885. grounded of human grounded is 3.779. detailed of human detailed is 3.878. focus of human focus is 4.003. coherence of human coherence is 4.057. share of human share is 4.072. human of human human is 3.976. grounded of human grounded is 3.994. detailed of human detailed is 4.068.
