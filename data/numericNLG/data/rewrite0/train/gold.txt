table 2 summarizes the performances of proposed model when different position features are exploited. as the table shows, the position feature on plain text is still effective in our model and we accredit its satisfactory result to the dependency information and tree-based kernels. the f1 scores of tree-based position features are higher since they are “specially designed” for our model. contrary to our expectation, the more fine-grained tpf2 does not yield a better performance than tpf1, and two kinds of tpf give fairly close results. 

table 3 presents the correlation results for the two modelsâ€™ preferences for each construction and the verb bias score. the ab model does not correlate with the judgments for the do. however, the model produces significant positive correlations with the pd judgments and with the verb bias score. the bfs model, on the other hand, achieves significant positive correlations on all measures, by both levels. as in the earlier experiments, the best correlation with the verb bias score is produced by the second level of the bfs model. 

table 4 presents the results of our rationale model. the first one achieves the highest map on the development set, the second run is selected to compare the models when they use roughly 10% of question text (7 words on average). the rationales achieve the map up to 56.5%, getting close to using the titles. the models also outperform the baseline of using the noisy question bodies, indicating the the modelsâ€™ capacity of extracting short but important fragments. 

table 2 shows the results of our contextdependent sense embedding models on the scws dataset. from table 2, we can observe that our model outperforms the other probabilistic models and is not as good as the best context clustering based model. 

from table 4, we can find that in the first hop the context words “great”, “but” and “dreadful” contribute equally to the aspect “service”. while after the second hop, the weight of “dreadful” increases and finally the model correctly predict the polarity towards “service” as negative. this case shows the effects of multiple hops. however, for food aspect, the content-based model also gives a larger weight to “dreadful” when the target we focus on is “food”. as a result, the model incorrectly predicts the polarity towards “food” as negative. 

table 2 presents overall system results according to hter and mter, as well as bleu computed against the original ted talks reference translation. we can see that nmt clearly outperforms all other approaches both in terms of bleu and ter scores. focusing on mter results, the gain obtained by nmt over the second best system (pbsy) amounts to 26%. it is also worth noticing that mter is considerably lower than hter for each system. 

we can see in table 4 that shift errors in nmt translations are definitely less than in the other systems. the error reduction of nmt with respect to the second best system (pbsy) is about 50% (173 vs. 354). looking at the last column of table 4, we can say that our observations on hter are confirmed by the krs results: the reorderings performed by nmt are much more accurate than those performed by any pbmt system. moreover, according to the approximate randomization test, krs differences are statistically significant between nmt and all other systems, but not among the three pbmt systems. 

table 2 shows the averaged scores over the responses. the checklist models outperform all baselines in generating recipes that follow the provided agenda closely and accomplish the desired goal, where nn in particular often generates the wrong dish. perhaps surprisingly, both the attention and encdec baselines and the checklist model beat the true recipes in terms of having better grammar. 

table 4 shows the performance for our system and those systems. our system achieves the best result in span and relatively lower performance in nucleus and relation identification comparing with the corresponding best results but still better than most systems. no system achieves the best result on all three metrics. 

when trained on nw and tested on df, supervised methods encounter out-of-domain situations. however, the msep system can adapt well. table 7 shows that msep outperforms supervised methods in out-of-domain situations for both tasks. 

table 2 shows the performance of the three supervised models in experiment 1. our approach achieves significantly better performance than yu’s method and word2vec method in terms of accuracy (t-test, p-value < 0.05) for both bless and entailment datasets. specifically, our approach improves the average accuracy by 4% compared to yu’s method, and by 9% compared to the word2vec method. the word2vec embeddings have the worst result because it is based only on co-occurrence based similarity, which is not effective for the classifier to accurately recognize all the taxonomic relations. our approach performs better than yu’s method and it shows that our approach can learn embeddings more effectively. moreover, from the experimental results of svm+our and svm+ourshort, we can observe that the offset vector between hypernym and hyponym, which captures the contextual information, plays an important role in our approach as it helps to improve the performance in both datasets. 

the experimental results in table 3 show that our term embedding learning approach performs better than other methods in accuracy. it also shows that the taxonomic properties identified by our term embedding learning approach have great generalization capability (i.e. less dependent on the training set), and can be used generically for representing taxonomic relations. 

table 5 shows the results on the common subset of the evaluation datasets, where all word pairs have images in each of the data sources. first, note the same patterns as before: multi-modal representations perform better than linguistic ones. even for the poorly performing esp game dataset, the vggnet representations perform better on both simlex and men (bottom right of the table). visual representations from google, bing, flickr and imagenet all perform much better than esp game on this common covered subset. in a sense, the fullcoverage datasets were “punished” for their ability to return images for abstract words in the previous experiment: on this subset, which is more concrete, the search engines do much better. to a certain extent, including linguistic information is actually detrimental to performance, with multi-modal performing worse than purely visual. again, we see the marked improvement with vggnet for imagenet, while google, bing and flickr all do very well, regardless of the architecture. 

we compare the performance of non-bilinear and bilinear pooling methods in table 1. we see that mcb pooling outperforms all non-bilinear pooling methods, such as eltwise sum, concatenation, and eltwise product. one could argue that the compact bilinear method simply has more parameters than the non-bilinear pooling methods, which contributes to its performance. however, even with similar parameter budgets, nonbilinear methods could not achieve the same accuracy as the mcb method. section 2 in table 1 also shows that compact bilinear pooling has no impact on accuracy compared to full bilinear pooling. section 3 in table 1 demonstrates that the mcb brings improvements regardless of the image cnn used. we primarily use resnet152 in this paper, but mcb also improves performance if vgg-19 is used. section 4 in table 1 shows that our soft attention model works best with mcb pooling. in fact, attending to the concatenation + fc layer has the same performance as not using attention at all, while attending to the mcb layer improves performance by 2.67 points. 

table 1 presents our results. in all three setups an swvp algorithm is superior. averaged accuracy differences between the best performing algorithms and csp are: 3.72 (b-wmr, (simple(++), learnable(+++))), 5.29 (b-wm, (simple(++), learnable(++))) and 5.18 (a-wm, (simple(+), learnable(+))). in all setups swvp outperforms csp in terms of averaged performance (except from b-wmr for (simple(+), learnable(+))). moreover, the weighted models are more stable than csp, as indicated by the lower standard deviation of their accuracy scores. finally, for the more simple and learnable datasets the swvp models outperform csp in the majority of cases (7-10/10). 

table 2 shows that, cross-lingual similarization results in grammars with much higher cross-lingual similarity, and the adaptive accuracies given by the adapted grammars approach to those of the original grammars. it indicates that the proposed algorithm improve the crosslingual similarity without losing syntactic knowledge. the last column of table 2 shows the performance of the grammars on machine translation. the cross-lingually similarized grammars corresponding to the configurations with projective searching for chinese always improve the translation performance, while non-projective grammars always hurt the performance. 

table 3 shows the performance of the crosslingually similarized grammar on dependency treebased translation, compared with previous work (xie et al., 2011). the original grammar performs slightly worse than the previous work in dependency tree-based translation, this can ascribed to the difference between the implementation of the original grammar and the dependency parser used in the previous work. however, the similarized grammar achieves very significant improvement based on the original grammar, and also significant surpass the previous work. 

table 1 summarizes the bleu scores of different systems on the chinese-english translation tasks. clearly vnmt significantly improves translation quality in terms of bleu on most cases, and obtains the best average results that gain 0.86 and 1.35 bleu points over moses and groundhog respectively. besides, without the kl objective, vnmt w/o kl obtains even worse results than groundhog. 

table 1 shows the alignment summary statistics for the 447 sentences present in the hansard test data. first, we note that in deciding the decoding style for ibm2-hmm, the hmm method is better than the joint method. from the results in table 1 we see that the hmm outperforms all other models, including ibm2-hmm and its convex relaxation. however, ibm2-hmm is not far in aer performance from the hmm and both it and its relaxation do better than fastalign. 

table 3 shows that the level of agreement as measured by the fleiss’κ score is much lower when the number of annotators is increased, particularly for the 4gs set of sentence pairs, as compared to scores noted in bowman et al. (2015). the decrease in agreement is particularly large with regard to contradiction. 

the theta scores from irt in table 5 show that, compared to amt users, the system performed well above average for contradiction items compared to human performance, and performed around the average for entailment and neutral items. for both the neutral and contradiction items, the theta scores are similar across the 4gs and 5gs sets, whereas the accuracy of the more difficult 4gs items is consistently lower. 

it can be seen from table 2 that adding the weight rw,c improves performance in all the cases, especially on the word analogy task. among the four ρ functions, ρ0 performs the best on the word similarity task but suffers notably on the analogy task, while ρ1 = log performs the best overall. 

table 3 gives the results on the two datasets. in terms of f-measure, we gain a 6% absolute improvement, and a 5% absolute improvement over the results of jamr on the two different experimental setups respectively. as expected, the results in table 3 show that we gain 3% improvement over the two different datasets respectively, by adding only some additional lexical features. 

from table 4 we can see that our parser achieves better performance than other approaches, even without utilizing any external semantic resources. 

the results in table 5 show that our approach outperforms camr*, and obtains comparable performance with camr. however, our approach achieves slightly lower performance, compared to the smbt-based parser, which adds data and features drawn from various external semantic resources. 

table 2 complements our results, providing uas values for each of the 8 languages participating in this setup. the uas difference between bgi-pp+i+b and the turboparser are (+0.24)-(- 0.71) in first order parsing and (+0.18)-(-2.46) in second order parsing. in the latter case, combining these two models (bgi+pp+i+b+e) yields improvements over the turboparser in 6 out of 8 languages. 

the mrr results in the left half of table 3 (“unfiltered”) show that for all languages and for all pos, form real has the worst performance among the form models. the form opt model improves these results based on the additional information it has access to (the mapping from lemma to its most frequent form). form sum performs similar to form opt. for czech, hungarian and spanish it is slightly better (or equally good), whereas for english and german there is no clear trend. there is a large difference between these two models on german nouns, with form sum performing considerably worse. we attribute this to the fact that many german noun forms are rare compounds and therefore lead to badly trained form embeddings, which summed up do not lead to high quality embeddings either. among the stemming models, stem real also is the worst performing model. we can further see that for all languages and almost all pos,stem sum performs worse than stem opt. that indicates that stemming leads to many low-frequency stems or many words sharing the same stem. this is especially apparent in spanish verbs. there, the stemming models are clearly inferior to form models. overall, lamb performs best for all languages and pos types. most improvements of lamb are significant. the improvement over the best formmodel reaches up to 6 points (e.g., czech nouns). in contrast to form sum, lamb improves over form opt on german nouns. this indicates that the sparsity issue is successfully addressed by lamb. 

table 5 lists the 10-fold cross-validation results (accuracy and macro f1) on the csfd dataset. lamb/stem results are consistently better than form results. on the semeval data, lamb improves the results over form and stem (cf. table 5). hence, lamb can still pick up additional information despite the simple morphology of english. the semeval 2015 winner (hagen et al., 2015) is a highly domain-dependent and specialized system that we do not outperform. 

table 2 shows the tagging accuracies and the averaged numbers of single-side tags for each token after pruning. we first fix λ = 0.98 and increase r from 2 to 8, leading to consistently improved accuracies on both ctb5-dev and pddev. no further improvement is gained with r = 16, indicating that tags below the top-8 are mostly very unlikely ones and thus insignificant for computing feature expectations. then we fix r = 8 and try different λ. we find that λ has little effect on tagging accuracies but influences the numbers of remaining single-side tags. we choose r = 8 and λ = 0.98 for final evaluation. the second major row tunes r and λ for offline pruning. different from online pruning, λ has much greater effect on the number of remaining single-side tags. under λ = 0.9999, increasing r from 8 to 16 leads to 0.20% accuracy improvement on ctb5-dev, but using r = 32 has no further gain. then we fix r = 16 and vary λ from 0.99 to 0.99999. we choose r = 16 and λ = 0.9999 for offline pruning for final evaluation, which leaves each word with about 5.2 ctb-tags and 7.6 pd-tags on average. 

table 3 summarizes the accuracies on the test data and the tagging speed during the test phase. it is clear that both online and offline pruning greatly improve the efficiency of the coupled model by about two magnitudes, without the need of a carefully predefined set of tag-to-tag mapping rules. moreover, the coupled model with offline pruning achieves 0.76% accuracy improvement on ctb5- test over the baseline model, and 0.48% over our reimplemented guide-feature approach of jiang et al. (2009). the gains on pd-test are marginal, possibly due to the large size of pd-train, similar to the results in li et al. (2015). 

table 4 shows results for tuning r and λ. from the results, we can see that in the online pruning method, λ seems useless and r becomes the only threshold for pruning unlikely single-side tags. the accuracies are much inferior to those from the offline pruning approach. based on the results, we choose r = 16 and λ = 1.00 for final evaluation. 

table 5 summarizes the accuracies on the test data and the tagging speed (characters per second) during the test phase. in terms of efficiency, the coupled model with offline pruning is on par with the baseline single-side tagging model. in terms of f-score, the coupled model with offline pruning achieves 0.67% (ws) and 1.09% (ws&pos) gains on ctb5-test over the baseline model, and 0.48% (ws) and 0.79% (ws&pos) over our reimplemented guide-feature approach of jiang et al. (2009). 

table 6 presents the f scores on ctb5x-test. we can see that the coupled model with offline pruning achieves 0.64% (ws) and 1.16% (ws&pos) f-score improvements over the baseline model, and 0.05% (ws) and 0.33% (ws&pos) over the guide-feature approach. the original guide-feature method in jiang et al. (2009) achieves 98.23% and 94.03% f-score, which is very close to the results of our reimplemented model. 

the results are shown in table 3. it can be clearly observed that binet-based approaches outperform baselines and perform comparably to the state-ofthe-art model on generating the summaries on most topics: arearank achieves the significant improvement over the state-of-the-art model on sports and disasters, and performs comparably on politics and military and noderank’s performance achieves the comparable performance to previous state-of-the-art model though it is inferior to arearank on most topics. among these five topics, almost all models perform well on disaster and military topics because disaster and military reference summaries have more entries than the topics such as politics and sports and topics of event entries in the summaries are focused. the high-quality training data benefits models’ performance especially for arearank which is purely data-driven. in contrast, on sports and politics, the number of entries in the reference summaries is small, which results in weaker supervision and affect the performance of models. it is notable that arearank does not perform well on generating the comprehensive summary in which topics of event entries are miscellaneous. 

table 2 summarizes the performances of scope detection on abstracts. it shows that our cnn-based models (both cnn_c and cnn_d) can achieve better performances than the baseline in most measurements. this indicates that our cnn-based models can better extract and model effective features. besides, compared to the baseline, our cnn-based models consider fewer features and need less human intervention. it also manifests that our cnn-based models improve significantly more on negation scope detection than on speculation scope detection. much of this is due to the better ability of our cnn-based models in identifying the right boundaries of scopes than the left ones on negation scope detection, with the huge gains of 29.44% and 25.25% on pcrb using cnn_c and cnn_d, respectively. table 2 illustrates that the performance of speculation scope detection is higher than that of negation (best pcs: 85.75% vs 77.14%). the imbalance between positive and negative instances has negative effects on both the baseline and the cnn-based models for negation scope detection. table 2 also shows that our cnn_d outperforms cnn_c in negation scope detection (pcs: 77.14% vs 70.86%), while our cnn_c performs better than cnn_d in speculation scope detection (pcs: 85.75% vs 74.43%). 

table 4 compares our cnn-based models with the  state-of-the-art  systems. it  shows  that  our  cnnbased  models  can  achieve  higher  pcss  (+1.54%)  than those of the state-of-the-art systems for speculation  scope  detection  and  the  second  highest  pcs  for  negation  scope  detection  on  abstracts,  and can get comparable pcss on clinical records  (73.92%  vs  78.69%  for  speculation  scopes,  89.66%  vs  90.74%  for  negation  scopes). it also displays that our cnn-based models perform worse than the state-of-the-art on full papers due to the complex syntactic structures of the sentences and the cross-domain nature of our evaluation. 

table 4 lists the effects of word embedding. we can see that the performance when updating the word embedding is better than when not updating, and the performance of word embedding is a little better than random word embedding. 

the compared results are shown in table 3. from the results, we can observe that our method could outperform all state-of-the-arts in both the hotel and restaurant domains. it proves that our method is effective. furthermore, the improvements in both the hotel and restaurant domains prove that our model possesses preferable domain-adaptability. 

table 4 shows the performances of our method utilizing bf+pe+bigram for hotel and restaurant domains. we found that dropping relations 1, 2 and 10 results in a relatively gentle reduction (about 2.2%) in f1-score. dropping other relations also result in a 2.5-4.0% performance reduction. 

table 4 shows the effect of unsupervised pre-training of word embeddings with a word2vec skip-gram model, and furthermore, the results of sharing of these representations between the tweets and targets, on the development set. the first set of results is with a uniformly random embedding initialisation in [âˆ’0.1, 0.1]. our results show that, in the absence of a large labelled training dataset, pretraining of word embeddings is more helpful than random initialisation of embeddings. 

table 7 shows all our results, including those using the unseen target setup, compared against the state-of-the-art on the stance detection corpus. table 7 further lists baselines reported by mohammad et al. (2016), namely a majority class baseline (majority baseline), and a method using 1 to 3-gram bag-of-word and character n-gram features (svm-ngrams-comb), which are extracted from the tweets and used to train a 3-way svm classifier. bag-of-word baselines (bowv, svm-ngrams-comb) achieve results comparable to the majority baseline (f1 of 0.2972), which shows how difficult the task is. by training conditional encoding models on automatically labelled stance detection data we achieve state-of-the-art results. the best result (f1 of 0.5803) is achieved with the bi-directional conditional encoding model (bicond). this shows that such models are suitable for unseen, as well as seen target stance detection. 

we can see that after incorporating subevent phrases, additional 10% of civil unrest stories were discovered, with a small precision loss, the f1-score on event detection was improved by 3%. 

table 1 compares our results on the awa benchmark against alternatives using the same visual features, and word vectors trained on the same corpus. we observe that: (i) our gaussian-embedding obtains the best performance overall. (ii) our method outperforms cme which shares an objective function and optimisation strategy with ours, but operates on vectors rather than gaussians. this suggests that our new distribution rather than vectorembedding does indeed bring significant benefit. 

table 1 shows the results of all systems on 5 million training set. the traditional syntax-based system achieves 9.45, 12.90, and 17.72 on mt06, mt08 news, and mt08 web sets respectively, and 13.36 on average in terms of (terbleu)/2. the large vocabulary nmt (lvnmt), our baseline, achieves an average (terbleu)/2 score of 15.74, which is about 2 points worse than the hybrid system. ugru improves the translation quality by 1.3 points on average over lvnmt. and ugru + usub achieves the best average score of 13.14, which is about 2.6 points better than lvnmt. all the improvements of our coverage embedding models over lvnmt are statistically significant with the signtest of collins et al. (2005). 

table 2 shows the results of 11 million systems, lvnmt achieves an average (ter-bleu)/2 of 13.27, which is about 2.5 points better than 5 million lvnmt. the result of our ugru coverage model gives almost 1 point gain over lvnmt. 

table 2 (next side) shows the average results over the different frequency ranges for the various dsms trained on the 1 billion-word ukwac data. as can be seen in the table, the most consistent model is isvd, which produces the best results in both the medium and mixed frequency ranges. the neural network models sgns and cbow produce the best results in the high and low range, respectively, with cbow clearly outperforming sgns in the latter case. clearly, the former approach is more beneficial for low-frequent items. the ppmi, tsvd and ri models perform similarly across the frequency ranges, with ri producing somewhat lower results in the medium range, and tsvd producing somewhat lower results in the low range. the co model underperforms in all frequency ranges. worth noting is the fact that all models that are based on an explicit matrix (i.e. co, ppmi, tsvd and isvd) produce better results in the medium range than in the high range. the arguably most interesting results are in the low range. unsurprisingly, there is a general and significant drop in performance for low frequency items, but with interesting differences among the various models. as already mentioned, the cbow model produces the best results, closely followed by ppmi and ri. it is noteworthy that the low-dimensional embeddings of the cbow model only gives a modest improvement over the highdimensional explicit vectors of ppmi. the worst results are produced by the isvd model, which scores even lower than the baseline co model. 

the evaluation results are listed in table 2. we make the following observations: lower annotation projection quality. we find that the f1-scores of bengali, malayalam and tamil are 6, 11 and 31 pp below that of an average highresource language (as exemplified by german in table 2). bengali and malayalam, however, do surpass hindi, for which only a relatively poor dependency parser was used. this suggests that syntactic annotation projection may be a better method for identifying predicate-argument structures in languages that lack fully developed dependency parsers. 

table 2 shows performance of sarcasm detection when our word embedding-based features are used on their own i.e, not as augmented features. using only unigrams as features gives a f-score of 72.53%, while only unweighted and weighted features gives f-score of 69.49% and 58.26% respectively. 

table 3 shows results for four kinds of word embeddings. all entries in the tables are higher than the simple unigrams baseline, i.e., f-score for each of the four is higher than unigrams - highlighting that these are better features for sarcasm detection than simple unigrams. in case of liebrecht et al. (2013) for word2vec, the overall improvement in f-score is 4%. precision increases by 8% while recall remains nearly unchanged. for features given in gonzalez- ´ ibanez et al. (2011a), there is a negligible degradation of ´ 0.91% when word embedding-based features based on word2vec are used. for buschmeier et al. (2014) for word2vec, we observe an improvement in f-score from 76.61% to 78.09%. precision remains nearly unchanged while recall increases. in case of joshi et al. (2015) and word2vec, we observe a slight improvement of 0.20% when unweighted (s) features are used. this shows that word embedding-based features are useful, across four past works for word2vec. table 3 also shows that the improvement holds across the four word embedding types as well. the maximum improvement is observed in case of liebrecht et al. (2013). it is around 4% in case of lsa, 5% in case of glove, 6% in case of dependency weight-based and 4% in case of word2vec. 

the correlation between s˜i,j and community feedback is reported for three models in table 4 for the thread level, and in table 5 for the user level. on the thread level, the hyb-500.30 style model consistently finds positive, statistically significant, correlation between the post’s stylistic similarity score and its karma. this result suggests that language style adaptation does contribute to being well-received by the community. none of the other models explored in the previous section had this property, and for the topic models the correlation is mostly negative. 

table 2 shows that capsule outperforms all four baseline methods. 

results from table 2 do not show significant differences between the two models. here we can see that the error rate of the most confident predicted label is about 30%, while human performed similarly by choosing from a two different label sets in a binary settings, so their task was inherently harder. 

the results in table 2 show that our method using all the column sets achieved the best average precision among the combination of column sets that include at least the base column set. this suggests that all of the clues introduced by our four column sets are effective for performance improvement. table 2 also demonstrates that our method using all the column sets obtained better average precision than the strongest baseline, ouchi’s method, in spite of an unfavorable condition for it. the results also show that our method with all of the column sets achieved a better f-score than iida’s method and the single-column baseline. however, it achieved a lower f-score than ouchi’s method. particularly, it got high precision in a wide range of recall levels (e.g., around 0.8 in precision at 0.25 in recall and around 0.7 in precision at 0.4 in recall), while the precision obtained by ouchi’s method at 0.25 in recall was just around 0.65. 

table 4 shows the performance of our combined model compared with several baselines. our combined model out-performed both luong et al. (2015) and gouws and sã¸gaard (2015) which represent the best published crosslingual embeddings trained on bitext and monolingual data respectively. 

table 6 shows the cldc results for various clwe. despite its simplicity, our model achieves competitive performance. 

table 3 gives the average accuracy of the unissnt+, bissnt+, vanilla encoder-decoder, and attention-based models. our bissnt+ model outperforms the vanilla encoder-decoder by a large margin and almost matches the state-of-the-art result on this task. 

table 4 shows the results of afet and its variants. afet outperforms both figer and hyena systems, demonstrating the predictive power of the learned embeddings, and the effectiveness of modeling type correlation information and noisy candidate types. we also observe that pruning methods do not always improve the performance, since they  ggressively filter out rare types in the corpus, which may lead to low recall. clustype is not as good as figer and hyena because it is intended for coarse types and only utilizes relation phrases. 

a breakdown by question type comparing the different data sources for kvmemnns is given in table 4. ie loses out especially to doc (and kb) on writer, director and actor to movie, perhaps because coreference is difficult in these cases – although it has other losses elsewhere too. doc loses out to kb particularly on tag to movie, movie to tags, movie to writer and movie to actors. 

the convergence results are shown in table 3 for four different temporal comparison intervals. comparison of the significant game 1 results shows that teams entrained on pitch min, pitch max, shimmer, and jitter in at least one of the intervals. both shimmer and jitter converged for all choices of temporal units. the only feature that diverged during game 1 is pitch-mean. the rest of the features did not show significant team-level partner differences during game 1 for any temporal interval and thus exhibited maintenance, meaning that the team members neither converged nor diverged. during game 2, we observed maintenance for all features except for intensity-mean and intensity-min, which diverged. 

table 1 shows the regression results. several interesting patterns were discovered in this analysis: (a) a positive correlation between the rank of “luxury” and “self-enhancement”, a trait often associated with people who pursue self-interests and value social status, prestige and personal success (p < 0.0001). (b) the rank of “safety” is positively correlated with “conservation”, a trait associated with people who conform to tradition and pursue safety, harmony, and stability (p < 0.005). (c) “self-transcendence”, a trait often associated with people who pursue the protection of the welfare of others and the nature, is positively correlated with the rank of “fuel economy” (p < 0.005) but negatively correlated with the rank of “style” (p < 0.005). other significant correlations uncovered in this analysis include a negative correlation between car “price” and “conservation” (p < 0.005), a negative correlation between car “safety” and “conscientiousness” (p < 0.05), and a positive correlation between “openness to change” and car “performance” (p < 0.05). 

table 9 shows an ablation of the alignment classifier features. entailment of arguments is the most informative feature for argument alignment. adding lexical and syntactic context compatibilities adds significant boosts in precision and recall. knowing that the arguments are retrieved by the same query pattern (sentence feature) only provides minor improvements. 

table 4 summarizes the empirical findings for our approach and s-mart (yang and chang, 2015) on the tweet entity linking task. for the systems with user-entity bilinear function, we report results obtained from embeddings trained on retweet+ in table 4, and other results are available in table 5. as presented in table 4, ntel-nonstruct performs 2.7% f1 worse than the ntel baseline on the two test sets, which indicates the non-overlapping inference improves system performance on the task. with structured inference but without embeddings, ntel performs roughly the same as s-mart, showing that a feedforward neural network offers similar expressivity to the regression trees employed by yang and chang (2015). as shown in table 4, by learning the interactions between mention and entity representations, ntel with mention-entity bilinear function outperforms the ntel baseline system by 1.8% f1 on average. specifically, the bilinear function results in considerable performance gains in recalls, with small compromise in precisions on the datasets. 

table 6 documents nist evaluation results on an unseen uyghur test set (with gold annotations) for the best transfer model configuration jointly trained on turkish and uzbek gold annotations and uyghur training annotations produced by a non-speaker linguist (non-gold). despite the noisy supervision provided in the target language, transferring from turkish and uzbek provides a +14.1 f1 improvement over a state of the art monolingual model trained on the same uyghur annotations. 

the results shown in table 3 generally confirm the conclusions we drew from the ptb experiments above. in particular, we can see that the proposed lsrc model largely outperforms all other models. in particular, lsrc clearly outperforms lstm with a negligible increase in the number of parameters (resulting from the additional 200 × 200 = 0.04m local connection weights ulc) for the single layer results. we can also see that this improvement is maintained for deep models (2 hidden layers), where the lsrc model achieves a slightly better performance while reducing the number of parameters by ≈ 2.5m and speeding up the training time by ≈ 20% compared to deep lstm. 

we present our results in table 2. our approach significantly outperforms the stanford parser (de marneffe et al., 2006) by 17.91% (28.69% relative) for pascal-50s, and 12.83% (25.28% relative) for pascal-context-50s. we also make small improvements over deeplabcrf (chen et al., 2015) in the case of pascal-50s. 

the results are shown in table 4. performance is similar across models. we found that adding a second fully-connected 150 dimensional layer to the charagram model improved results slightly. 

table 1 reports the translation quality for different methods. comparing the first two lines in table 1, it is obvious that the nmt method rnnsearch performs much worse than the smt model moses on chinese-to-english translation. the gap is as large as approximately 2.0 bleu points (28.38 vs. 30.30). clearly, rnnsearch-mono-sl outperforms rnnsearch in most cases. the best performance is obtained if the top 50% monolingual data is used. the biggest improvement is up to 4.05 bleu points (32.43 vs. 28.38 on mt03) and it also significantly outperforms moses. for example, rnnsearchmono-mtl using the top 50% monolingual data can remarkably outperform the baseline rnnsearch,

table 2 reports the results. we can see from the table that closely related source-side monolingual data (the top 50%) can also boost the translation quality on all of the test sets. the performance improvement can be more than 1.0 bleu points. compared to the results on small training data, the gains from source-side monolingual data are much smaller. 

table 4 summarizes the best results reported in this paper for the conll-2014 test set (column 2014) before and after adding the common crawl n-gram language model. the combination of sparse features and web-scale monolingual data marks our best result, outperforming previously published results by 8% m2 using similar training data. while our sparse features cause a respectable gain when used with the smaller language model, the web-scale language model seems to cancel out part of the effect. 

table 3 shows the results of re-scoring. re-scoring with the transfer nmt model yields an improvement of 1.1–1.6 bleu points above the strong sbmt system, we find that transfer nmt is a better re-scoring feature than baseline nmt or neural language models. 

table 2 shows the results of an ablation study when removing some groups of features. we can see that the lexical similarity features (which we modeled by mt evaluation metrics), have a large impact: excluding them from the network yields a decrease of over eight map points. as expected, eliminating the domain-specific features also hurts the performance greatly: by six map points absolute. eliminating the use of distributed representation has a lesser impact: 3.3 map points absolute. we have also found that there is an interaction between features and similarity relations. for example, for relatedness, lexical similarity is 2.6 map points more informative10 than distributed representations. 

however, a pairwise comparison between full and -syn (table 2) reveals that human subjects consistently prefer the output of full instead of -syn both for startest and cartoon. 

table 3 shows the results of the detailed comparison of thematicity, coherence, and solvability. this table clearly shows the strong contribution of the semantic component of our system. the specific contribution of the syntactic component is to pro duce overall more solvable and thematically satisfying problems, although it can slightly affect coherence especially when automatic parses fail. finally, the overall high ratings for human-authored stories across all three dimensions, confirm the high quality of the crowd-sourced stories. 

table 1 shows the classification performance on the sst2 dataset. from rows 1-3 we see that our proposed sentiment model that integrates the diverse set of knowledge (section 4) significantly outperforms the base cnn (kim 2014). compared with the baselines trained in the same setting (rows 4-6), our model with the full knowledge, cnn+rel+lex, performs the best. our enhanced framework enables richer knowledge and achieves much better performance. our method further outperforms the base cnn that is additionally trained with dense phrase-level annotations (row 7), showing improved generalization of the knowledge-enhanced model from limited data. 

table 2 shows model performance on the cr dataset. our model again surpasses the base network and several other competitive neural methods by a large margin. 

table 5 shows the results on the word to sense dataset of the semeval-2014 clss task, according to pearson (r × 100) and spearman (rho × 100) correlation scores and for the four strategies. despite this, deconf provides consistent improvement over the comparison sense representation techniques according to both measures and for all the strategies. across the four strategies, s2a proves to be the most effective for deconf and the representations of rothe and schutze (2015). the representations of chen et al. (2014) perform best with the s2w strat egy whereas those of iacobacci et al. (2015) do not show a consistent trend with relatively low performance across the four strategies. also, a comparison of our results across the s2w and s2a strategies reveals that a word’s aggregated representation, i.e., the centroid of the representations of its senses, is more accurate than its original word representation. 

table 1 shows a number of features extracted from the aligned dependency trees in figure 1 and highlights that adjectives and nouns do not share many features if only first order dependencies would be considered. 

table 3 highlights the effect of the sppmi shift parameter k, while keeping the number of neighbours fixed at 30 and using the static top n neighbour retrieval function. for the apt model, a value of k = 40 performs best (except for simlex-999, where smaller shifts give better results), with a performance drop-off for larger shifts. 

table 4 shows that distributional inference successfully infers missing information for both model types, resulting in improved performance over models without the use of di on all datasets. the improvements are typically larger for the apt model, suggesting that it is missing more distributional knowledge in its elementary representations than untyped models. the density window and static top n neighbour retrieval functions perform very similar, however the static approach is more consistent and never underperforms the baseline for either model type on any dataset. the wordnet based neighbour retrieval function performs particularly well on simlex-999. 

table 6 shows that the static top n and density window neighbour retrieval functions perform very similar again. the density window retrieval function outperforms static top n for composition by intersection and vice versa for composition by union. the wordnet approach is competitive for composition by union, but underperfoms the other approaches for composition by intersection significantly. table 6 also shows that while composition by intersection is significantly improved by distributional inference, composition by union does not appear to benefit from it. 

table 7 shows that composition by intersection with distributional inference considerably improves upon the best results for apt models without distributional inference and for untyped count-based models, and is competitive with the state-of-the-art neural network based models of hashimoto et al. (2014) and wieting et al. (2015). distributional inference also improves upon the performance of an untyped vsm where composition by pointwise multiplication is outperforming the models of mitchell and lapata (2010), and blacoe and lapata (2012). table 7 furthermore shows that di has a smaller effect on the apt model based on composition by union and the untyped model based on composition by pointwise addition. 

using a standard logistic regression classifier, we achieve an 11% improvement in accuracy over the baseline approach, as can be seen in table 2. the rule-based approach was insufficient for be and have vpe, where logistic regression provides the largest improvements. although we improve upon the baseline by 29%, the accuracy achieved for bevpe is still low, this occurs mainly because: (i) be is the most commonly used auxiliary, so the number of negative examples is high compared to the number of positive examples, and, (ii) the analysis of the some of the false positives showed that there may have been genuine cases of vpe that were missed by the annotators of the dataset (bos and spenader, 2011). 

in table 3, we compare our results to those achieved by liu et al. (2016) when using wsj sets 0-14 for training and sets 20-24 for testing. we improve on their overall accuracy by over 11%, due to the 25% improvement in recall achieved by our method. 

in table 6 we can see that the syntactic features were essential for obtaining the best results, as can be seen by the 8.3% improvement, from 73.4% to 81.7%, obtained from including these features. 

table 7 presents the results from a feature ablation study on antecedent

table 1 summarizes 1-best supertagging results. our baseline blstm model without attention achieves the same level of accuracy as lewis et al. (2016) and the baseline blstm model of vaswani et al. (2016). for training and testing the local attention model (blstm-local), we used an attention window size of 5 (tuned on the dev set), and it gives an improvement of 0.94% over the brnn supertagger (xu et al., 2016), achieving an accuracy on par with the beam-search (size 12) model of vaswani et al. (2016) that is enhanced with a language model. despite being able to consider wider contexts than the local model, the global attention model (blstmglobal) did not show further gains, hence we used blstm-local for all parsing experiments below. 

table 4 also shows the results for the xf1 models (lstm-xf1), which use all four types of embeddings. decoding the xf1 model with greedy inference only slightly decreased recall and f1, and this resulted in a highly accurate deterministic parser. on the test set, our xf1 greedy model gives 2.67% f1 improvement over the greedy model in xu et al. (2016), and the beam-search xf1 model achieves an f1 improvement of 1.34% compared with the xf1 model of xu et al. (2016). 

the overall performances of all parsers are shown in table 1. table 1 shows that, for both training conditions, the parser that has the best robustness score in the esl domain has also high robustness for the mt domain. the training conditions do matter – malt performs better when trained from tweebank than from the ptb. when trained on the ptb, all parsers are comparably robust on esl data, while they exhibit more differences on the mt data, and, as expected, everyone’s performance is much lower because mt errors are more diverse than esl errors. we expected that by training on tweebank, parsers will perform better on esl data (and maybe even mt data), since tweebank is arguably more similar to the test domains than the ptb, we also expected tweebo to outperform others. on the one hand, the highest parser score increased from 93.72% (turbo trained on ptb) to 94.36% (malt trained on tweebank), but the two neural network parsers performed significantly worse, most likely due to the small training size of tweebank. interestingly, although syntaxnet has the lowest score on esl, it has the highest score on mt, showing promise in its robustness. 

table 2 shows the test results using our best performing model (ensemble with syntax features). our performance is comparable to the cky parser of (artzi et al., 2015), which we use to bootstrap our system. this demonstrates the ability of our parser to match the performance of a dynamic-programming parser, which executes significantly more operations per sentence. 

table 6 shows the results. the first two use random word embedding for initialization, and the first one does not use label refinement. from the results, it can be concluded that queryparser consistently outperformed competitors on query parsing task. pretrained word2vec embeddings improve performance by 3-5 percent, and the postprocess of label refinement also improves the performance by 1-2 percent. table 6 also shows that conventional depencency parsers trained on sentence dataset relies much more on the syntactic signals in the input. while stanford parser and mstparser have similar performance to our parser on func dataset, the performance drops significantly on all and nofunc dataset, when the majority of input has no function words. 

table 2 shows our experimental results comparing trem and baseline models using descriptions. in general, contingency-based methods (trem, trem w/o sr and trem w/o cc) outperform summarization-based methods. our contingency assumptions are verified as adding cc and sc both improve trem with summarization component only. moreover, the best result is achieved by the complete trem model with both contingency factors. from the summarization-based methods, we can see that our trem-summ gets higher rouge scores than two ilp approaches. additionally, we note that the performance of ilp-ext is poor. 

table 3 reports the spam and nonspam review detection accuracy of our methods smtl-llr and mtllr against all other baseline methods. in terms of 5% significance level, the differences between smtl-llr and the baseline methods are considered to be statistically significant. under symmetric multi-task learning setting, our methods smtl-llr and mtl-lr outperform all other baselines for identifying spam reviews from nonspam ones. mtl-lr achieves the average accuracy of 85.2% across the three domains, which is 3.1% and 3.4% better than lr and svm trained in the single task learning setting, and 1.2% higher than mtrl. training with a large quantity of unlabeled review data in addition to labeled ones, smtl-llr improves the performance of mtl-lr, and achieves the best average accuracy of 87.2% across the domains, which is 3.2% better than that of mtrl, and is 4.3% better than tsvm, a semi-supervised single task learning model. pu gives the worst performance, because learning only with partially labeled positive review data (spam) and unlabeled data may not generalize as well as other methods. 

as table 5 shows, the best performance of annotators is highlighted and regarded as the upper bound performance (ub) of the nld task on our dataset. 

the results are listed in table 6. the tests reveal that the performance of nlds is significantly better than sts and rae, no significant differences could be found between ub and nlds. these results demonstrate that nlds would represent an effective approach for nld that is on pair with annotator judgement and overcomes state-of-the-art approaches for related tasks. 

table 3 compares the bleu scores for various translation systems. the orthographic syllable level system is clearly better than all other systems. it significantly outperforms the character-level system (by 46% on an average). the system also outperforms two strong baselines which address data sparsity: (a) a word-level system with transliteration of oov words (10% improvement), (b) a morph-level system with transliteration of oov words (5% improvement). the os-level representation is more beneficial when morphologically rich

in table 3 we compare the binary classification model (our system binary) against the alignment model (our system alignment) and show that the latter outperforms the former by a margin of 3.2 points in f-score, achieving a micro f1-score of 28.58 across the three test corpora, thus confirming the benefits of joint inference. the only corpus in which joint inference did not help was stock which has on average shorter event chains per document (minard et al., 2015) and thus renders joint anchoring less likely to be useful. 

comparison of the proposed techniques in table 1 shows that deep fusion performs well on both meteor and bleu, incorporating glove embeddings substantially increases meteor, and combining them both does best. 

table 2 presents the results of the cross-domain experiment, whereby we train a model on mr and test on cr, and vice versa, to measure the robustness of the different regularization methods in a more real-world setting. once again, we see that our regularization method is superior to word-level dropout and the baseline cnn, and the techniques combined do very well, consistent with our findings for synthetic noise. 

table 2 shows the results of word alignment evaluations, where none denotes that the model has no constraint. in kftt and btec corpus, itg achieved significant improvement against sym and none on ibm model 4 (p ? 0.05). however, in the hansard corpus, itg shows no improvement against sym. 

the results of this experiment reported in table 2 are similar to the cross-fold evaluation, and in this case the contribution of n features is even more accentuated. indeed, the absolute f1 of n and b ? n is slightly higher on test data, while the f-measure of b decreases slightly. on test data, n \ s is not found to outperform n. interestingly, n \ s is the only configuration having higher recall than precision. 

table 3 shows the empirical benchmark results. the dynamic model achieves the best results in all the metrics. the static model outperforms the baseline, but is inferior to the dynamic model. in addressee selection (adr), the baseline model achieves around 55% in accuracy. compared with the baseline, our proposed models achieve better results, which suggests that the models can select the correct addressees that spoke at more previous time steps. in particular, the dynamic model achieves 68% in accuracy, which is 7 point higher than the accuracy of static model. in response selection (res), our models outperform the baseline. compared with the static model,the dynamic model achieves around 0.5 point higher in accuracy. 

table 4 compares the performance of the models for different numbers of agents in a context. in addressee selection, the performance of all models gradually gets worse as the number of agents in the context increases. however, compared with the baseline, our proposed models suppress the performance degradation. in particular, the dynamic model predicts correct addressees most robustly. in response selection, unexpectedly, the performance of all the models gets better as the number of agents increases. 

by comparing the rows labeled da and da +ds in table 2 (a) and table 2 (b), we see that in both the headlines and the images datasets, adding sentence level information improves the untyped score, lifting the stricter typed score f1. on the headlines dataset, incorporating sentence-level information degrades both the untyped and typed alignment quality because we cross-validated on the typed score metric. from the row da + ds in table 2(a), we observe that the typed score f1 is slightly behind that of rank 1 system while all other three metrics are significantly better, indicating that we need to improve our modeling of the intersection of the three aspects. further, we see that even our base model that only depends on the alignment data offers strong alignment f1 scores. 

we first compare our parser with state-of-the-art neural transition-based dependency parsers on ptb and ctb. for english, we also compare with state-of-the-art graph-based dependency parsers. the results are shown in table 1. it can be seen that the biatt-dp outperforms all other graph-based parsers on ptb. compared with the transition-based parsers, it achieves better accuracy than chen and manning (2014), which uses a feed-forward neural network, and dyer et al. (2015), which uses three stack lstm networks. compared with the integrated parsing and tagging models, the biatt-dp outperforms bohnet and nivre (2012) but has a small gap to alberti et al. (2015). 

it can be observed from table 3 that the biattdp has highly competitive parsing accuracy as stateof-the-art parsers. moreover, it achieves best uas for 5 out of 12 languages. for the remaining seven languages, the uas gaps between the biatt-dp and state-of-the-art parsers are within 1.0%, except swedish. the biatt-dp consistently outperforms both parser by up to 5% absolute uas score. the last three columns of table 3 show the recall of crossed arcs, that of uncrossed arcs, and the percentage of crossed arcs in the test set. for these four languages, the biatt-dp achieves better uas than that reported in (pitler and mcdonald, 2015). more importantly, we observe that the improvement on recall of crossed arcs (around 10–18% absolutely) is much more significant than that of uncrossed arcs (around 1–3% absolutely), which indicates the effectiveness of the biatt-dp in parsing languages with non-projective trees. 

table 4 shows a comparison between the performance of our system on this subset of the jsem problems and the performance of the rte system for english in mineshima et al. (2015) on the corresponding problems in the fracas dataset. the total accuracy of our system is comparable to that of mineshima et al. (2015). comparison between gold parses and system parses shows that correct syntactic disambiguation improves performance. 

the top section of table 2 shows development set results comparing modeling effectiveness for atomic  and sequence model architectures and different features. the fourier feature transformation generally improves on raw hsv vectors and discretized embeddings. 

the results are shown in table 4. the proposed method achieved significantly better automatic evaluation scores than the baseline for all the language pairs except the bleu score of en → ja direction. also, the decoding time is reduced by about 60% relative to that of the baseline. our tree-based model is better than the conventional models except zh → ja, where the accuracy of chinese parsing for the input sentences has a bad effect. 

experimental results in table 1 show some interesting results. first, with the same alignment, j joint optimization works best than other optimization strategies (lines 3 to 6). unfortunately, breaking down the network into two separate parts (a and t) and optimizing them separately do not help (lines 3 to 5). second, when we change the training alignment seeds (zh → en, gdfa, and maxent) nmt model does not yield significant different results (lines 6 to 8). third, the smoothed transformation (j + gau.) gives some improvements over the simple transformation (the last two lines), and achieves the best result (1.2 better than lvnmt, and 0.3 better than tree-to-string). in terms of bleu scores, we conduct the statistical significance tests with the signtest of collins et al. (2005), the results show that the improvements of our j + gau. over lvnmt are significant on three test sets (p < 0.01). at last, the brevity penalty (bp) consistently gets better after we add the alignment cost to nmt objective. 

the rows in table 1 show, respectively, the results for the original embeddings, the basic mapping proposed by mikolov et al. (2013b) (cf. section 2) and the addition of orthogonality constraint (cf. section 2.1), with and without length normalization and, incrementally, mean centering. the results show that the orthogonality constraint is key to preserve monolingual performance, and it also improves bilingual performance by enforcing a relevant property (monolingual invariance) that the transformation to learn should intuitively have. the contribution of length normalization alone is marginal, but when followed by mean centering we obtain further improvements in bilingual performance without hurting monolingual performance. 

table 2 shows the results for our best performing configuration in comparison to previous work. 

table 2 shows the results of mt n combining our bisyndata (denoted as mt nbi) on the pdtb. on the macro f1, mt nbi gains an improvement of 4.17% over st n. the improvement is significant under one-tailed t-test (p<0.05). a closer look into the results shows that mt nbi performs better across all relations, on the precision, recall and f1 score, except a little drop on the recall of cont. the greatest improvement is observed on comp, up to 6.36% f1 score. overall, using bisyndata under our multi-task model achieves significant improvements on the english drrimp. 

finally, table 3 compares the syntactic fluency of the output. the models with no knowledge of syntax are able to recover a higher proportion of gold arcs. 

evaluation results are shown in table 3 on only yg15k due to limited space, where we report hits@1 instead of hits@10. 

table 5 reports the results on the test sets. the results indicate that time-aware embedding outperforms all the baselines consistently. 

table 4 compares the different violation-based learning objectives, as discussed in section 5. our novel all-violation updates outperform the alternatives. 

table 5 shows the performance of our models alongside human performance on the v1.0 of development and test sets. the logistic regression model significantly outperforms the baselines, but underperforms humans. 

table 3 shows the performance on the test set for variations of our method and that of the human annotators. our method is denoted as proposed, while its variations include a method with only monotonic alignment (monotonic), without em (w/o em), and a method aligning only 1-best trees (1-best tree). our method significantly outperforms the others as it achieved the highest recall and precision for alignment quality. our recall and precision reach 92% and 89% of those of humans, respectively. comparing our method and the one aligning only 1-best trees demonstrates that the alignment of parse forests largely contributes to the alignment quality. although we confirmed that aligning larger forests slightly improved recall and precision, the improvements were statistically insignificant. finally, our method achieved 98% reachability, where 2% of unreachable cases were due to the beam search. these results show the ability of our method to accurately align paraphrases with divergent phrase correspondences. 

table 4 summarizes our results. we observe that all neural models achieve higher f1 scores than the logistic regression and patterns systems, which demonstrates the effectiveness of neural models for relation extraction. although positional embeddings help increase the f1 by around 2% over the plain cnn model, a simple (2-layer) lstm model performs surprisingly better than cnn and dependency-based models. lastly, our proposed position-aware mechanism is very effective and achieves an f1 score of 65.4%, with an absolute increase of 3.9% over the best baseline neural model (lstm) and 7.9% over the baseline logistic regression system. we also run an ensemble of our position-aware attention model which takes majority votes from 5 runs with random initializations and it further pushes the f1 score up by 1.6%. 

table 5 presents our results. we find that: (1) by only training our logistic regression model on tacred (in contrast to on the 2 million bootstrapped examples used in the 2015 stanford system) and combining it with patterns, we obtain a higher hop-0 f1 score than the 2015 stanford system, and a similar hop-all f1; (2) our proposed position-aware attention model substantially outperforms the 2015 stanford system on all hop-0, hop-1 and hop-all f1 scores. combining it with the patterns, we achieve a hop-all f1 of 26.7%, an absolute improvement of 4.5% over the previous state-of-the-art result. 

5.4 final results table 3 shows the final results on the g&c 16 and c&j08 datasets, respectively. we compare the results of our final model with the following baselines:. it can be seen from the table that the statistical counting-based models pmi and bigram significantly underperform the neural network models event-comp, rnn and memnet, which is largely due to their sparsity and lack of semantic representation power. under our event representation, bigram does not outperform pmi significantly either, although considering the order of event pairs. direct comparison between event-comp and rnn shows that the event-pair model gives comparable results to the strong-order lstm model. by considering both pairwise relations and chain temporal orders, our method significantly outperform both event-comp and rnn (p − value < 0.01 using t-test), giving the best reported results on both datasets. 

table 1 shows that swear outperforms the best, results for various models reported in both publications, including the hierarchical models softattend and reinforce presented by choi et al. (2017). interestingly, softattend computes an attention over sentence encodings, analogous to swear’s attention over overlapping window encodings, but it does so on the basis of less powerful encoders (bow or convolution vs rnn), suggesting that the extra computation spent by the rnn provides a meaningful boost to performance. 

hewlett et al.(2016) grouped properties by answer distribution: categorical properties have a small list of possible answers,  such as countries, relational properties have an open set of answers, such as spouses or places of birth, and date properties (a subset of relational properties) have date answers, such as date of birth. we reproduce this grouping in table 2 to show that swear improves performance for relational and date properties, demonstrating that it is better able to extract precise information from documents. 

table 4 and 5 show the results of swear and semi-supervised models with pretrained and fixed embeddings. results show that swear-ss always improves over swear at small data sizes, with the difference become dramatic as the dataset becomes very small. vrae pretraining yields the best performance. however, initialization with pretrained vrae model leads to a substantial improvement on both subsamples. 

table 6 shows the results of semi-supervised reviewer models. when trained on 1% of the training data, swear-mlr and the supervised swear model perform similarly. without using skip connections between embedding and hidden layers, the performance drops. the swearpr model further improves mean f1 and outperforms the strongest swear-ss model,  even without fine-tuning the weights initialized from the autoencoder. 

table  5  shows  the  results  of  evaluating  the four  main  models  on  adversarial  examples  generated by running either addsent or addany against  each  model.  for both  addsent and  addany,  examples  transferred slightly better between single and ensemble versions of the same model. 

table 4 shows the results. 1) our model outperforms all baselines significantly. compared with baselines, the accuracy improvement on test set is at least 13.7%. compared with narrative event chain model, our model achieves a 16.3% accuracy improvement by considering richer commonsense knowledge, rather than only narrative event knowledge.  compared with dssm and rnn, which model all relations  between  two  elements  using  a  single  semantic similarity score, our model achieves significant accuracy improvements by modeling, distinguishing and selecting different types of commonsense relations between  different kinds of elements. 

table 5 shows the results. we can see that using a single kind of knowledge is insufficient for commonsense machine comprehension: all single-knowledge settings cannot achieve competitive performance to the all-knowledge setting. 

table 7 show the results. we can see that: 1) the minimum cost mechanism cannot achieve competitive performance, we believe this is because the selection of rules should not depend on the cost of them, and considering all valid inferences is critical for reasoning; 2) our attention mechanism can effectively model the inference rule selection possibility. compared with the average cost mechanism, our method achieved a 6.36% accuracy improvement. 

table 8 show the results. we can see that removing negation rules will significantly drop the system performance, which confirm the effectiveness of our proposed negation rules. 

table 3 lists the results of the best feature-based (lr all features) and deep learning (cnn-rand) systems, as well as single feature groups (averages over all source domains, results for individual source domains can be found in the supplementary material to this article). we note the biggest performance drops on the datasets which performed best in the indomain setting (mt and pe). for the lowest scoring datasets, oc and wtp, the differences are only marginal when trained on a suitable dataset (vg and  oc,  respectively). in particular, as opposed to the in-domain experiments, the difference of the claim-f1 measure between the feature-based approaches and the deep learning approaches is striking. in the feature-based approaches, on average, a combination of all features yields the best results for both macro-f1and claim-f1. looking at the best overall system (lr withall features), the average test results when training on different source datasets are between 54% macro-f1 resp. 23% claim-f1 (both mt) and 58% (vg) resp. 34% (oc). depending on the goal that should be achieved, training on vg (highest average macro-f1) or oc (highest average claim-f1) seems to be the best choice when the domain of test data is unknown (we analyze this finding inmore depth in¤6). mt clearly gives the best results as target domain, followed by pe and vg. 

table 2 shows that the qg system incorporating our best performing sentence extractor outperforms its lreg counterpart across metrics. 

firstly, we perform evaluation on the whole articles and table 1 shows the comparison results. we can see that our approach outperforms all the baseline methods with respect to rouge-2 and rouge-su4. the submodular method achieves the highest rouge-1 score, but our approach also achieves very high rouge-1 score, which is very close to that of the submodular method. 

table 2 shows the comparison results based on this evaluation protocol (two part evaluation i). we then compute and average the rouge scores of the matched parts. 

table 4 shows the manual evaluation results. we can see that our proposed approach can produce news overview articles with better content coverage, readability and overall responsiveness than baseline methods. 

table 1 shows the comparisons of feature weights. we can see that within-document event linking mainly relies on the euclidean distance and cosine similarity scores calculated using event word features, with a reasonable amount of weight assigned to overlapped arguments’ embedding as well. however, only very small weights were assigned to the similarity and distance scores calculated using context embeddings. in contrast, in the classifier trained with cross-doc coreferent event mention pairs, the highest weight was assigned to the cosine similarity score calculated using context embeddings of two event mentions. additionally, both the cosine similarity score calculate dusing event word embeddings and the overlapped argument features were assigned high weights as well. 

table 3 shows the comparison results for both within-document and cross-document event coreference resolution. in the first stage of iterative merging, using two distinct wd and cd classifiers for corresponding wd and cd merges yields clear improvements for both wd and cd event coreference resolution tasks, compared with using one common classifier for both types of merges. in addition, the second stage of iterative merging further improves both wd and cd event coreference resolution performance stably by leveraging second order event inter-dependencies. the improvements are consistent when measured using various coreference resolution evaluation metrics. 

table 3 shows the mos results with standard error. it can be seen that all the systems based on selective sampling are significantly better than vanilla sampling baseline. when restricting outputâ€™s style and/or topic, the mos score results of most systems do not decline significantly except singer-songwriter, which attempts to generate lyrics-like outputs in response to political debate questions, resulting in uninterpretable strings. table 3 also shows the likelihood of being labeled as jfk for different methods. it is encouraging that finetune based approaches have similar chances as the rank system which retrieves sentences directly from jfk corpus, and are significantly better than the selective sampling baseline. 

table 1 compares our results with those obtained by the baselines. our two models, uncertainty propagation and average embedding, outperform all the baselines. among these two models, uncertainty propagation, which is more analytically grounded, outperforms the average embedding model. using the true current label during training seems to degrade performance compared to using the predicted label, which is expected, since the true label is not available during testing. the scheduled sampling method performs similarly to the predicted-label method for the maptask corpus, and outperforms this method for the switchboard corpus. 

table 1 shows the performance on test data. for all types of users, the hrl-based agent yielded more robust dialogue policies outperforming the hand-crafted rule-based agents and flat rl-based agent measured on success rate. it also needed fewer turns per dialogue session to accomplish a task than the rule-based agents and flat rl agent. 

we observe that while in the sb bucket both the baseline and u u r perform equally well, for all the other buckets u u r massively outperforms the baseline. the overall macro and micro precision/recall as shown in table 4 further strengthens our observation that u u r is a better metric than the baseline. 

table 6 shows the bucket-wise precision and recall for u u r and the baseline metrics with respect to two new ground truths. for the young population once again the number of words in each bucket for all the three sets is the same thus making the values of the precision and the recall same. in fact, the precision/recall for this ground truth is exactly same as in the case of the original ground truth. 

table 7 presents results per dimension with the best overall combination of features (verb + personx + persony + personx persony). all dimensions obtain overall f-measures between 0.65 and 0.83 (last column). results per label are heavily biased towards the most frequent label per dimension (figure2), although it is the case that the models we experiment with predict both 1 and -1 for all dimensions. as stated above,none  of  them  predict 0,  but  this  limitation  does not substantially penalize overall performance because of the low frequency of this label. the model obtains the same f-measures for1and-1with concurrent dimension (0.76), and the labels of this dimension are virtually distributed uniformly (46.4% vs. 47.1%, figure 2). similarly, f-measures for 1 and -1 with spatially near and active dimensions are similar (0.67 vs. 0.76 and 0.76  vs. 0.58), and the labels are distributed relatively evenly in our corpus (40.4% vs 51.4% and.58.4% vs. 35.3%). finally, f-measures per label with other dimension  are  biased  towards  the  most  frequent  label. for example, only 15% of all pairs of people have an enduring relationship (figure 2), and the f-measure for 1 with temporary dimension is much higher (0.91) that for -1 (0.16). 

table 4 reports on the obtained results with the different configurations. 

table 5 reports on the results obtained by the best configuration, i.e., lr + all features, per each category. 

to address the task of factual vs opinion arguments classification, we apply the supervised classification algorithms described in section 2.1. table 6 reports on the obtained results. 

table 8 reports on the obtained results. 

table 2 is the evaluation results of different ordered rules. the rule r4 can significantly boost the success rate (comparing line 2 with line 1),while the rule r1* can both boost the success rate and decrease the dialogue length (comparing line 3 with line 1). the combination of r4 and r1* takes respective advantages (comparing line 4 with line 1, line 2 and line 3). 

table 3 shows the results. lexstat performs slightly better than the heuristic baseline, but both are limited by their inability to relate words that have non-identical definitions. our system, semaphor, finds approximately three times as many cognate sets as lexstat, and over 75% of those sets are complete with respect to the gold annotation. the purity of the produced clusters indicates that there are many more hits than misses in the system output. 

table 1 compares a simple direct transfer baseline, the previous state-of-the-art in cross-lingual ner, and our proposed algorithm. for these languages, we beat the baseline by 25.4 points, and the state-of-the-art by 5.9 points. 

table 4 shows the distribution of training data and the f-score of each single type. we can see that, for some slot types, such as per:dateofbirthandper:age, the entity types of their candidate fillers are easy to learn and differentiate from other slot types, and their indicative words are usually explicit, thus our approach can get high f-score with limited training data (less than 507 instances). in contrast, for some slots,such as org:location of headquarters, their clues are implicit and the entity types of candidate filler sare difficult to be inferred. although the size of training data is larger (more than 1,433 instances), the f-score remains quite low. 

table 2 shows development set results on darkode for each of the four systems for each metric described in section 3. the post-level system underperforms the binary classifier on the token evaluation, but is superior at not only postlevel accuracy but also product type f1. comparing the automatic systems with human annotator performance we see a substantial gap. note that our best annotatorâ€™s token f1 was 89.8, and np post accuracy was 100%; a careful, well-trained annotator can achieve very high performance, indicating a high skyline. 

table 5 confirms this intuition: it shows product out-of-vocabulary rates in each of the four forums relative to training on both darkode and hack forums, along with recall of an np-level system on both previously seen and oov products. as expected, performance is substantially higher on in vocabulary  products. oov rates of a darkode-trained system are generally lower on new forums,indicating that that forum has better all-around product coverage. a system trained on darkode is therefore in some sense more domain-general than one trained on hack forums. 

table 3 shows the results of running the models with f1-score tuning on genia dataset. besides the mention hypergraph baseline, we also make comparisons with the system of finkel and manning (2009) that can also support overlapping mentions. we see that the mention hypergraph model matches the performance of the constituency parser-based model of finkel and manning

table 1 reports the results of our neural sequence tagging model nn-crf in both supervised and semi-supervised learning (ulm and graph-based), and compares them with the baselines and the state-of-the-art (best semeval system (augenstein et al., 2017)). augenstein and sã¸gaard (2017) use a multi-task learning strategy to improve the performance of supervised keyphrase classification, but they only report dev set performance on semeval task 10, we also include their result here and refer it as

in table 2 we present results for our models on ace-2004. our model outperforms the wikifier and vinculum systems that only use information from wikipedia, and aida, by a significant margin, indicating its possible over-fitting to the conll domain. hence, it shows our model's ability to perform accurate linking across different datasets without using domain-specific information. 

we compare our relation extraction models against previous work on the cause effect subset of the data, table 2 shows our relational similarity model, without the use of sparse features or external resources such as wordnet, outperforms recent state-of-the-art treelstm model (miwa and bansal, 2016). it also shows bigru model is reasonably competitive on this dataset, which is why we use it in our baseline system for comparison purpose. 

table 2 shows performance of different outlier document detection methods. it can be observed that our method outperforms all the baselines in both data sets. in both data sets, vmf-q can achieve a 45% to 135% increase from baselines in terms of recall by examining the top 1% outliers. generally, performances of most methods are lower in the arnet data set comparing to nyt, potentially because the relatively short document lengths and more technical terminologies in arnet. 

we also conduct a human assessment on the explanation chains produced by the two reasoners, asking people to choose more convincing explanation chains for each feature-target pair. table 9 shows their relative preferences. 

table 2 reports the results of thread-subtitle matching. we can notice that there are some anomalies in p@3 and p@5 results. in the first mooc (people and network), video subtitles contain relatively less words, and therefore it is hard to get effective representations. overall, the proposed models can achieve better performance than baselines, and we highlight the precision@1 results. compared to hdv which also considers the streaming documents, our model is better at every task. 

the results of these tests are presented in table 2 as precision, recall, and f1 scores for the positive class, i.e., the opt-out class. using the f1 scores as the primary evaluation metric, it appears that all features help in classification. the unigram, topic distribution, nonterminal, and modal verb and optout phrase features contribute the most to performance. including all the features results in an f1 score of 0.735. ablation test without unigram features resulted in the lowest f1 score of 0.585, and by analyzing features with higher logistic regression weights, we found n-grams such as unsubscribe to have intuitively high weights. 

table 4 shows the result of our model on both the development and test sets. beam search improves the f-score form 87.1% to 87.5%, which is consistent with the finding of buckman et al.(2016) on the lstm parser of (dyer et al., 2015) (improvements by about 0.3 point). scheduled sampling achieves the same improvements compared to beam-search. 

we are curious about the performance of our system using all the dps files. table 6 shows the result on the dps files. our model achieves a 88.1% f-score by using more training data, obtaining 0.6 point improvement compared with the system training on mrg files. the performance is far better than the sequence labeling methods that use dps files for training. 

table 7 shows the results of chinese disfluency detection. our model obtains a 2.4 point improvement compared with the baseline bi-lstm model and a 5.3 point compared with the baseline crf model. 

table 6 presents the performance for a subset of error types that are affected the most re-ranking camb16smt on before and after the fce test set. the largest improvement is observed in replacement errors referring to possessive nouns (r:noun:poss) and verb agreement (r:verb:sva); and in unnecessary errors referring to adverbs (u:adv), determiners (u:det), pronouns (u:pron), and verb tense (u:verb:tense). 

table 1 shows the full set of experimental results on the aesw development and test data. the char+bi+dom model is stronger than the word+bi+dom and charcnn+dom models by 2.9 m2 (0.2 gleu) and 3.3 m2 (0.3 gleu), respectively. the sequence-to-sequence models were also more effective than the smt models, as shown in table 1. we find that training with target diffs is beneficial across all models, with an increase of about 5 m2 points for the word+bi model, for example. adding +dom information slightly improves effectiveness across models. 

table 1 shows that all models based on the vector space achieve similar performance in terms of spearman's ρ (except svm-w2v which yields lower performance). the baseline method based on unigram models was outperformed by 0.1+ point. so we select densifier-lsa as a representative for our fsmt system. 

table 1 shows the translation performance in terms of bleu score. clearly, the proposed approaches significantly outperforms baseline in all cases. 

table 6 shows results with or without using the predicted pos tags in the preorderer, as well as including the features used by the tagger in the reorderer directly and only training the downstream task. the preorderer that includes a separate network for pos tagging and then extracts features over the predicted tags is more accurate and smaller than the model that includes all the features that contribute to a pos tag in the reorderer directly. 

table 5 summarizes the performance on the development set. the lstm outperforms the other models when only using text as input; however the other two models improve substantially with adding liwc features, particularly in the case of the multinomial naive bayes model. in contrast, the liwc features do not improve the neural model much, indicating that some of this lexical information is perhaps redundant to what the model was already learning from text. 

as shown in table 1, our system outperforms the other methods with an accuracy of 0.86 in the word-intrusion task with four key concepts in each cluster, while it decreases to 0.67 if we extend the evaluation to include eight key concepts. 

our quantitative results are summarized in the table 2. on the development set, the number of verbs whose bias exceed the original bias by over 5% decreases 30.5% (viol.). overall, we are able to significantly reduce bias amplification in vsrl by 52% on the development set (amp. bias). 

we report results for the main task accuracy and the representation privacy in table 3 for the +demo setting and in table 4 for the raw setting. the ‘standard’ columns contain the accuracy and privacy of the base model described in section 2. the next columns present the absolute variation in accuracy and privacy for the three defense methods presented in section 3: multidetasking, adversarial generation, and declustering. despite a model selection based on accuracy, they lead to an improvement in privacy on all datasets, except on the france subcorpus. in most cases,we observe only a small decrease in accuracy,or even an improvement at times (e.g. multidetasking on the germany dataset, raw setting), thus improving the tradeoff between the utility and the privacy of the text representations. 

we report results for the main task accuracy and the representation privacy in table 3 for the +demo setting and in table 4 for the raw setting. the ‘standard’ columns contain the accuracy and privacy of the base model described in section 2. the next columns present the absolute variation in accuracy and privacy for the three defense methods presented in section 3: multidetasking, adversarial generation, and declustering. despite a model selection based on accuracy, they lead to an improvement in privacy on all datasets, except on the france subcorpus. in most cases,we observe only a small decrease in accuracy,or even an improvement at times (e.g. multidetasking on the germany dataset, raw setting), thus improving the tradeoff between the utility and the privacy of the text representations. 

all methods are effective to some extent, table 4 summarizes the results. increasing the capacity of the adversarial network helped reduce the protected attributeâ€™s leakage, though different capacities work best on each setup. on the sentiment/race task, none of the higher dimensional adversaries worked better than the 300-dim one, on the pan16 dataset it did. on pan16/gender the 8000-dim adversary performed best, and on pan16/age, the 500-dim one. increasing the weight of the adversary through the lambda parameter also has a positive effect on the result (except on the sentiment/race pair). the adversarial ensemble method with 2 adversaries achieves 57.4% on sentiment/race, as opposed to 56.0% with a single one, but when using 5 different adversaries, we achieve 54.8%. on the pan16 dataset larger ensembles are more effective. 

table 1 shows the precision, recall, and f1 for all models on the the test partition. prostruct significantly outperforms the baselines, suggesting that world knowledge helps prostruct avoid spurious predictions. this hypothesis is supported by the fact that the proglobal model has the highest recall and worst precision, indicating that it is over-generating state change predictions. conversely, the prolocal model has the highest precision, but its recall is much lower, likely because it makes predictions for individual sentences, and thus has no access to information in surrounding sentences that may suggest a state change is occurring. 

table 2 compares our wikipage retriever with the one in (thorne et al., 2018), which used a document retriever from drqa (chen et al., 2017). our document retrieval module surpasses the competitor by a big margin in terms of the coverage of gold passages: 89.63% vs. 55.30% (k = 5 in all experiments). 

table 3 lists the performances of baselines and the twowingos variants on fever (dev&test). from the dev block, we observe that:. twowingos (from "share-cnn") surpasses prior systems in big margins. in the three setups - "pipeline", "diff-cnn" and "share-cnn" - of coarse-coarse, "pipeline" gets better scores than (thorne et al., 2018) in terms of evidence identification. "share-cnn" has comparable f1 as "diff-cnn" while gaining a lot on noscoreev (72.32 vs. 39.22) and scoreev (50.12 vs. 21.04). both "diff-cnn" and "share-cnn" perform better than "pipeline" (except for the slight inferiority at scoreev: 21.04 vs. 22.26). two-channel fine-grained representations show more effective than the single-channel counterpart in claim verification (noscoreev: 78.77 vs. 75.65, scoreev: 53.64 vs. 52.65). in the last three rows of dev, there is no clear difference among their evidence identification scores. in the test block, our system (fine & fine (two)) beats the prior top system across all measurements by big margins f1: 47.15 vs. 17.47; scoreev: 54.33 vs. 31.87; noscoreev: 75.99 vs. 50.91. in both dev and test blocks, we can observe that our evidence identification module consistently obtains balanced recall and precision.  in contrast, the pipeline system by thorne et al. (2018) has much higher recall than precision (45.89 vs. 10.79). 

table 2 shows the results on coco. among the directly comparable models, our model is arguably the best and outperforms the existing models except in terms of bleu-4. especially, our model outperforms the state-of-the-art substantially in spice and meteor. 

table 6 shows the performance on the online coco evaluation server. nonetheless, our model is second only to up-down (anderson et al. 2018) and surpasses almost all the other models in published work, especially when 40 references are considered. 

table 1 illustrates the performance comparisons on the didemo dataset. in addition to mcn, we also compare with the baseline moment frequency prior (mfp) in (hendricks et al., 2017), which selects segments corresponding to the positions of videos in the training dataset with most annotations. first, tgn with different features can significantly outperforms the "prior baseline" mfp, which retrieves segments corresponding to the most common start and end points in the dataset. second, it can be observed that with the same visual features, specifically vgg16 and optical flow, tgn significantly outperforms mcn. and the performance of tgn with optical flow is better than that with vgg16. by fusing the results obtained by vgg16 and optical flow together, the performance can be further boosted, as demonstrated by tgn-fusion and mcn-fusion. with tef, the performance of mcn can be significantly improved. however, it is still inferior to our proposed tgn. 

table 2 shows the experimental results for named entities on the original weiboner dataset. in the first block of table 2, we give the performance of the main model and baselines proposed by peng and dredze (2015). in the second block of table 2, we report the performance of the main model and baselines proposed by peng and dredze (2016). aiming to incorporate word boundary information into the ner task, they propose an integrated model that can joint training cws task, improving the f1 score from 46.20% to 48.41% as compared with pipeline model (pipeline seg.repr.+ner). in the last block of table 2, we give the experimental result of our proposed model (bilstm+crf+adversarial+self-attention). we can observe that our proposed model significantly outperforms other models. compared with the model proposed by peng and dredze (2016), our method gains 4.67% improvement in f1 score. 

table 3 lists the performance of the latest models and our proposed model on the updated dataset. in the first block of table 3, we report the performance of the latest models. the model proposed by peng and dredze (2015) achieves f1 score of 56.05% on overall performance. the unified model improves the f1 score from 54.82% to 58.23% compared with the model proposed by he and sun (2017a). in the second block of table 3, we give the result of our proposed model. it can be observed that our proposed model achieves a very competitive performance. compared with the latest model proposed by he and sun (2017b), our model improves the f1 score from 58.23% to 58.70% on overall performance. 

table 4 lists the comparisons on sighanner dataset. we observe that our proposed model achieves new state-of-the-art performance. in the first block, we give the performance of previous methods for chinese ner task on sighanner dataset. although the model achieves competitive performance, giving the f1 score of 89.21%, it suffers from the error propagation problem. in the second block, we report the result of our proposed model. compared with the state-of-the-art model proposed by luo and yang (2016), our method improves the f1 score from 89.21% to 90.64% without any additional features, which demonstrates the effectiveness of our proposed model. 

table 5 provides the experimental results of our proposed model and baseline as well as its simplified models on sighanner dataset and weiboner dataset. from the experimental results of table 5, we have following observations:. bilstm+crf+transfer improves f1 score from 89.13% to 89.89% as compared with bilstm+crf on sighanner dataset and achieves 1.08% improvement on weiboner dataset, which indicates the word boundary information from cws is very effective for chinese ner task. by introducing adversarial training, bilstm+crf+adversarial boosts the performance as compared with bilstm+crf+transfer model, showing 0.15% and 0.36% improvement on sighanner dataset and weiboner dataset, respectively. when compared with bilstm+crf, the bilstm+crf+self-attention significantly improves the performance on the two different datasets with the help of information learned from self-attention, which verifies that the self-attention mechanism is effective for chinese ner task. we observe that our proposed adversarial transfer learning framework and self-attention lead to noticeable improvements over the baseline, improving f1 score from 51.01% to 53.08% on weiboner dataset and giving 1.51% improvement on sighanner dataset. 

table 2 shows our results on the ud datasets. there are some differences across languages. for french, german, indonesian and russian, vslg does not show improvement when using unlabeled data. this may be resolved with better tuning, since the model actually shows improvement on the dev set. note that results reported by zhang et al. (2017) and ours are not strictly comparable as their word embeddings were only pretrained on the ud training sets while ours were pretrained on wikipedia. we include these results to show that our baselines are indeed strong compared to prior results reported in the literature. 

the results in table 3 suggest that attaching the reconstruction and classification losses to the same latent variable (z) harms accuracy although attaching the classifier to z effectively gives the classifier an extra layer. 

for following experiments, we use multicluster and multicca as baselines 14. table 4 shows the results. we observe that both multicca and corrnet approaches are sensitive to the size of the bilingual lexicons. our approach on the other hand can maintain high performance, even when the size of bilingual lexicons is reduced to 250. the performances of multicluster based on various sizes of bilingual dictionary are close because it jointly trains the embedding of multiple languages from scratch and by default takes advantage of identical strings among all the languages. 

table 7 shows the performance. for most testing languages, our approach achieves better performance than multicca and multicluster. the closer that the languages are, such as amharic and tigrinya, the better performance is achieved. 

the results listed in table 4 indicate several trends. first and foremost, in terms of model-wise comparisons, we observe that our proposed alterations of both vecmap and muse improve their quality in a consistent manner, across most metrics and data configurations. in italian our proposed model shows an improvement across all configurations. however, in spanish vecmap emerges as a highly competitive baseline, with our model only showing an improved performance when training data in this language abounds (in this specific case there is an increase from 17.2 to 19.5 points in the mrr metric). concerning the other baseline, muse, the contribution of our proposed model is consistent for both languages, again becoming more apparent in the italian split and in a fully cross-lingual setting, where the improvement in mrr is almost 3 points (from 10.6 to 13.3). finally, it is noteworthy that even in the setting where no training data from the target language is leveraged, all the systems based on cross-lingual embeddings outperform the best unsupervised baseline, which is a very encouraging result with regards to solving tasks for languages on which training data is not easily accessible or not directly available. it was shown in table 4 that the performance of our model gradually increased alongside the size of the training data in the target language until surpassing vecmap in the most informed configuration (i.e., en+2k). for example, the english-only vecmap configuration (en), unlike ours, correctly discovered the following hypernyms for francesc maci`a (a spanish politician and soldier): politician, ruler, leader and person. these were missing from the prediction of our model in all configurations until the most informed one (en+2k). 

table 1 exhibits the evaluation results of crosslingual lexical sememe prediction with different seed lexicon sizes in {1000, 2000, 4000, 6000}. from the table, we can clearly see that:. (1) our two models perform much better compared with bilex in all the seed lexicon size settings. (2) clsp-se model achieves better results than clsp-wr model. 

table 1 reports the main translation results of chen translation. we first compare baseline+mem with baseline. as shown in row 1 and row 5 in table 1, baseline+mem can improve over baseline on all test datasets, and the average improvement is 1.37 bleu points. the results show that our method could significantly outperform the baseline model. the results are shown in row 4 and row 7. from the results, baseline(sub-word)+mem outperforms baseline(sub-word) by 1.01 bleu points, indicating that adopting sub-words as translation units still faces the problem of troublesome tokens, and our method could alleviate this problem. comparing arthur(test) (row 2 in table 1) and baseline+mem (row 5 in table 1), we can see that our proposed method can surpass arthur(test) with 1.05 bleu points. comparing the results of these two methods in table 1 (line 3 and 6), our method is still effective on the retrained model. the average gains are 0.92 bleu points. 

table 3 and table 4 has the bleu (papineni et al., 2002) and ribes (isozaki et al., 2010) scores. in our news commentary v8 tests, the same relative performance from seq2drnn(sync) can be observed. the seq2drnn+sync model is also able to out-perform the str2tree model proposed by aharoni and goldberg (2017) and nmt+rnng by eriguchi et al. (2017) in most cases. 

we report the perplexity of our language model on all versions of the source data in table 2. the results show that beam outputs receive higher probability by the language model compared to sampling, beam+noise and

this results in a new state of the art of 35 bleu on newstest2014 by using only wmt benchmark data. for comparison, deepl, a commercial translation engine relying on high quality bilingual training data, achieves 33.3 tokenized bleu. table 6 summarizes our results and compares to other work in the literature. 

results in the unconstrained inference (u) setting (table 2 top 5 rows) shows seq2seq(hma),denoted by “ours”, outperforms previous approaches on hindi, kannada, and bengali, with almost 3-4% gains. i. improvements over the seq2seq with attention (seq2seq w/ att) model demonstrate the benefit of imposing the monotonicity constraint in the generation model. on tamil and hebrew, seq2seq(hma) is at par with the best approaches, with negligible gap (∼0.3) in scores. overall, we see that seq2seq(hma) can achieve better (and sometimes competitive) scores than state-of-the-art approaches in full supervision settings. when comparing approaches which use constrained inference (table 2, rows 6 and 7), we see that using dictionary-constrained inference (as in ours(dc)) is more effective than using a entitylinking model for re-ranking (rpi-isi + el). in table 2 (rows under “low-resource setting”), we evaluate different models in a low-resource setting when provided only 500 name pairs as supervision. results are averaged over 5 different random sub-samples of 500 examples. the results clearly demonstrate that all generation models suffer a drop in performance when provided limited training data. note that models like seq2seq with attention suffer a larger drop than those which enforce monotonicity, suggesting that incorporating monotonicity into the inference step in the low-resource setting is essential. after bootstrapping our weak generation model using algorithm 1, the performance improves substantially (last row in table 2). on almost all languages, the generation model improves by at least 6%, with performance for hindi and bengali improving by more than 10%. bootstrapping results for the languages are within 2-4% of the best model trained with all available supervision.". 

table 3 shows that our model performs significantly better on native names for all the languages. 

table 3 shows that norma-linear outperforms (artetxe et al., 2018a) by over 10 points on the rare words dataset. on the regular muse dictionary, (artetxe et al., 2018a) is ahead by about 5 points. on rare, (lazaridou et al., 2015) is behind norma-linear by 9 points, whereas on the muse dictionary performance of (lazaridou et al., 2015) and norma-linear is about the same. 

as shown in table 3, using the same data, our approach achieves significant improvements over the original transformer model (vaswani et al. 2017) (p < 0.01). the gain on the concatenated test set (i.e., “all”) is 1.96 bleu points. it also outperforms the cache-based method (kuang et al. 2017) adapted for transformer significantly (p < 0.01), which also uses the two-step training strategy. 

table 5 shows the results of subjective evaluation. on average, around 19% of transformerâ€™s translations are better than that of our model, 51% are equal, and 31% are worse. 

table 1 shows the results for the piqa baselines (top) and the unconstrained state of the art (bottom). first, the tf-idf model performs poorly, which signifies the limitations of traditional document retrieval models for the task. second, we note that the addition of self-attention makes a significant impact on results, improving f1 by 2.6%. next, we see that adding elmo gives 3.7% and 2.9% improvement on f1 for lstm and lstm+sa models, respectively. lastly, the best piqa baseline model is 11.7% higher than the first (unconstrained) baseline model (rajpurkar et al., 2016) and 26.6% lower than the state of the art (yu et al., 2018). 

table 1 shows the evaluation results of word similarity tasks and word analogy tasks. in addition to glove and swivel, the evaluations of sgns are also reported for reference. as can be seen from the table, the context overlap information enhanced word embeddings perform better in most word similarity tasks and get higher analogy accuracy in semantic aspect at the cost of syntactic score. 

table 5 reports the breakdown of performance by pos tags. adposition is the easiest to identify as metaphorical and is also the most frequently metaphorical class (28%). 

table 6 shows performance on the verb classification task for three datasets (moh-x , trofi and vua). our models achieve strong performance on all datasets, outperforming existing models on the moh-x and vua datasets. on the moh-x dataset, the cls model outperforms the seq model, likely due to the simpler overall sentence structure and the fact that the target verbs are the only words annotated for metaphoricity. for the vua dataset, where we have annotations for all words in a sentence, the seq model significantly outperforms the cls model. we hypothesize that koper et al. (2017) outperforms our models on the trofi dataset for a similar reason:. 

table 2 summarizes the performance of baseline models and our approach. as we can see from the table, our model could achieve superior performance compared with other baseline models. 

table 3 shows the performance of several deep learning models trained on either sst or tsa datasets and evaluated on the opt dataset. as can be seen from the table, the models trained on the sentiment datasets perform poorly on the optimism/pessimism dataset. for example, there is a drop in performance from 80.19% to 67.60% when training on tsa (with an even larger decrease when we train on sst). 

table 1 depicts the human annotations (t-test: p < 0.05 for c and l, p < 0.01 for e). overall, e-scba outperforms s2s-aw on all three metrics, where the compound information plays a positive role in the comprehensive promotion. however, in surprise and angry, the grades of consistency and logic are not satisfactory, since the data for them are much less than others (surprise (1.2%) and angry (0.7%)). besides, the score of emotion in surprise has a big difference from others. 

the results of our experiments are summarized in the table 3. findings indicate that our proposed approach leads to a small performance boost after using the topic embeddings. we keep the same experimental steps and use the parameters that achieved the best performances in the table 3 to train the models. 

table 3 shows the results of human evaluation. first, it is clear that the aem model outperforms the seq2seq model with a large margin, which proves the effectiveness of the aem model on. second, it is interesting to note that with the attention mechanism, the coherence is decreased slightly in the seq2seq model but increased significantly in the aem model. 

table 2 shows the accuracy of all w2v configurations. representing an argument using its more verbose several-sentences-long content outperforms using its short single-sentence title. we compared the results of the best w2v-based configuration (arg-sentence), to the performance of the skip-thought auto-encoder. in this setting, encoding individual speech sentences and an argument, the accuracy of skip-thought was 60.2%. the highest scoring method, w2v arg-sentence, reaches, then, a rather modest accuracy of 64.6%. it is nevertheless substantially superior to the trivial all-yes baseline, as well as its all-no counterpart. 

table 1 shows the main experimental results. our baseline cross-entropy captioning model gets similar scores to the original flat model. when the repetition penalty is applied to a model trained with cross-entropy, we see a large improvement on cider and a minor improvement on other metrics. when combining the repetition penalty with scst, we see a dramatic improvement across all metrics, and particularly on cider. 

we evaluate rouge-g, against the top metrics (c s iiith3, demokritosgr1, catolicasc1) among the 23 metrics participated in tac aesop 2011, rouge, and the most recent related work (rouge-we) (table 1). overall results support our proposal to consider semantics besides surface with rouge. we analyze the correlation results reported in table 1 in the following. rouge-g-2 achieves the best correlation with pyramid, regarding all correlation metrics. moreover, every rouge-g variant outperforms its corresponding rouge and rouge-we variants, regardless of the correlation metric used. however, the only exception is rouge-su4, which correlates slightly better with pyramid when measuring with pearson correlation. for responsiveness, rouge-g-su4 achieves the best correlation when measuring with pearson. we also observe that rouge-g-2 obtains the best correlation with responsiveness while measuring with the spearman and kendall rank correlations. we also see that every variant of rouge-g outperforms its corresponding rouge and rouge-we variants. although our main goal is not to improve the readability, rouge-g-su4 and rouge-g-2 are observed to correlate very well with this metric when measured with the pearson and spearman/kendall rank correlations, respectively. besides, every variant of rouge-g represents the best correlation results comparing to its corresponding variants of rouge and rouge-we for all correlation metrics. 

table 1 reports bleu scores comparing our model against previous works. here, we see that our model achieves a bleu score comparable with the state-of-the-art, and thus we argue that it is sufficient to be used in our subsequent experiments with guidance. 

table 2 reports performance for all models. the difference between the guided and the unguided model is 16.2 points in bleu and 9.9 points in rouge-2, while there is room for improvement as evidenced by the difference between the oracle and non-oracle result. 

table 3 shows the results of our model on the second type of adversarial examples, i.e., the paraphrased atis development set. we also report the result of our model on the original atis development set. we can see that (1) no matter which feature our model uses, the performance degrades at least 2.5% on the paraphrased dataset;. (2) the model that only uses word order features achieves the worst robustness to the paraphrased queries while the dependency feature seems more robust than other two features. (3) simultaneously utilizing three syntactic features could greatly enhance the robustness of our model. 

table 2 shows that recode outperforms the baselines in both bleu and accuracy, providing evidence for the effectiveness of incorporating retrieval methods into tree-based approaches. we also compare recode with rabinovich et al. (2017)â€™s abstract syntax networks with supervision (asn+supatt) which is the state-of-the-art system for hs. recode exceeds asn without extra supervision though asn+supatt has a slightly better result. 

table 1 summarizes the results of our models and baselines. although the template-based method achieves decent bleu scores, its grammaticality score is substantially worse than other baselines. we can see that on both two datasets, our graph2seq models perform significantly better than the seq2seq and tree2seq baselines. we also conducted an experiment that treats the sql query graph as an undirected graph and found the performance degrades. by manually analyzing the cases in which the graph2seq model performs better than seq2seq, we find the graph2seq model is better at interpreting two classes of queries:. 

table 6 shows binary classification f1 on spr1. all new results outperforming prior work (crf) in bold. 

the main results are reported in table 1. our neural transition-based model achieves the best results in ace datasets and comparable results in genia dataset in terms of f1 measure. this observation helps explain why our model achieves greater improvement in ace than in genia in table 1 since the former has much more nested structures than the latter. our model turns out to be around 3-5 times faster than theirs, showing its scalability. the results are listed in table 1. from the performance gap, we can conclude that these components contribute significantly to the effectiveness of our model in all three datasets. 

as shown in table 1, this gru model gets a score of 73.4 on the wikilinksned development set. in table 1, we see that our model with attention (gru+attn) outperforms our basic gru model by around 1% absolute. it also outperforms the roughly similar model of eshel et al. (2017) on the test set:. table 1 shows the impact of incorporating character cnns (gru+attn+cnn). surprisingly, these have a mild negative impact on performance. table 1 shows the results of stacking these features on top of our model with attention (gru+attn+feats). we see our highest development set performance and correspondingly high test performance from this model. our model augmented with these sparse features achieves state-of-the-art results on the test set. 

table 2 shows the full results of our best models on the lambada and cbt-ne datasets, and compares them to recent, best-performing results in the literature. without features, attsum + l2 achieves the best test results, whereas with features attsum-feat + l1 performs best on cbtne. the results on lambada indicate that entity tracking is a very important overlooked aspect of the task. interestingly, with features included, attsum-feat + l2 appears to hurt test performance on lambada and leaves cbtne performance essentially unchanged, amounting to a negative result for l2. on the other hand, the effect of attsum-feat + l1 is pronounced on cbt-ne, and while our simple models do not increase the state-of-the-art test performance on cbt-ne, they outperform “attentionover-attention” in addition to reranking (cui et al. 2016), and is outperformed only by architectures supporting “multiple-hop” inference over the document (dhingra et al. 2016). our best model on cbt-ne test set, attsum-feat + l1, is very close to the current state-of-the-art result. on the validation sets for both lambada and cbt-ne, the improvements from adding features to attsum + li are statistically significant (for full results refer to our supplementary material). on lambada, the l1 multi-tasked model is a 3.5-point increase on the state of the art. 

table 3 considers the performance of the different models based on a segmentation of the data. we see that both the additional features and multi-task objectives independently result in a clear improvement in all categories, but that the gains are particularly pronounced for named entities and specifically for speaker and quote examples. here we see sizable increases in performance, particularly in the speaker category. we see larger increases in the more dialog heavy lambada task. 

table 2 shows the evaluation results. our model has obvious advantage over the baseline systems in content preservation, and also performs well in other aspects. 

table 1 shows the results of our experiments, as well as the results of our strong baselines. note that the majority class baseline already provides good results. over all architectures, we observe a comparable or better performance when using fasttext embeddings instead of word2vec or glove. leaving everything else unchanged, we can furthermore see an increase in performance for all settings, when switching from the pipeline to an end-to-end approach. the best performance (marked in bold) is achieved by a combination of cnn and fasttext embeddings, which outperforms the highly adapted winning system of the shared task. 

table 2 shows the results for this task. first of all, we can see that the svm-based germeval baseline model has very decent performance as it is practically on par with the best submission for the synchronic and even outperforms the best submission on the diachronic test set. comparing our architectures, we see again that fasttext embeddings always lead to equal or better performance. and even though we do not directly optimize our models for this task only, our best model (cnn+fasttext) outperforms all baselines, as well as the germeval winning system. 

table 2 shows the average f1 scores of different models versus sommeliers’ performance. likewise, the sommeliers’ performance measures represent a conservative(ly higher) estimate since scores lower than 60% in section 1 were removed. we find the hslda model, especially of monovarietals, outperforms sommeliers by 6.3%, as measured by f1. 

table 3 shows the results of this comparison. as can be seen from the table, convlexlstm achieves the best results consistently throughout all experiments in terms of all compared measures. for example, removing the seven lexicon features from convlexlstm, which yields convlstm, results in a drop in f1-score by 5.8% on joy in b-ds, and by 3.9% on sadness in b-ds. still, convlstm is the second performing model in terms of f1-score. not surprisingly, the svm with the sevenlexicon based features (denoted as seven-lexicon) performs the worst among the compared models, suggesting that capturing the semantic information from text via deep neural networks improves emotion detection. table 3 shows the results of this comparison as well. as can be seen, convlexlstm outperforms all three baselines on both datasets, and more importantly, the character-level cnn-lstm by kim et al. (2016) (i.e., the c-convlstm model). it is worth mentioning that all deep neural networks, convlexlstm, convlstm, cnn, lstm, and c-convlstm, that capture high-level semantic features perform better than the traditional models on emotion detection. 

in table 4 we remove the 30+ tweet requirement from the “tweet to county” and “county” methods and compare against the “user to county” method (with the 30+ tweet requirement). again we see the “user to county” method outperforms all others in spite of the fact that the “user to county” approach uses less data than both “all” approaches, which contains 108 million more tweets. 

in table 5 we repeat the above experiment on a 1% twitter sample. here we see that the “user to county” method outperforms both the “tweet to county” and “county” methods (with all three tasks using the same number of tweets). when we compare the “user to county” and “county (all)” methods we see the “user to county” outperforming on two out of four tasks (income and life satisfaction). again, we note that the “user to county” is using less data than the “county (all)”. while, across the board, the performance increase is not as substantial as in the 10% results, we see comparable performance between “user to county” and “county (all)” methods despite the difference in the number of tweets. 

table 3 summarizes language modeling perplexities on ptb test set. the middle block compares all models with two layers and 10m trainable parameters. rrnn(b) and rrnn(c) achieve roughly the same performance; interpolating both unigram and bigram features, rrnn(f) outperforms others by more than 2.9 test perplexity. for the three-layer and 24m setting (the bottom block), we observe similar trends, except that rrnn(c) slightly underperforms rrnn(b). here rrnn(f) outperforms others by more than 2.1 perplexity. using a max-plus semiring, rrnn(b)m+ underperforms rrnn(b) under both settings. finally, most compared models outperform the lstm baselines, whose numbers are taken from lei et al. (2017b). 

table 1 shows the overall performance comparing to the above state-of-the-art methods with golden-standard entities. from the table, we can see that our jmee framework achieves the best f1 scores for both trigger classification and argumentrelated subtasks among all the compared methods. there is a significant gain with the trigger classification and argument role labeling performances, which is 2% higher over the best-reported models. 

table 2 shows the results. 1) compared with lstm+softmax, lstm-based collective ed methods (lstm+crf, lstm+tlstm, lstm+htlstm, lstm+htlstm+bias) achieves a better performance. surprisingly, the lstm+htlstm+bias yields a 14.9% improvement on the sentence contains multiple events over the lstm+softmax. 2) the lstm+tlstm achieve better performances than lstm+crf. and the lstm+htlstm achieve better performances than lstm+tlstm. 3) compared with lstm+htlstm, the lstm+htlstm+bias gains a 0.9% improvement on all sentence. 

table 5 presents the results for different combinations of valency relation subsets. we find that pp-attachment decisions are generally harder to make, compared with core and functional relations. however, they do permit improvements on precision for pp attachment by 3.30, especially with our proposed joint decoding. 

human accuracy on our test set under these 4 settings are reported in table 5. as expected, compared to human accuracy based only on question-answer pairs (q), adding videos (v+q), or subtitles (s+q) significantly improves human performance. adding both videos and subtitles (v+s+q) brings the accuracy to 89.41%. we also observe that workers obtain 31.84% accuracy given questionanswer pairs only, which is higher than random guessing (20%). 

table 6 compares performance on tempo - hl. performance on tempo-hl is considerably lower than tempotl suggesting that tempo-hl is harder than tempo-tl. on tempo - hl, we observe similar trends as on tempo-tl. when considering all sentence types, mllc has the best performance across all metrics. in particular, our model has the strongest performance for all sentence types considering the miou metric. in addition to performing better on temporal words, our model also performs better on the original didemo dataset. the final row of table 6 shows an upper bound in which the ground truth context is used at test time instead of the latent context. we note that results improve for “before”, “after”, and “then”, suggesting that learning to better localize context will improve results for these sentence types. 

table 2 shows the results of domain specific named entity recognition. used for pre-training embeddings, mem2vec achieves higher f1-score than all the baselines. it first surpasses cre and darep that only bring slight improvements over word2vec. 

table 2 shows a similar pattern as we observed the naive baseline outperforms the with nli:. the naive baseline outperforms the single-embedding encoders;. the dme methods outperform the naive baseline, with the contextualized version appearing to work best. finally, we experiment with replacing ï† in eq. 1 and 2 with a sigmoid gate instead of a softmax, and observe improved performance on this task, outperforming the comparable models listed in the table. 

table 3 shows the results, comparing against vse++. first, note that the imagenet-only embeddings don’t work as well as the fasttext ones, which is most likely due to poorer coverage. we observe that dme outperforms naive and fasttext-only, and outperforms vse++ by a large margin. 

the results across different datasets are shown in table 2. we perform evaluations under two settings: by considering (i) word similarity between visual words only and (ii) between all words (column 100% in table 2). the two last rows correspond to the multimodal embeddings inferred from our model. overall, we note that pixie⊕ offers the best performance in almost all situations. as table 2 shows, pixie and v-sgns are often the best performing multimodal models, which provides empirical evidence that accounting for perceptual information while learning word embeddings from text is beneficial. moreover, the superior performance of pixie⊕ over v-sgns suggests that our model does a better job at combining perception and language to learn word representations. pixie⊕ outperforms vae⊕sgns in almost all cases, which demonstrates the importance of joint learning. on datasets that focus on semantic/taxonomic similarity, our approach dominates all other methods. on datasets focusing on general relatedness, our approach obtains mixed results. while dominating other approaches on men, it tends to perform worst than sgns on mturk and rel (under the 100% setting). the low performance of cnn and vae confirms this hypothesis. on the vissim dataset focusing on visual similarity, both cnn⊕sgns and vae⊕sgns perform the best, strongly suggesting that visual and linguistic data are complementary. 

table 6 summarizes the results. the evaluation metrics are accuracies at top-k (k=1, 5, or 10) retrieved sentences or images. our model consistently outperforms sgns and other competing multimodal methods, which provides additional support for the benefits of our approach. 

table 3 shows the results of embedding-based solvers on the anomia and crowdsourced datasets, using several different pre-trained embedding maps. we find the best performance on anomiacommon and anomiaproper using the word2vec vectors trained on 100 billion tokens from the google news corpus. based on this tuning, we fixed the value of k to 5, and repeated the experiments from section 5 (see the first row in table 3). elmo sense vectors clearly outperform all previous baselines on all odd-man-out datasets. 

table 3 shows the performance of the systems. the results are reported with the precision (p), recall (r), and their harmonic mean f1 score (f1). the two feature based methods perform differently. baseline1 performs poorly. baseline2 considers context windows and outperforms baseline1 largely. furthermore, we have other observations:. (1) neural network based approaches largely outperform feature-based classifiers;. (2) multitask learning approaches outperform every single task approach and other baselines. combining them together can achieve the best performance. the improvement of f1 score can reach to 3.3% compared with the best single task model. 

table 4 shows the results of various systems and settings on two test sets. the first dataset consists of all manually labeled simile sentences in the test set and the second dataset is the whole test set. first, we can compare the results in the middle column and the rightmost column in table 4. second, we can see that both pipelines (the feature-based and the neural network based) achieve a better performance compared with extracting components directly using either the crf model or the neural single task model. third, multitask(ce+sc) doesn’t bring significant improvements compared with the single task neural model. we can see that even on gold simile sentences, the rule based method doesn’t work well. the crf method performs significantly better, because it considers more contextual signals. our neural single task model achieves large improvements on both datasets. it gains a 1.3% f1 improvement on the gold simile sentences due to the improvement on the precision and a 3% f1 improvement on the whole test set due to a large improvement on the recall. as shown in table 4, the optimized pipeline performs better than the strongest multitask learning setting. however, in all settings, the precision scores are lower compared with the recall scores. 

experimental results are listed in table 1. we measure performance by the mean average precision (map) and mean reciprocal rank (mrr) using the standard trec evaluation script. in the first block of table 1, we compare our model and variants thereof against several baselines. as shown in table 1, we use two variants of the structured alignment model, since the structure of the question and the answer sentence may be different;the first model shares parameters across the question and the answer for computing the structures, while the second one uses separate parameters. table 1 (second block) also reports the performance of various comparison systems and stateof-the-art models. as can be seen, on both map and mrr metrics, structured alignment models perform better than the decomposable attention model, showing that structural bias is helpful for matching a question to the correct answer sentence. we also observe that using separate parameters achieves higher scores on both metrics. the simple span alignment model obtains results similar to the decomposable attention model, suggesting that the shallow softmax distribution is ineffective for capturing structural information and may even introduce redundant noise. the pipelined model with an external parser also slightly improves upon the baseline, but still cannot outperform the end-to-end trained structured alignment model which achieves results comparable with several strong baselines with fewer parameters. 

table 2 reports our results on the multinli and scitail datasets. on multinli, cafe significantly outperforms esim, a strong state-of-the-art model on both settings. we also outperform the esim + read model (weissenborn, 2017). an ensemble of cafe models achieve competitive result on the multinli dataset. on scitail, our proposed cafe model achieves state-of-the-art performance. the performance gain over strong baselines such as decompatt and esim are ≈ 10% − 13% in terms of accuracy. cafe also outperforms dgem, which uses a graph-based attention for improved performance, by a significant margin of 5%. as such, empirical results demonstrate the effectiveness of our proposed cafe model on the challenging scitail dataset. 

table 2 shows the results of different models on the train set and test set of snli. the first row gives a baseline model with handcrafted features presented by bowman et al. (2015). in table 2, the second block gives the single models. as we can see, our proposed model cin achieves 88.0% in accuracy on snli test set. compared to the previous work, cin obtains competitive performance. ensemble systems obtained the best performance on snli. our ensemble model obtains 89.1% in accuracy and outperforms the current state-of-the-art model. overall, single model of cin performs competitively well and outperforms the previous models on ensemble scenarios for the natural language inference task. 

table 3 shows the performance of different models on multinli. the first line of table 3 gives a baseline model without interaction. the second block of table 3 gives the attention-based models. the proposed model, cin, achieves the accuracies of 77.0% and 77.6% on the match and mismatch test sets respectively. 

table 4 shows the performance of different models on the quora test set. the baselines on table 4 are all implemented in zhiguo wang (2017). as we can see, our model outperforms the base lines and achieve 88.62% in the test sets of quora

table 1 reports the experimental results on the test set. overall, our proposed model outperforms the variants in every aspect. compared to our model which employs the coreference annotating mechanism, these two variants suffer notable loss in the precision of s metric. as a result, their performance drops on the other metrics. as expected, it shows a significant performance drop. 

table 4 shows the performances of the new models, which are all worse than our full models minv +infgibbs and minv +infem. the model minv +nn is not as competitive as our full models. 

table 1 presents the results on the development set. lines 1-3 depict the results for local models using averaged fasttext embedding initialization, showing that the best performance in terms of mrr and top-rank hits is achieved by transe. mean rank does not align with the other metrics;. there is a clear advantage to re-ranking the top local candidates using the score signal from the m3gm model (line 4). these results are further improved when the graph score is weighted against the association component per relation (line 5). 

table 2 shows that our main results transfer onto the test set, with even a slightly larger margin. 

results in table 5 show that our simple extractive method ourextractive significantly outperforms state-of-the-art neural extractive baselines, which demonstrates the effectiveness of the information selection component in our model. moreover, ourextractive significantly outperforms the two comparison systems which remove different components of our model one by one. 

table 1 compares xsum with the cnn, dailymail, and ny times benchmarks. as can be seen, xsum contains a substantial number of training instances, similar to dailymail; documents and summaries in xsum are shorter in relation to other datasets but the vocabulary size is sufficiently large, comparable to cnn. 

table 2 provides empirical analysis supporting our claim that xsum is less biased toward extractive methods compared to other summarization datasets. we report the percentage of novel n-grams in the target gold summaries that do not appear in their source documents. there are 36% novel unigrams in the xsum reference summaries compared to 17% in cnn, 17% in dailymail, and 23% in ny times. this indicates that xsum summaries are more abstractive. the proportion of novel constructions grows for larger n-grams across datasets, however, it is much steeper in xsum whose summaries exhibit approximately 83% novel bigrams, 96% novel trigrams, and 98% novel 4-grams (comparison datasets display around 47?55% new bigrams, 58?72% new trigrams, and 63?80% novel 4-grams). we further evaluated two extractive methods on these datasets. table 2 reports the performance of the two extractive methods using rouge-1 (r1), rouge2 (r2), and rouge-l (rl) with the gold summaries as reference. the lead baseline performs extremely well on cnn, dailymail and ny times confirming that they are biased towards extractive methods. ext-oracle further shows that improved sentence selection would bring further performance gains to extractive approaches. interestingly, lead and ext-oracle perform poorly on xsum underlying the fact that it is less biased towards extractive methods. 

we report results using automatic metrics in table 4. we evaluated summarization quality using f1 rouge (lin and hovy, 2003). on the xsum dataset, seq2seq outperforms the lead and random baselines by a large margin. ptgen, a seq2seq model with a “copying” mechanism outperforms ext-oracle, a “perfect” extractive system on rouge-2 and rouge-l. ptgen-covg, the best performing abstractive system on the cnn/dailymail datasets, does not do well. convs2s, the convolutional variant of seq2seq, significantly outperforms all rnn-based abstractive systems. table 4 shows several variants of t-convs2s including an encoder network enriched with information about how topical a word is on its own (enct') or in the document (enc(t',td)). we also experimented with various decoders by conditioning every prediction on the topic of the document, basically encouraging the summary to be in the same theme as the document (dectd) or letting the decoder decide the theme of the summary. interestingly, all four t-convs2s variants outperform convs2s. t-convs2s performs best when both encoder and decoder are constrained by the document topic (enc(t',td),dectd). 

in table 3 we compare the performance of different extractors using the averaging encoder, when the word embeddings are held fixed or learned during training. in all but one case, fixed embeddings are as good or better than the learned embeddings. 

table 5 shows the results of the shuffling experiments. the news domains and pubmed suffer a significant drop in performance when the document order is shuffled. by comparison, there is no significant difference between the shuffled and inorder models on the reddit domain, and shuffling actually improves performance on ami. 

table 2 shows the results. dpl substantially outperformed peng et al. (2017), improving sample precision by ten absolute points and raising absolute recall by 25%. combining disparate indirect supervision strategies is key to this performance gain, as evident from the ablation results. while distant supervision remained the most potent source of indirect supervision, data programming and joint inference each contributed significantly. replacing out-of-domain (wikipedia) word embedding with in-domain (pubmed) word embedding (pyysalo et al., 2013) also led to a small gain. 

table 5 compares system performance on this test set. the string-matching baseline has a very low precision, as gene mentions are highly ambiguous, which explains why peng et al. (2017) resorted to heavy filtering. by combining indirect supervision strategies, dpl improved precision by over 50 absolute points, while retaining a reasonably high recall (86%). 

in table 5 we present the results obtained on simulated data from the conll-2012 test set. the results follow a similar trend to those observed using actual annotations: a much better quality of the chains produced using the mention pairs inferred by our mpa model, across all the simulated scenarios. furthermore, the mv baseline achieves better chains compared to the stanford system in 3 out of 4 simulation settings, again showcasing the potential of crowdsourced annotations. 

table 6 lists the best results of the two models for bridging anaphora resolution from hou et al. (2013b). it models that semantically or syntactically related anaphors are likely to share the same antecedent and achieves an accuracy of 41.32% on the isnotes corpus. the results for glove gigawiki14 and glove giga are similar on two settings (using np head vs. using np head + modifiers). for embeddings pp, the result on using np head + modifiers (31.67%) is worse than the result on using np head (33.03%). finally, our method based on embeddings bridging achieves an accuracy of 39.52%, which is competitive to the best result (41.32%) reported in hou et al. (2013b). 

table 8 lists the results of different systems for bridging anaphora resolution in isnotes. it shows that combining our deterministic approach (np head + modifiers) with mln ii slightly improves the result compared to hou (2018). although combining np head + modifiers with mln ii achieves significant improvement over np head + modifiers, we think the latter has its own value. 

table 9 lists the results of bridging anaphora resolution in the bashi and arrau corpora, respectively. and our algorithm achieves an accuracy of 32.39% using only embeddings bridging. overall, the reasonable performance on these two corpora demonstrates thatembeddings bridging is a general word representation resource for bridging. 

table 2 shows the results, based on a ms-lstm setup similar to that of ยง4.1. note that the mslstm achieves 0.95-0.96 top-10 accuracy for the seen evaluation, significantly higher not only than the best model of hill et al. (2016), but also higher than onelook, a commercial system with access to more than 1000 dictionaries. it also presents considerably higher performance in the unseen evaluation. 

in table 3 we report results for two evaluation settings. in evaluation 1, we provide a comparison with the method of yang et al. (2015) who include textual features in graph embeddings based on matrix factorisation, and two topic models used as baselines in their paper. in evaluation 2, using the same linear svm classifier and λ as before, we reduce the training ratio to 0.05 in order to make our task comparable to the experiments reported by velickovic et al. (2018) for a number of deep learning models: specifically, the graph attention network (gat) of veličkovic et al. (2018), the graph convolutional network (gcn) of kipf and welling (2017), and the planetoid model of yang et al. (2016). again, our simple setting presents results within the state of the art range, comparable to (or better than) those of much more sophisticated models that have been specifically designed for the task of node classification. 

experimental results for relational triples, instanceof triples, and subclassof triples are shown in table 2, table 3, and table 4 respectively. in table 3 and table 4, a rising arrow means performance of this model have a promotion from yago39k to m-yago39k and a down arrow means a drop. from table 3 and table 4, we can conclude that: (1) on yago39k, some compared models perform better than transc in instanceof triple classification. transc can find a balance between them and all triples achieve a good performance. (3) on m-yago39k, transc outperforms previous work in both instanceof triple classification and subclassof triple classification, which indicates that transc can handle the transitivity of isa relations much better than other models. (4) after comparing experimental results in yago39k and m-yago39k, we can find that most previous work’s performance suffers a big drop in instanceof triple classification and a small drop in subclassof triple classification. (5) in transc, nearly all performances have a significant promotion from yago39k to myago39k. both instanceof-subclassof transitivity and subclassof-subclassof transitivity are solved well in transc. 

the ranks are reported in table 4 for both the datasets. the results depict the effectiveness of tdns. 

table 2 shows the entity linking performance of different methods on yelp-el. within the compared methods, linkyelp performs substantially better. the accuracy of directlink means that many mentions (about 67%) in yelp-el simply refer to the corresponding reviewed businesses.". 

we present f1, precision, and recall scores on all datasets in table 2 and table 3. from both tables, one can find the autoner achieves the best performance when there is no extra human effort. fuzzy-lstm-crf does have some improvements over the dictionary match, but it is always worse than autoner. even though swellshark is designed for the biomedical domain and utilizes much more expert effort, autoner outperforms it in almost all cases. moreover, autonerâ€™s performance is competitive to the supervised benchmarks. for example, on the bc5cdr dataset, its f1 score is only 2.16% away from the supervised benchmark. 

in table 3 we study entity typing for the coarse types on three datasets. we focus on three types that are shared among the datasets: per, loc, org. as expected, the supervised systems have strong in-domain performance. however, they suffer a significant drop when evaluated in a different domain. our system, while not trained on any supervised data, achieves better or comparable performance compared to other supervised baselines in the out-of-domain evaluations. 

as table 6 shows, both factors are crucial and complementary for the system. however, the contextual information seems to have a bigger role overall. 

our results, shown in table 3 indicate that systems that return contiguous spans from the rule text perform better according to our bleu metric. we speculate that the logical forms in the data are challenging for existing models to extract and manipulate, which may suggest why the explicit rule-based system performed best. 

table 4 shows the result of our baseline models on the entailment corpus of sharc test set. results show poor performance especially for the macro accuracy metric of both simple baselines and neural state-of-the-art entailment models. 

table 2 shows the experimental results with three different multi-answer loss functions introduced in section 3.5.1. all of them offer improvement over the single-answer baseline, which shows the effectiveness of utilizing multiple answers. the average loss performs better than the min loss, which suggests that forcing the model to predict all possible answers is better than asking it to just find the easiest one. not surprisingly, by taking into account the quality of different answer spans, the weighted average loss outperforms the average loss and achieves the best result among the three. 

table 4 shows the performance of our model and other state-of-the-art models on the dureader test set. based on the same bidaf model described in section 3.4, our method (pe+bidaf) significantly outperforms the trained model from wang et al. (2018b) (pr+bidaf) on the dureader test set. as we can see, our complete model achieves the state-of-the-art performance in both rouge-l and bleu-4, and greatly narrows the performance gap between mrc system and human in the challenging realworld setting. 

table 1 shows that our model perfectly answers all test questions, demonstrating that it can learn challenging semantic operators and induce parse trees from end task supervision. performance drops when using external parser, showing that our model learns an effective syntactic model for this domain. the relation network also achieves good performance, particularly on questions involving relations. lstm baselines work well on questions not involving relations. 

table 3 shows the performance of our model on complex human-generated queries in genx. our approach outperforms strong lstm and semantic parsing baselines, despite the semantic parserâ€™s use of hard-coded operators. 

table 7 compares performance of bcr + ecr + v + e + sst system on semeval to that of kelp and convkn, the two top systems in the semeval 2016 competition, and also to the performance of the recent dnn-based hyperqa and ai-cnn systems. in the semeval 2016 competition, our model would have been the first, with #1 kelp system being 0.6 map points behind. then, it would have outperformed the state-of-theart ai-cnn system by 0.35 map points. 

we present our main results on the tacred test set in table 1. we observe that our gcn model outperforms all dependency-based models by at least 1.6 f1. by using contextualized word representations, the c-gcn model further outperforms the strong pa-lstm model by 1.3 f1, and achieves a new state of the art. in addition, we find our model improves upon other dependency based models in both precision and recall. comparing the c-gcn model with the gcn model, we find that the gain mainly comes from improved recall. this simple interpolation between a gcn and a pa-lstm achieves an f1 score of 67.1, outperforming each model alone by at least 2.0 f1. an interpolation between a c-gcn and a pa-lstm further improves the result to 68.2. this complementary performance explains the gain we see in table 1 when the two models are combined. 

the performance of our model on the benchmark settings is reported in table 4, where all numbers are obtained with strong supervision over supporting facts. from the distractor setting to the full wiki setting, expanding the scope of the context increases the difficulty of question answering. the performance in the full wiki setting is substantially lower, which poses a challenge to existing techniques on retrieval-based question answering. we also investigate the explainability of our model by measuring supporting fact prediction performance. our model achieves 60+ supporting fact prediction f1 and âˆ¼40 joint f1, which indicates there is room for further improvement in terms of explainability. 

table 9 shows the comparison between train-medium split and hard examples like dev and test under retrieval metrics in full wiki setting. as we can see, the performance gap between trainmedium split and its dev/test is close, which implies that train-medium split has a similar level of difficulty as hard examples under the full wiki setting in which a retrieval model is necessary as the first processing step. 

table 3 presents the results on english out-of-domain test set. our models outperform the highest records achieved by he et al. (2018), with absolute improvements of 0.2-0.5% in f1 scores. these favorable results on both in-domain and outof-domain data demonstrate the effectiveness and robustness of our proposed unified framework. 

the corresponding results of our models are also summarized in table 6 for comparison. note that the first row is the results of our syntax-agnostic model. surprisingly, we observe a dramatical performance decline of 1.2% f1 for our syn-gcn model with m&t encoder. a less significant performance loss for our salstm (−0.4%) and tree-lstm (−0.5%) models shows that the syn-gcn is more sensitive to contextual information. 

table 7 presents the comprehensive results of our syn-gcn model on the four syntactic inputs aforementioned of different quality together with previous srl models. first, our model gives quite stable srl performance no matter the syntactic input quality varies in a broad range, obtaining overall higher scores compared to previous state-of-the-arts. second, it is interesting to note that the sem-f1/las score of our model becomes relatively smaller as the syntactic input becomes better. third, when we adopt a syntactic parser with higher parsing accuracy, our srl system will achieve a better performance. notably, our model yields a sem-f1 of 90.5% taking gold syntax as input. 

table 3 depicts our model's performance when linearizing the graphs according to the different traversal orders discussed and exemplified in table 2. overall, we find that the “smaller-first” approach performs best across all datasets, and that imposing one of our orders is always preferable over random permutations. 

table 4 presents the performance of our complete model (“mtl primary+aux”) versus peng et al. (2017a). on average, our model performs within 1% f1 point from the state-of-the art (outperforming it on the harder psd task), despite using the more general sequence-to-sequence approach instead of a dedicated graph-parsing algorithm. in addition, an ablation study shows that multi-tasking the primary tasks is beneficial over a single task setting, which in turn is outperformed by the inclusion of the auxiliary tasks. 

the results in table 5 show that our approach is more resilient to the decrease in annotation overlap, outperforming the state-of-the-art model on the dm and psd task, as well as on the average score. 

table 3 shows the intrinsic evaluation results, in which our alignment intrinsically outperforms jamr aligner by achieving better alignment f1 score and leading to a higher scored oracle parser. 

table 4 shows the results. from this table, we can see that our alignment consistently improves all the parsers by a margin ranging from 0.5 to 1.7. 

table 3 and table 4 summarize the results of all the methods on the lex-c dataset. firstly, the performance scores on lex-c are not necessarily consistent with those on lex-z (table 2) even if the methods and the language pairs are the same; this is not surprising as the two datasets differ in query words, word embedding quality, and training-set sizes. secondly, the performance gap between the best supervised methods and the best unsupervised methods in both table 3 and table 4 are larger than that in table 2. thirdly, the average performance in table 3 is lower than that in table 4, indicating that the language pairs in the former are more difficult than that in the latter. nevertheless, we can see that our method has much stronger performance than other unsupervised methods in table 3, i.e., on the harder language pairs, and that it performed comparably with the model by conneau et al. (2017) in table 4 on the easier language pairs. 

table 3 and table 4 summarize the results of all the methods on the lex-c dataset. firstly, the performance scores on lex-c are not necessarily consistent with those on lex-z (table 2) even if the methods and the language pairs are the same; this is not surprising as the two datasets differ in query words, word embedding quality, and training-set sizes. secondly, the performance gap between the best supervised methods and the best unsupervised methods in both table 3 and table 4 are larger than that in table 2. thirdly, the average performance in table 3 is lower than that in table 4, indicating that the language pairs in the former are more difficult than that in the latter. nevertheless, we can see that our method has much stronger performance than other unsupervised methods in table 3, i.e., on the harder language pairs, and that it performed comparably with the model by conneau et al. (2017) in table 4 on the easier language pairs. 

table 5 summarizes the performance of all the methods in cross-lingual word similarity prediction. we can see that the unsupervised methods, including ours, perform equally well as the supervised methods, which is highly encouraging. 

table 7 shows zero-shot xel results on all datasets, both with and without using the prior during inference. note that zero-shot xel (with prior) is close to sota (sil et al. (2018)) on tac15-test, which also uses the prior for zeroshot xel. however, for zero-shot xel (without prior) performance drops by more than 20% for tac15-test, 2.4% for th-test and by 2.1% for mcn-test. 

table 5 illustrates that, for three different testing datasets, the entire generated corpus d achieves 74.1%, 80.6% and 84.2% on ctrain:test, respectively, which are much higher than that of trn13, trn14 and trn15. 

table 7 shows the detection performance on three different testing datasets. we have the following observations. for tst13, d-10k achieves a better f1 score than trn13. besides, we can see that the detection performance shows a stable improvement as the size of our generated corpus is continuously enlarged. from table 7, although the overall performance (f1 score) keeps improving as the size of our generated corpus increases, the precision and the recall demonstrate different changing trends. it is observed that as the size of training dataset increases, the model achieves a better performance in terms of the recall. however, the improvement of the precision is not so obvious as that of the recall. specifically, in tst14 and tst15, d-50k does not achieve a higher precision than d-40k. from table 7, we can see that with a certain size of our generated corpus, it can train a model that achieve better detection performance than with the manually annotated datasets provided in the corresponding shared tasks. 

table 7 compares performance of the same experiments on the wsj and noun-verb challenge test sets, tuned either using the wsj or the noun-verb development set. when we tuned on the noun-verb development set, the wsj results remained almost unchanged, while the nounverb test set results increased significantly. the effect was greatest on the unenhanced model, which improved 2.9% absolute on the noun-verb evaluation. 

table 6 summarizes the results on czech, german, and russian. we find augmenting the charlstm model with either oracle or predicted case improve its accuracy, although the effect is different across languages. the mtl models achieve similar or slightly better performance than the character-only models, suggesting that supplying case in this way is beneficial. finally, we observe that augmenting the char-lstm with either gold or predicted case improves the parsing performance for all languages, and indeed closes the performance gap with the full oracle, which has access to all morphological features. 

table 2 presents our proposed model in comparison with state-of-the-art results. these experiments show that our char-intnet generally improves results across different models and datasets. the improvement is more pronounced for non-english datasets, for example, intnet improves the f-1 score over the stateof-the-art results by more than 2% for dutch and spanish. it also shows that the results of lstm-crf are significantly improved after adding character-to-word models, which confirms that word shape information is very important for sequence labeling. 

tables 4 and 5 present the results on the iemocap and semaine testing sets, respectively. in table 4, we evaluate the mean classification performance using weighted accuracy (acc.) and f1-score (f1) on the discrete emotion categories. icon performs better than the compared models with significant performance increase in emotions (âˆ¼2.1% acc.). for each emotion, icon outperforms all the compared models except for happiness emotion. however, its performance is still at par with clstm without a significant gap. 

table 6 presents the results for different combinations of modes used by icon on iemocap. as seen, the trimodal network provides the best performance which is preceded by the bimodal variants. among unimodals, language modality performs the best, reaffirming its significance in multimodal systems. interestingly, the audio and visual modality, on their own, do not provide good performance, but when used with text, complementary data is shared to improve overall performance. 

table 5 gives the mean accuracy obtained on the test data (of 100 runs). our imgobjloc vectors outperform all comparison models on motion verbs, including cnn-based image features and the best-performing models of (gella et al., 2018), namely gella–cnn+o and gella–cnn+c (cnn features concatenated with predicted object labels and image captions, respectively). on non-motion verbs, the best models, including our own, perform only comparably to the most frequent sense heuristic. 

table 4 shows the comparison results among various models and the upper bound where the gold commonsense evidence provided to the human. itâ€™s not surprising that performance on common ground is worse in the hard configuration as the distracting verbs are more similar to the target action. the cvae-based method is better than the attention-based method in facilitating common ground. 

table 3 reports the results and the average score of the two metrics. as it can be seen, the model is very accurate and achieves a very high correlation (>0.56 with p <0.001) with an average error difference (avg mean abs err) below 1. in particular, the model obtained higher performance in predicting the ranking of features extracted from sentences at agreement 14. 

table 1 shows parsing results on the wsj20dev dataset. there is almost a point of gain in precision going from best to best with pioc with virtually no recall loss, showing that the posterior uncertainty is helpful in flattening binary trees. as more samples from the posterior are collected, as shown in all with pioc without best, the precision gain is even more substantial. 

table 2 presents recognition results, comparing baseline lstm-lms to the full session-based lstm-lms. both the letter-trigram and one-word word encoding versions are reported. the different models may also be used jointly, using log-linear score combination in rescoring, shown in the third section of the table. we also tried iterating the session lm rescoring, after the recognized word histories were updated from the first rescoring pass (shown as “2nd iteration” in the table). results show that the session-based lm yields between 1% and 4% relative word error reduction for the two word encodings, and test sets. iterating the session lm rescoring to improve the word histories did not give consistent gains. even though the session-based lstm subsumes all the information used in the standard lstm, there is an additional gain to be had from combining those two model types (last row in the table). thus, the overall gain from adding the session-based models to the two baseline models is 3-5% relative word error reduction. 

see table 2 for multi-label classification results, where the hamming score for the multi-label cnn-rnn model is 82.5%, showing potential for real-world use as well as substantial future research scope. 

table 2 shows the comparison of our model with several previous state-of-the nested ner models on the test dataset. our model outperforms the state-of-the-art models in terms of f-score. 

table 3 describes the performances of our model on different entity levels on the test dataset. the model performs well on multi-token and top-level entities. 

table 4 shows the performances on the five entity types on the test dataset. we here show the performance by finkel and manning (2009) (f&m) for the reference. our system performs better than their model except for the rna type. 

table 5 shows the coverage ratio and the performance with different maximum region sizes. fortunately, the performance did not degrade with the long maximum region size, despite the fact that it introduces more out-of-entity regions. 

ablations on character embeddings in table 6 also show the importance of character embeddings. it also shows that both the boundary information and the inside information, i.e., average of the embeddings in a region, are necessary to improve the performance. 

table 7 shows the performances of our model on jnlpba dataset. we compared our result with the state-of-the-art result of gridach (2017) which achieved 75.8% in f-score, where our model obtained 78.4% in terms of f-score. 

as the results in table 1 show, our results very closely replicate those reported by malouf (2017). 

table 4 shows results for completing tables for common lexemes. our system significantly outperforms the baseline on all other datasets apart from german nouns. 

table 3 shows the attack success rate and mean percentage of modified words on each task. we compare to the perturb baseline, which greedily applies the perturb subroutine, to validate the use of population-based optimization. as can be seen from our results, we are able to achieve high success rate with a limited number of modifications on both tasks. in addition, the genetic algorithm significantly outperformed the perturb baseline in both success rate and percentage of words modified, demonstrating the additional benefit yielded by using population-based optimization. speaking of which, our success rate on textual entailment is lower due to the large disparity in sentence length. with sentences that short, applying successful perturbations becomes much harder, however we were still able to achieve a success rate of 70%. for the same reason, we didnâ€™t apply the perturb baseline on the textual entailment task, as the perturb baseline fails to achieve any success under the limits of the maximum allowed changes constraint. 

table 3 shows the effect of gated attention mechanism. we compared models with gated c+ p2c and simple c+ p2c. the miu accuracy of the p2c model has over 10% improvement when changing the operate pattern of the extra information proves the effect of ga mechanism. the gated c+ p2c achieves the best in dc corpus, suggesting that the gated-attention works extremely well for handling long and diverse context. our model is compared to other models in table 3. we list the top-5 accuracy contrast to all baselines with top10 results, and the comparison indicates the noticeable advancement of our p2c model. to our surprise, the top-5 result on pd of our best gated c+ p2c system approaches the top-10 accuracy of google ime. on dc corpus, our gated c+ p2c model with the best setting achieves 90.14% accuracy, surpassing all the baselines. the comparison shows our gated-attention system outperforms all state-of-the-art baselines with better user experience. 

we present the lm results for the standard nontied model, the tied model as in inan et al. (2017) and press and wolf (2017), and our tied model with an additional linear transformation (tied+l) in tables 1 (ptb) and 2 (wiki). table 1 confirms that tying generally brings gains with respect to not tying. this is also true for the cases when the hidden and embedding sizes are different (e.g. 400/200 and 600/400), where our tied+l model outperforms the non-tied model by 5 to 6.4 points having around 40% less parameters. furthermore, our decoupled model slightly but consistently improves results with respect to standard tying, confirming our intuition that the coupling of the hidden state to the embedding representation is a limiting constraint. smaller tied+l models perform well compared to larger tied models. in particular, the tied+l model with 600/400 units has perplexity of 76.0, compared to 76.1 of the tied 600/600 model, with 55% the number of parametres. note that our results are comparable to previously reported perplexity values on ptb for similar models. our best results of 75.5 test perplexity is only 1.2 points behind the large tied model with 1500 units reported in press and wolf (2017) and is only 1.6 points behind the medium tied model with 650 units and variational dropout (gal and ghahramani, 2016) reported in inan et al. (2017). 

table 3 shows the performance of our best han model with a varying number k of previous sentences in the test-set. we can see that the best performance for ted talks and news is archived with 3, while for subtitles it is similar between 3 and 7. 

table 1 reports the main translation results of zh→en/ja and en→de/fr translation tasks. from the first two lines, we can see that the o2m method cannot perform on par with the individually trained systems in most cases. we can observe from the table that all our proposed strategies (last part in table 1) improve the translation performance compared to the baseline (o2m). specifically, the combined use of three strategies performs best and it can achieve the improvements up to 1.96 bleu points (45.51 vs. 43.55 on zh→en mt04). as for languagedependent positional embedding, we find that both fixed and dynamic styles perform similarly. table 1 demonstrates some encouraging results. it is shown in the table that the universal one-to-many architecture enhanced with our strategies can outperform the individually trained models on three out of four language translations (zh→en, zh→ja, en→fr). 

4.2 the muse benchmark,2,

table 3 shows that rcsls is on par with the state of the art. 

table 1 shows the result on different training datasets to compare our model against the baselines. in all settings, our mt+ag+lm models outperforms the mt+ag and monolingual/multi-source seq2seq models. specifically, our model outperform mt+ag in 500k+12k training condition by almost 1 bleu score on test2017. as expected, the models trained on 23k data perform better than those trained on 12k; further gains are obtained by adding 500k synthetic data. interestingly, training mt+ag and mt+ag+lm models on 23k data lead to better ter/bleu than those trained on 500k+12k. 

we compare the clmâ€™s entity identification against two state-of-the-art ner systems: cogcompner (khashabi et al., 2018) and lstmcrf (lample et al., 2016). as table 2 shows, the result of ngram clm, which yields the highest performance, is remarkably close to the result of state-of-theart ner systems (especially for english) given the simplicity of the model. 

the results in table 3 show that for six of the eight languages we studied, the baseline ner can be significantly improved by adding simple clm features; for english and arabic, it performs better even than the neural ner model of (lample et al., 2016). for tagalog, however, adding clm features actually impairs system performance. in the same table, the rows marked “unseen” report systems’ performance on named entities in test that were not seen in the training data. by this measure, farsi ner is not improved by nameonly clm features and tagalog is impaired. benefits for english, hindi, and somali are limited, but are quite significant for amharic, arabic, and bengali. 

table 4 compares our model against the best performing models in the literature (dernoncourt et al. 2016; liu et al. 2013). we have evaluated both model variants on all datasets. and as evidenced by table 4, our best model can improve the f1 scores by 2%-3% in absolute number compared with the previous best published results for all datasets. for the pubmed 20k and 200k datasets, our hsln-rnn model achieves better results; however, for the nicta dataset, the hsln-cnn model performs better. 

table 9 gives the performance of six different word embeddings for our hsln-rnn model trained on the pubmed 20k dataset. according to table 9, the training methods that create the word embeddings do not have a strong influence on model performance, but the corpus they are trained on does. the combination of wikipedia and pubmed abstracts as the corpus for unsupervised word embedding training yields the best result, and the individual use of either the wikipedia corpus or the pubmed abstracts performs much worse. although the dataset we are using for evaluation is also from pubmed abstracts, using only the pubmed abstracts together with mimic notes without the wikipedia corpus does not guarantee better result (see the “fasttextp.m.+mimic” embeddings in table 9), which may be because the corpus size of pubmed abstracts plus mimic notes (about 12.8 million abstracts and 1 million notes) is not large enough for good embedding training compared with the corpus consisting of at least billion tokens such as the wikipedia. 

table 2 shows the results for mimic ii. table 2 results do not rely on thresholding because we evaluate using the relative ranking of groups with similar frequencies. acnn performs best on frequent labels. for few-shot labels, zagcnn outperforms acnn by over 10% in r@10 and by 8% in r@5; compared to these r@k gains for few-shot labels, our loss on frequent labels is minimal (< 1%). we find that the word embedding derived label vectors work best for eszsl on zero-shot labels. however, this setup is outperformed by grals derived label vectors on the frequent and few-shot labels. on zero-shot labels, zagcnn outperforms the best eszsl variant by over 16% for both r@5 and r@10. finally, similar to the setup in xian et al. (2017), we also compute the harmonic average across all r@5 and all r@10 scores. we find that zagcnn outperforms zacnn by 4% for r@10. 

table 3 gives human evaluation results. mrl achieves better results than the other two models. the biggest gap between mrl and gt lies on meaning. 

finally, the evaluation results in table 6 lead to the following findings:. (1) our models outperform other baselines on wn11 and fb15k, and obtain comparable results with baselines on fb13, which validate the effectiveness of our models;. (2) the extended models transe-hrs, transh-hrs and distmult-hrs achieve substantial improvements against the original models. on wn11, transe-hrs outperforms transe with a margin as large as 10.9%. 

table 4 compares our models on some of the most frequent relations. as shown, the model that includes textual description significantly benefits isaffiliatedto, and playsfor relations, as this information often appears in text. moreover, images are useful for hasgender and ismarriedto, while for the relation isconnectedto, numerical (dates) are more effective than images. 

table 4 compares the results of our model with the state of the art on the semeval 17 dataset for tasks of span identification, keyphrase extraction and relation extraction as well as the overall score. our model outperforms all the previous models that use hand-designed features. we observe more significant improvement in span identification than keyphrase classification. moreover, we have competitive results compared to the previous state of the art in relation extraction. 

table 2 shows the evaluation results of our proposed approach and the baselines. the top part presents embedding based approaches and the bottom part presents multi-hop reasoning approaches. we find embedding based models perform strongly on several datasets, achieving overall best evaluation metrics on umls, kinship, fb15k237 and nell-995 despite their simplicity. while previous path based approaches achieve comparable performance on some of the datasets (wn18rr, nell-995, and umls), they perform significantly worse than the embedding based models on the other datasets (9.1 and 14.2 absolute points lower on kinship and fb15k-237 respectively). in particular, neurallp and ntp-λ failed to scale to the larger datasets and their results are omitted from the table, as das et al. (2018) reported. ours is the first multi-hop reasoning approach which is consistently comparable or better than embedding based approaches on all five datasets. the best single model, ours(conve), improves the sota performance of path-based models on three datasets (umls, kinship, and fb15k-237) by 4%, 9%, and 39% respectively. on nell-995, our approach did not significantly improve over existing sota. while a better reward shaping module typically results in a better overall model, an exception is wn18rr, where complex performs slightly worse on its own but is more helpful for reward shaping. 

table 5 shows the percentage of examples associated with seen and unseen queries on each dev dataset and the corresponding mrr evaluation metrics of previously studied models. on most datasets, the ratio of seen vs. unseen queries is similar to that of to-many vs. to-one relations (table 4) as a result of random data split, with the exception of wn18rr. on some datasets, all models perform better on seen queries (umls, kinship, wn18rr) while others reveal the opposite trend. on nell-995 both of our proposed enhancements are not effective over the seen queries. in most cases, our proposed enhancements improve the performance over unseen queries, with ad being more effective. 

our results are shown in table 1. for set1, sig17+ship obtains the highest accuracy, while, for set2 and set3, med+pt+ship performs best. overall, however, sig17+ship, med+pt, and med+pt+ship all outperform the baselines by a wide margin for all settings. on average, med+pt clearly outperforms sig17, the strongest baseline: by .0796 (.5808-.5012) on set1, .0910 (.7486-.6576) on set2, and .0747 (.8454-.7707) on set3. conversely, if we consider only the languages with an average input subset size of more than 15 (basque, haida, hindi, khaling, persian, and quechua), the average accuracy of med+pt for set1 is 0.9564, compared to an overall average of 0.5808. further, table 1 shows that sig17+ship is better than sig17 by .0959 (.5971-.5012) on set1, .0779 (.7355-.6576) on set2, and .0301 (.8008-.7707) on set3. in contrast, the performance of med, the neural model, is relatively independent of the choice of source; this is in line with earlier findings (cotterell et al., 2016). however, even for med+pt, adding ship (i.e., med+pt+ship) slightly increases accuracy by .0061 (.7547-.7486) on set2, and .0029 (.8483-.8454) on set3 (l53). med does not perform well for either set1 or set2. however, med loses against med+pt in all cases, highlighting the positive effect of paradigm transduction. looking at pt next, even though pt does not have a zero accuracy for any setting or language, it performs consistently worse than med+pt. for set3, pt is even lower than med on average, by .3436 (.4211-.0775). however, in most cases, pt does not perform well on its own. med+pt outperforms both med and pt. 

table 3 shows our results on all languages. we get +5.8 f1 points for turkish, +4.8 f1 for uyghur, +0.8 f1 for hindi and +0.7 f1 for bengali over the existing methods. we observe that a combination of character-ngrams, lemma and morphological properties gives the best performance for uyghur and bengali. adding morph hurts in turkish, in contrast to hindi, where it helps. 

the results are shown in table 1 and table 2. in fact, in table 1, the accuracy scores based on quantity-quality vectors (qq column) suggest that our handcrafted features are actually useful for detecting hyperboles. the three models become comparable to, and yet do not outperform, the second and third baselines. an interesting trend appears both for hype-par and hype-min: with skip-gram+qq, algorithms perform better than relying on skip-gram or qq alone, and the same happens for glove+qq. lr outstands other classifiers in the skipgram+qq combination, reaching .72 mean accuracy and .76 average f1-score (see table 3). 

table 4 shows cross-genre experimental results of our neural network models on the training set of masc+wiki by treating each genre as one crossvalidation fold. as we expected, both the macroaverage f1-score and class-wise f1 scores are lower compared with the results in table 2 where in-genre data were used for model training as well. but the performance drop on the paragraph-level models is little, which clearly outperform the previous system (friedrich et al., 2016) and the baseline model by a large margin. 

table 4 shows the thread reconstruction results of our model and the baseline models in the wikipedia conversation dataset. from the f1node score of the results, we establish our model performs better than other baseline models. 

bottom rows in table 5 report the end-to-end performance of our five systems on both domains. on both labeled and unlabeled parsing, our basic neural model with only lexical input performs comparable to the logistic regression model. and our enriched neural model with only three simple linguistic features outperforms both the logistic regression model and the basic neural model on news, improving the performance by more than 10%. however, our models only slightly improve the unlabeled parsing over the simple baseline on narrative grimm data. finally, attention mechanism improves temporal relation labeling on both domains. to facilitate comparison with previous work where gold events are used as parser input, we report our results on temporal dependency parsing with gold time expression and event spans in table 5 (top rows). these results are in the same ballpark as what is reported in previous work on temporal relation extraction. our best performing model (neural-attention) reports 0.81 and 0.70 f-scores on unlabeled and labeled parses respectively, showing similar performance. 

table 2 lists the results on the four datasets. by leveraging multimodal correlations, lrmm significantly outperforms mf-based models (i.e. nmf, svd++) and topic-based methods (i.e., urp, ctr, rmr, and hft). lrmm also outperforms recent deep learning models (i.e., nrt, deepconn) with respect to almost all metrics. while the cold-start recommendation is more challenging, lrmm(-u) and lrmm(-o) are still able to achieve a similar performance to the baselines in the standard recommendation setting. for example, rmse 1.101 (lrmm(-o)) to 1.107 (nrt) on electronics, mae 0.680 (lrmm(-o)) to 0.667 (deepconn) on s&o. table 5 lists some randomly selected rating predictions. 

table 3 lists the results of training lrmm with missing data modalities for the modality dropout ratio pm = 0.5 on the s&o and h&p datasets, respectively. both rmse and mae of lrmm deteriorate but are still comparable to the mf-based approaches nmf and svd++. 

in table 1 we report results from the experiments on the 10m web documents dataset for prec and ndcg-idf metrics for k = 1, 3, 5, 7, limiting k to small values as is common in the recommendation problems from large sets of items (jain et al., 2016). for example, in 36.76% of cases entity research (the most popular future entity from the corpus) is in the future of the document (as can be also seen from figure 1, where the entity research corresponds to the leftmost point of the graph). among the linear models, p(f|c) yields the highest scores, significantly outperforming the baselines. ppmi(f|c) model yields relatively high ncg-idf scores (although in most cases lower than p(f|c)), and very low precision scores. notice that the svd methods are consistently worse than the linear models. when experimenting with higher ranks for svd decomposition we found the performance increases, but does not improve over the linear models. nns improve over linear models according to both ncg-idf and prec scores. this is especially apparent for ncg-idf, where the relative improvement is very significant. xml-cnn is in all cases better than the youtube model, which shows how utilizing more linguistic structure than simply bag of entities is helpful in the nn framework. both youtube and xml-cnn models with a modified loss function improve over the basic nn models in terms of the ncg-idf metrics, showing that a simple adjustment of a loss function in the nn framework can lead to more rare entities being recommended. this comes at the cost of lowering prec@k scores, which however correlates with user judgments to a lesser extent, as we showed in section 3.2. fastxml performs particularly well on the precision scores, which however is not necessarily useful, as demonstrated in the examples discussed later. last, we inspect the sizes of the different models reported in the rightmost column of table 1. pfastrexml model takes 150gb, by far the most of all methods, resulting in its capability in recommending tail entities. the linear models take 4.7gb related to the fact that in the full 100k ã— 100k co-occurrence matrix approximately 11% of entries are non-zero. the nn models take around 2gb, significantly less than the random forests. 

table 8 compares limbic with la, lad, and law. we observe that for both datasets, incorporating discourse relations improves accuracy. by incorporating word embeddings, law yields better accuracy than lad, showing that word embeddings add more value to limbic than discourse relations do. 

table 2 shows the performance comparison results of mgan with other baseline methods. we can have the following observations. (1) majority performs worst since it only utilizes the data distribution information. feature+svm can achieve much better performance on all the datasets, with the well-designed feature engineering. our method mgan outperforms majority and feature+svm since mgan could learn the high quality representation for prediction. (2) atae-lstm is better than lstm since it employs attention mechanism on the hidden states and combines with attention embedding to generate the final representation. td-lstm performs slightly better than atae-lstm, and it employs two lstm networks to capture the left and right context of the aspect. td-lstm performs worse than our method mgan since it could not properly pay more attentions on the important parts of the context. (3) ian achieves slightly better results with the previous lstm-based methods, which interactively learns the attended aspect and context vector as final representation. our method consistently performs better than ian since we utilize the finegrained attention vectors to relieve the information loss in ian. bilstm-att-g models left context and right context using attention-based lstms, which achieves better performance than memnet. ram performs better than other baselines. our proposed mgan consistently performs better than memnet, bilstm-att-g and ram on all three datasets. 

table 3 shows the performance comparison among the variants of mgan model. we can have the following observations. (1) the proposed fine-grained attention mechanism mgan-f, which is responsible for linking and fusing the information between the context and aspect word, achieves competitive performance compared with mgan-c, especially on laptop dataset. it demonstrates mgan-f has better performance on aspects with more words, and make use of the word-level interactions to relieve the information loss occurred in coarsegrained attention mechanism. (2) mgan-cf is better than both mgan-c and mgan-f, which demonstrates the coarsegrained attentions and fine-grained attentions could improve the performance from different perspectives. compared with mgan-cf, the complete mgan model gains further improvement by bringing the aspect alignment loss, which is designed to capture the aspect level interactions. 

table 1 and table 2 report the results of our experiments. firstly, we observe that our proposed aglr outperforms all neural baselines on 3-way classification. the overall performance of aglr achieves state-of-the-art performance. on average, aglr outperforms lexicon rnn and at-bilstm by 1% âˆ’ 3% in terms of f1 score. we also observe that aglr always improves at-bilstm which ascertains the effectiveness of learning auxiliary lexicon embeddings. we also observe that lexicon rnn does not handle 3-way classification well. even though it has achieved good performance on binary classification, the performance on 3-way classification is lackluster (the performance of aglr outperforms lexicon rnn by up to 8% on semeval16 train). finally, we observe that bilstm and at-bilstm outperform lexicon rnn on average with lexicon rnn being slightly better on binary classification. 

table 3 reports the results of our proposed approach against the top team of each semeval run, i.e., nrc-canada (mohammad et al., 2013) for 2013 task 2, team-x (miura et al., 2014) for 2014 task 9, swisscheese (deriu et al., 2016) for 2016 task 4. following the competition setting, with the exception of accuracy for semeval 2016, all metrics reported are the macro averaged f1 score of positive and negative classes. we observe that aglr achieves competitive performance relative to the top runs in semeval 2013, 2014 and 2016. on the other hand, our proposed model is a single neural model. given these conditions, we find it remarkable that our single model is able to achieve competitive performance relative to the extensively engineered approach of swisscheese. moreover, we actually outperform significantly in terms of pure accuracy. aglr performs competitively on semeval 2013 and 2014 as well. 

we describe the task setting results in table 2, and detailed per-event results in table 3. although tfg does not achieve the highest f1-score in the task setting, it is mainly due to the split of the dataset. in the event setting, which has a more fair comparison, tfg outperformed other methods with a big margin (p<0.001). experiment results using the task setting are shown in table 2 and the detailed per-event results are listed in table 3. the performance of combo is also listed in table 2. since combo would contain noise introduced from combining webpages indexed by different search engines, it is not surprising that combo performs slightly worse than tfg extracted from google webpages which already cover a wide range of information solely. however, combo performs much better than tfb which only leverages baidu webpages. 

table 5 lists the detailed results of our transfer learning experiment. we achieved much better performance compared to the baseline with statistical significance (p<0.001), which indicates that our cross-lingual cross-platform feature set can be generalized to rumors in different languages. in event 11 (pig fish), transfer achieves much higher performance than the random baseline. transfer obtains pretty low f1-scores in event 07 (passport hoax). since transfer is pre-trained using twitter dataset, it is not surprising that transfer achieves 0 in f1-score on this event. 

table 1 displays the results. the difference in performance between the three baselines that don't use a rnn generator and the three model variants that do demonstrates the importance of context in recognizing personal attacks within text. the relative performance of the three variants of the lei et al. model show that both modifications, setting the bias term and the addition of the adversarial predictor, lead to marginal improvements in tokenwise f1. the best-performing model approaches average human performance on this metric. the phrasewise metric is relaxed. the results on this metric show that in an absolute sense, 87% of personal attacks are at least partially captured by the algorithm. 

in table 7 we present results on classifying privacy policy sentences into four categories: clear, somewhat clear, vague, and extremely vague. we compare ac-gan with three baselines: cnn and lstm trained on human-annotated sentences, and a majority baseline that assigns the most frequent label to all test sentences. we observe that the acgan models (using cnn discriminator) perform strongly, surpassing all baseline approaches. cnn shows strong performance, yielding an f-score of 50.92%. comparing "full model" with "vagueness only",we found that allowing the ac-gan to only discriminate sentences of different levels of vagueness, but not real/fake sentences, yields better results. 

table 1 shows the results of the evaluation. first note that the logistic regression classifier and the cnn model using the title outperforms the chance classifier significantly (f1: 59.12,59.24 vs 34.53). second, only modeling the network structure yields a f1 of 55.10 but still significantly better than the chance baseline. third, note that modeling the content (hdam) significantly outperforms all previous baselines (f1:68.92). finally, all flavors of our model outperform the baselines. it is also worth noting that without the network, only the title and the content show only a small improvement over the best performing baseline (69.54 vs 68.92) suggesting that the network yields distinctive cues from both the title, and the content. finally, the best performing model effectively uses all three modalities to yield a f1 score of 79.67 outperforming the state of the art baseline by 10 percentage points. 

we present in table 4 the results of using features from the different sources proposed in section 3. we can see that the textual features extracted from the articles yielded the best performance on factuality. they also perform well on bias, being the only type that beats the baseline on mae. they also show that using the titles only is not enough, and that the article bodies contain important information that should not be ignored. overall, the wikipedia features are less useful for factuality, and perform reasonably well for bias. interestingly, the has page feature alone yields sizable improvement over the baseline, especially for factuality. the twitter features perform moderately for factuality and poorly for bias. this is not surprising, as we normally may not be able to tell much about the political ideology of a website just by looking at its twitter profile (not its tweets!) unless something is mentioned in its description, which turns out to perform better than the rest of the features from this family. we can see that the has twitter feature is less effective than has wiki for factuality, which makes sense given that twitter is less regulated than wikipedia. note that the counts features yield reasonable performance, indicating that information about activity (e.g., number of statuses) and social connectivity (e.g., number of followers) is useful. overall, the twitter features seem to complement each other, as their union yields the best performance on factuality. the url features are better used for factuality rather than bias prediction. overall, this feature family yields slight improvements, suggesting that it can be useful when used together with other features. finally, the alexa rank does not improve over the baseline, which suggests that more sophisticated traffic-related features might be needed. 

table 1 compares results in terms of variance explained, when using the three hand-picked factors vs. using all 11 extra-linguistic factors (since past work has also used the pearson-r metric, table 2 shows the same results for all factors in terms of pearson-r). as the table shows, fa outperforms controls only, added-controls, and residualized control. rfa does even better and outperforms fa on both the hand-picked factors and when using the entire set of factors. even though adding controls directly, as in the “added-controls” column, works better than language-only and controls-only models, it is worse than any other model that exploits both language and extra-linguistic data. overall, these results show the power of rfa over the other models. 

table 2 depicts the results obtained by combining character trigrams, tokens, and spelling features (sections 3.5.1, 3.5.2). as expected, these content features yield excellent results in-domain, but the accuracy deteriorates out-of-domain, especially in the most challenging task of nli. 

table 4 shows the results obtained by combining the spelling features with the grammar features (section 3.5.2). clearly, these two feature types reflect somewhat different phenomena, as the results are better than using any of the two alone. 

table 5 shows the accuracy obtained by all the centrality features (section 3.5.4), excluding the most popular subreddits. as expected, the contribution of these features is small, and is most evident on the binary task. the signal of the native language reflected by these features is very subtle, but is nonetheless present, as the results are consistently higher than the baseline. 

the results are summarized in table 3. we have several findings from table 3 as follows:. the accuracy of both left and right half tokens in the normal translation is lower than that in teacher forcing, which feeds the ground-truth tokens as inputs. taking en-zh and the left-to-right nmt model as an example, the bleu score improvement of the right half (the second half of the generation) of teacher forcing over normal translation is 2.64, which is much larger than the accuracy improvement of the left half (the first half of the generation): 1.70. similarly, for en-zh with the right-to-left nmt model, the bleu score improvement of the left half (the second half of the generation) of teacher forcing over normal translation is 2.82, which is much larger than the accuracy improvement of the right half (the first half of the generation): 1.77. taking en-de translation with the left-to-right model as an example, the accuracy of the left half (9.43) is higher than that of the right half (8.36) when there is no error propagation with teacher forcing. similar results can be found in other language pairs and models. 

table 2 shows that the learning of baseline reward does not help rl training. 

from table 3 and 4, we have several observations. first, monolingual data helps rl training, improving bleu score from 25.04 to 25.22 (ρ < 0.05) in table 3. second, when we only add monolingual data for rl training, the model achieves similar performance compared to mle training with bilingual and monolingual data (e.g., 25.15 vs. 25.24 (ρ < 0.05) in table 4). 

the results are reported in table 5. from table 5, we can observe that the sequential training of monolingual data can benefit the model performance. taking the last three rows as an example, the bleu score of the mle model trained on the combination of bilingual data and target-side monolingual data is 25.24; based on this model, rl training using the source-side monolingual data further improves the model performance by 0.7 (ï < 0.01) bleu points. 

so as to put our results into perspective, table 3 comprises the results of different supervised methods in the same test sets. quite surprisingly, our proposed system, trained exclusively on monolingual corpora, is relatively close to a comparable phrase-based smt system trained on europarl, with differences below 5 bleu points in all cases and as little as 2.5 in some. in addition to that, the results for the constrained variants of this smt system justify some of the simplifications required by our approach. in particular, removing lexical reordering and constraining the phrase table to the most frequent n-grams, as we do for our initial system, has a relatively small effect, with a drop of less than 1 bleu point in all cases, and as little as 0.28 in some. replacing standard mert tuning with our unsupervised variant does cause a considerable drop in performance, although it is below 2.5 bleu points even in the worst case, and our unsupervised tuning method is still better than using default weights as reported in table 2. 

table 4 (top) reports the results using micro-averaged f1. our models outperform both variants of abae across domains. abaeinit improves upon the vanilla model, affirming that informed aspect initialization can facilitate the task. the richer multi-seed representation of mate, however, helps our model achieve a 3.2% increase in f1. further improvements are gained by the multi-task model, which boosts performance by 2.7%. the combined use of polarity and aspect information improves the retrieval of salient opinions across domains, as all model variants that use our salience formula of equation (12) outperform the milnet and aspect-only baselines. when comparing between aspect-based alternatives, we observe that the extraction accuracy correlates with the quality of aspect prediction. in particular, ranking using milnet+mate+mt gives best results, with a 2.6% increase in map against milnet+mate and 4.6% against milnet+abaeinit. 

table 5 presents rouge-1, rouge-2 and rouge-l f1 scores, averaged across domains. our model (milnet+mate+mt) significantly outperforms all comparison systems (p < 0.05; paired bootstrap resampling; koehn 2004), whilst using a redundancy filter slightly improves performance. assisting opinosis with aspect predictions is beneficial, however, it remains significantly inferior to our model (see the supplementary material for additional results). 

table 2 compares the performances of our nrr model to the state-of-the-art results reported by paetzold and specia (2017). our full model (nrrall+binning+wc) exhibits a statistically significant improvement over the state-of-the-art for both measures. we also conducted ablation experiments to show the effectiveness of the gaussianbased feature vectorization layer (+binning) and the word-complexity lexicon (+w c). 

table 4 shows the performance of our model compared to simpleppdb and other baselines. our neural readability ranking model alone with gaussian binning (nrrall+binning) achieves better accuracy and precision while using less features. leveraging the lexicon (nrrall+binning+wc) shows statistically signi?cant improvements over simpleppdb rankings based on the paired bootstrap test. the accuracy increases by 3.2 points, the precision for ‘simplifying’ class improves by 7.4 points and the precision for ‘complicating’ class improves by 4.0 points. 

table 5 shows the comparison of simpleppdb and simpleppdb++ on the number of substitutions generated for each target, the mean average precision and precision@1 for the final ranked list of candidate substitutions. this is a fair and direct comparison between simpleppdb++ and simpleppdb, as both methods have access to the same paraphrase rules in ppdb as potential candidates. the better nrr model we used in creating simpleppdb++ allows improved selections and rankings of simplifying paraphrase rules than the previous version of simpleppdb. 

we compare our enhanced approaches (sv000gg+w c and nc+w c) and lexicon only approach (w c-only), with the state-of-the-art and baseline threshold-based methods. table 6 shows that the wordcomplexity lexicon improves the performance of sv000gg and the nearest centroid classifier in all the three metrics. the word-complexity lexicon alone (wc-only) performs satisfactorily on the cwig3g2 dataset, which effectively is a simple table look-up approach with extreme time and space efficiency. for cwi semeval 2016 dataset, wc-only approach gives the best accuracy and fscore, though this can be attributed to the skewed distribution of dataset (only 5% of the test instances are 'complex'. 

table 2 reports results using gold predicates. we obtain further improvement of 0.8 absolute f1 with the best syntactic scaffold from the frame srl task. 

table 9 further shows f-scores for the baseline and the both-retrained model relative to each role type in detail. from the figure we can observe that, all the semantic roles achieve significant improvements in performances. 

table 1 shows the details of the test performance. as shown in table 1, s1 achieves higher dialog success rate and rewards when testing with sim1. 

the results of separate training for slot filling and intent detection are reported in table 1 and table 2 respectively. table 1 compares f1-score of slot filling between our proposed architecture and some previous works. our model achieves state-of-the-art results and outperforms previous best model by 0.56% in terms of f1-score. 

table 4 shows the joint learning performance of our model on atis data set by removing one module at a time. we find that all variants of our model perform well based on our gate mechanism. as listed in the table, all features contribute to both slot filling and intent classification task. if we remove the self-attention from the holistic model or just in the intent-augmented gating layer, the performance drops dramatically. if we remove character-level embeddings and only use word-level embeddings, we see 0.22% drop in terms of f1-score. 

our results for our proposed model and comparison with other models for permuted-babi dialog task are given in table 2. we show results for three models memn2n, memn2n + all-answers and our proposed model, mask-memn2n. from table 2, we observe that the memn2n + all-answers model performs poorly, in comparison to the memn2n baseline model both in standard setup and oov setting, as well as with and without match-type features. our proposed model performs better than both the baseline models. in the standard setup, the perdialog accuracy increases from 22% to 32%. using match-type features, the per-dialog accuracy increases considerably from 30.3% to 47.3%. in the oov setting, all models perform poorly and achieve per-dialog accuracy of 0-1% both with and without match-type features. these results are similar to results for original-babi dialog task 5 from bordes and weston (2016b) and our results with the baseline model. overall, mask-memn2n is able to handle multiple correct next utterances present in permutedbabi dialog task better than the baseline models. 

our results for ablation study are given in table 3. we show results for mask-memn2n in various settings - a) without entropy, b) without pre-training mask c) reinforcement learning phase only. when we remove l2 mask pre-training, there is a huge drop in performance. from table 3 it is clear that the rl phase individually does not perform well; it is the combination of both the phases that gives the best performance. 

table 2 shows the performances of the models on quora datasets. in both settings, we find that the proposed rbm-sl and rbm-irl models outperform the baseline models in terms of all the evaluation measures. particularly in quora-ii, rbm-sl and rbm-irl make significant improvements over the baselines, which demonstrates their higher ability in learning for paraphrase generation. on quora dataset, rbm-sl is constantly better than rbm-irl for all the automatic measures, which is reasonable because rbm-sl makes use of additional labeled data to train the evaluator. 

table 4 demonstrates the average ratings for each model, including the ground-truth references. our models of rbm-sl and rbm-irl get better scores in terms of relevance and fluency than the baseline models, and their differences are statistically significant (paired t-test, p-value < 0.01). we note that in human evaluation, rbm-sl achieves the best relevance score while rbm-irl achieves the best fluency score. 

table 1 shows evaluation results for different models on squad split1. s2s-a vs. s2s: we can see attention brings in large improvement on both sentence and paragraph inputs. s2s-a-at vs. s2s-a: answer tagging dramatically boosts the performance, which confirms the importance of answer-aware qg: to generate good question, we need to control/learn which part of the context the generated question is asking about. s2s-a-at-cp vs. s2s-a-at: as expected, copy mechanism further improves the performance on the qg task. more interestingly, the performance is lower when paragraph is given as input than sentence as input. the maxout pointer mechanism outperforms the basic copy mechanism in all metrics. moreover, the effectiveness of maxout pointer is more significant when paragraph is given as the model input, as it reverses the performance gap between models trained with sentence and paragraph inputs, table 1. s2s-a-at-mp-gsa vs. s2s-a-at-mp: the results demonstrate the effectiveness of gated selfattention, in particular, when working with paragraph inputs. the observation is consistent across all metrics. 

the results of our experiments are summarized in table 8 - 10. the first column for each table shows the manner in which the noisy training data was created. the second column shows the bleu4 score of the noisy questions when compared to the original reference questions (thus it tells us the perceived quality of these questions under the bleu4 metric). similarly, the third column tells us the perceived quality of these questions under the q-bleu4 metric. we observe that the general trend is better w.r.t. the q-bleu4 metric than the bleu4 metric (i.e., in general, higher q-bleu4 indicates better performance and lower q-bleu4 indicates poor performance). 

the comparison of our method with various baselines and state-of-the-art methods is provided in table 2 for vqa 1.0 and table 3 for vqg-coco dataset. in both the tables, the first block consists of the current stateof-the-art methods on that dataset and the second contains the baselines. we observe that for the vqa dataset we achieve an improvement of 8% in bleu and 7% in meteor metric scores over the baselines, whereas for vqg-coco dataset this is 15% for both the metrics. in the vqg-coco dataset, we improve over (mostafazadeh et al., 2016) by 3.7% and (jain et al., 2017) by 3.5% in terms of meteor scores. 

table 2 shows the performances of template generator based on coarse-grained and finegrained type respectively, and figure 5 shows an example of the template generated. the template generator based on coarse-grained entity type outperforms the one based on fine-grained entity type for two reasons: (1) fine template relies on edl, and incorrect linkings import noise; (2) named entities usually has multiple types, but we only choose one during generalization. 

results in table 4 show that hnnattti outperforms the random baseline, while hnnatttc and hnnatttic perform worse. to show the influence of our oov replacement mechanism, we eliminate the mechanism from our models, and show the evaluation results in table 4 and table 5. we can see from the two tables that the scores are lower than the corresponding scores in table 2 and table 3. 

table 1 shows the performance comparison of our model with other baselines on the dailymail dataset with respect to rouge score at 75 bytes and 275 bytes of summary length. our model performs consistently and significantly better than other models on 75 bytes, while on 275 bytes, the improvement margin is smaller. 

table 5 shows the percentages of summaries of different models under each rank scored by human experts. it is not surprising that gold standard has the most summaries of the highest quality. our model has the most summaries under 2nd rank, thus can be considered 2nd best, following are hybrid memnet and lead-3, as they are ranked mostly 3rd and 4th. 

table 2 shows experiments with the same systems on the nyt corpus. we see that the 2 point improvement compared to the baseline pointergenerator maximum-likelihood approach carries over to this dataset. here, the model outperforms the rl based model by paulus et al. (2017) in rouge-1 and 2, but not l, and is comparable to the results of (celikyilmaz et al., 2018) except for rouge-l. the same can be observed when comparing ml and our pointer-generator. 

table 3 shows the performance of rl and ilp on the duc’04 dataset. td(λ) significantly outperforms lstd(λ) in terms of all rouge scores we consider. td(λ) also significantly outperforms ilp in terms of all metrics except rouge-2. 

the experimental results are shown in table 4 which indicate that: 1) though trained on scientific papers, our models still have the ability to generate keyphrases for news articles, illustrating that our models have learned some universal features between the two domains; and 2) semi-supervised learning by leveraging unlabeled data improves the generation performances more, indicating that our proposed method is reasonably effective when being tested on cross-domain data. though unsupervised methods are still superior, for future work, we can leverage unlabeled out-of-domain corpora to improve cross-domain keyphrase generation performance, which could be a promising direction for domain adaption or transfer learning. 

the results of our model on both narrativeqa and wikihop with and without commonsense incorporation are shown in table 2 and table 3. we see empirically that our model outperforms all generative models on narrativeqa, and is competitive with the top span prediction models. furthermore, with the noic commonsense integration, we were able to further improve performance (p < 0.001 on all metrics6), establishing a new state-of-the-art for the task. 

table 2 shows the results of automatic evaluation. the proposed model performs the best according to bleu. in particular, the differences between the existing state-of-the-art models are within 0.07, while the proposed model supersedes the best of them by 0.13. 

table 6 shows the human evaluation results. the slight improvement with the skeleton extraction module in bleu reflecs as the decreases in both fluency and coherence. finally, when the skeleton extraction module is trained on the target domain using reinforcement learning, the human evaluation is improved significantly by 14% on g-score. 

table 1 reports the embedding scores on both datasets. nexus network significantly outperforms the best baseline model in most cases. 

table 2 reports the bleu 1-3 scores. as can be seen, nexus network achieves best or near-best performances with only greedy decoders. nexus-h generally outperforms nexus-f as the connection with future context is not explicitly addressed by the bleu score metric. mmi and vhred bring minor improvements over the seq2seq model. even when evaluated on multiple references, rl still performs worse than most models. 

table 1 summarizes the results of different systems for the readability assessment task. cohemb significantly outperforms the graph-based coherence model proposed by mesgar and strube (2016) by a large margin (6%), showing that our model captures coherence better than their model. cohlstm significantly outperforms both the coherence model proposed by mesgar and strube (2016) and the cohemb model by 11% and 5%, respectively, and defines a new state-of-the-art on this dataset. cohlstm, which captures exclusively local coherence, even outperforms the readability system proposed by de clercq and hoste (2016), which relies on a wide range of lexical, syntactic and semantic features. 

table 4 summarizes our performances on arxiv abstract and sind caption. as we see, all models show fair well in predicting the first sentence, and the prediction accuracy declines for the last one. it is observed that attordernet still achieves a boost in predicting two positions compared to the previous state-of-the-art system on both datasets. 

table 5 reports the results of attordernet and currently competing architectures in this evaluation task. attordernet also achieves the stateof-the-art performance, showing a remarkable advancement of about 1.8% gain on accident dataset and further improving the pairwise accuracy to 99.8 on earthquake dataset. lstm+ptrnet and cnn+ ptrnet (gong et al., 2016) fall short of varient-lstm+ptrnet (logeswaran et al., 2018) in performance. compared to the result in the sentence ordering task, entity grid (barzilay and lapata, 2008) achieves a good performance in this task and even outperforms recurrent neural networks and recursive neural networks (li and hovy, 2014) on accident dataset. 

table 5 shows the results for these experiments. as shown, this method works poorly, since the τ grid-search could not find a reasonable τ which worked well for every possible language pair. this yielded our best crosslingual score of f1=84.0, confirming that crosslingual similarity is of higher quality between each language and english, for the embeddings we used. 

as table 4 shows, before update denotes the model trained on the old tasks before the new tasks are involved, so only evaluations on the old tasks are conducted. cold update re-trains the model of before update with both the old tasks and the new tasks, thus achieving similar performances with the last line in table 3. different from cold update, hot update resumes training only on the new tasks, requires much less training time, while still obtains competitive results for all tasks. the new tasks like imdb and kitchen benefit more from hot update than the old tasks, as the parameters are further tuned according to annotations from these new tasks. zero update achieves competitive performances in case 1 (90.9 for imdb) and case 2 (86.7 for kitchen), as tasks from these two cases all belong to sentiment datasets of different cardinalities or domains that contain rich semantic correlations with each other. however, the result for imdb in case 3 is only 74.2, as sentiment shares less relevance with topic and question type, thus leading to poor transferring performances. 

as table 5 shows, mtle achieves competitive or better performances on most tasks except for qc, as it contains less correlations with other tasks. tree-lstm outperforms our model on sst1 (50.6 against 49.8), but it requires an external parser to get the sentence topological structure and utilizes treebank annotations. pv slightly surpasses mtle on imdb (91.7 against 91.3), as sentences from imdb are much longer than sst and mdsd, which require stronger abilities of long-term dependency learning. 

we present the results of the evaluation on table 5, where it can be found that our model with fewer parameters still outperforms the hierarchical model with the deterministic setting of sentence or phrase. the results in table 5 show that the hierarchical models achieve similar performances, which are all higher than the performances of the baselines. 

as table 2 shows, our single-task network enhanced by capsules is already a strong model. capsnet1 that has one kernel size obtains the best accuracy on 2 out of 6 datasets, and gets competitive results on the others. and capsnet-2 with multiple kernel sizes further improves the performance and get best accuracy on 4 datasets. particularly, our capsule network outperforms conventional cnns like dcnn, cnn-mc and vd-cnn with a large margin (by average 1.1%, 0.7% and 1.0% respectively), which shows the advantages of capsule network over conventional cnns for clustering features and leveraging the position information. we conduct the ablation experiment on orphan category, and result (table 2) shows that network with orphan category perform better than the without one by 0.4%. 

table 5 shows that the acnn model is competitive with recent models from the literature. 

the values of word similarity on isear and youtube are respectively shown in table 7 and table 8, where the best results are highlighted in boldface. we can observe that sltm outperforms baselines for all cases. the results indicate that word embeddings learned from the global label-specific topic information are better than those from the local context information without any external corpora. 

table 3 shows the performance of our model with different sample size for a mini-corpus. for the 20 newsgroups dataset, the best performance is achieved when the sample size is 3. when we do not use our sample strategy (mini-corpus is 1), the performance drops by a large margin. when sample size increases, the performance drops again. compared to 20 newsgroups dataset, documents in the all news dataset is longer and carried more topic information, so the best performance is achieved without sampling. 

in table 6 we present cross auc evaluations. rows correspond to the embedding used and columns to the aspect evaluated against. as expected, aspect-embeddings perform better w.r.t. the aspects for which they code, suggesting some disentanglement. however, the reduction in performance when using one aspect representation to discriminate w.r.t. others is not as pronounced as above. 

table 4 summarizes our results on pos tagging. again, our approach consistently achieves the best performance across different settings and tasks. adding twitter as a source leads to a drop in performance for the unified model, as a result of negative transfer. 

table 1 shows the results of our model and the baselines in the emoji prediction task for the different evaluation splits. the evaluation metrics used are: f1, accuracy@k (a@k, where k ∈ {1, 5}), and coverage error (ce) (tsoumakas et al., 2009). the results show that our proposed 2-bilstmsl method outperforms all baselines for f1 in three out of four settings, and for ce in all of them. 

table 1 shows the translation performance on test sets measured in bleu score. simply training nmt model by the probabilistic 2-gram precision achieves an improvement of 1.5 bleu points, which significantly outperforms the reinforcement-based algorithms. we also test the precision of other n-grams and their combinations, but do not notice significant improvements over p-p2. 

table 3 and 4 list the results for the kg completion tasks. ta-transe and ta-distmult systematically improve transe and distmult in mrr, hits@10 and hits@1 in almost all cases. this explains that ttranse is only competitive in yago15k, wherein the number of distinct timestamps is very small (see #distinct ts in table 2) and thus enough training examples exist to learn robust timestamp embeddings. ttranseâ€™s performance is similar to that of ta-transe, our time-aware version of transe, in wikidata. 

we find that almost all of the proposed loss functions outperform the vanilla autoencoder trained with cross-entropy on all three tasks (see table 1). the only exception is the weighted similarity loss function. among the models we tested, the best scores were achieved by the weighted cross-entropy loss for msrp (68.2%), the weighted similarity loss for snli (69.1%) and by the soft label loss for sick-e (72.4%). 

table 3 contains results achieved without using any pretrained embeddings. our model achieves the best results among nn models on 6/7 datasets. 

the results are summarized in table 6. negative numbers in table 6 correspond to decreases in performance for the ablated system. note that although each of the components help performance on average, there are cases where we observe no impact. for example using recurrent dropout on as and msr rarely affects accuracy. 

table 2 shows the results. we can find from the table that only 1024 guideline sentences can improve the performance of “slm-4” significantly. while rule-based post-processing is very effective, “slm-4†” can outperform “slm-4*” on all the four datasets. moreover, performance drops when applying the rule-based post-processing to “slm-4†” on three datasets. 

table 1 reports the results of this experiment. we see that models trained with joint objective (jx) improve over baseline models (bx), both in terms of f1 and average disagreement rate. these improvements provide evidence for answering (q1-3) favorably. 

table 2 shows our results for language modeling. prpn-up, configured as-is with parsing criterion and language modeling criterion, performs dramatically worse than the standard prpn-lm (a vs. d and e). adjusting the vocabulary of prpnup down to 10k to make a fairer comparison possible, the ppl of prpn-up improves significantly (c vs. d), but not enough to match prpn-lm (a vs. c). we also observe that early stopping on parsing leads to incomplete training and a substantial decrease in perplexity (a vs. b and d vs. e). both prpn models trained on allnli do even worse (f and g), though the mismatch in vocabulary and domain may explain this effect. 

in addition, table 3 shows that the prpn-up models achieve the median parsing f1 scores of 46.3 and 48.6 respectively on the multinli dev set while prpn-lm performs the median f1 of 45.7; setting the state of the art in parsing performance on this dataset among latent tree models by a large margin. 

table 4 presents the results of a various of model architectures and shows several challenges. as expected, the model achieves almost perfect score on the inform metric on the cam676 dataset taking the advantage of an oracle belief state signal. however, even with the perfect dialogue state tracking of the user intent, the baseline models obtain almost 30% lower score on the inform metric on the new corpus. the addition of the attention improves the score on the success metric on the new dataset by less than 1%. nevertheless, as expected, the best model on multiwoz is still falling behind by a large margin in comparison to the results on the cam676 corpus taking into account both inform and success metrics. moreover, the bleu score on the multiwoz is lower than the one reported on the cam676 dataset. 

we submitted the best bison model out of the random three of table 1 to be evaluated on the hidden test set and report results in comparison to the best model on the leaderboard,3 e3 (zhong and zettlemoyer, 2019) in table 2. bison outperforms e3 by 5.6 bleu-4 points, while it is only slightly worse than e3 in terms of accuracy. 

table 1 compares masked lm perplexity for knowbert with bertbase and bertlarge. overall, knowbert improves the masked lm perplexity, with all knowbert models outperforming bertlarge, despite being derived from bertbase. 

the results of this experiment are reported in table 4 as pearson and spearman correlation and mean squared error (mse). we used the ? configuration of our model with chen2014 to represent senses and bert-l-u-4 to represent words. as we can see the simplicity of the method leads to low performances for both representations, but sense vectors correlate better than word vectors. 

the performance of each approach that interact swith the agenda-based user simulator is shown in table 3. gdpl achieves extremely high perfor-mance in the task success on account of the substantial improvement in inform f1 and match rateover the baselines. surprisingly, gdpl even outperforms human in completing the task,  and its average dialog turns are close to those of humans, though gdpl is inferior in terms of match rate. humans almost manage to make a reservation in each session, which contributes to high task success.  however,  it  is  also  interesting  to  find  that human have low inform f1, and that may explain why the task is not always completed successfully. actually, there have high recall (86.75%) but low precision  (54.43%)  in  human  dialogs  when  answering  the  requested  information. 

table 3 shows the comparison with previous work on the pgr testset, where our models are significantly better than the existing models. 

table 1 shows the experimental results, from which we can observe that:. (1) rsn models outperform all baseline models on precision, recall, and f1-score, among which weakly-supervised rsn (sn-l+cv) achieves state-of-the-art performances. compared with rw-hac, sn-hac achieves better clustering results because of its supervised relational representation and similarity metric. (3) louvain outperforms hac for clustering with rsn, comparing sn-hac with sn-l. 

table 1 shows the results of baseline as well as our proposed models on tacred dataset. it is observed that our proposed knowledge-attention encoder outperforms all cnn-based and rnnbased models by at least 1.3 f1. meanwhile, it achieves comparable results with c-gcn and selfattention encoder, which are the current start-ofthe-art single-model systems. 

table 2 shows the overall performance of our proposed model as well as the baseline methods (p and r denote precision and recall). our method consistently outperforms all baselines in five languages w.r.t f1, mainly because we greatly improve recall (2.7% to 9.34% on average) by taking best advantage of wl data and being robust to noise via two modules. as for the precision, partial-crfs perform poorly compared with crfs due to the uncertainty of unlabeled words, while our method alleviates this issue by introducing linguistic features in non-entity sampling. 

table 3 shows the results on the five crossdomain datasets. as shown, none of existing methods can consistently win on all datasets. dca-based models achieve state-of-the-art performance on the msbnc and the ace2004 dataset. on remaining datasets, dca-rl achieves comparable performance with other complex global models. in addition, rl-based models show on average 1.1% improvement on f1 score over the sl-based models across all the crossdomain datasets. 

table 4 shows the effectiveness of this strategy. we observe that incorporating these neighbor significantly improve the performance (compared to 1-hop) by introducing more related information. and our analysis shows that on average 0.72% and 3.56% relative improvement of 2-hop dca-(sl/rl) over 1-hop dca-(sl/rl) or baseline-sl (without dca) is statistically significant (with p-value < 0.005). 

table 2 shows the performance of different bootstrapping methods on google web 1t. we can see that our full model outperforms three baseline methods: comparing with pos, our method achieves 41% improvement in p@100, 35% improvement in p@200 and 45% improvement in map; comparing with meb, our method achieves 24% improvement in p@100 and 18% improvement in p@200; comparing with cob, our method achieves 3% improvement in both p@100 and p@200 metrics, and 2% improvement in map. the above findings indicate that our method can extract more correct entities with higher ranking scores than the baselines. 

table 7 shows the results of event argument role labeling on chinese and arabic entity mentions automatically extracted by stanford corenlp instead of manually annotated mentions. the system extracted entity mentions introduce noise and thus decrease the performance of the model, but the overall results are still promising. 

table 4 describes the performances of our model on the five categories on the test dataset. our model outperforms the model described in ju et al. (2018) and sohrab and miwa (2018) with f-score value on all categories. 

table 5 shows the results of boundary detection on genia test dataset. our model locates entities more accurately with a higher recall value (76.9%) than the comparing methods. it gives a reason why our model outperforms other state-of-the-art methods in recall value. 

table 7 shows the performance of our pipeline model and multitask model on genia development set and test set. our multitask model has a higher f value both in development set and test set. 

the p@n results in table 3 indicate that ccl further improves the model's performance when compared to pcnn+att/one+selfatt as well. 

table 3 shows the performance of our models on the enteval tasks. pretrained cwrs (elmo, bert) perform the best on enteval overall, indicating that they capture knowledge about entities in contextual mentions or as entity descriptions. bert performs poorly on entity similarity and relatedness tasks. bert large is better than bert base on average, showing large improvements in ert and ned. the large model appears to be handling these capabilities better than the base model. entelmo improves over the entelmo baseline (trained without the hyperlinking loss) on some tasks but suffers on others. the hyperlink-based training helps on cerp, efp, et, and ned. since the hyperlink loss is closely-associated to the ned problem, it is unsurprising that ned performance is improved. overall, we believe that hyperlink-based training benefits contextualized entity representations but does not benefit descriptive entity representations (see, for example, the drop of nearly 2 points on esr, which is based solely on descriptive representations). 

in table 4 we further show the breakdown performances for each positive relation on tb-dense. before, after and vague are the three dominant label classes in tb-dense. we observe that the linguistic rule-based model, caevo, tends to have a more evenly spread-out performance, whereas our neural network-based models are more likely to have concentrated predictions due to the imbalance of the training sample across different label classes. 

table 1 reports the results of our model on different datasets comparing with the widely used text classification methods and state-of-the-art approaches. we can have the following observations. our hcapsnet achieves the best results on 5 out of 6 datasets, which verifies the effectiveness of our model. in particular, hcapsnet outperforms vanilla capsule network capsule-b[yang et al., 2018] by a remarkable margin, which only utilizes the dynamic routing mechanism without hyperplane projecting. 

the bottom part of table 5 shows the scores when restricting the evaluation to sentences with score greater than equal 10. we observed that this threshold is a good trade-off in both the amount of kept sentences (above the threshold) and average bleu score increase (presumably sentence quality). 

firstly, results in table 5 show that with little dependency information (dep), lisa performs better, while incorporating richer syntactic knowledge (dep&rel or dep&relpath), three methods achieve similar performance. overall, relawe achieves best results given enough syntactic knowledge. 

table 7 shows that our open model achieves more than 3 points of f1-score than the stateand relawe with depof-the-art path&relpath achieves in both closed and open settings. besides, performance gap between three models under open setting is very small. at last, the gold result is much higher than the other models, indicating that there is still large space for improvement for this task. 

table 8 presents results obtained with the neural network when predicting temporal anchors. the image components are beneficial with all anchors, especially before (f1:0.47 vs. 0.59, +25%) and after (0.55 vs. 0.67, +22%), and to a lesser degree during (0.48 vs. 0.52; 8%). f1 scores are higher for yes label than no label across all temporal anchors. 

table 6 shows that training with the curiosity-encouraging objective reduces the chance of the agent looping and making the same decisions repeatedly. as a result, its success rate is greatly boosted (+4.33% on test unseenall) over no curiosity-encouraging. 

to explore our lexical mapping method, we compare the performance of several variant systems retrieving a different number of candidates (ranging from 1 to 5) and the embedding-projection method (embedding proj). table 2 summarizes the results. from the results, we observe that even though both of cl trans (1 cand.) and embedding proj are content-independent mapping methods, the former outperforms the latter by a margin (+3.2% on f1). but when too many candidates (e.g., 5) are added, the precision drops, which harms the overall f1 measure. 

table 2 compares the performance of the discussed models against the baselines, evaluating per-step entity prediction performance. using the ground truth about ingredient's state, we also report the uncombined (ur) and combined (cr) recalls, which are per-timestep ingredient recall distinguished by whether the ingredient is explicitly mentioned (uncombined) or part of a mixture (combined). note that exact match and first occ baselines represent high-precision and high-recall regimes for this task, respectively. 

table 9 presents these ablation studies. we only observe a minor performance drop from 84.59 to 82.71 (accuracy) when other ingredients are removed entirely. removing verbs dropped the performance to 79.08 and further omitting both leads to 77.79. 

table 1 shows the average of precision over the 10 random splits @k = 1, 5 and 10. the rgp column refers to our model without the piecewise mapping, which we discuss later in this section. in all cases, except czech (cs) and bulgarian (bg) @1 where muse has a slight edge over llmap, our method achieved higher precision on average over 10-fold cross-validation than the muse algorithm. in the majority of the cases the improvements are statistically significant. we can see the most significant improvements (over 8%) are observed in japanese (ja) language and chinese (zh). the other languages mostly see between 1%-3% improvement in the precision. the average gain in precision @10 sits at 3.7%. 

table 2 shows the precision@5 for the pre-split dictionaries. in all 14 languages the llmap outperforms the muse algorithm for recovering both all senses and any sense of a word with significant gains in chinese and japanese. 

table 6 provides bleu and consistency scores for the docrepair model trained on different amount of data. we see that even when using a dataset of moderate size (e.g., 5m fragments) we can achieve performance comparable to the model trained on a large amount of data (30m fragments). moreover, we notice that deixis scores are less sensitive to the amount of training data than lexical cohesion and ellipsis scores. 

results in table 4 show that models with ds-init yield the best perplexity on both training and development set, and those with t2t achieve the best bleu on the training set. however, dsinit+matt performs best in terms of bleu on the development set. 

table 4 shows that while ner systems trained on projected data do categorically worse than an ner system trained on gold-standard data, the higherquality alignments obtained from discalign lead to a major improvement in f1 when compared to fastalign. 

table 3 lists the results. we compare our models and the best external zp prediction approach. as seen, our models also significantly improve translation performance, demonstrating the effectiveness and universality of the proposed approach. 

table 4 shows the results, where the uas values are reported. the first block shows several models by directly transferring gold-standard source treebank knowledge into the target side, including the models of guo15 (guo et al., 2015), guo16 (guo et al., 2016b) and ta16 (tiedemann and agic´, 2016). our model gives the best performance with one exception on the german language. 

table 2 presents the results on english penn treebank. our model h-ptrnet-pst (gate) outperforms the baseline by 0.09 and 0.08 in terms of uas and las, respectively. performance of h-ptrnet-pst (sgate) is close to that of h-ptrnet-pst (gate), though we see slight improvement. we also test h-ptrnetps (gate), the model with parent and sibling connections only, which further improves the performance to 96.09 and 95.03 in uas and las. 

table 4 presents the results of our experiments (without elmo) on chinese, german, and spanish. although we have not performed detailed parameter selection in these languages (i.e., we used the same parameters as in english), our model achieves state-of-the-art performance across all three languages. 

we employ two baselines: a monolingual model (ยง3.1) and a cross-lingual model (ยง2.3), both without data augmentation. we compare both baselines to models trained with morph and nonce augmentation methods. table 3 reports our results. we see that the cross-lingual training (cross-base) performs better than monolingual models even with augmentation. for the t10 setting, cross-base achieves almost twice as much as the monolingual baseline (mono-base). the benefits of data augmentation are less evident in the cross-lingual setting, but in the t10 scenario, data augmentation still clearly helps. overall, cross-lingual combined with data augmentation yields the best result. 

table 7 reports the las performance on the development sets. morph augmentation improves performance over the zero-shot baseline and achieves comparable or better las with a cross-lingual model trained with pre-trained word embeddings. in the zero-shot experiments, simply mapping both turkish and kazakh characters to the latin alphabet improves accuracy from 12.5 to 21.2 las. cross-lingual training with morph further improves performance to 36.7 las. 

table 2 shows the full results. our unsupervised approach achieves a test set f1 score of 78.8, comparable to the 79.4 f1 score found by the supervised prototypical approach. the factorized and dnn models significantly outperformed our approach with f1 scores of 89.2 and 89.0, respectively. 

we first use the three meta-learning algorithms with pps sampling and present in table 1 the experimental results on the glue test set. generally, the meta-learning algorithms achieve better performance than the strong baseline models, with reptile performing the best. since the mt-dnn also uses pps sampling, the improvements suggest meta-learning algorithms can indeed learn better representations compared with multi-task learning. reptile outperforming maml indicates that reptile is a more effective and efficient algorithm compared with maml in our setting. 

based on table 1, mpwimseq clearly outperforms sse on twitter, pit-2015, sts-2014, wikiqa, and trecqa. however, for the snli and quora datasets, sse slightly exceeds mpwim by 0.4% and 1.6%, respectively. 

table 7 shows the results in terms of accuracy for the various experimental set-ups. the results reflect the difference between the two subsets: the results on the gcs data fluctuate more (between 0.79 and 0.94 accuracy) when the amt or the gcs data is used for training, while amt is rather stable (0.86 – 0.89 accuracy). using all available training data leads to best results on both test subsets, for both simple and complex sentences. 

table 2 show the f1 score of slot filling performance comparison results on atis dataset. the results show that progmodel consistently outperforms attrnn, where the improvement gain is up to 4.24% in atis. as expected, progmodel continuously improves performance with moreand more new batches of training data, even though it is only trained on new data at each batch. among all competitors, ft-cp-attrnn achieves the closest performance to progmodel by using much larger model size (shown in section 3.4). the values in pink show that the performance of ftattrnn and ft-cp-attrnn drops up to 3.82% and 5.38% respectively. as a result, their f1 scores are significantly reduced in the end. at last, we observe that progmodel is quite close to upper bound performance (note that this is only for reference rather than comparison since upper bound performance assumes the availability of all training data while progmodel does not). 

table 2 presents results for all models across the four variations of the dataset. first, bert is consistently the best approach for in-scope, followed by mlp. second, out-of-scope query performance is much lower than in-scope across all methods. training on less data (small and imbalanced) yields models that perform slightly worse on in-scope queries. the trend is mostly the opposite when evaluating out-of-scope, where recall increases under the small and imbalanced training conditions. 

table 3 compares classifier performance using the oos-binary scheme. in-scope accuracy suffers for all models using the undersampling scheme when compared to training on the full dataset using the oos-train and oos-threshold approaches shown in table 2. however, out-of-scope recall improves compared to oos-train on full but not oos+. augmenting the out-of-scope training set appears to help improve both in-scope and out-of-scope performance compared to undersampling, but outof-scope performance remains weak. 

automatic results: table 1 shows that all data augmentation approaches (last 3 rows) improve statistically significantly (p < 0.01) 10 over the strongest baseline vhred (w/ attention). moreover, our input-agnostic autoaugment is statistically significantly (p < 0.01) better (on activity and entity f1) than the strong manual-policy alloperations model, while the input-aware model is stat.signif.(p < 0.01) better on activity f1.11 . 

table 2 shows our classification results, presenting the f1 scores obtained by the different mt-based approaches in the two training conditions. when nmt is trained on 100% of the parallel data, for both languages reinforce produces translations that lead to classification improvements over those produced by the generic model (+0.5 de-en, +0.8 it-en). although the scores are considerably better than those obtained by the original classifiers (+9.3 de-en, +7.2 it-en), the gap with respect to the english classifier is still quite large (-1.4 de-en and -2.3 it-en). 

comparison to prior work: table 2 shows the performance of previous models trained on the same data. for arabic, both a-tcn and bilstm provide significantly better performance than madamira (pasha et al., 2014), which is a morphological disambiguation tool for arabic. the performance of zalmout and habash (2017)’s model falls in between bilstm and atcn. for vietnamese, when we re-train naplava et al. ´ (2018) model on the same sample discussed in section 4, both a-tcn and bilstm provide significantly better results. for yoruba, both character-based architectures provide lower performance than orife (2018)’s model. 

table 3 shows the performance of our model on german-english document-level translation and the baseline here refers to the transformer model. from the results, our proposed hmgdc model can help improve the transformer model on german-english document-level translation by 0.90 bleu points. 

table 1 shows the performance of our model and other competitive approaches on the development and test sets. mtmsn outperforms all existing approaches by a large margin, and creates new state-of-the-art results by achieving an em score of 75.85 and a f1 score of 79.88 on the test set. as we can see, our model obtains 12.07/13.19 absolute gain of em/f1 over the baseline, demonstrating the effectiveness of our approach. however, as the human achieves 95.98 f1 on the test set, our results suggest that there is still room for improvement. 

table 4 shows that our gains mainly come from the most frequent number type, which requires various types of symbolic, discrete reasoning operations. moreover, significant improvements are also obtained in the multi-span category, where the f1 score increases by more than 40 points. this result further proves the validity of our multi-span extraction method. 

the results are given in table 5 where we report the accuracy (p@1), averaged over the five datasets. interestingly, we do not observe large differences between supervised training and ws-tb for both models when they use the same number of positive training instances (ranging from 2.8k to 5.8k). further, when we use all available title-body pairs, the bilstm model substantially improves by 5pp, which is only slightly worse than coala (which was designed for smaller training sets). 

table 8 shows that seq2seq achieves low bleu scores, which indicates its tendency to generate irrelevant text. transformer achieves higher performance than seq2seq. our proposed coarse-to-fine model demonstrates a new state of the art, improving the current highest baseline result by 3.35 and 0.60 bleu scores, respectively. 

the results in table 2 demonstrate the strong performance gains obtained with mgt. with l = 5 granularities, mgt outperforms a similarly sized ensemble of dual encoders. 

the results shown in table 5 demonstrate that mgt results in more general representations of language, thereby facilitating better transfer. however, there is room for improvement when comparing to models fine-tuned on the downstream task. 

the results in table 6 demonstrate that mgt learns general representations which effectively transfer to downstream tasks, especially more difficult tasks such as dialog act prediction. 

table 1 lists the performance of our system and the comparison systems. cvae and laed inject seq2seq with stochastic latent variable, resulting in more informative responses and better performance on distinct-{1, 2, 3}. ta-seq2seq incorporates seq2seq with the outsourcing topic information from lda. it is not surprising that it performs much better on the response relevance (bleu, average, greedy, extrema), while its improvements on the informativeness are limited. dom-seq2seq builds multiple domain-specific encoder-decoders. it gains improvements on both the relevance metrics and informativeness metrics. 

table 1 shows the experiment result, which indicates that our rewriting method outperforms heuristic methods. moreover, a 54.2 bleu-4 score means that the rewritten sentences are very similar to the human references. crn-rl has a higher score than crn-pre-train on bleu4, it proves reinforcement learning promotes our model effectively. 

table 2 presents the evaluation results of our reproduced imn model (gu et al., 2019) and previous methods on persona-chat dataset without using personas. it can be seen that the imn model outperformed other models on this dataset by a margin larger than 28.9% in terms of hits@1. 

table 3 presents the evaluation results of our proposed and previous methods on personachat under various persona configurations. we can see that the fine-grained persona fusion at the utterance level rendered a hits@1 improvement of 2.4% and an mrr improvement of 1.9% by comparing imnctx and imnutr conditioned on original self personas. the dim model outperformed its baseline imnctx by a margin of 14.5% in terms of hits@1 and 10.5% in terms of mrr. compared with the ft-pc model (mazare et al. â´ , 2018) which was first pretrained using a large-scale corpus and then fine-tuned on the persona-chat dataset, the dim model outperformed it by a margin of 10.0% in terms of hits@1 conditioned on revised self personas. lastly, the dim model outperforms previous models by margins larger than 27.7% in terms of hits@1 conditioned on original self personas. 

table 6 presents the bleu-2 scores (as recommended in the prior work (liu et al., 2016)), perplexity (ppl), and distinct scores. the results show that all models have similar levels of bleu2 and ppl, while qadpt+multi has slightly better distinct scores. 

table 2 report the results of automatic evaluation on sentiment response generation task. we can see that s2s-temp outperforms all baseline models in terms of all metrics, and the improvements are statistically significant (t-test with p-value< 0.01). although lacking fine-grained check, from the comparison among s2s-temp-none, s2s-temp-50%, and s2s-temp, we can conclude that the performance of s2s-temp improves with more unpaired data. moreover, without unpaired data, our model is even worse than cvae since the structured templates cannot be accurately estimated from such a few data, and as long as half of the unpaired data are available, the model outperforms the baseline models on most metrics. 

table 4 shows the consistency between human inference and model predictions. w2w also outperforms the baselines with a larger margin on longer conversation scenarios, which is consistent with the phenomenon of automatic evaluation. 

as shown in table 2, persona model only achieves comparable results with seq2seq in terms of bleu scores and bow scores. for personawae and diawae-gmd, incorporating personalizations in both decoder and the latent space yields a performance improvement. for the bleu-recall, which personawae does not outperform than diawae-gmd, a possible explanation for this might be that personawae model the personalization information and generation may be more limited. 

in table 2, we report the mrr numbers of several models that use the dialogue acts in different settings. more specifically, we show how the following models i.e., siamese with actual dialogue act (siamese-ada), siamese with predicted dialogue acts in single-task setting (siamese-pda-st) and siamese with predicted dialogue act in multi-task setting (siamesepda-mt) perform when they are given the dialogue acts of only context (context-da), dialogue acts of only response (response-da), and dialogue acts of both (crossway). results in table 2 indicate that the crossway always outperforms the context-da or the response-da, for both datasets. for dailydialog dataset, context-da performs better than response-da for all three models, whereas in the swda dataset, responseda does a relatively better job than contextda (two out of three models). despite their different behavior for different datasets, when we combine response-da and context-da in a crossway fashion, it outperforms the both, giving the best of both worlds. 

table 2 shows the comparison with selfretrieval. as our baseline model (denoted as baseline), we train a model only with policy gradient method without the proposed gan model. when only using the 100% paired ms coco dataset (denoted as w/o unlabeled), our model already shows improved performance over self-retrieval. moreover, when adding unlabeled-coco images (denoted as with unlabeled), our model performs favorably against self-retrieval in all the metrics. 

the results of our experiments are presented in table 4. results demonstrate that both context and punchline information are important as c-mfn outperforms c-mfn (p) and c-mfn (c) models. punchline is the most important component for detecting humor as the performance of c-mfn (p) is significantly higher than c-mfn (c). models that use all modalities (t+a+v) outperform models that use only one or two modalities (t, t+a, t+v, a+v). between text (t) and nonverbal behaviors (a+v), text shows to be the most important modality. most of the cases, both modalities of visual and acoustic improve the performance of text alone (t+v, t+a). based on the above observations, each neural component of the c-mfn model is useful in improving the prediction of humor. the results also indicate that modeling humor from a multimodal perspective yields successful results. furthermore, both context and punchline are important in understanding humor. 

4.5 quantitative analysis in table 2 and table 3, we compare our asgn+lna model with the state-of-the-art models on the youtube2text and msr-vtt datasets. following the operation of (gan et al., 2016; pasunuru and bansal, 2017), asgn+lna is the average ensemble of 5 asgn+lna (rl) models trained with different initializations. from the results, our method achieves the competitive performance on the two datasets. compared with the other interpretable improvement methods (dong et al., 2017; wu et al., 2018), interpretability of our neural network is explicitly improved, and the performance of our model is more competitive. 

table 3 gives the result of the comparison experiment. from the result of gate-mechanism row, we can observe that without the stack-propagation learning and simply using the gate-mechanism to incorporate the intent information, the slot filling (f1) performance drops significantly, which demonstrates that directly leverage the intent information with stack-propagation can improve the slot filling performance effectively than using the gate mechanism. besides, we can see that the intent detection (acc) and overall accuracy (acc) decrease a lot. we attribute it to the fact that the bad slot filling performance harms the intent detection and the whole sentence semantic performance due to the joint learning scheme. besides, from the pipeline model row of table 3, we can see that without shared encoder, the performance on all metrics declines significantly. this shows that stack-propagation model can learn the correlation knowledge which may promote each other and ease the error propagation effectively. 

table 1 shows the experimental results of answer generation on tacos-multilevel and youtubeclip datasets, and table 2 shows the question generation results on same datasets. our method (rict) outperforms all above models in almost all metrics. this fact shows the effectiveness of our overall network architecture. and we find that the image dialog models perform better than video qa models in answer generation. 

table 1 shows the experimental results of answer generation on tacos-multilevel and youtubeclip datasets, and table 2 shows the question generation results on same datasets. our method (rict) outperforms all above models in almost all metrics. this fact shows the effectiveness of our overall network architecture. and we find that the image dialog models perform better than video qa models in answer generation, but worse in question generation on both datasets. 

table 2 shows the performance comparison of various negative focus detection models. we can see that all of contextual attention based models (row 7-9 in table 2) achieve better perfomances than existing methods (row 1-3 in table 2) and models without contextual attention (row 4-6 in table 2). in addition, both word-level and topiclevel contextual attention based models outperform the state-of-the-art system (wtgm in table 2) with about 2% accuracy gain at least. the results demonstrate the effectiveness of these two types of contextual attention mechanisms. moreover, to better quantify the contribution of the different attention mechanisms of our approach, we also conduct several attention variants. comparing the three types of attention mechanisms (row 7-9 in table 2), the topic-level attention based model achieves the best performance. we also observe that the word-level attention based model also achieves comparative performance with the topic-level one. however, when combining the two types of attention mechanisms, the performance declines. it indicates that both the word-level attention mechanism and the topiclevel one can capture the contextual information effectively, but applying such information repeatedly might lead to feature redundancy. in addition to the methods that take advantage of the contextual features in adjacent sentences, we also compare the performances of different frameworks for negative focus detection (row 4-6 in table 2), which only apply the features in current sentence. the results indicate that the bilstm-crf framework is a better fit for encoding order information and long-range context dependency for such sequence labeling task. 

table 7 show the performances of the t-att bilstm-crf model with different pre-trained word embeddings, including senna, glove, word2vec, and bert. we can see that elmo achieves the best performance, but the performance gaps between different pre-trained word embeddings and different dimensions are not significant. 

table 3 shows the results in accuracy on the local discrimination task. from the table, we see that existing models including our global model perform poorly compared to our proposed local models. this observation is further bolstered by the performance of our local coherence models, which show higher sensitivity in discriminating locally coherent texts and achieve significantly higher accuracy compared to the baseline models and our global model. 

table 4 shows rst-dt test set labelled attachment metrics for various parsers. our model outperforms all of the published neural models that do not use additional training data in morey et al. (2017)'s replication study on all of the metrics. on span accuracy (s), we outperform all of the other parsers except for feng and hirst (2014a)'s graph crf model. on spans with nuclearity (n), the equivalent of the unlabelled attachment score for discourse dependencies, we outperform all of the parsers in the study. we perform competitively on spans with relations (r), and we outperform all of the published parsers that do not use additional data on spans with nuclearity and relations (f). our model also outperforms the discriminative baseline using the same features and implementation on all metrics by between 1.9% and 2.7%. 

as seen in table 2 on stac test data, gen dramatically outperformed our deep learning baselines, bilstm, bert, and bert + logreg* architectures on gold labels, as well as the last baseline, which attaches every du in a dialogue to the du directly preceding it. in addition, stand alone gen also outperformed all the coupled snorkel models, in which gen is combined with an added discriminative step, by up to a 30 point improvement in f1 score (gen vs. gen+bilstm). 

table 4 reports precision, and recall and f1-measure, of all the baselines in comparison to proposed approach crf+vae in twitter and reddit dataset. we make the following observations: quickumls is outperformed by all the other methods. 

table 6 shows that the performance of kbqg is degraded without transe embeddings. in comparison, elsahar et al. (2018) obtain obvious degradation on all metrics while there is only a slight decline in our model. we believe that it may owe to the contextaugmented fact encoder since our model drops to 40.87 on the bleu4 score without contextaugmented fact encoder and transe embeddings. 

table 3 show it can bring a significant boost in cross-lingual language understanding performance. with the help of multi-language finetuning, unicoder is been improved by 1.6% of accuracy on xnli and 3.3% on xqa. in table 3, we proved that multi-language fine-tuning with 15 languages is better than translate-train who only fine-tune on 1 language. 

table 2 summarizes the experimental results. we observe that the proposed method consistently outperforms san and the synnet+san model on all datasets. in the squad → newsqa setting, where the sourcedomain dataset is squad and the target-domain dataset is newsqa, adamrc achieves 38.46% and 54.20% in terms of em and f1 scores, outperforming the pre-trained san by 1.78% (em) and 1.41% (f1), respectively, as well as surpassing synnet by 3.27% (em) and 4.59% (f1), respectively. similar improvements are also observed in newsqa → squad, squad → ms marco and ms marco → squad settings, which demonstrates the effectiveness of the proposed model. 

table 3 reports the human evaluation scores of keag and state-of-the-art answer generation models. the keag model surpasses all the others in generating correct answers syntactically and substantively. on the other hand, keag significantly outperforms all compared models in generating substantively correct answers, which demonstrates its power in exploiting external knowledge. 

a  summary  of  the  results  is  reported in table 3 and table 4. in both cases, our  attentive  ranker  model  outperforms  the  current  state-of-the-art  (sota)  approach  proving  that,  indeed,  performing  a  semantic  ranking  is  very  effective  for  qa  systems. 

table 6 shows that using documents ranked by  our  attentive  neural  network  always  leads  to  a  performance  increase  in  downstream  models,  compared  to  tf-idf. on  the  validation  set,  the  improvement is considerably higher (+7.79) due to  a  possible  over-fitting  of  the  hyperparameters  during the attentive ranker's training. 

human performance on the test set of pqa-l is shown in table 4. under reasoning-free setting where the annotator can see the conclusions, a single human achieves 90.4% accuracy and 84.2% macrof1. 

4.4 results table 2 shows the performances of different methods on the link prediction task according to various metrics. from table 2 we could observe that: (1) ptranse performs better than its basic model transe, and rpe outperforms its original method transr. (2) optranse performs better than previous pathbased models like rtranse, ptranse, paskoge and rpe on all metrics. 

table 2 shows overall average results by model. as seen in table 2, both smerti variations achieve higher stes and outperform the other models overall, with the wordnet models performing the worst. the transformer variation achieves slightly higher slor, while the rnn variation achieves slightly higher css. 

5 results system comparison table 2 summarizes our results on the pmb gold data (v.2.1.0, test set). we compare our graph decoder against the system of van noord et al. (2018) and our implementation of a seq2seq model, enhanced with a copy mechanism. overall, we see that our graph decoder outperforms both models. moreover, it reduces the number of illformed representations without any specific constraints or post-processing in order to ensure the well-formedness of the semantics of the output. 

table 1 shows that our ka gne t-based methods using fixed pre-trained language encoders outperform fine-tuning themselves in all settings. furthermore, we find that the improvements in a small data situation (10%) is relatively limited, and we believe an important future research direction is thus few-shot learning for commonsense reasoning. 

table 4 shows training method significantly outperforms all the weaklysupervised learning algorithms, including 10% gain over the previous state of the art. comparing to previous models with full supervision, our results are still on par and outperform most of the published results. 

we report the results of our binary classification task in table 3 in terms of precision, recall and f-score for the “true” class, i.e., when a relation is present. we report results given both gold premises and predicted premises (using our best model from 5.1). our best results are obtained from ensembling the rst classifier with bert fine-tuned on imho+context, for statistically significant (p < 0.001) improvement over all other models. we obtain comparable performance to previous work on relation prediction in other argumentative datasets (niculae et al., 2017; morio and fujita, 2018). 

table 2 presents results for our binary classification task. the bert classifier has higher f1-score and precision than other classifiers. the bertsa model after three epochs only achieves an f1 score of 0.491, which confirms the difference between sentiment analysis and our good/bad task, i.e., even if the segment has positive sentiment, it might be not suitable as a justification. 

table 1 lists the accuracy (acc), precision (prec), recall (rec), and f1 of the compared methods. the section method achieved 100% precision, which indicates that our technique of exploiting causality-describing sections in wikipedia could accurately extract causalities. as the method’s recall indicates, however, it covered only a small portion of our target causalities. the infobox method also achieved 100% precision, though its coverage was also quite limited. the related method exhibited the highest recall, but the precision was unacceptably low for the subsequent manual labor that would be required to construct the ckb. the oracle re method achieved 100% precision by design. its recall was rather low because 67.3% of the entity pairs in the data (§3.1) consisted of entities that did not co-occur in a sentence. this means that most re methods that work sentence-wise will miss a large portion of causalities, regardless of their accuracy. finally, prop achieved the best f1 score, though its recall still had room for improvement. the fact that prop outperformed the baselines, especially oracle re, clearly shows the effectiveness of our method. 

as we can see in these tables, on both datasets, the neural extractive models outperforms the traditional extractive models on informativeness (rouge-1,2) by a wide margin, but results are mixed on rouge-l. similarly, the neural extractive models also dominate the neural abstractive models on rouge-1,2, but these abstractive models tend to have the highest rouge-l scores, possibly because they are trained directly on gold standard abstract summaries. compared with other neural extractive models, our models (both with attentive context and concatenation decoder) have better performances on all three rouge scores, as well as meteor. interestingly, just the baseline model already achieves a slightly better performance than previous works; possibly because the auto-regressive approach used in those models is even more detrimental for long documents. 

we first report the rouge metrics on the combined cnn/dailymail test sets in table 1 and the separate results in table 2. we can get several observations from these two tables. firstly, our model generally performs the best and even surpasses 42 on rouge-1 score on the combined cnn/dailymail dataset. rlbased methods, refresh (narayan et al., 2018) and rnes (wu and hu, 2018), perform better than the sequence labeling methods like summarunner (nallapati et al., 2017). banditsum (dong et al., 2018) generally performs better than the other baselines, and it reports that framing the extractive summarization based on contextual bandit is more suitable than sequential labeling setting and also has more search space than other rlbased methods (narayan et al., 2018; yao et al., 2018; wu and hu, 2018). 

next, we conduct ablation test by removing the modules of the proposed her step by step. based on her-3, we also remove bandit policy, local net, general net gradually, and denote them as her-3 w/o policy, her3 w/o policy & local net and her-3 w/o policy & rough reading individually. the results are reported in table 3 and it proves the effectiveness of each proposed module. firstly, her constructed with an automatic termination mechanism is more ï¬‚exible and reliable in extracting various numbers of sentences varying different documents. 

the experimental results on the english evaluation sets are listed in table 1. we report the full-length f-1 scores of rouge-1 (r-1), rouge2 (r-2), and rouge-l (r-l) on the evaluation set of the annotated gigaword, while report the recall-based scores of the r-1, r-2, and r-l on the evaluation set of duc2004 to follow the setting of the previous works. the results of our works are shown at the bottom of table 1. the performances of the related works are reported in the upper part of table 1 for comparison. abs and abs+ are the pioneer works of using neural models for abstractive text summarization. factaware generates summary words conditioned on both the source text and the fact descriptions extracted from openie or dependencies. besides the above rnn-based related works, cnn-based architectures of convs2s and convs2sreinforcetopic are included for comparison. table 1 shows that we build a strong baseline using transformer alone which obtains the state-of-the-art performance on gigaword evaluation set, and obtains comparable performance to the state-of-the-art on duc2004. when we introduce the contrastive attention mechanism into transformer, it significantly improves the performance of transformer, and greatly advances the state-of-the-art on both gigaword evaluation set and duc2004, as shown in the row of “transformer+contrastive attention”. 

table 2 presents the evaluation results on lcsts. table 2 shows that transformer also sets a strong baseline on lcsts that surpasses the performances of the previous works. when transformer is equipped with our proposed contrastive attention mechanism, the performance is significantly improved and drastically advances the state-of-the-art on lcsts. 

table 4 presents the human evaluation results. summaries generated by neuraltd receives significantly higher human evaluation scores than those by refresh and extabsrl. also, the average human rating for refresh is significantly higher than extabsrl. 

table 5 compares the rouge scores of using different rewards to train the extractor in extabsrl (the abstractor is pre-trained, and is applied to rephrase the extracted sentences). again, when rouge is used as rewards, the generated summaries have higher rouge scores. it is clear from table 5 that using the learned reward helps the rl-based system generate summaries with significantly higher human ratings. qualitative analysis suggests that the poor overall rating may be caused by occasional information inconsistencies between a summary and its source text: for instance, a sentence in the source article reads “after mayweather was almost two hours late for his workout , pacquiao has promised to be on time”, but the generated summary outputs “mayweather has promised to be on time for the fight”. 

table 4 compares the performance of selector with the state-of-the-art bottom-up content selection of gehrmann et al. (2018) in abstractive summarization. we set k, the number of mixtures of selector, to 1 to directly compare it with the previous work (bottom-up (gehrmann et al., 2018)). we observe that selector not only outperforms bottom-up in every metric, but also achieves a new state-of-the-art rouge-1 and rouge-l on cnn-dm. 

table 5 shows that selector trains up to 3.7 times faster than mixture decoder (shen et al., 2019). training time of mixture decoder linearly increases with the number of decoders, while parallel focus inference of selector makes additional training time negligible. 

in the evaluation, we also distinguished the results for seen and unseen entities. we trained the model on a training set contains 64,353 referring expressions and evaluate the model on a test set with 3,591 referring expressions related to seen entities and 2,955 expressions related to unseen entities. table 4 shows the evaluation results. from table 4, it is easy to see that the model performs better when generating referring for seen entities. among four referring expression types, the accuracy of description type drops dramatically for unseen entities, from 48.72% to 20.54%. 

table 3 shows scores on twitter dataset. our model achieves the best bleu score and outperforms all baselines for meteor. for diversity, our model again performs considerably better than other models. paraphrases generated by our model have not only more diverse expression but also better quality compared to those generated by other models. 

table 4 shows our model achieves better scores in both meaning similarity and fluency than baseline models. 

table 1 shows the evaluation results with their consistency. the results indicate that the proposed method improves the consistency of the three generated outputs 1. 

table 2 shows the evaluation result along with the adequacy and ﬂuency. the proposed method improves the adequacy by 0.42pt and the occupation adequacy by 0.44pt. proposed method can generate more adequate outputs, particularly for the occupation. 

table 3 shows the effect of the proposed methods: multi-task learning (mtl), scheduling strategy (sd) and hierarchical consistency loss (hcl). from this result, the proposed method (mtl + sd + hcl) achieves the best score on all three tasks. mtl and hcl improve for all three tasks, and sd improves the score of the headline generation. 

table 4 shows the results of the cnn and dailymail datasets, respectively. for both datasets, the proposed method improves the rouge scores of the summarization and headline generation. 

table 2 shows the results. it turns out that the simple application of the sequence-to-sequence model does not work well at all on this task; note that it is provided with the information about which sentence should have a feedback comment (i.e., tested on only the sentences having feedback comments). nevertheless, its performance is very poor. case frame-based successfully generates feedback comments in some cases. however, its recall is quite low. in contrast, the neural retrieval-based method achieves a far better performance in recall, achieving a precision comparable to that of case frame-based. at the same time, table 2 shows that there is still room for improvement. subsect. 7.1 will investigate the generation results to reveal what has been solved by the methods. 

table 4 shows automatic evaluation results for our model and baselines (copied from their papers). our proposed model which combines structured answer-relevant relations and unstructured sentences achieves significant improvements over proximity-based answer-aware models (zhou et al., 2017; sun et al., 2018) on both dataset splits. 

table 5 shows the experimental results. our model outperforms baselines in terms of coverage and diversity; it manages to use more given ingredients and generates more diversified cooking steps. 

table 5 shows the experimental results on these datasets. our models still yield strong performance compared to baselines and past work on the cnndmdataset. the extraction model achieves comparable results to past successful extractive approaches on cnndm and jecs improves on this across the datasets. in some cases, our model slightly underperforms on rouge-2. finally, we note that our compressive approach substantially outperforms the compression-augmented latsum model. 

table 2 shows the main translation results of en¨zh/ja and en¨de/fr on iwslt datasets. compared with indiv, we can see that multi achieves better results on all cases, which can be attributed to that the encoder can be enhanced by extra training data from the other language pair. as for our proposed method, the synchronous translation method performs significantly better than both indiv and multi baseline methods, and it can achieve the improvements up to 2.75 bleu points (19.31 vs. 16.56) on en_ja. 

the final results are shown in table 3. from table 3 we can see our model is robust to k and outperforms multihop in every case. 

table 2 show the results of human evaluation. we find that: 1) pun-gan achieves the best ambiguity score. 2) compared with clm+jd which is actually the same as our pre-trained generator, pun-gan has a large improvement in unusualness. 

table 2 shows that d-at-gru model outperforms all baseline methods. given that at-lstm (wang et al., 2016) has strong correlation to our base model (at-gru), their work can be categorized as a baseline to our model. in contrast, the results prove that the additional components are helpful to recognize conﬂict opinions. we also compare our model to the recently proposed gcae (xue and li, 2018), which is based on gated cnn. d-at-gru performs competitively with gcae overall and significantly better on conﬂict category. 

results table 2 reports the results of our models against other baseline methods. we can see that our model can achieve the state-of-the-art result. we  note  that  the  results  of  graph-based  models  are  better  than  traditional  models  like  cnn, lstm, and fasttext. 

the results with 30 and 50 topics are shown in table 1. it can be observed from table 1 that nvdm and ngtm achieve better perplexities compared to lda. however, in terms of topic coherence measure, nvdm and ngtm perform slightly worse than lda. a similar observation has been reported in (card et al., 2017). scholar achieves better coherence compared to other neural models. nevertheless, after using reinforcement learning based on the topic coherence scores in our proposed model, vtmrl outperforms all the other models on the topic coherence measure by a large margin. as such, we observe worse perplexity results for models trained with rl-based vocabulary compared to frequency-based vocabulary in 20 newsgroups, though the converse is true for nips. nevertheless, the coherence scores improve for all the models with rl-based vocabulary. 

table 3 compares ukb + syntagnet against the best supervised english wsd systems (yuan et al., 2016; melacci et al., 2018; uslu et al., 2018). none of the differences across datasets between the best performing supervised system and syntagnet is statistically significant according to chi-square test (p < 0.01), meaning that syntagnet enables knowledge-based wsd to rival current supervised approaches. 

it could be seen in table 2 that jobi complex outperforms both complex and dist-mult on all three standard datasets, on all the metrics we consider. for hits@1, jobi complex out-performs baseline complex by 4% on fb15k-237, 6.4% on fb15k and 5.6% on yago3-10. moreover, results in table 2 demonstrate that jobi improves performance on distmult and simple. it should be noted that on fb15k-237, all jobi models outperform all the baseline models, regardless of the base model used. 

in table 6 it can be seen that joint on its own gives a slight performance boost over the baseline, and biasedneg performs slightly under the baseline on all measures. however, combining our two techniques in jobi gives 5.6% points improvement on hits@1. 

in table 2 we show the results of exploring this hypothesis on ptb. even with encoder pretraining, we see that posterior collapse occurs immediately after beginning to update both encoder and decoder using the full elbo objective. when pretraining is combined with annealing, ppl improves substantially. however, the pretraining and anneal combination only has 2 active units and has small kl value â€“ the latent representation is likely unsatisfactory. 

the results on the pun of the day dataset are shown in table 3 above. it shows an accuracy of 93 percent, close to 4 percent greater accuracy than the best cnn model proposed. although the cnn model used a variety of techniques to extract the best features from the dataset, we see that the self-attention layers found even greater success in pulling out the crucial features. 

the results in table 1 make it clear that the bert-based  model  for  each  task  is  a  solid  win  over  a meta-lstm model in both the per-language and multilingual settings. however, the number of parameters of the bert model is very large (179m parameters), making deploying memory intensive and inference slow: 230ms on an intel xeon cpu. 

table 3 shows the f1 results. palm outperforms the right branching baseline, but is not as accurate as the other models. 

table 3 shows the f1 scores averaged across all the target domains. the transductive models (t) consistently outperformed the domain-adapted models (cu). this demonstrates that adapting lms directly to test sets is more effective than adapting them to target domain unlabeled data. 

table 4 shows the f1 scores averaged across all the target domains. fine-tuning the lms on the target domain unlabeled data as well as each test set (u + t) showed better performance than fine-tuning them only on the target domain unlabeled data (u). 

table 5 shows the f1 scores of our models and those of existing models. the results of the baseline model were comparable with those of the state-of-the-art models, and the transductive model consistently outperformed the baseline model. these results, however, demonstrate that transductive lm fine-tuning improves state-of-the-art chunking and srl models. 

table 4 shows the best results for the various models as reported in kayal and tsatsaronis (2019), in addition to the best performance of our model denoted as ck. note that the dct-based model, dct*, described in kayal and tsatsaronis (2019) performed relatively poorly in all tasks, while our model achieved close to state-of-the-art performance in both the 20-ng and r-8 tasks. our model outperformed eignsent on all tasks and generally performed better than or on par with p-means, elmo, bert, and eigensent⊕avg on both the 20-ng and r-8. on the other hand, both eigensent⊕avg and elmo performed better than all other models on sst-5. 

table 1 shows the event detection performance of the models on the test set. our model achieves performance comparable to the state-of-the-art tees event detection module without the use of any syntactic and hand-engineered features, suggesting it can be applied to other domains with no need for feature engineering. we validated it to have no significant statistical difference with the tees model (the approximate randomisation test (yeh, 2000; noreen, 1989)). 

table 3 shows the number of classifications (or action scoring function calls in our model) performed by each model with the corresponding actual running time. sbnn performs fewer classifications and in less time than tees, implying it is more computationally efficient. 

table 4 summarizes results on cspub-sum. following   collins   et   al.   (2017)   we take the top   10 predicted sentences   as   the summary  and  use   rouge-l scores for evaluation. it   is   clear   that   our   approach   outperforms bert+transformer. as in collins et al. (2017), we  found  the  abstract-rouge feature  to  be useful. our  model  augmented  with  this  feature slightly outperforms  collins et al. (2017)’s model, which is a relatively complex ensemble model and uses a number of carefully engineered features for the task. 

table 3 presents results on the nyt dataset. following the evaluation protocol in durrett et al. (2016), we use limited-length rouge recall, where predicted summaries are truncated to the length of the gold summaries. again, we report the performance of the oracle upper bound and lead-3 baseline. the second block in the table contains previously proposed extractive models as well as our own transformer baseline. compress (durrett et al., 2016) is an ilp-based model which combines compression and anaphoricity constraints. the third block includes abstractive models from the literature, and our transformer baseline. bert-based models are shown in the fourth block. again, we observe that they outperform previously proposed approaches. on this dataset, abstractive bert models generally perform better compared to bertsumext, almost approaching oracle performance. 

table 4 lists the average scores of each model, showing that pesg outperforms the other baseline models in both fluency and consistency. 

4.2 additional experiments conll-2003 english table 6 shows the performance on the conll-2003 english dataset. with the contextualized word representations, dglstm-crf outperforms bilstm-crf with 0.2 points in f1 (p < 0.09). such modification improves the f1 to 92.7, which is significantly better (p < 0.05) than the bilstm-crf. 

table 4 considers the effect of the document representation component of the transformation discussed in section 3.2.3 across the tasks, datasets, and translation methods. examining the table, we observe that using a non-uniform weighting scheme generally gives improved performance over the naive unweighted baseline. 

we compare our on-device model against state-of-cdfczxybvgthe-art on-device short text classification approach sgnn (ravi and kozareva, 2018). we directly compare performance on the same swda and mrda datasets. as shown in table 2 proseqo reaches +5.3% improvement for swda and +3.4% accuracy improvement for mrda. we run experiments on atis and snips intent prediction tasks and reported results in table 2 in italic to indicate that this is a re-implementation of (ravi and kozareva, 2018) and previously these ondevice results were not reported. as shown in table 2, sgnn consistently performs well on dialog act and intent prediction tasks. overall, proseqo reached +8.9% accuracy improvements on atis and +4.5% accuracy improvements on snips compared to sgnn. this shows that proseqo’s recurrent dynamic projections learn more powerful representations than the static sgnn ones, this also leads to significant performance improvements on multiple tasks. 

table 3 shows the results on three tasks and datasets (ag, y!a, amzn). overall, proseqo significantly improved upon the ondevice neural model sgnn (ravi and kozareva, 2018) with +23% to +35.9% accuracy. proseqo also reached comparable performance to prior non-on-device neural lstms and character cnns approaches (zhang et al., 2015; bui et al., 2018). 

table 3 shows the results. the learned-mixin method was highly effective, boosting performance on vqa-cp by about 9 points, and the entropy regularizer can increase this by another 3 points, significantly surpassing prior work. 

table 5 shows the performance of subjective evaluations. looking at the first column of the table, our model is better in confusing human, which gives a higher rate in selecting "unk". furthermore, the results also show that human evaluators offer worse sentiment predictions on the proposed approach, which is also desired and expected. 

table 6 compares our model with topperforming methods reported in the literature. yasunaga et al. (2018) demonstrate that adversarial training can improve the tagging accuracy. bilstm-lan gives highly competitive result on wsj without training on external data. 

table 1 shows the average cumulative reward for humans interacting with lid vs a random policy for this experiment. we note that lid leads to better performance on average. this trend is the same as in the simulated analysis, although we note that the learning is slower with real teachers than in the simulated setting on the same tasks, and the gain in performance is substantially smaller. we note that the learned policy was rated by human users as more natural than a random policy on a likert scale (with range 1-5). 

the results in table 4 show that both the basickd method and fine-grained methods achieve performance improvements through domain adaptation. compared with the basickd method, fgkf behaves better (+1.1% f and +2.8% roov v.s. takes multilevel relevance discrepancies into account. the sample-q method performs better than the domainq method, which shows the domain feature is better represented at the sample level, not at the domain level. 

table 5 shows that removing gradient meta decreases 29.3% and 15% on two dataset settings, and further removing relation meta continuous decreases the performance with 55% and 72% compared to the standard results. thus both relation meta and gradient meta contribute significantly and relation meta contributes more than gradient meta. without gradient meta and relation meta, there is no relation-specific meta information transferred in the model and it almost doesn't work. this also illustrates that relation-specific meta information is important and effective for few-shot link prediction task. 

table 1 provides the bleu scores of flowseq with argmax decoding, together with baselines with purely non-autoregressive decoding methods that generate output sequence in one parallel pass. without using knowledge distillation, flowseq base model achieves significant improvement (more than 9 bleu points) over cmlm-base and lv nar. it demonstrates the effectiveness of flowseq on modeling the complex interdependence in target languages. 

table 1 reports results of all models on the new dependency task. xpad significantly outperforms the strongest baselines, proglobal and prostruct, by more than 3 points f1. xpad has much higher precision than proglobal with similar recall, suggesting that xpad dependency-aware decoder helps it select more accurate dependencies. compared with prostruct, it yields more than 11.6 points improvement on recall. 

we compare the two afore mentioned models with (khatri et al., 2018) who conducted their experiments with a bilstm with glove pre-trained word vectors (pennington et al., 2014). results are listed in table 2 and we compare them using the weighted-f1, i.e. the sum of f1 score of each class weighted by their frequency in the dataset. we also report the f1 of the offensive-class which is the metric we favor within this work, although we report both. our bert-based model outperforms the method from khatri et al. (2018); throughout the rest of the paper, we use the bertbased architecture in our experiments. 

table 3 shows results of models on babi tasks. hmns and mem2seq adopt one hop attention only and note that all results are the best performance of each model in 100 epochs. hmns achieved the best results on most tasks except t5. hmns-cfo also outperforms the other models. the improvements in per-dialogue accuracy on out-of-vocabulary tests are even more significant. 

table 4 shows our model gets the best f1 score on dataset dstc 2, while seq2seq with attention gets the best bleu result. 

table 2 shows our experimental results of alsc in single-task settings. firstly, we observe that by introducing attention regularizations (either rs or ro), most of our proposed methods outperform their counterparts. particularly, at-can-rs and at-can-ro outperform at-lstm in all results; atae-canrs and atae-can-ro also outperform ataelstm in 15 of 16 results. in the rest15 dataset, atae-can-ro outperforms atae-lstm by up to 5.39% of accuracy and 6.46% of the f1 score in the 3-way classification. secondly, regularization ro achieves better performance improvement than rs in all results. thirdly, our approaches, especially atae-can-ro, outperform the state-ofthe-art baseline model gcae. finally, the lstm method outputs the worst results in all cases, because it can not distinguish different aspects. 

multi-task settings table 3 shows experimental results of alsc in multi-task settings. second, in almost all cases, applying attention regularizations to both tasks gains more performance improvement than only to the alsc task, which shows that our attention regularization approach can be extended to different tasks which involving aspect level attention weights, and works well in multi-task settings. for example, for the binary classification in the rest15 dataset, m-at-lastm outperforms atlstm by 3,57% of accuracy and 4,96% of the f1 score, and m-can-2ro further outperforms mat-lstm by 3:28% of accuracy and 4,0% of the f1 score. 

table 4 shows the results of the acd task in multi-task settings. our proposed regularization terms can also improve the performance of acd. regularization ro achieves the best performance in almost all metrics. 

table 3 displays their results on development set. in comparison, we consider bilstm over turn sequence (only chronological order encoded and henceforth bilstm), glstm (state number g = 6), gcn (layer number set to 3) without bilstm-encoded temporal representations (henceforth gcn (w/o bilstm)), and the full gcn described in section 3.3 (henceforth gcn (with bilstm) and layer number set to 1). from the results, we find that bilstm exhibits the worst results for not encoding replying relations. the best performance is achieved for gcn (with bilstm), with relatively less training time. this shows the effectiveness and efficiency to explore the order of turns with bilstm and the user interactions with gcn. 

table 4 shows the conversation recommendation results with baselines and state of the arts. our model exhibits the best results on both datasets, significantly outperforming all the comparison models. it indicates the usefulness to encode user interactions for conversation recommendation. particularly, convmf is able to encode turns' temporal orders yet ignores how they reply with each other in conversation history. it is outperformed by our model, showing the benefit to capture users' replying patterns for predicting what conversations will draw their engagement. 

performance comparison table 2 shows the results of different methods for rumor stance classification. clearly, the macro-averaged f1 of conversational-gcn is better than all baselines. further, conversational-gcn also achieves higher f1 score for querying stance (fq). 

performance comparison table 3 shows the comparisons of different methods. by comparing single-task methods, hierarchical gcn-rnn performs better than td-rvnn, which indicates that our hierarchical framework can effectively model conversation structures to learn high-quality tweet representations. the recursive operation in td-rvnn is performed in a fixed direction and runs over all tweets, thus may not obtain enough useful information. moreover, the training speed of hierarchical gcn-rnn is significantly faster than td-rvnn: in the condition of batch-wise optimization for training one step over a batch containing 32 conversations, our method takes only 0.18 seconds, while td-rvnn takes 5.02 seconds. 

table 2 shows the classification accuracy of different methods on 6 benchmark datasets. we can see that our methods significantly outperform all the baselines by a large margin, which shows the effectiveness of our proposed method on semisupervised short text classification. the traditional method svms based on the human-designed features, achieve better performance than the deep models with random initialization, i.e., cnn-rand and lstm-rand in most cases. while cnn-pretrain and lstm-pretrain using the pre-trained vectors achieve significant improvements and outperform svms. our model hgat consistently outperforms all the state-ofthe-art models by a large margin, which shows the effectiveness of our proposed method. 

we report the results on the full benchls dataset in the upper half of table 7. table 7 shows that ranking with s+ c works best according to all measures and across both sets. 

table 3 shows the performance of disp and sc in discriminating perturbations. compared to sc, disp has an absolute improvement by 35% and 46% on sst-2 and imdb in terms of f1score, respectively. an interesting observation is that sc has high recall but low precision scores for character-level attacks because it is eager to correct misspellings while most of its corrections are not perturbations. conversely, disp has more balances of recall and precision scores since it is optimized to discriminate the perturbed tokens. for the word-level attacks, sc shows similar low performance on both random and embed attacks while disp behaves much better. moreover, disp works better on the random attack because the embeddings of the original tokens tend to have noticeably greater euclidean distances to randomlypicked tokens than the distances to other tokens. 

4 results table 1 depicts the performance of our proposed model on the cdr test set, in comparison with the state-of-the-art. we directly compare our model with models that do not incorporate external knowledge. verga et al. (2018) and nguyen and verspoor (2018) consider a single pair per document, while gu et al. (2017) develops separate models for intra- and inter-sentence pairs. as it can be observed, the proposed model outperforms the state-of-the-art in cdr dataset by 1.3 percentage points of overall performance. our model performs significantly better on intraand inter-sentential pairs, even compared to most of the models with external knowledge, except for li et al. (2016b). 

table 3 presents the evaluation results of automatic metrics on the models. it can be seen that the bleu scores and gleu scores of the semi-supervised models on almost all the datasets are better than the baseline s2s model. one interesting thing is that the overall bleu scores on the ancient poems and modern chinese datasets are lower than other datasets. among three semi-supervised models, cpls model achieves the greatest improvement, verifying the effectiveness of the projection functions. however, the gain of cpls model in the aspect of style accuracy is not that significant. 

table 4 compares the human evaluation results of s2s model and cpls model on all the datasets, which are calculated by the average score of the human annotations. as shown in the table 4, the cpls model outperforms the s2s model in the aspects of the content preservation and style strength, and is on par in terms of fluency. 

table 4 shows the results for edit anchoring. our method, cmntedit-ea, outperforms the best baseline method, gated-rnn, by 5.5% on f1 and 6.9% on accuracy. the improvements over all the baselines are statistically significant at a p-value of 0.01. the baseline classifiers including passiveaggressive, random forest and adaboost have high accuracies, but low f1 scores. in fact, adaboost actually outperforms our models on accuracy when the candidate set size is 10, but yields a much lower f1 score. 

for text classification, we observed that training with quantization  significantly  improves  accuracy  as  shown in table 3. we believe that this is due to the improved regularization as quantization has the highest  impact  on  yelp. 

table 2 lists the results for the dfwiki dataset. we obtain new sota results with lasertaggerar, outperforming the previous sota 7-layer transformer model from geva et al. (2019) by 2.7% exact score and 1.0% sari score. we also find that the pretrained seq2seqbert model yields nearly as good performance, demonstrating the effectiveness of unsupervised pretraining for generation tasks. 

table 5 compares our taggers against two baselines. again, the tagging approach clearly outperforms the bert-based seq2seq model, here by being more than seven times as accurate in the prediction of corrections. this can be accounted to the seq2seq model's much richer generation capacity, which the model can not properly tune to the task at hand given the small amount of training data. the tagging approach on the other hand is naturally suited to this kind of problem. 

table 5 reports the results on automatic metrics. we can see that all variants suffer from performance drop and no reading is the worst among the three variants. 

table 1 shows the performance of previous structured prediction models, current state-of-theart models, our baseline models and the softlabel chain crf model. training a non-crf model on soft-label target distributions improves  accuracy  by  a  further  2.08%. on  top of that, soft-label chain crf improves accuracy by another 0.40%, which shows the effectiveness of treating phrase grounding as a sequence labeling task and using crfs to capture entity dependencies. we  also  observe  that  the  hard-label chain crf outperforms the hard-label baseline by a mere margin of 0.05%, so our conjecture is that using chain crfs works well only with a suitable choice of training regime.  soft-label chain crf gives an overall improvement of 2.48% over the hard-label  baseline;  it  significantly  outperforms previous  structured  prediction  models  including structured matching (wang et al., 2016), phrase-region  cca  (plummer  et  al.,  2017a)  and  qrc net (chen et al., 2017b), and surpasses the state-of-the-art ban (kim et al., 2018) and ddpn (yu et al., 2018b) models by a margin of 5.00% and about 1.4%, respectively. 

table 7 shows ablation results on bling-kpe’s variations. each variation removes a component and keeps all others unchanged. we first verify the effectiveness of using elmo embedding by replacing elmo with the wordpiece token embedding (wu et  al.,  2016). the  accuracy  of  this  variation  is much  lower  than  the  accuracy  of  the  full  model and others. the result is shown in the first row of table 7. the context-aware word embedding is a necessary component of bling-kpe. the second part of table 7 studies the contribution of transformer and position embedding. the position embedding barely helps, since real-world web pages are often not one text sequence. as shown in the second part of table 7, both visual features and search pretraining contribute significantly to bling-kpe’s  effectiveness. without  either  of them, the accuracy drops significantly. visual features even help on query prediction, though users issued the click queries and clicked on the documents before seeing its full page. the crucial role of elmo embeddings confirm the  benefits  of  bringing  background  knowledge and general language understanding, in the format of pre-trained contextual embedding, in keyphrase extraction. the importance of visual features and search weak supervisions confirms the benefits of going beyond language understanding in modeling real-world web documents. 

as table 6 shows, our sciresrec framework outperforms the two baselines. and we can observe that the feature of 2-nd category role label has the largest impact on performance indicating that capturing fine-grained role types is important for recognizing specific resources. 

table 2 shows the performances of different supervised hashing models on three datasets under different lengths of hashing codes. we observe that all of the vae-based generative hashing models (i.e vdsh, nash, gmsh and bmsh) exhibit better performance, demonstrating the effectiveness of generative models on the task of semantic hashing. it can be also seen that bmsh-s achieves the best performance, suggesting that the advantages of bernoulli mixture priors can also be extended to the supervised scenarios. 

table 2 reports the simlex-999 spearman rank-order correlation coefficients rs (all are significant, p < 0.01). surprisingly, the wed40 and 70 methods outperform the unmitigated embedding, although the difference in result is small (0.386 and 0.395 vs. 0.385 on gigaword, 0.371 and 0.367 vs. 0.368 on wikipedia). nwed70, on the other hand, performs worse than the unmitigated embedding (0.384 vs. 0.385 on gigaword, 0.367 vs. 0.368 on wikipedia). cda and cds methods do not match the quality of the unmitigated space, but once again the difference is small. 

table 2 shows our wsd results in f1 measure. it is shown in the table that with the nearest neighbor matching model, bert outperforms context2vec and elmo. when we include surrounding sentences, one to the left and one to the right, we get improved f1 scores consistently. we also show that linear projection to the sense output vector further improves wsd performance, by which our best results are achieved. it is worthwhile to note that our more advanced linear projection, by means of layer weighting (§4.2.2 and gated linear unit (§4.2.3) gives the best f1 scores on all test sets. all our bert wsd systems outperform glossenhanced neural wsd, which has the best overall score among all prior systems. 

table 1 shows ansacc results of competitive baselines on the test set. compared with them, star achieves the highest, 65.05%, which demonstrates its superiority. compared with concat, our approach boosts over 39.81% on coarse2fine for the capability of context-dependent semantic parsing. table 1 also shows symacc and bleu of different methods on the dev and test sets. as observed, star significantly outperforms all baselines, demonstrating its effectiveness. for example, star achieves an absolute improvement of 8.03% bleu over the state-ofthe-art baseline fanda on testing. moreover, the rewriting-based baselines, even the simplest concat, perform better than the generation-based ones. it suggests that the idea of rewriting is more reasonable for the task, where precedent and follow-up queries are of full utilization. 

table 3 presents all test results on seven languages of conll-2009 datasets. so far, the best previously reported results of catalan, japanese and spanish are still from conll-2009 shared task. compared with previous methods, our baseline yields strong performance on all datasets except german. especially for catalan, czech, japanese and spanish, our baseline performs better than existing methods with a large margin of 3.5% f1 on average. nevertheless, applying our argument pruning to the strong syntax-agnostic baseline can still boost the model performance, which demonstrates the effectiveness of proposed method. on the other hand, it indicates that syntax is generally beneficial to multiple languages, and can enhance the multilingual srl performance with effective syntactic integration. 

table 3 shows the results, with comparison to previous published elmobase results (peters et al., 2018) and the bert models. both of our stacking methods outperform the previous state of the art, but fine tuning gives the biggest gain. 

table 4 shows the results. here, fine tuning is required to achieve gains over the previous state of the art, which used elmo embeddings. 

table 5 shows that the cloze loss performs significantly better than the bilm loss and that combining the two loss types does not improve over the cloze loss by itself. 

our main results on the trecqa, twitterurl, and quora datasets are shown in table 2 and results on trec microblog 2013â€“2014 are shown in table 3. from table 2, we can see that on all three datasets, relevance matching (rm) achieves significantly higher effectiveness than semantic matching (sm). it beats other competitive baselines (infersent, decatt and esim) by a large margin on the trecqa dataset, and is still comparable to those baselines on twitterurl and quora. this finding suggests that soft term matching signals alone are fairly effective for many textual similarity modeling tasks. however, sm performs much worse on trecqa and twitterurl, while the gap between sm and rm is reduced on quora. by combining sm and rm signals, we observe consistent effectiveness gains in hcan across all three datasets, establishing new state-of-the-art (non-bert) results on trecqa. 

table 1 shows the results of these syntax-aware methods on cpb1.0 dataset. first, the first line shows the results of our baseline model, which only employs the word embeddings and char representations as the inputs of the basic srl model. second, the tree-gru method only achieves 80.06 f1 score on the test data, which even didnâ€™t catch up with the baseline model. third, the fir approach outperforms the baseline by 2.17 f1 score on the test data, demonstrating the effectiveness of introducing fixed implicit syntactic representations. forth, the hps strategy achieves more significant performance by 83.51 f1 score. finally, our proposed framework achieves the best performance of 83.91 f1 score among these methods, outperforming the baseline by 3.43 f1 score. 

table 2 shows the results of our baseline model and proposed framework using external dependency trees on cpb1.0, as well as the corresponding results when adding bert representations. it is clear that adding dependency trees into the baseline srl model can effectively improve the performance (p < 0.0001), no matter whether employ the bert representations or not. especially, our proposed framework (iir) consistently outperforms the hard parameter sharing strategy. our final results outperforms the best previous model (xia et al., 2017) by 7.87 and 4.24 f1 scores with bert representations or not, respectively. 

table 4 shows the results of our framework and comparison with previous works on the conll-2009 chinese test data. our baseline achieves nearly the same performance with cai et al. (2018), which is an endto-end neural model that consists of bilstm encoder and biaffine scorer. our proposed framework outperforms the best reported result (cai et al., 2018) by 0.8 f1 score and brings a significant improvement (p < 0.0001) of 0.9 f1 score over our baseline model. our experimental result boosts to 88.5 f1 score when the framework is enhanced with bert representations. however, compared with the results in the settings without bert, the improvement is fairly small (88.53 - 88.47 = 0.06 f1 score, p > 0.1) 8 of the proposed framework, which we will discuss in section 5.3. 

table 3 shows fine-tuning results on glue; our model, denoted as transfer fine-tuning, is compared against bert-base and bert-large. the first set of columns shows the results of semantic equivalence assessment tasks. our model outperformed bert-base on mrpc (+0.9 points) and sts-b (+2.7 points). furthermore, it outperformed even bert-large by 0.6 points on mrpc and by 1.4 points on sts-b, despite bert-large having 3.1 times more parameters than our model. 

table 2 shows the results. first, the overall accuracy can be improved by 3.2% and 2.5% respectively. furthermore, performances on medium, hard and extra hard sql queries achieve more improvement than that on easy sql queries, indicating that our approach is more helpful for solving complicated cases. 

table 5 shows that dae significantly outperforms typesql and annotatedseq2seq on all the evaluation metrics. first, for accsc, dae outperforms typesql and anotatedseq2seq by 16% and 3.5% on test data; for accoc, dae outperforms typesql and annotatedseq2seq by 0.8% and 28% on test data. moreover, dae can achieve around 86% for accce, while other methods fail to recognize cells when the table content is not available due to the privacy problem. 

table 5 gives the results of the proposed capsulenet srl (with global node) on the in-domain test sets of all languages from conll-2009. interestingly, the effectiveness of the refinement method does not seem to be dependent on the dataset size: the improvements on the smallest (japanese) and the largest datasets (english) are among the largest. 

table 1 presents exact match and bleu scores on the original concode train/validation/test split. iyer-simp yields a large improvement of 3.9 em and 2.2 bleu over the best model of iyer et al. (2018), while also being significantly faster (27 hours for 30 training epochs as compared to 40 hours). furthermore, using 200 code idioms further improves bleu by 2.2% while maintaining comparable em accuracy. 

table 5 shows the results. in all settings, misp-sql improves the base parserâ€™s performance, demonstrating the benefit of involving human interaction. however, we also notice that the gain is not as large as in simulation, especially on sqlova. 

table 2 presents the results of the ablation test on the development set of ldc2015e86 by either removing bpe, or vocabulary sharing, or both of them from the baseline system. from the results we can see that bpe and vocabulary sharing are critical to building our baseline system (an improvement from 18.77 to 24.93 in bleu), revealing the fact that they are two effective ways to address the issue of data sparseness for amr-to-text generation. 

table 3 presents the comparison of our approach and related works on the test sets of ldc2015e86 and ldc2017t10. from the results we can see that the transformer-based baseline outperforms most of graph-to-sequence models and is comparable with the latest work by guo et al. (2019). all of them achieve significant improvements over the baseline: the biggest improvements are 4.16 and 4.39 bleu scores on ldc2015e86 and ldc2017t10, respectively. methods using continuous representations (such as sa-based and cnn-based) outperform the methods using discrete representations (such as feature-based). compared to the baseline, the methods have very limited affect on the sizes of model parameters (see the column of #p (m) in table 3). finally, our best-performing models are the best among all the single and supervised models. 

table 4 compares the performance of our approach with or without modeling structural information of indirectly connected concept pairs. it shows that by modeling structural information of indirectly connected concept pairs, our approach improves the performance on the test set from 29.92 to 31.82 in bleu scores. it also shows that even without modeling structural information of indirectly connected concept pairs, our approach achieves better performance than the baseline. 

to show the effect of different actions, we take rnn and cnn on sst-2, sst-5, and rt as examples and conduct experiments by dropping one action at a time. as table 7 shows, only some actions are useful for the robustness improvement. the average performance becomes better after dropping subordinate words under two different settings. surprisingly, the neighbor words largely contribute to the performance. without neighbor words, the average accuracies are dropped from 66.17 to 65.38 for rnn, and from 65.63 to 65.22 for cnn. 

hoteluser contains 28165 reviews posted by 202 randomly selected reviewers, each of whom contributes at least 100 hotel reviews. hotelloc contains a total of 136446 reviews about seven us cities, split approximately equally. it contains 23874 restaurant reviews posted by 144 users, each of whom contributes at least 100 reviews. table 1 summarizes our datasets. 

table 2 presents results on the libcon and the balanced (b) and imbalanced (i) beauty, book and music data sets and table 3 presents results on the sst and mr data sets. from table 2, it is observed that on the libcon data set, where we have a considerable difference in language use between the two groups of users, the adapted bilstm and adapted cnn perform much better than the vanilla baselines. furthermore, our proposed adaptation layer improves the performance of the vanilla bilstm to surpass the performance of fine-tuned bert. 

the experimental results on both datasets are shown in table 2 and table 3, respectively. rb yields high precision but with low recall. cb has an opposite scenario from rb. 

table 6 shows that joint learning (bert-joint) hurts the performance compared to single-task bert. however, using additional information from the sentence-level for the token-level classification (bert-granularity) yields small improvements. the multi-granularity models outperform all baselines thanks to their higher precision. 

from table 2 and figure 4, it is clear that gcn complements the bilstm to improve model performance. this means that the bilstm can identify opinion words within the context with respect to a specific aspect. however, in some complicated contexts, it might perform poorly. 

ctc table 1 shows the main bleu results of different methods on the test set. however, we cannot identify the best da method because their rankings across the four translation tasks vary a bit. the ctc for the bleu measure is 0.62, which is of weak consistency. 

next, we evaluate online decoding with a noisy channel setup compared to just a direct model (dir) as well as an ensemble of two direct models (dir ens). table 1 shows that adding a language model to dir (dir+lm) gives a good improvement (gulcehre et al., 2015) over a single direct model but ensembling two direct models is slightly more effective (dir ens). the noisy channel approach (ch+dir+lm) improves by 1.9 bleu over dir on news2017 and by 0.9 bleu over the ensemble. without per word scores, accuracy drops because the direct model and the channel model are not balanced and their weight shifts throughout decoding. our simple approach outperforms strong online ensembles which illustrates the advantage over incremental architectures (yu et al., 2017) that do not match vanilla seq2seq models by themselves. 

in table 3 we compare our method with recent state-of-the-art approaches on the ud treebank dataset: convex-mst (grave and elhadad, 2015), lc-dmv (noji et al., 2016) and d-j (jiang et al., 2017). our g+i model performs better than convex-mst and lc-dmv on average, even though additional priors and delicate biases are integrated into the two methods (e.g, the universal linguistic prior for convexmst and the limited center-embedding for lcdmv). our method also slightly outperforms d-j on average, even though d-j combines convexmst and lc-dmv and therefore utilizes even more linguistic prior knowledge. 

table 3 shows accuracy. we can see that the seed lexicon itself had practically no impact on prediction. 

table 1 presents the performance comparison between different methods. we can see that moganed achieves 1.6% and 1.7% improvement on precision and f1-measure, respectively, compared with the best baselines. moganed reaches a lower recall than two sequence based methods, jrnn and deeb-rnn. 

table 1 shows test set f1 on the entity, relation and event extraction tasks. our framework establishes a new state-of-the-art on all three high-level tasks, and on all subtasks except event argument identification. relative error reductions range from 0.2 - 27.9% over previous state of the art models. 

table 1 shows that bert-base offers an improvement of 0.9% over the elmo-based c2fcoref model. given how gains on coreference resolution have been hard to come by as evidenced by the table, this is still a considerable improvement. bert-large, however, improves c2f-coref by the much larger margin of 3.9%. we also observe that the overlap variant offers no improvement over independent. 

table 3 shows performance comparison among different delta operations: subtract, add, and mlp which is a multi-layer perceptron network. while add shows good performance on meteor, subtract does on the soft metric (i.e., vecext), indicating that subtraction can help the model capture the better semantics than the other functions. 

table 5 shows that proposedru+bk improved the average precision over proposedru by about 2.5% (i.e., proposedru+bk significantly outperformed the state-of-the-art method, mcnn, by about 5%), suggesting that background knowledge in the form of text fragments is still useful, at least in our current experimental setting. 

table 1 shows quantitative results on our lconvqa and cs-convqa datasets. the state-of-the-art vqa has low consistency. the baseline vqa system (row a) retains similarly high top-1 accuracy on the convqa splits (63.58% on vqav2 vs 70.34% / 60.03% on lconvqa / cs-convqa); however, it achieves only 26.13% perfect consistency on the human generated cs-convqa questions. finetuning is an effective strategy for the synthetic l-convqa split. finetuning on lconvqa train results in 18.43% gains in perfect consistency on l-convqa test (row c vs a). this is unsurprising given the templated questions and simple concepts in l-convqa; however, perfect consistency is low in absolute terms at 54.68%. finetuning does not lead to significant gains in consistency for human-generated questions. finetuning the vqa model on cs-convqa (row b) leads to an improvement in consistency of only 0.26%. likewise, adding l-convqa (row c) and extra visual genome questions (row e) actually reduces consistency. ctm-based training preserves or improves consistency when leveraging additional data. when we apply ctm to the finetuned l/csconvqa model, we improve cs-convqa perfect consistency by 1.24% (row d vs c) while modestly improving other metrics. extending to visual genome questions, the ctm augmented model improves perfect consistency in cs-convqa by 2.27% over the finetuned model (row f vs e). interestingly, the ctm modules were never trained with the human-annotated cs-convqa questions and yet lead to this improvement on csconvqa by acting as an intelligent data augmenter/regularizer. 

model (10) in table 1 shows the result of jointly training the bert encoder and the qanet model. the result is very poor, likely because the parameters in bert are catastrophically forgotten while training the qanet model. to tackle this issue, we fix parameters in bert, and only update parameters for qanet. the result is listed as model (11). it works better than model (10), but still worse than multi-passage bert in model (6). we design another model by starting from model (11), and then jointly fine-tuning the bert encoder and qanet. model (12) in table 1 shows the result. it works better than model (11), but still has a big gap with multi-passage bert in model (6) . therefore, we conclude that the explicit inter-sentence matching is not helpful for multi-passage bert. one possible reason is that the multi-head self-attention layers in bert has already embedded the inter-sentence matching. 

the automatic evaluation results based on the dev and test set are shown in table 2. in terms of rouge score model s3 outperforms model s1 but perform worse than model s2. 

table 4 shows that the proposed model with contextualized word embeddings outperforms all previous models. 

table 7 shows the comparison of copying accuracies between ms uedin, copynet, and our approach. we find that our approach outperforms the two baselines. however, the copying accuracy of our approach is almost 20% lower than the prediction accuracy (i.e., 65.61% vs. 85.09%), indicating that it is much more challenging to place the copied words in correct positions. 

we conducted separate evaluations for two (set2singleseq, pairs of (set2singleseq, set2multipleseq) set2multipleseq+opt). the results are shown in tables 5 and 6 respectively. results shown in table 5 explains that incorporating neural components for subset selection and content ordering helps in improving informative instruction generation. we observe that conducting content selection multiple times during each time step through content ordering rnn helps in generating a discrete set of instructions (set2multipleseq). 

we cannot determine recall for our approaches, instead we compute precision, recall and f-score based on the baseline dataset (see section3), which is shown in table 2 as well as precision of the baseline dataset. the automated approaches using wikidata and named entity recognition can boost the precision significantly with a moderate loss in recall. the blstm approach performs best. although the loss in recall is higher than with the wd approach, the precision reaches almost 87% and is thus raised to a new level. 

table 1 shows the experimental results. we can see that hanpane+p showed the highest accuracy and hanpane+p and ve+p, with consideration of paraphrases, showed a higher accuracy than baseline. in contrast, hanpane-p and ve-p, without consideration of paraphrases, did not. the results indicate that the use of paraphrases contributed to improved accuracy. then, based on the assumption that outputs have the binomial distribution, we apply a binomial test. 

table 4 shows that our proposed sentence-level blc with structural information performs best regarding macro f1 in either boundary identification or unit classification tasks. for the post-level blc, the model yields a better score in predicting non-euparts because post-level discrimination can capture the entire post and thus identify irrelevant boundaries, such as supplementary notes, in posts. 

table 1 shows the results of various models. our model based on the unified objective of eq. (4) offers better balanced results compared to its variants. when removing lsentiment, our model degrades to an input copy-like method, resulting in low classification accuracies but the highest bleu scores. when removing lcontent, the bleu scores drop, indicating that the model cannot maintain a sufficient number of content words. without lalignment, we observe a reduction in both accuracy and bleu on yelp. however, this tendency is inconsistent on amazon (i.e., -2.2 accuracy and +0.56 bleu). when using only lsentiment, our model falls back to the vanilla encoder-decoder model with a single loss, yielding poorer results on both datasets. 

in table 1 we report the test set results for all models. overall, mtl-lp is the best performing multilabel classification method across all the datasets. mtllp is also better than the best performing model seq2seq reported in sobhani et al. (2019) for the etc dataset. mtl-xld improves on the baseline models for the bbc and mftc datasets, but performs slightly worse than mtl on the etc dataset. 

table 3 presents the results of the total term prediction. although our method is not directly trained to make the final prediction, the performance of our model surpasses all baselines, which confirms that the breakdown charge-based analysis can indeed help the total prison term prediction. 

table 2 shows the results obtained on testing the final classifier system on the test subset of sl. we obtain an overall high accuracy of 86.6% at the sentence level. while result and conclusion obtained f-measures above 90%, objective and method reported reasonable f-measures above 80%. design obtained the lowest precision, recall and f-measure. 

table 3 shows the performance of our model on this task. as expected a topic's label distribution over its entire life-time is very informative with respect to classifying the topic as growing or declining. we achieve a significant improvement over the baselines on the full dataset (32.3% relative improvement over majority prediction), and this trend holds across each field separately. 

table 4 shows the performance of our model on this task. these results show that we can accurately predict whether a topic will grow or decline using only a small amount of data. moreover, we see that both percentage and delta features are necessary for this task. 

table 1 contains results for experiments comparing different composition architectures on the task of error detection. the crf has the lowest f0.5 score compared to any of the neural models. it memorises frequent error sequences with high precision, but does not generalise sufficiently, resulting in low recall. 

table 2 contains results obtained by incrementally adding training data to the bi-lstm model. we found that incorporating the nucle dataset does not improve performance over using only the fce-public dataset, which is likely due to the two corpora containing texts with different domains and writing styles. the differences in the datasets offset the benefits from additional training data, and the performance remains roughly the same. 

the results, summarized in table 7, indicate that pathlstm performs better than the system by bjorkelund et al. (2009) in all cases. for german and chinese, pathlstm achieves the best overall f1-scores of 80.1% and 79.4%, respectively. 

table 1 shows the overall performance on the blind test set. we compare our results with the jet baseline as well as the cross-event, crossentity, and joint methods. when adding the event type classifier, in the line titled “+ et”, we see a significant increase in the three measures over the jet baseline in recall. although our trigger’s precision is lower than rbpb(jet), it gains 5.2% improvement on the trigger’s f1 measure, 10.6% improvement on argument identification’s f1 measure and 9.7% improvement on argument classification’s f1 measure. the line titled + regu in table 1 represents the performance when we only use the regularization method. in table 1, compared to the four baseline systems, the argument identification’s f1 measure of “+ regu” is significantly higher. the complete approach is denoted as “rbpb” in table 1. remarkably, our approach performances comparable in trigger classification with the state-of art methods: cross-event, cross-entity, joint model, dmcnn and significantly higher than them in argument identification as well as classification although we did not use the cross-document, cross-event information or any global feature. 

we test the performance with argument candidates automatically extracted by jet in table 2, our approach “+ et” again significantly outperforms the jet baseline. remarkably, our result is comparable with the joint model although we only use lexical features. the line titled + regu in table 2 represents the performance when we only use the regularization method. in table 2, the “+ regu” again gains a higher f1 measure than the jet, cross-document, joint model baseline and “+ et”. the complete approach is denoted as “rbpb” in table 2. remarkably, our approach performances comparable in trigger classification with the state-of art methods: cross-document, joint model, and significantly higher than them in argument identification as well as classification although we did not use the cross-document, cross-event information or any global feature. 

table 4 summarizes the system performance on newswire and clinical data. we observe that systems that did well on rte datasets, were mediocre on the clinical dataset. it is interesting to see though that ent did well (comparatively) on both domains. the performance of all systems on the clinical data is noticeably low as compared to the newswire data. 

table 3 summarizes acc1 and acc10 for each model. as can be seen, cue/liu(bistm) and cue/liu(bistm+ts) significantly outperform cue/liu(bilda) (p < 0.01 in the sign test). this indicates that bistm and bistm+ts improve the performance of translation extraction for both the cue and liu methods by assigning more suitable topics. 

table 3 provides a detailed comparison of our multi-level attention cnn model with previous approaches. we observe that our novel attention-based architecture achieves new state-of-the-art results on this relation classification dataset. with att-input-cnn, we achieve an f1-score of 87.5%, thus already outperforming not only the original winner of the semeval task, an svm-based approach (82.2%), but also the well-known cr-cnn model (84.1%) with a relative improvement of 4.04%, and the newly released drnns (85.8%) with a relative improvement of 2.0%, although the latter approach depends on the stanford parser to obtain dependency parse information. our full dual attention model att-pooling-cnn achieves an even more favorable f1-score of 88%. 

table 4 provides the experimental results for the two variants of the model given by eqs.(7) and (8) in section 3.3. our main model outperforms the other variants on this dataset, although the variants may still prove useful when applied to other tasks. 

the number of clusters in the hmm approach8 is set to 100 and the results are shown in table 3 as means across 10 runs. as the table shows, although the hmm does not reach the same performance as svm, it performs similarly to logistic regression and meets or exceeds its f1-score for five categories. 

results on test data table 2 shows the test results of spo and our different systems over the seven domains. it can be seen that all of our sequence-based systems are performing better than spo by a large margin on these tests. when averaging over the seven domains, our ‘worst’ system dsp scores at 64.7% compared to spo at 57.1%. the lfp and cfp models, with higher performance than dsp, also may generate ungrammatical sequences. 

table 3 shows the results, with w/o denotes experiments without the corresponding group of features. we can observe that both the traditional features and the novel features contribute useful information for learning to rank models. 

table 2 shows the translation results using bootstrap resampling (koehn, 2004). generally, the csrs model outperformed the mers model and the csrs-mini model outperformed the mersmini model on different translation tasks. 

we show the results of two of the best proposed parsers: third-order adding (o3-adding) and third-order perceptron (o3-perceptron) methods, and compare with the reported results of some previous work in table 2. we compare with three categories of models: other graph-based nn (neural network) models, traditional graph-based linear models and transition-based nn models. from the comparison, we see that the proposed parser has output competitive performance for different dependency conversion conventions and treebanks. the results and comparisons in table 2 demonstrate the proposed models can obtain comparable accuracies, which show the effectiveness of combining local and global features through window-based and convolutional neural networks. 

as table 3 shows, external information improve the performance of grsemi-crfs for both tasks. compared to text chunking, we can find out that external information plays an extremely important role in ner, which coincides with the general idea that ner is a knowledge-intensive task (ratinov and roth, 2009). another interesting thing is that, brown clusters generated from nyt corpus performs better on the conll 2000 task while those generated from reuters rcv1 dataset performs better on the conll 2003 task. 

as table 4 shows, a grsemi-crf using vectorial gating coefficients (i.e., eq. (7)) performs better than that using scalar gating coefficients (i.e., eq. (6)), which provides evidences for the theoretical intuition that vectorial gating coefficients can make a detailed modeling of the combinations of segment-level latent features and thus performs better than scalar gating coefficients. 

table 4 shows a runtime comparison of the losses and sampling strategies. we find random sampling to be orders of magnitude faster than the others while also performing the best. 

the result in table 4 shows that now the character-based model cannot perform as well as the original character-based autoencoder representation does, which again proves that the order of the word form is necessary for learning the grammatical function of a word. 

table 5 reveals that phonological knowledge can be transferred if two languages share similar bigram and trigram character frequency distribution. for example, finnish and english are both indo-european language. 

we demonstrate our emotion model prediction quality using 10-fold c.v. on our hashtag emotion dataset and compare it to other existing datasets in table 4. our results significantly outperform the existing approaches and are comparable with the state-of-the-art system for twitter sentiment classification (mohammad et al., 2013; zhu et al., 2014). 

table 4 shows that svm outperforms blstm. 

it is clear from table 3 that copynet beats the competitor models with big margin. 

table 5 shows the results of subjective evaluation. the two human evaluators made close judgements: around 54% of mle translations are worse than mre, 23% are equal, and 23% are better. 

table 7 shows the results on english-french translation. they differ in network architectures and vocabulary sizes. our rnnsearch-mle system achieves a bleu score comparable to that of jean et al. (2015). rnnsearch-mrt achieves the highest bleu score in this setting even with a vocabulary size smaller than luong et al. (2015b) (2014). 

table 2 shows the obtained results. the source-context model does not help in the small data setting but brings a substantial improvement of 0.7-0.8 bleu points for the medium and full data settings, which is an encouraging result. target-side context information allows our model to push the translation quality further: even for the small data setting, it brings a substantial improvement of 0.5 bleu points and the gain remains significant as the data size increases. even in the full data setting, target-side features improve the score by roughly 0.2 bleu points. our results demonstrate that feature-rich models scale to large data size both in terms of technical feasibility and of translation quality improvements. target side information seems consistently beneficial, adding further 0.2-0.5 bleu points on top of the source-context model. 

table 2 shows the evaluation metrics for various settings of cross-lingual transfer learning. as you can see, it outperformed the other two models for every metric. in particular, the cider-d score was about 4% higher than that for the monolingual baseline. surprisingly, this model had lower performance than the baseline model. 

table 3 shows the results of testing the state-of-the-art msr captioning system on the captionsbing-5000 dataset as compared to the ms coco dataset, measured by the standard bleu (papineni et al., 2002) and meteor (denkowski and lavie, 2014) metrics. the wide gap in the results further confirms that indeed the v qgbing-5000 dataset covers a new class of images. 

table 5 shows the results. to our surprise, both parsers perform very well on the learner corpora despite the fact that it contains a number of grammatical errors and also syntactic tags that are not defined in ptb-ii. their performance is comparable to, or even better than, that on the penn treebank (reported in petrov (2010)). 

table 6 presents the rmse for each aspect, for two different sets of feature: a standard bow and the shallow features described previously, as well as the baselinem. in particular, the model trained on the bow features achieves an rmse that is very close to that of the baselinem, whereas  model trained on the shallow features outperforms all other models. 

table 3 gives the performance of the sentence boundary detectors on test sets. on wsj all systems are close to 98 and this high number once again affirms that the task of segmenting newspaper-quality text does not leave much space for improvement. however, when real syntax is used (joint) we see a huge improvement in f1, 10 points absolute, which is significantly better than both nosyntax and marmot. on switchboard marmot is much lower and both parsing models outperform it significantly. surprisingly the nosyntax system achieves a very high result beating the baseline significantly by almost 4.5 points. the usage of syntax in the joint model raises this gain to 4.8 points. 

table 2 shows the results on the rst corpus. our system is roughly comparable to tree knapsack here, and we note that none of the differences in the table are statistically significant. we also observed significant variation between multiple runs on this corpus, with scores changing by 1-2 rouge points for slightly different system variants. 

our results are shown in table 6 and compared to results reported in previous work by johannsen et al. (2014), with two additional baselines: the semcor system of ciaramita and altun (2006) and the most frequent sense. our system achieves comparable performance to the best previously used supervised systems, without using any explicit gazetteers. 

table 3 shows automatic evaluation metrics for our model and baselines. code-nn outperforms all the other methods in terms of meteor and bleu-4 score. the neural models have better performance on c# than sql. 

table 6 presents the results where we measure precision, recall and f1. compared with ace-ann-fn, events from sf and rf hurt the performance. moreover, ace-sl-fn obtains a score of 70.3% in f1 measure, which outperforms ace-ann-fn. finally and most importantly, consistent with the results of manual evaluations, ace-psl-fn performs the best, which further proves the effectiveness of our proposed approach for event detection in fn. 

table 2 shows the different evaluation results with standard metric and our balanced metric. we can see that the proposed evaluation metric generally gives lower and more distinguishable score, compared with the standard metric. 

table 3 shows the results obtained by different rnn models with only character level word embedding features. for the task a (disease name recognition) bi-lstm and nn models gave competitive performance on the test set, while bi-rnn and bi-gru did not perform so well. on the other hand for the task b, there is 2.08% âˆ’ 3.8% improved performance (f1-score) shown by rnn models over the nn model again on the test set. bi-lstm model obtained f1-score of 59.78% while nn model gave 57.56%. 

table 3 lists the performances of our model as well as previous state-of-the-art systems on on pennym, penn-sd and ctb5. overall, our model achieves competitive accuracy on all three datasets. although our model is slightly lower in accuracy than unlimited-order double beam model (zhang and mcdonald, 2014) on penn-ym and ctb5, our model outperforms their model on penn-sd. it seems that our model performs better on data sets with larger label sets, given the number of labels used in penn-sd data set is almost four times more than penn-ym and ctb5 data sets. 

table 4 lists the uas of two methods on test set. as we can see, lstm-minus shows better performance because our method further incorporates more sentence-level information into our model. 

as shown in table 1, when structured inference is augmented with the unstructured inference, we see an improvement of 2.9% (from 44.1% to 47.0%). and when structured + joint uses unstructured inference, the performance boosts by 6.2% (from 47.1% to 53.3%) achieving a new state-of-the-art result. 

in table 2 we show the overall performance of our proposed ncm system compared with strong competing methods as described above. we see that, for perplexity, bleu and human judgments, our system outperforms other baseline models. 

table 2 presents our main results. the conventional feature-based classifier obtains 67.9% accuracy on the cnn test set. not only does this significantly outperform any of the symbolic approaches reported in (hermann et al., 2015), it also outperforms all the neural network systems from their paper and the best single-system result reported so far from (hill et al., 2016). more dramatically, our single-model neural network surpasses the previous results by a large margin (over 5%), pushing up the state-of-the-art accuracies to 72.4% and 75.8% respectively. 

table 4 displays performance scores of hypenet and the baselines. comparing the path-based methods shows that generalizing paths improves recall while maintaining similar levels of precision, reassessing the behavior found in nakashole et al. (2012). hypenet path-based outperforms both path-based baselines by a significant improvement in recall and with slightly lower precision. 

table 2 lists these methods and their performance on the sst2 task. we see that: 1) although all methods lead to different degrees of improvement, our framework outperforms all other competitors with a large margin. 2) in particular, compared to the pipelined method in row 6 which is in analogous to the structure compilation work (liang et al., 2008), our iterative distillation (section 3.2) provides better performance. 3) the distilled student network “-rule-p” achieves much superior accuracy compared to the base cnn, as well as “-project” and “-opt-project” which explicitly project cnn to the rule-constrained subspace. the inferior accuracy of “-opt-project” can be partially attributed to the poor performance of its neural network part which achieves only 85.1% accuracy and leads to inaccurate evaluation of the “-but-clause” rule in eq.(5). 

here we investigate further the accuracy of the model in predicting the subjective success rate. an evaluation of the on-line gp reward model between 1 and 850 training dialogues is presented in table 2. the results shown in the table are thus the average over 2096 dialogues. as can be seen, there was a significant imbalance between success and fail labels since the policy was improving along with the training dialogues. this lowered the recall on failed dialogue prediction as the model was biased to data with positive labels. nevertheless, its precision scores well. on the other hand, the successful dialogues were accurately predicted by the proposed model. 

in table 1 we compare our model to a linear crf and to the compositional character-to-word lstm model of ling et al. (2015). our local model already compares favorably against these methods on average. using beam search with a locally normalized model does not help, but with global normalization it leads to a 7% reduction in relative error, empirically demonstrating the effect of label bias. the set of character ngrams feature is very important, increasing average accuracy on the conll 09 datasets by about 0.5% absolute. 

table 4 shows our sentence compression results. our globally normalized model again significantly outperforms the local model. the lstm and our global model perform on par on both the automatic evaluation as well as the human ratings, but our model is roughly 100 times faster. 

table 1 shows the performance of intent classification across domains. for the baseline, svm without embedding (w/o embed) achieved 91.99% accuracy, which is already very competitive. however, the models with word embedding trained on 6 billion tokens (6b-50d) and 840 billion tokens (840b-300d) (pennington et al., 2014) achieved 92.89% and 93.00%, respectively. surprisingly, when we use sentence representation (sent) induced from the sketching method with our data set, we can boost the performance up to 93.49%, corresponding to a 18.78% decrease in error relative to a svm without representation. also, we see that the extended sentence representation (sent+) can get additional gains. 

as in table 2 , we also measured performance of our method (sent+) as a function of the percentage of unlabeled data we used from total unlabeled sentences. the overall trend is clear: as the number of sentences are added to the data for inducing sentence representation, the test performance improves because of both better coverage and better quality of embedding. we believe that if we consume more data, we can boost up the performance even more. 

table 2 shows results for english penn treebank using stanford dependencies. despite the minimally designed feature representation, relatively few training iterations, and lack of precomputed embeddings, the parser performed on par with state-of-the-art incremental dependency parsers, and slightly outperformed the state-of-the-art greedy parser. 

table 3 shows the comparison results between our model and the state-of-the-art methods (li et al., 2013; chen et al., 2012). we can see that our method outperforms methods based on human designed features for event trigger identification and achieves comparable f-score for event classification. 

table 3 shows the performance of the algorithms with the manually designed features against the automatically induced ones with lstm-crf. we show the performance of each individual product entity category. compared to all models and settings, lstm-crf reaches the best performance of 90.92 f1 score. the most challenging entity types are product family and model, due to their “wild” and irregular nature. 

we compare the performance of the best programs found with and without curriculum learning in table 4. we find that the best programs found with curriculum learning are substantially better than those found without curriculum learning by a large margin on every metric. 

finally, table 4 presents our results on graphquestions. we report f1 for scanner,the neural baseline model, and three symbolic systems presented in su et al. (2016). scanner achieves a new state of the art on this dataset with a gain of 4.23 f1 points over the best previously reported model. 

table 6 reports scanner's performance on spades. for all freebase related datasets we use average f1 (berant et al., 2013) as our evaluation metric. as can be seen, scanner outperforms all ccg variants (from unsupervised to fully supervised) without having access to any manually annotated derivations or lexicons. again, we observe that scanner outperforms this baseline. 

as shown in table 8, on spades and webquestions, the predicates learned by our model match the output of easy ccg more closely than the heuristic baseline. but for graphquestions which contains more compositional questions, the mismatch is higher. to further analyze how the learned predicates differ from syntax-based ones, we grouped utterances in spades into four types of linguistic constructions: coordination (conj), control and raising (control), prepositional phrase attachment (pp), and subordinate clauses (subord). table 8 also shows the breakdown of matching scores per linguistic construction, with the number of utterances in each type. 

results are shown in table 2 where performance on all three tasks (event coreference, trigger detection, and anaphoricity determination) is expressed in terms of f-score. table 2 shows the results on the english evaluation set. specifically, row 1 shows the performance of the best event coreference system participating in kbp 2016 (lu and ng,2016). as we can see, this system achieves an avg-f of 30.08 for event coreference and an f-score of 46.99 for trigger detection. row 2 shows the performance of the independent models, each of which is trained independently of the other models. as we can see, the independent models outperform the top kbp 2016 system by 1.2 points in avg-f for event coreference and 1.83 points for trigger detection. results of our joint model are shown in row 3. the absolute performance differences between the joint model and the independent models are shown in row 4. as we can see, the joint model outperforms the independent models for all three tasks: by 1.80 points for event coreference, 0.48 points for trigger detection and 4.59 points for anaphoricity determination. most encouragingly, the joint model outperforms the top kbp 2016 system for both event coreference and trigger detection. for event coreference, it outperforms the top kbp system w.r.t. all scoring metrics, yielding an improvement of 3 points in avg-f. for trigger detection, it outperforms the top kbp system by 2.31 points. 

table 3 shows the results on the english and chinese datasets when we add each type of joint factors to the independent model and remove each type of joint factors from the full joint model. among the three types of factors, coref-trigger interactions contributes the most to coreference performance, regardless of whether it is applied in isolation or in combination with the other two types of factors to the independent coreference model. in addition, it is the most effective type of factor for improving trigger detection. when applied in combination, it also improves anaphoricity determination, although less effectively than the other two types of factors. when applied in isolation to the independent models, coref-anaphoricity interactions improves coreference resolution but has a mixed impact on anaphoricity determination. when applied in combination with other types of factors, it improves both tasks, particularly anaphoricity determination. its impact on trigger detection, however, is generally negative. when applied in isolation to the independent models, trigger-anaphoricity interactions improves both trigger detection and anaphoricity determination. when applied in combination with other types of factors, it still improves anaphoricity determination (particularly on chinese), but has a mixed effect on trigger detection. among the three types of factors, it has the least impact on coreference resolution. 

table 6 shows the evaluation results of aes on three datasets. we can see that the blrr algorithm performs better than the svr algorithm. no matter  which  algorithm  is  adopted,  adding  discourse mode features make positive contributions for aes compared  with using  basic feature sets. the trends are consistent over all three datasets. 

table 1 shows that a single-layer convolutional model with position embeddings (convolutional) can outperform both a uni-directional lstm encoder (lstm) as well as a bi-directional lstm encoder (bilstm). this configuration outperforms bilstm by 0.7 bleu (deep convolutional 6/3). among recurrent encoders, the bilstm is 2.3 bleu better than the uni-directional version. the simple pooling encoder which does not contain any parameters is only 1.3 bleu lower than a unidirectional lstm encoder and 3.6 bleu lower than bilstm. the results without position embeddings (words) show that position information is crucial for convolutional encoders. 

the results (table 2) show that a deep convolutional encoder can perform competitively to the state of the art on this dataset (sennrich et al., 2016a). our bi-directional lstm encoder baseline is 0.6 bleu lower than the state of the art but uses only 512 hidden units compared to 1024. a singlelayer convolutional encoder with embedding size 256 performs at 27.1 bleu. increasing the number of convolutional layers to 8 in cnn-a and 4 in cnn-c achieves 27.8 bleu which outperforms our baseline and is competitive to the state of the art. 

table 3 summarizes our amr generation results on the development and test set. we outperform all previous state-of-theart systems by the first round of self-training and further improve with the next rounds. our final model trained on giga-20m outperforms tsp and treetostr trained on ldc2015e86, by over 9 bleu points. 

based on the results in table 5, we make the following observations. comparing morse-cv to morse reflects the fundamental difference between sd17 and mc datasets. comparing morse-cv to morfessor, we observe a significant jump in performance (an increase of 24%). 

we first report the error rates of our full model (dpcnn with 15 weight layers plus unsupervised embeddings) on the larger five datasets (table 2). to put it into perspective, we also show the previous results in the literature. on all the five datasets, dpcnn outperforms all of the previous results, which validates the effectiveness of our approach. 

table 2 shows the results on two relation detection tasks. our proposed hr-bilstm outperformed the best baselines on both tasks by margins of 2-3% (p < 0.001 and 0.01 compared to the best baseline bilstm w/ words on sq and wq respectively). note that using only relation names instead of words results in a weaker baseline bilstm model. the model yields a significant performance drop on simplequestions (91.2% to 88.9%). however, the drop is much smaller on webqsp, and it suggests that unseen relations have a much bigger impact on simplequestions. 

table 1 compares the performance of our system with respect to the baselines on ace05 dataset. we find that our joint model significantly outperforms the joint structured perceptron model (li and ji, 2014) on both entities and relations, despite the unavailability of features such as dependency trees, pos tags, etc. however, if we compare our model to the sptree models, then we find that their model has better recall on both entities and relations. 

as shown in table 5 all the features contribute to the performance of our final system. without the aligned question embedding feature (only word embedding and a few manual features), our system is still able to achieve f1 over 77%. more interestingly, if we remove both faligned and fexact match, the performance drops dramatically. 

table 6 presents the results. despite the difficulty of the task compared to machine comprehension (where you are given the right paragraph) and unconstrained qa (using redundant resources), drqa still provides reasonable performance across all four datasets. however performance when training on squad alone is not far behind, indicating that task transfer is occurring. nevertheless, the best single model that we can find is our overall goal, and that is the multitask (ds) system. we compare to an unconstrained qa system using redundant resources (not just wikipedia), yodaqa (baudis, 2015), giving results which were previously reported on curatedtrec and webquestions. despite the increased difficulty of our task, it is reassuring that our performance is not too far behind on curatedtrec (31.3 vs. 25.4). the gap is slightly bigger on webquestions, likely because this dataset was created from the specific structure of freebase which yodaqa uses directly. 

table 3 gives bleu scores on the europarl corpus of our best performing sentence-level method (sent-beam) and word-level method (word-sampling) compared with pivot-based methods (cheng et al., 2016a). we find that both the sent-beam and word-sampling methods outperform the pivot-based approaches in a zero-resource scenario across language pairs. our word-sampling method improves over the best performing zero-resource soft method on spanish-french translation by +3.29 bleu points and german-french translation by +3.24 bleu points. in addition, the word-sampling mothod surprisingly obtains improvement over the likelihood method, which leverages a source-target parallel corpus. 

table 2 presents the comparison of the methods in terms of it. we see that our method outperforms the competitors on all datasets except for men dataset where it obtains slightly worse results. moreover, it is important that the higher dimension entails higher performance gain of our method in comparison to the competitors. 

table 1 shows the results of these models for word similarity computation. from the results we can observe that: (1) our sat model outperforms other models, including all baselines, on both two test sets. in general, ssa model performs slightly better than baselines, which tentatively proves that sememe information is helpful. (3) the sat model performs much better than ssa and sac. this indicates that sat can obtain more precise sense distribution of a word. (4) sat works better than mst, and we can conclude that a soft disambiguation over senses prevents inevitable errors when selecting only one most-probable sense. 

table 2 shows the evaluation results of these models for word analogy inference. from the table, we can observe that: (1) the sat model performs best among all models, and the superiority is more significant than that on word similarity computation. (2) the sat model does well on both classes of capital and city, because some words in these classes have low frequencies, while their sememes occur so many times that sememe embeddings can be learned sufficiently. (3) it seems that cbow works better than sat on relationship class. whereas for the mean rank, cbow gets the worst results, which indicates the performance of cbow is unstable. on the contrary, although the accuracy of sat is a bit lower than that of cbow, sat seldom gives an outrageous prediction. 

table 2 gives the results of experiment 1. we see that precision on cpb with automatic pos tagging is about 0.9 percentage point higher than that on csb, while recall is about 0.4 percentage point lower, and the gap between f1 scores on cpb and csb is not significant, which is only about 0.3 percentage point, although the size of csb is smaller. 

table 3 summarizes the srl performance of previous benchmark methods and our experiments described above. our approach significantly outperforms sha et al. (2016) by a large margin (wilcoxon signed rank test, p < 0.05), even without using gra. the results of methods using external language resources are also presented in table 3. without gra, the f1 drops 0.37% percentage point to 79.30, confirming that gated recurrent adapter structure is more suitable for our task because it can remember what has been transferred in previous time steps. 

the results are shown in table 2. they show that dropping entire word embeddings and scrambling input sequences is very effective in improving the result of the lstm, while neither type of dropout improves avg. moreover, averaging the hidden states of the lstm is the most effective modification to the lstm in improving performance. all of these modifications can be combined to significantly improve the lstm, finally allowing it to overtake avg. 

in table 3, we compare the various gran architectures. we find that the gran provides a small improvement over the best lstm configuration, possibly because of its similarity to avg. it also outperforms the other gran models, despite being the simplest. 

in table 6 we compare the various gran architectures under the same settings as the previous experiment. we find that the gran still has the best overall performance. 

table 1 shows that our proposed token level embedding scheme ontolstm-pp outperforms the better variant of our baseline lstm-pp (with glove-retro intialization) by an absolute accuracy difference of 4.9%, or a relative error reduction of 32%. ontolstm-pp also outperforms hpcd (full), the previous best result on this dataset. 

table 2 shows the effect of using the pp attachment predictions as features within a dependency parser. however, when gold pp attachment are used, we note a large potential improvement of 10.46 points in pp attachment accuracies (between the ppa accuracy for rbg and rbg + oracle pp), which confirms that adding pp predictions as features is an effective approach. our proposed model rbg + ontolstm-pp recovers 15% of this potential improvement, while rbg + hpcd (full) recovers 10%, which illustrates that pp attachment remains a difficult problem with plenty of room for improvements even when using a dedicated model to predict pp attachments and using its predictions in a dependency parser. for example, the unlabeled attachment score (uas) of the baselines rbg and rbg + hpcd (full) are 94.17 and 94.19, respectively, in table 2, compared to 93.96 and 94.05, respectively, in belinkov et al. (2014). 

table 2 contains results for evaluating the different architectures on ner and chunking. while these results are comparable to the respective state-of-the-art results on most datasets, we did not fine-tune hyperparameters for any specific task, instead providing a controlled analysis of the language modeling objective in different settings. for jnlpba, the system achieves 73.83% compared to 72.55% by zhou and su (2004) and 72.70% by rei et al. (2016). on conll-03, lample et al. (2016) achieve a considerably higher result of 90.94%, possibly due to their use of specialised word embeddings and a custom version of lstm. however, our system does outperform a similar architecture by huang et al. (2015), achieving 86.26% compared to 84.26% f1 score on the conll-03 dataset. 

table 6 presents the result of end-to-end problem solving on the univ data. once a problem-level logical form was produced, the system yielded a correct solution for 27.60% of such problems in dev and 11.40% in test. 

table 2 shows the detailed comparison to their work. our system achieves higher performance on ‘after’, ‘vague’, while lower on ‘before’, ‘includes’ (5% of all data) and ‘is included’ (4% of all data). on the whole, our system reaches better ‘overall’ on both e-e and e-d. 

table 1 compares the performance of the monolingual sequence-to-tree model (dong and lapata, 2016), single, and our multilingual model, multi, with separate and shared output parameters under the single-source setting as described in section 3.1. on average, both variants of the multilingual model outperform the monolingual model by up to 1.34% average accuracy on geo. parameter sharing is shown to be helpful, in particular for geo. we observe that the average performance increase on atis mainly comes from chinese and indonesian. we also learn that although including english is often helpful for the other languages, it may affect its individual performance. 

in table 6, we report the accuracy of the 3 runs for each model and dataset. in both settings, we observe that the best accuracy on both datasets is often achieved by multi. this is the same conclusion that we reached when averaging the results over all runs. 

table 1 shows accuracy, precision, recall and f1-score for the entailment and non-entailment class on the rte-3 dataset. it is also noticeable that the different compound splitters yield different results in the downstream task, with ff2010 being the most beneficial and significantly outperforming the initial rte setup without prior compound splitting (init) by up to four percentage points in accuracy and f1-score. as expected, manual splitting performs best overall. the performance difference with ff2010 is however not statistically significant. 

table 2 shows the results in the low-resource setting, where the bpe2tree model is consistently better than the bpe2bpe baseline. 

we present our experimental results and analyze the results in terms of the lexico-functional linguistic patterns we learn. rows 1-3 of table 4 show the results for the three baselines, in terms of f-score for each class and the macro f. stanford outperforms both nrc and svm, but misses many cases of positive sentiment. row 4 of table 4 shows the results for the autoslog classifier. although autoslog itself does not perform highly, the patterns that it learns represent a different type of knowledge than what is contained in many sentiment analysis tools. row 5 of table 4 shows the results for retrained stanford. the f-scores for retrained stanford are almost identical to the standard stanford classifier. 

table 3 shows our results. nmn is the best performing model using images. for models using the structured representation, the maxent model provides the best performance. 

table 2 shows the results. extractive oracle summaries achieved near perfect scores. although the scores of compressive oracle summaries are inferior to those of extractive oracle summaries, they achieved good enough score, around 4. 

as table 3 shows, our model trained on atomic data outperforms the baseline in all but one project with an average gain of 5 bleu points. in particular, we observe bigger gains for java projects such as corenlp and guava. 

table 3 contains final results on the held-out evaluation data. trans features provide a large f1 improvement of 17.4 over the baseline (base), similar to the benchmark lexical generalisation features (lex). they differ in precision-recall tradeoff, with higher recall but lower precision from trans. lex and trans are complementary, giving f1 of 55.0. this is 20.6 points higher than the baseline features alone, and improves both the precision of lex and the recall of trans. 

table 5 summarizes the results of evinets and some baseline methods on the created yahoo! answers dataset. as we can see, knowledge base data is not enough to answer most of these questions, and a state-of-the-art kbqa system aqqu gets only 0.116 precision. adding textual data helps significantly, and text2kb improves the precision to 0.17, which roughly matches the results of the askmsr system, that ranks candidate entities by their popularity in the retrieved documents. using text along with kb evidence gave higher performance metrics, boosting f1 from 0.271 to 0.291. evinets significantly improves over the baseline approaches, beating askmsr by 28% and kv memn2n by almost 80% in f1 score. 

table 3 summarizes our results. starting with the baseline, we incrementally add the type pair, graph-based, and set size features discussed in 2.1. adding type pair features results in an appreciable performance gain, while the graph features bring little benefit—potentially because pairwise correlations suffice to summarize the set structure when the number of types is moderately low. 

table 2 shows the performance of our crf-based method in finding the correct relation cardinality, evaluated on manually annotated 20 (has part), 100 (admin. terr. entity) and 200 (child and spouse) randomly selected subjects that have at least one object. the random-number baseline achieves a precision of 5% (has part), 3.5% (admin. territ. entity), 0% (spouse) and 11.2% (child). compared to that, especially using only-nummod, our method gives encouraging results for has part, admin. territ. entity and child, with 30-50% precision and around 30% f1-score. for spouse, the performance is significantly lower, reasons are discussed below. furthermore, we can observe that using manual ground truth as training data for the child relation can boost performance considerably. as shown by the last row of table 2, higher quality of training data can considerably boost the performance of cardinality extraction. 

table 1 presents the test-set accuracies obtained by different strategies. results in table 1 indicate that the agt method achieved very competitive accuracy (with 50.5%), when compared to the state-of-the-art results obtained by the tree-lstm (51.0%) (tai et al., 2015, zhu et al., 2015) and high-order cnn approaches (51.2%) (lei et al., 2015). 

we compare our neural combination system with the best individual engines, and the state-of-the-art traditional combination system jane (freitag et al., 2014). table 1 shows the bleu of different models on development data and test data. the bleu score of the multi-source neural combination model is 2.53 higher than the best single model hpmt. the source language input gives a further improvement of +1.12 bleu points. as shown in table 1, jane outperforms the best single mt system by 1.92 bleu points. however, our neural combination system with source language gets an improvement of 1.67 bleu points over jane. furthermore, when augmenting our neural combination system with ensemble decoding 2, it leads to another significant boost of +1.69 bleu points. 

table 2 summarizes the experimental results. the results show that ja-generator, that is, the approach in which japanese captions were used as training data, outperformed en-generator → mt, which was trained without japanese captions. 

table 3 shows that both forms of distributional inference significantly outperform a baseline without di. on average, offset inference outperforms the method of kober et al. (2016) by a statistically significant margin on both datasets. 

table 2 provides the spearman’s correlation scores for different models against the human ranking. we see that with dimensions 100 and 300, two of our models obtain improvements over the baseline. the mssg model of neelakantan et al. (2014) performs only slightly better than our hlte model by requiring considerably more parameters (600 vs. 100 embedding size). 

as can be seen from table 1, sphred outperforms both hred and lm over all the three embedding-based metrics. the last 4 rows in table 1 display the results of our framework applied in two scenarios mentioned in section 2.3 and 2.4. both successfully predict the sentiment with very minor mismatches (0.2% and 0.8%). the high accuracy further demonstrated sphred’s capability of maintaining individual context information. the embedding based scores of our framework are still comparable with sphred and even better than vhred. 

table 4 shows the transfer learning results of bidaf-t on sick dataset (marelli et al., 2014), with various pretraining routines. (a) bidaf-t pretrained on squad outperforms that without any pretraining by 6% and that pretrained on squad-t by 2%, which demonstrates that the transfer learning from large span-based qa gives a clear improvement. (b) pretraining on squad+snli outperforms pretraining on snli only. (c) we outperform the previous state of the art by 2% with the ensemble of squad+snli pretraining routine. 

table 1 shows the classification accuracy of the nine variants of our model on the maptask corpus. table 1 shows that adding the attention mechanism is beneficial, as the traditional attention models always outperform their non-attention counterparts. the gated attention configurations, in turn, outperform those with the traditional attention mechanism by 0.49%-1.21%. as seen in table 1, the performance gain from the hmm connection is larger than the gain from the attention mechanism. without the attention mechanism, the hmm connection brings an increase of 3.63% with the gated bias hmm configuration and 2.58% with the fully gated hmm configuration. with the use of traditional attention, the improvement is 3.01% for the bias hmm configuration and 3.47% for the gated hmm configuration. finally with the gated attention in place, the two hmm configurations improve the accuracy by 3.73%. 

as table 4 demonstrates, our method shows promising results (87.0 f1 score) on the balanced data set. 

table 1 presents mt results using various segmentation strategies. compared to the unseg system, the morph system improved translation quality by 4.6 and 1.6 bleu points in arabic-to-english and english-to-arabic systems, respectively. the results also improved by up to 3 bleu points for ccnn and char systems in the arabic-to-english direction. however, the performance is lower by at least 0.6 bleu points compared to the morph system. in the english-to-arabic direction, where ccnn and char are applied on the target side, the performance dropped significantly. surprisingly, the ccnn system results were inferior to the unseg system for english-to-arabic. indeed, in the ar-toen case ccnn significantly reduces the unknown words in the test sets, while in the english-to-arabic case the number of unknown words remains roughly the same between unseg and ccnn. the bpe system outperformed all other systems in the ar-to-en direction and is lower than morph by only 0.2 bleu points in the opposite direction. 

table 4 compares our final results (greedy search is adopted by setting k=1) to prior neural models. pre-training character embeddings on large scale unlabeled corpus (not limited to the training corpus) has been shown helpful for extra performance improvement. we also show the state of the art results in (zhao and kit, 2008b) of traditional methods. the comparison shows our neural word segmenter outperforms all state of the art neural systems with much less computational cost. 

table 3 shows final results. our model achieves 74.7%, which is close to the state of the art result of 75.2%. we also report the results of stripping away the plots and running our system on just the endings (“ending only”). we achieve 72.5%, matching the similar ending-only result of schwartz et al. (2017b). 

table 1 shows the results of our models and baseline systems. we can see srb outperforms both rnn and rnn context in the f-score of rouge-1, rouge-2 and rouge-l. with a gated attention encoder, srb achieves a better performance with 33.3 f-score of rouge-1, 20.0 rouge-2 and 30.1 rouge-l. 

table 2 summarizes the results of our model and state of the art systems. copynet has the highest socres, because it incorporates copying mechanism to deals with out of vocabulary word problem. 

table 2 presents classification results for task 1 (binary) suspicious vs. verified news posts and task 2 (multi-class) four types of suspicious tweets e.g., propaganda, hoaxes, satire and clickbait. we report performance for different model and feature combinations. we find that our neural network models (both cnns and rnns) significantly outperform logistic regression baselines learned from all feature combinations. the accuracy improvement for the binary task is 0.2 and f1 macro boost for the multi-class task is 0.07. we report 0.05 accuracy improvement for task 1, and 0.02 f1 boost for task 2. adding linguistic cues to basic tweet representations significantly improves results across all models. finally, by combining basic content with network and linguistic features via late fusion, our neural network models achieve best results in binary experiments. interestingly, models perform best in the multiclass case when trained on tweet embeddings and fused network features alone. however, unlike earlier work we find that fusing these features into our models significantly decreases performance by 0.02 accuracy for the binary task and 0.02 f1 for multi-class. 

a combined approach gives the best result. as table 3 shows our whole pipeline (‘cf parser’ in table 3) obtained the best overall performance with the combination of both approaches. 

table 4 shows the spearmanfs correlation results of our models. we outperform fasttext on many word similarity benchmarks. our results are also significantly better than the dictionary-based models, w2g and w2gm. we hypothesize that w2g and w2gm can perform better than the current reported results given proper pre-processing of words due to special characters such as accents. 

from table 1 we see that, while our method is consistently better than both the additive baseline and nonce2vec, removing stop-words from the additive baseline leads to stronger performance for more sentences. since the `a la carte algorithm explicitly trains the transform to match the true word embedding rather than human similarity measures, it is perhaps not surprising that our approach is much more dominant on the definitional nonce task. 

in table 4 we display the result of running cross-validated, (cid:96)2-regularized logistic regression on documents from mr movie reviews (pang and lee, 2005), cr customer reviews (hu and liu, 2004), subj subjectivity dataset (pang and lee, 2004), mpqa opinion polarity subtask (wiebe et al., 2005), trec question classification (li and roth, 2002), sst sentiment classification (binary and fine-grained) (socher et al., 2013), and imdb movie reviews (maas et al., 2011). despite the simplicity of our embeddings (a concatenation over sums of a la carte n-gram vectors), we find that our results are very competitive with many recent unsupervised methods, achieving the best word-level results on two of the tested datasets. 

the results are summarized in table 1 in terms of accuracy and (macro-averaged) precision, recall and f1 score. as can be observed, our model outperforms the baselines, with the r2 ik variant outperforming the others. 

in table 2 we show the specialization performance of the er-cnt models (h = 5, é = 0.3),using different types of constraints on simlex999 (sl) and simverb-3500 (sv). we compare the standard model, which exploits both synonym and antonym pairs for creating training instances, with the models employing only synonym and only antonym constraints, respectively. clearly, we obtain the best specialization when combining synonyms and antonyms. 

table 2 shows the bleu scores on english-german, english-french and english-to-chinese test sets. as it can be seen, the proposed approach obtains significant improvements than the word-by-word baseline system, with at least +5.01 bleu points in english-to-german translation and up to +13.37 bleu points in english-to-french translation. this shows that the proposed model only trained with monolingual data effectively learns to use the context information and

table 5 shows the comparison on different segmentation algorithms: word, character, mixed word/character (wu et al., 2016), bpe (sennrich et al., 2016) and our unigram model with or without subword regularization. the bleu scores of word, character and mixed word/character models are cited from (wu et al.,2016). as german is a morphologically rich language and needs a huge vocabulary for word models, subword-based algorithms perform a gain of more than 1 bleu point than word model. among subword-based algorithms, the unigram language model with subword regularization achieved the best bleu score (25.04), which demonstrates the effectiveness of multiple subword segmentations. 

table 2 shows our results on the wmtf14 en¨de task. the transformer base model improves over gnmt and convs2s by more than 2 bleu points while the big model improves by over 3 bleu points. rnmt+ further outperforms the transformer big model and establishes a new state of the art with an averaged value of 28.49. in this case, rnmt+ converged slightly faster than the transformer big model and maintained much more stable performance after convergence with a very small standard deviation, which is similar to what we observed on the en-fr task. 

from table 4 we draw the following conclusions about the four techniques:. we observed that label smoothing improves both models, leading to an average increase of 0.7 bleu for rnmt+ and 0.2 bleu for transformer big models. ? multi-head attention multi-head attention contributes significantly to the quality of both models, resulting in an average increase of 0.6 bleu for rnmt+ and 0.9 bleu for transformer big models. since by design, we remove one technique at a time in our ablation experiments, we were unable to quantify how much layer normalization helped in either case. ? synchronous training removing synchronous training has different effects on rnmt+ and transformer. for rnmt+, it results in a significant quality drop, while for the transformer big model, it causes the model to become unstable. 

results table 3 shows the performance of our model and our reimplementation of attentivener. our model, which uses a multitask objective to learn finer types without punishing more general types, shows recall gains at the cost of drop in precision. the mrr score shows that our model is slightly better than the baseline at ranking correct types above incorrect ones. 

table 4 shows the performance breakdown for different type granularity and different supervision. overall, as seen in previous work on finegrained ner literature (gillick et al., 2014; ren et al., 2016a), finer labels were more challenging to predict than coarse grained labels, and this issue is exacerbated when dealing with ultra-fine types. all sources of supervision appear to be useful, with crowdsourced examples making the biggest impact. head word supervision is particularly helpful for predicting ultra-fine labels, while entity linking improves fine label prediction. the low general type performance is partially because of nominal/pronoun mentions (e.g. gith), and because of the large type inventory (sometimes glocationh and gplaceh are annotated interchangeably). 

results table 6 shows the overall performance on the test set. our combination of model and training data shows a clear improvement from prior work, setting a new state-of-the art result. 

table 6 shows the results for entity level typing on our wikipedia typenet dataset. we see that both the basic cnn and the cnn+complex models perform similarly with the cnn+complex model doing slightly better on the full data regime. we also see that both models get an improvement when adding an explicit hierarchy loss, even before adding in the transitive closure. the transitive closure itself gives an additional increase in performance to both models. in both of these cases, the basic cnn model improves by a greater amount than cnn+complex. 

table 3 shows the interannotator agreement rates, averaged across all pairs of annotators. average agreement is 74.4% on the scene role and 81.3% on the function (row 1). agreement is higher on the function slot than on the scene role slot, which implies that the former is an easier task than the latter. results show that most confusions are local with respect to the hierarchy. 

table 5 shows the performance values for a generic classifier that predicts fake news across orientations, and orientation-specific classifiers that have been individually trained on articles from either orientation. although all classifiers outperform the naive baselines of classifying everything into one of the classes in terms of precision, the slight increase comes at the cost of a large decrease in recall. while the orientation-specific classifiers are slightly better for most metrics, none of them outperform the naive baselines regarding the f measure. 

table 1 shows the results on the test set. our approach significantly outperforms the s2s baselines both with individual models and ensembles, while using a comparable number of parameters. in particular, we obtain these results without relying on scoping heuristics. table 1 also show bleu scores reported in previous work. it is also most informative to compare our s2s model with konstas et al.(2017), since this baseline is very similar to theirs. we expected our single model baseline to outperform theirs since we use a larger training set but we obtained similar performance. 

table 1 shows our main experimental results. in two of the cases (sst and roc), sopa outperforms all models. on amazon, sopa performs within 0.3 points of cnn and bilstm, and outperforms the other two baselines. the table also shows the number of parameters used by each model for each task. however, sopa performs better or roughly the same as a bilstm, which has 3 to 6 times as many parameters. 

hyperparameters: table 2 shows the development results of various s-lstm settings, where time refers to training time per epoch. without the sentence-level node, the accuracy of s-lstm drops to 81.76%, demonstrating the necessity of global information exchange. adding one additional sentence-level node as described in section 3.2 does not lead to accuracy improvements, although the number of parameters and decoding time increase accordingly. the accuracies of s-lstm increases as the hidden layer size for each node increases from 100 to 300, but does not further increase when the size increases beyond 300. without using hsi and h=si, the performance of s-lstm drops from 82.64% to 82.36%, showing the effectiveness of having these additional nodes. 

as shown in table 3, bilstm gives significantly better accuracies compared to uni-directional lstm, with the

from table 1, we can see that stamp performs better than existing systems on wikisql. incorporating rl strategy does not significantly improve the performance. our simplified model, stamp (w/o cell), achieves better accuracy than aug.pntnet, which further reveals the effects of the column channel. results also demonstrate the effects of incorporating the column-cell relation, removing which leads to about 4% performance drop in terms of accex. 

overall results are shown in table 6. from the table, we can see that our final model (seq2seq lf+attreg+iter) outperforms the neural-based baseline models (wang et al. (2017) 4 and seq2seq equ). on number word problem dataset, our model already outperforms the state-of-the-art feature-based model (huang et al., 2017) by 40.8% and is comparable to the ruled-based model (shi et al., 2015). from the first two rows, we can see that the seq2seq model which is trained to generate intermediate forms (seq2seq lf) greatly outperforms the same model trained to generate equations(seq2seq equ). the use of intermediate forms helps more on numword than on dolphin18k. 

table 7 shows the results of the data augmentation model and the gan-based model. our gen+adv model performs better than the data augmented model. 

table 2 shows the precision, recall and f1 value of noveltagging model (zheng et al., 2017) and our onedecoder and multidecoder models. as we can see, in nyt dataset, our multidecoder model achieves the best f1 score, which is 0.587. there is 39.8% improvement compared with the noveltagging model, which is 0.420. besides, our onedecoder model also outperforms the noveltagging model. in the webnlg dataset, multidecoder model achieves the highest f1 score (0.371). multidecoder and onedecoder models outperform the noveltagging model with 31.1% and 7.8% improvements, respectively. these observations verify the effectiveness of our models. we can also observe that, in both nyt and webnlg dataset, the noveltagging model achieves the highest precision value and lowest recall value. by contrast, our models are much more balanced. 

table 1 shows the trigger identification performance. it can be observed that self outperforms other models, with a performance gain of no less than 1.1% f-score. frankly, the performance mainly benefits from the higher recall (78.8%). but in fact the relatively comparable precision (75.3%) to the recall reinforces the advantages. by contrast, although most of the compared models achieve much higher precision over self, they suffer greatly from the substantial gaps between precision and recall. the advantage is offset by the greater loss of recall. 

the middle block of table 1 shows the performance of the pairwise model after applying double-checking. the bottom block of table 1 presents the results, showing that all models from the present paper outperform existing models from the literature. we show a comparison to a baseline model which adds two dense layers on top of the pairwise model, without the gcl. the result shows that the performance is slightly higher than what we get from the pairwise model, but the difference is smaller than what we get from gcl models -suggesting that the performance improvement with gcl models is not just due to more parameters. 

table 5 shows that by incorporating our learned event knowledge, the overall prediction accuracy was improved by 1.1%. 

table 4 shows the performance of our model with different random seeds on the test dataset. we report the minimum, the maximum, the median f-scores results and the standard deviation ƒðof f-scores. the maximum f-score is 57.5% and the minimum on is 56.5%. 

neusum achieves 19.01 rouge-2 f1 score on the cnn/daily mail dataset. compared to the unsupervised baseline methods, neusum performs better by a large margin. in terms of rouge2 f1, neusum outperforms the strong baseline lead3 by 1.31 points. neusum also outperforms the neural network based models. compared to the state-of-the-art extractive model nnse (cheng and lapata, 2016), neusum performs significantly better in terms of rouge-1, rouge2 and rouge-l f1 scores. 

in table 5, we show the substantial test-time speed-up of our model compared to see et al. (2017).18. our model without reranking is extremely fast. from table 5 we can see that we achieve a speed up of 18x in time and 24x in word generation rate. even after adding the (optional) reranker, we still maintain a 6-7x speed-up (and hence a user can choose to use the reranking component depending on their downstream applicationfs speed requirements).20. 

we also show human evaluation results on the gigaword dataset in table 4 (again based on pairwise comparisons for 100 samples), where we see that our mtl model is better than our state-of-theart baseline on both relevance and readability.7. 

 table 6 compares our modelâ€™s performance to pasunuru and bansal (2017). our pointer mechanism gives a performance boost, since the entailment generation task involves copying from the given premise sentence, whereas the 2-layer model seems comparable to the 1-layer model. 

as can be seen in table 9, our multi-task model improves upon the baseline in the aspect of being entailed by the source document (with statistical significance p < 0.001). 

 table 6 shows an overview of the average results of our supervised experiments for five of the psl models. the first column lists the svm or psl model. the second column presents the results of a given model when using the mfd as the source of the unigrams for the initial model (m1). the final column shows the results when the ar unigrams are used as the initial source of supervision. the first two rows show the results of predicting the morals present in tweets using a bag-of-words (bow) approach. both the svm and psl models perform poorly due to the eleven predictive classes and noisy input features. the third row shows the results when taking a majority vote over the presence of mfd unigrams, similar to previous works. this approach is simpler and less noisy than m1, the psl model closest to this approach. 

table 9 shows the macro-weighted average f1 scores for three different models. the joint model using ar unigrams outperforms the baseline, showing that there is some benefit to modeling moral foundations and frames together, as well as using domain-specific unigrams. as expected, the joint model does not outperform the skyline model which is able to use the known values of the frames in order to accurately classify the moral foundations associated with the tweets. 

table 3 reports results on django where we observe similar tendencies. coarse2fine outperforms onestage by a wide margin. it is also superior to the best reported result in the literature (snm+copy; see the second block in the table). again we observe that the sketch encoder is beneficial and that there is an 8.9 point difference in accuracy between coarse2fine and the oracle. 

table 5 shows the relative importance of individual metrics in the regression model. the results indicate that model uncertainty (noise/dropout/posterior/perplexity) plays the most important role. on ifttt, the number of unknown tokens (#unk) and the variance of top candidates (var(k-best)) are also very helpful because this dataset is relatively noisy and contains many ambiguous inputs. 

table 3 shows the results of the proposed method in comparison to previous systems, including those with different degrees of supervision. we focus on the widely used english-italian dataset of dinu et al(2015) and its extensions. despite being fully unsupervised, our method achieves the best results in all language pairs but one, even surpassing previous supervised approaches. the only exception is english-finnish, where artetxe et al.(2018a) gets marginally better results with a difference of 0.3 points, yet ours is the only unsupervised system that works for this pair. at the same time, it is remarkable that the proposed system gets substantially better results than artetxe et al.(2017), the only other system based on selflearning, with the additional advantage of being fully unsupervised. 

as table 6 shows, adding each component usually enhances the performance (f-score, %), while the impact also depends on the size of the target task data. for example, the language-specific layer slightly impairs the performance with only 10 training sentences. 

table 2 compares its performance with our adapted bwes, with both cosine similarity and classification based systems. "top"f1 scores are based on the most probable word as prediction only; "all"f1 scores use all words as prediction whose probability is above the threshold. it can be seen that the cosine similarity based system using adapted bwes clearly outperforms the nonadapted bwes which were trained in a resource poor setup.4. the classification based system performs significantly better comparing to cosine similarity by exploiting the seed lexicon better. using adapted bwes as input word embeddings for the system further improvements were achieved which shows the better quality of our bwes. this shows the significance of the medical seed lexicon for this system. on the other hand, adapted bwes have better performance compared to non-adapted ones using the best translation while they have just slightly lower f1 using multiple translations. this result shows that while with adapted bwes the system predicts better gtoph translations, it has a harder time when predicting gallh due to the increased vocabulary size. 

results in table 5 show that adding semisup to the classifier further increases performance for bli as well. for the baseline system, when using only in-domain text for creating bwes, only the medical unlabeled set was effective, general domain word pairs could not be exploited due to the lack of general semantic knowledge in the bwe model. on the other hand, by using our domain adapted bwes, which contain both general domain and in-domain semantical knowledge, we can exploit word pairs from both domains. results for adapted bwes increased in 3 out of 4 cases, where the only exception is when using multiple translations for a given source word (which may have been caused by the bigger vocabulary size). 

table 4 shows that the combination of different interactions between ctx and ctx+kn representations leads to clear improvement over the w/o knowledge setup, in particular for the common nouns dataset. 

table 6 compares our model (knowledgeable reader) to previous work on the cbt datasets. we show the results of our model with the settings that performed best on the dev sets of the two datasets ne and cn: for ne, (dctx+kn, qctx) with 100 facts; for cn the full model with 50 facts, both with cn5sel. knreader clearly outperforms prior single-hop models on both datasets. while we do not improve over the state of the art, our model stands well among other models that perform multiple hops. 

the scores for the retrieval task with all models are summarized in table 2. the results clearly demonstrate the superiority of dpcca (with a slight advantage to the more complex variant b) and of the concatenation of their representation with that of the dcca noi (strongest) baseline. 

the results on the pos classes represented in simlex-999 (nouns, verbs, adjectives, table 3) form our main finding: conditioning the multilingual representations on a shared image leads to improvements in verb and adjective representations. while for nouns one of the dpcca variants is the best performing model for both languages, the gaps from the best performing baselines are much smaller. 

further, table 4 presents results on all simlex word pairs. first, the results over the initial monolingual embeddings before training (init emb) clearly indicate that multilingual information is beneficial for the word similarity task. we observe improvements with all models (the only exception being extremely lowscoring ppcca and ncca, not shown). 

table 3 displays our results, from which several observations can be made. first, we observe that combining glove and picturebook leads to improved similarity across most categories. for adjectives and the most abstract category, glove performs significantly better, while for the most concrete category picturebook is significantly better. this result confirms that glove and picturebook capture very different properties of words. next we observe that the performance of picturebook gets progressively better across each concreteness quartile rating, with a 20 point improvement over glove for the most concrete category. for the hardest subset of words, picturebook performs slightly better than glove while glove performs better across all pairs. we also compare to a convolutional network trained with visual similarity. we observe a performance difference between our visual and semantic embeddings: on all categories except verbs, the semantic embeddings outperform visual ones, even on the most concrete categories. this indicates the importance of the type of similarity used for training the model. finally we note that adding more images nearly consistently improves similarity scores across categories. 

table 4 displays our results. for bow models, adding picturebook embeddings to glove results in significant gains across all three tasks. while non-contextual gating is sufficient to improve bag-of-words methods, with bilstm-max it slightly hurts performance over the glove baseline. adding contextual gating was necessary to improve over the glove baseline on snli. finally we note the strength of our own glove baseline over the reported results of conneau et al.(2017a), from which we improve on their accuracy from 85.0 to 86.8 on the development set. 

table 6 displays our  results on this task. our glove baseline was able to match or outperform the reported results in faghri et al.(2017) with the exception of recall@10 for image annotation, where it performs slightly worse. glove+picturebook improves over the glove baseline for image search but falls short on image annotation. however, using contextual gating results in improvements over the baseline on all metrics except r@1 for image annotation. 

on the english ¨ german tasks, we find our picturebook model to perform on average 0.8 bleu or 0.7 meteor over our baseline. on the german task, compared to the previously best published results (caglayan et al.,2017) we do better in bleu but slightly worse in meteor. 

as shown in table 3, both tnet-lf and tnet-as consistently achieve the best performance on all datasets, which verifies the efficacy of our whole tnet model. moreover, tnet can perform well for different kinds of user generated content, such as product reviews with relatively formal sentences in laptop and rest, and tweets with more ungrammatical sentences in twitter.  indeed, we can also observe that another cnn-based baseline, i.e., cnnasp implemented by us, also obtains good results on twitter. for the tweet in twitter, the competitive bilstmatt-g and ram cannot perform as effective as they do for the reviews in laptop and rest, due to the fact that they are heavily rooted in lstms and the ungrammatical sentences hinder their capability in capturing the context features. to investigate the impact of each component such as deep transformation, context-preserving mechanism, and positional relevance, we perform comparison between the full tnet models and its ablations (the third group in table 3). after removing the deep transformation (i.e., the techniques introduced in section 2.2), both tnet-lf and tnetas are reduced to tnet w/o transformation (where position relevance is kept), and their results in both accuracy and f1 measure are incomparable with those of tnet. comparing the results of tnet and tnet w/o context (where tst and position relevance are kept), we observe that the performance of tnet w/o context drops significantly on laptop and rest7, while on twitter, tnet w/o context performs very competitive (p-values with tnetlf and tnet-as are 0.066 and 0.053 respectively for accuracy). tnet w/o context performs consistently better than tnet w/o transformation, which verifies the efficacy of the target specific transformation (tst), before applying context-preserving. 

table 2 shows the human evaluation results. it can be clearly seen that the proposed method obviously improves semantic preservation. the semantic score is increased from 3.87 to 5.08 on the yelp dataset, and from 3.22 to 4.67 on the amazon dataset. in general, our proposed model achieves the best overall performance. furthermore, it also needs to be noticed that with the large improvement in content preservation, the sentiment accuracy of the proposed method is lower than that of caae on the two datasets. 

table 3 reports a performance comparison of all benchmarked models on the reddit datasets. our proposed siarn and miarn models achieve very competitive performance on the reddit datasets, with an average of ? 2% margin improvement over the best baselines. notably, the baselines we compare against are extremely competitive state-of-the-art neural network models. this further reinforces the effectiveness of our proposed approach. 

table 4 reports a performance comparison of all benchmarked models on the debates datasets. the performance improvement on debates (long text) is significantly larger than short text (i.e., twitter and reddit). for example, miarn outperforms grnn and cnn-lstm-dnn by 8% to 10% on both iac-v1 and iac-v2. 

the effectiveness of various inference approaches can be observed by comparing the results in table 2 by column. compared to the normal seq2seq inference and seq2seq (+lm) baselines, fluency boost inference brings about on average 0.14 and 0.18 gain on f0.5 respectively, which is a significant6 improvement, demonstrating multi-round edits by fluency boost inference is effective. take our best system (the last row in table 2) as an example, among 1,312 sentences in the conll-2014 dataset, seq2seq inference with shallow fusion lm edits 566 sentences. in contrast, fluency boost inference additionally edits 23 sentences during the second round inference, improving f0.5 from 52.59 to 52.72. 

table 1 shows the evaluation results of dam as well as all comparison models. as demonstrated, dam significantly outperforms other competitors on both ubuntu corpus and douban conversation corpus, including smndynamic, which is the state-of-the-art baseline, demonstrating the superior power of attention mechanism in matching response with multi-turn context. besides, both the performances of damf irst and damself decrease a lot compared with dam, which shows the effectiveness of self-attention and cross-attention. both damf irst and damlast underperform dam, which demonstrates the benefits of using multigrained representations. also the absence of self-attention-match brings down the precision, as shown in damcross, exhibiting the necessity of jointly considering textual relevance and dependency information in response selection. one notable point is that, while damf irst is able to achieve close performance to smndynamic, it is about 2.3 times faster than smndynamic in our implementation as it is very simple in computation. 

table 2 reports our results compared to other benchmarks. to the best of our knowledge, we set a new stateof-the-art for single-model parsing achieving 86.5 f1 on the test set. 

surprisingly, with only 50 annotated questions (see table 4), performance on qbankdev jumps 5 points, from 89.9% to 94.9%. this is only 1.5% below training with all of wsjtrain and qbanktrain. 

on the more difficult genia corpus of biomedical abstracts (tateisi et al., 2005), we see a similar, if somewhat less dramatic, trend. see table 5. with 50 annotated sentences, performance on geniadev jumps from 79.5% to 86.2%, outperforming all but one parser from david mccloskyfs thesis (mcclosky, 2010) ? the one that trains on all 14k sentences from geniatrain and self-trains using 270k sentences from pubmed. that parser achieves 87.6%, which we outperform with just 500 sentences from geniatrain. 

table 2 displays the performance of the proposed method and the baselines in the two evaluation settings. our method outperforms all the methods in the isomorphic setting. in the nonisomorphic setting, it outperforms the other two systems that score reasonably on the isomorphic setting (sfs and iiith) but cannot compete with the systems that focus on achieving high precision. 

table 4 displays the methods' performance on the two versions of the tratz (2011) dataset and the two dataset splits. the paraphrase model on its own is inferior to the distributional model, however, the integrated version improves upon the distributional model in 3 out of 4 settings, demonstrating the complementary nature of the distributional and paraphrase-based methods. the contribution of the paraphrase component is especially noticeable in the lexical splits. as expected, the integrated method in shwartz and waterson (2018), in which the paraphrase representation was trained with the objective of classification, performs better than our integrated model. 

for each corpus, we create two subcorpora: with sentiment contains only the sentences with at least one word from the sentiment lexicon, while without sentiment is the complement. table 3 shows the results, including that of random word embeddings for reference. sentiment lexicon has a significant impact on the performance of sentiment and subjectivity classifications, and a smaller impact on topic classification. without sentiment, the subjective embeddings prove more robust, still outperforming the objective on sentiment classification, while the objective performs close to random word embeddings on amazon. 

table 4 shows the unfolded results for the 24 classification datasets of amazon, as well as for rotten tomatoes. for each classification dataset (row), and for the objective and subjective embedding corpora respectively, the best word embedding methods are shown in bold. an asterisk indicates statistically significant8 results at 5% in comparison to word2vec. both sentivec variants outperform word2vec in the vast majority of the cases. the degree of outperformance is higher for the objective than the subjective word embeddings. sentivec also outperforms the two baselines that benefit from the same lexical resources. retrofitting does not improve upon word2vec, with the two embeddings essentially indistinguishable (the difference is only noticeable at the second decimal point). refining makes the word embeddings perform worse on the sentiment classification task. 

table 5 shows that the unfolded results for topic classification on the six datasets, and the result for subjectivity classification are similar across methods. neither the sentivec variants, nor retrofitting and refining, change the subjectivity and topic classification capabilities much, which means that the used sentiment lexicon is targeted only at the sentiment subspace of embeddings. 

table 1 shows the performance of our model and the baselines on the task of metaphor identification. for sentence level metaphor identification, it can be observed that all our models outperform the baseline (melamud et al., 2016), with sim-cbowi+o giving the highest f1 score of 75% which is a 6% gain over the baseline. we also see that models based on both input and output vectors (i.e.,sim-cbowi+o and sim-sgi+o) yield better performance than the models based on input vectors only (i.e., sim-cbowi and sim-sgi ). when comparing cbow and skip-gram based models, we see that cbow based models generally achieve better performance in precision whereas skip-gram based models perform better in recall. in terms of phrase level metaphor identification, we compare our best performing models (i.e.,

word similarity is conducted to test the semantic information which is encoded in word embeddings, and the results are listed in table 2 (first 6 rows). we observe that our models surpass the comparative baselines on five datasets. compared with the base model cbow, it is remarkable that our models approximately achieve improvements of more than 5% and 7%, respectively, in the performance on the golden standard wordsim-353 and rg-65. on ws-353-rel, the difference between cbow and lmm-s even reaches 8%. by incorporating mophemes, emm also performs better than other baselines but fails to get the performance as well as ours. specially, because of the medium size corpus and the experimental settings, glove

table 4 shows comparison of our memory-to-context model variants source context-nmt models (jean et al., 2017; wang et al., 2017). for german→english, our s-nmt+src mem model is comparable to jean et al. (2017) but outperforms wang et al. (2017) for one test set according to bleu, and for both test sets according to meteor. for estonian→english, our model outperforms jean et al. (2017). our global source context model has only surface-level sentence information, and is oblivious to the individual words in the context since we do an offline training to get the sentence representations (as previously mentioned). 

each system geoparses its particular majority of the dataset to obtain a representative data sample, shown in table 1 as strongly correlated scores for subsets of different sizes, with which to assess model performance. table 1 also shows scores in brackets for the overlapping partition of all systems in order to compare performance on identical instances: geovirus 601 (26%), lgl 787 (17%) and wiktor 2,202 (9%). the geocoding difficulty based on the ambiguity of each dataset is: lgl (moderate to hard), wik (very hard), geo (easy to moderate). table 1 shows the effectiveness of this heuristic, which is competitive with many geocoders, even outperforming some. however, the baseline is not effective on wiktor as the dataset was deliberately constructed as a tough ambiguity test. table 1 shows how several geocoders mirror the behaviour of the population baseline. the rule-based (edinburgh, geotxt, clavin), (topocluster), machine learning (camcoder, santos) and other (yahoo!, population) geocoders occupy different ranks across the three datasets. camcoder proved to be robust to reduced context, with only a small performance decline. using the same format as table 1, auc errors for lgl increased from 22 (18) to 23 (19), wik from 33 (37) to 37 (40) and geo remained the same at 31 (32). 

table 2 shows our ptb experimental results. from this result, we can see that the ensemble model outperforms the baseline model by 1.90 in las. for our distillation from reference, when setting alpha = 1.0, best performance on development set is achieved and the test las is 91.99. we also compare our parser with the other parsers in table 2. the second group shows the greedy transition-based parsers in previous literatures. andor et al. (2016) presented an alternative state representation and explored both greedy and beam search decoding. (ballesteros et al., 2016) explores training the greedy parser with dynamic oracle. our distillation parser outperforms all these greedy counterparts. the third group shows parsers trained on different techniques including decoding with beam search (buckman et al.,2016; andor et al., 2016), training transitionbased parser with beam search (andor et al.,2016), graph-based parsing (dozat and manning,2016), distilling a graph-based parser from the output of 20 parsers (kuncoro et al., 2016), and converting constituent parsing results to dependencies (kuncoro et al., 2017). our distillation parser still outperforms its transition-based counterparts but lags the others. 

table 3 shows the experimental results on iwslt 2014 dataset. similar to the ptb parsing results, the ensemble 10 translators outperforms the baseline translator by 3.47 in bleu score. distilling from the ensemble by following the reference leads to a single translator of 24.76 bleu score. table 3 shows the exploration result of a bleu score of 24.64 and it slightly lags the best reference model. distilling from both the reference and exploration improves the single modelâs performance by a large margin and achieves a bleu score of 25.44. we also compare our model with other translation models including the one trained with reinforcement learning (ranzato et al., 2015) and that using beam search in training (wiseman and rush, 2016). our distillation translator outperforms these models. 

the comparison in table 4 shows that the ensemble model significantly outperforms the baseline on ambiguous and non-optimal states. we also observe that the distillation model perform better than both the baseline and ensemble. 

table 1 illustrates the uas and las of the four versions of our model (with decoding beam size 10) on the three treebanks, together with previous top-performing systems for comparison. our full model significantly outperforms all the transition-based parsers on all three languages, and achieves better results than most graph-based parsers. our re-implementation of biaf obtains better performance than the original one in dozat and manning (2017), demonstrating the effectiveness of the

table 2 gives results of the parsers with different versions of pos tags on the test data of ptb. the parser with gold-standard pos tags significantly outperforms the other two parsers, showing that dependency parsers can still benefit from accurate pos information. the parser with predicted (imperfect) pos tags, however, performs even slightly worse than the parser without using pos tags. it illustrates that an end-to-end parser that doesnft rely on pos information can obtain competitive (or even better) performance than parsers using imperfect predicted pos tags, even if the pos tagger is relative high accuracy (accuracy > 97% in this experiment on ptb). 

table 4 summarizes the results of the stackptr parser, along with biaf for comparison, on both the development and test datasets for each language. first, both biaf and stackptr parsers achieve relatively high parsing accuracies on all the 12 languages ? all with uas are higher than 90%. on nine languages ? catalan, czech, dutch, english, french, german, norwegian, russian and spanish ? stackptr outperforms biaf for both uas and las. on bulgarian, stackptr achieves slightly better uas while las is slightly worse than biaf. on italian and romanian, biaf obtains marginally better parsing performance than stackptr. 

table 2 indicates that, given enough capacity, lstm language models without explicit syntactic supervision are able to perform well in number agreement. for cases with multiple attractors, we observe that the lstm language model with 50 hidden units trails behind its larger counterparts by a substantial margin despite comparable performance for zero attractor cases, suggesting that network capacity plays an especially important role in propagating relevant structural information across a large number of steps.5 . as demonstrated on the last row of table 2, we find that the character lstm language model performs much worse at number agreement with multiple attractors compared to its word-based counterparts. 

in table 5, our model can achieve highest 12.6 bleu score. in addition, mem2seq has shown promising results in terms of entity f1 scores (33.4%), which are, in general, much higher than those of other baselines. the other baselines such as seq2seq or ptrunk especially have worse performances in this dataset since it is very inefficient for rnn methods to encode longer kb information, which is the advantage of mem2seq. furthermore, we observe an interesting phenomenon that humans can easily achieve a high entity f1 score with a low bleu score. 

table 3 displays the accuracy and recall of entities on factoid question answering dialogues. the performance of nkd is slightly better than the specific qa solution gends, while lstm and hred which are designed for chi-chat almost fail in this task. all the variants of nkd models are capable of generating entities with an accuracy of 60% to 70%, and nkd-gated achieves the best performance with an accuracy of 77.6% and a recall of 77.3%. 

table 4 lists the accuracy and recall of entities on the entire dataset including both the factoid qa and knowledge grounded chit-chats. not surprisingly, both nkd-ori and nkd-gated outperform gends on the entire dataset, and the relative improvement over gends is even higher than the improvement in qa dialogues. all the nkd variants in table 4 generate more entities than gends. lstm and hred also produce a certain amount of entities, but are of low accuracies and recalls. we also noticed that nkdgated achieves the highest accuracy and recall, but generates fewer entities compared with nkdori and nkd-gated, whereas nkd-atte generates more entities but also with relatively low accuracies and recalls. 

the results of human evaluation in table 5 also validate the superiority of the proposed model, especially on appropriateness. responses generated by lstm and hred are of high fluency, but are simply repetitions, or even dull responses as gi donft know.h, ggood.h. nkd-gated is more adept at incorporating the knowledge base with respect to appropriateness and correctness, while nkdatte generates more fluent responses. nkd-ori is a compromise, and obtains the best correctness in completing an entire dialogue. 

table 3 presents evaluation results for roundtrip translation and sentiment analysis. validity of roundtrip (rt) evaluation results. rtsimple (line 1) is not competitive; e.g., its accuracy is lower by almost half compared to n(t). we also see that rt is an excellent differentiator of poor multilingual embeddings (e.g., bow) vs. higher-quality ones like s-id and n(t). the concept-based multilingual embedding learning algorithms clique and n(t)(lines 5-6) consistently (except s1 word) outperform bow and s-id (lines 2-3) that are not based on concepts. bow performs poorly in our low-resource setting; this is not surprising since bow methods rely on large datasets and are therefore expected to fail in the face of severe sparseness. s-id performs reasonably well for word, but even in that case it is outperformed by n(t), in some cases by a large margin, e.g., µ of 63 for s-id vs. 80 for n(t) for s4. for char, s-id results are poor. on sentiment classification, n(t) also consistently outperforms s-id. while s-id provides a clearer signal to the embedding learner than bow, it is still relatively crude to represent a word as - essentially - its binary vector of verse occurrence. comparison of graph-theoretic definitions of concepts:n(t)-clique, n(t)-cc. n(t) (line6) has the most consistent good performance across tasks and evaluation measures. postfiltering target neighborhoods down to cliques (line 7) and ccs (line 8) does not work. the reason is that the resulting number of concepts is too small; see, e.g., low coverages of n = 18 (n(t)-clique) and n = 5 (n(t)-cc) for word and n = 21 (n(t)-cc) for char. n(t)-clique results are highly increased for char, but still poorer by a large margin than the best methods. comparison of graph-theoretic definitions of concepts: clique. clique has strong performance for a subset of measures, e.g., ranks consistently second for rt (except s1 word) and sentiment analysis in word. comparison of graph-theoretic definitions of concepts: n(t) vs. n(t)-edge. this gqualityh filter does seem to work in some cases, e.g., best performance s16 md for char. but results for word are much poorer. sample performs best for char: best results in five out of eight cases. however, its coverage is low: n = 58. this is also the reason that it does not perform well on sentiment analysis for char (f1 = 77 for pos). target neighborhoods n(t). the overall best method is n(t). it is the best method more often than any other method and in the other cases, it ranks second. 

as shown in table 4, without using word segmentation, a characterbased lstm-crf model gives a development f1- score of 62.47%. adding character-bigram and softword representations as described in section 3.1 increases the f1-score to 67.63% and 65.71%, respectively, demonstrating the usefulness of both sources of information. in addition, a combination of both gives a 69.64% f1-score, which is the best among various character representations. table 4 shows a variety of different settings for word-based chinese ner. with automatic segmentation, a word-based lstm crf baseline gives a 64.12% f1-score, which is higher compared to the character-based baseline. the two methods of using character lstm to enrich word representations in section 3.2, namely word+char lstm and word+char lstm(cid:48), lead to similar improvements. a cnn representation of character sequences gives a slightly higher f1-score compared to lstm character representations. on the other hand, further using character bigram information leads to increased f1-score over word+char lstm, but decreased f1-score over word+char cnn. as a result, we use word+char+bichar lstm for wordbased ner in the remaining experiments, which gives the best development results, and is structurally consistent with the state-of-the-art english ner models in the literature. as shown in table 4, the lattice lstm-crf model gives a development f1-score of 71.62%, which is significantly higher compared with both the word-based and character-based methods, despite that it does not use character bigrams or word segmentation information. the fact that it significantly outperforms char+softword shows the advantage of lattice word information as compared with segmentor word information. 

table 3 shows the results on kbp2017eval. we can see that npn(task-specific) outperforms other methods significantly. 

table 6 shows the experiment results. we can see that neither character-level or wordlevel representation can achieve competitive results with the npns. this verified the necessity of hybrid representation. 

table 2 compares our final results with existing work. graph2seq+charlstm+copy achieves a bleu score of 23.3, which is 1.3 points better than mseq2seq+anon trained on the same amr corpus. in addition, our model without character lstm is still 0.7 bleu points higher than mseq2seq+anon. following konstas et al. (2017), we also evaluate our model using both the amr corpus and sampled sentences from gigaword. using additional 200k or 2m gigaword sentences, graph2seq+charlstm+copy achieves bleu scores of 28.2 and 33.0, respectively, which are 0.8 and 0.7 bleu points better than mseq2seq+anon using the same amount of data, respectively. the bleu scores are 5.3 and 10.1 points better than the result when it is only trained with the amr corpus, respectively. 

table 4 shows the results of the human evaluations. the results confirm the automatic evaluation in which our proposed model achieves the best scores. 

to investigate the effect of individual discriminators on the overall performance, we report the results of ablations of our model in table 4. interestingly, most discriminators help with most aspects of writing, but all except repetition fail to actually improve the overall quality over adaptivelm. 

table 3 shows the bleu and bleu-2 scores for the proposed model under different subsets of features. overall bleu scores are low, likely due to the inherent variance in the language generation task (novikova et al., 2017) , although a precursory examination of the outputs for data points selected randomly from test set indicated that they were reasonable. 

in table 1, we compare various rc datasets with two embodiments of our dataset i.e. the selfrc and paraphraserc. we use ner and noun phrase/verb phrase extraction over the entire dataset to identify key entities in the question, plot and answer which is in turn used to compute the metrics mentioned in the table. it is evident that tackling paraphraserc is much harder than the others on account of (i) larger distance between the query and answer, (ii) low word-overlap between query & passage, and (iii) higher number of sentences required to infer an answer. 

spanmodel v/s genmodel:. comparing the first two rows (selfrc) and the last two rows (paraphraserc) of table 3 we see that the spanmodel clearly outperforms the genmodel. selfrc v/s paraphraserc:. comparing the selfrc and paraphraserc numbers in table 3, we observe that the performance of the models clearly drops for the latter task, thus validating our hypothesis that paraphraserc is a indeed a much harder task. finally, comparing the spanmodel with and without 1691 paraphrasing in table 3 for paraphraserc, we observe that the pre-processing step indeed improves the performance of the span detection model. 

the main results in terms of em and f1 are shown in table 1. we observe that san achieves 76.235 em and 84.056 f1, outperforming all other models. standard 1-step model only achieves 75.139 em and dynamic steps (via reasonet) achieves only 75.355 em. san also outperforms a 5-step memory net with averaging, which implies averaging predictions is not the only thing that led to sanfs superior results; indeed, stochastic prediction dropout is an effective technique. 

table 4 shows the development set scores for t = 1 to t = 10. we observe that there is a gradual improvement as we increase t = 1 to t = 5, but after 5 steps the improvements have saturated. in fact, the em/f1 scores drop slightly, but considering that the random initialization results in table 3 show a standard deviation of 0.142 and a spread of 0.426 (for em), we believe that the t = 10 result does not statistically differ from the t = 5 result. 

the results in table 5 show that san achieves the new state-of-the-art performance and sanfs superior result is mainly attributed to the multi-step answer module, which leads to significant improvement in f1 score over the standard 1-step answer module, i.e., +1.2 on addsent and +0.7 on addonesent. 

the results in table 7 show that san outperforms v-net (wang et al., 2018) and becomes the new state of the art. 

table 5 shows results in the task of qa on squad and newsqa. minimal is more efficient in training and inference than full. on squad, s-reader achieves 6.7 training and 3.6 inference speedup on squad, and 15.0 training and 6.9 inference speedup on newsqa. in addition to the speedup, minimal achieves comparable result to full (using s-reader, 79.9 vs 79.8 f1 on squad and 63.8 vs 63.2 f1 on newsqa). 

table 8 shows results on triviaqa (wikipedia) and squad-open. first, minimal obtains higher f1 and em over full, with the inference speedup of up to 13.8. second, the model with our sentence selector with dyn achieves higher f1 and em over the model with tf-idf selector. for example, on the development-full set, with 5 sentences per question on average, the model with dyn achieves 59.5 f1 while the model with tf-idf method achieves 51.9 f1. third, we outperforms the published state-of-the-art on both dataset. 

table 9 shows that minimal outperforms full, achieving the new state-of-the-art by large margin (+11.1 and +11.5 f1 on addsent and addonesent, respectively). 

table 4 lists the results for this simulation experiment in rows 2-5 (s). if unlimited clean feedback was given (rl with direct simulated rewards), improvements of over 5 bleu can be achieved. when limiting the amount of feedback to a log of 800 translations, the improvements over the baseline are only marginal (opl). when replacing the direct reward by the simulated reward estimators from 5, i.e. having unlimited amounts of approximately clean rewards, however, improvements of 1.2 bleu for mse estimators (rl+mse) and 0.8 bleu for pairwise estimators (rl+pw) are found. table 4 shows the results for training with human rewards in rows 6-8: . the improvements for opl are very similar to opl with simulated rewards, both suffering from overfitting. for rl we observe that the msebased reward estimator (rl+mse) leads to significantly higher improvements as a the pairwise reward estimator (rl+pw) ? the same trend as for simulated ratings. finally, the improvement of 1.1 bleu over the baseline showcases that we are able to improve nmt with only a small number of human rewards. 

table 4 shows the overall results on 12 translation directions. we also provide the results from wmt17 winning systems4. although different languages have different linguistic and syntactic structures, our model consistently yields rather competitive results against the transformer on all language pairs in both directions. particularly, on the de¨en translation task, our model achieves a slight improvement of 0.10/0.07 case-sensitive/case-insensitive bleu points over the transformer. the largest performance gap between our model and the transformer occurs on the en→tr translation task, where our model is lower than the transformer by 0.52/0.53 case-sensitive/case-insensitive bleu points. in all, these results suggest that our aan is able to perform comparably to transformer on different language pairs with different scales of training data. 

table 1 shows the phase-wise accuracy of our sequence-to-sequence model. we can see that the hard attention model outperforms the soft attention model in all phases, which shows that the single-pointer attention finds more relevant information than the soft attention on the relatively small dataset. the sequence-to-sequence models perform better than the feedforward model of peng et al. (2018) on shiftorpop and arcbinary, which shows that the whole-sentence context information is important for the prediction of these two phases. on the other hand, the sequence-tosequence models perform worse than the feedforward models on pushindex and arclabel. 

table 4 shows the comparison with other amr parsers. the first three systems are some competitive neural models. we can see that our parser significantly outperforms the sequence-to-action-sequence model of buys and blunsom (2017). our model also outperforms the stack-lstm model by ballesteros and al-onaizan (2017), while their model is evaluated on the previous release of ldc2014t12. we also show the performance of some of the best-performing models. while our hard attention achieves slightly lower performance in comparison with wang et al. (2015a) and wang and xue (2017), it is worth noting that their approaches of using wordnet, semantic role labels and word cluster features are complimentary to ours. 

table 2 compares our spigot method to three baselines. pipelined semantic dependency predictions brings 0.9% absolute improvement in classification accuracy, and spigot outperforms all baselines. in this task ste achieves slightly worse performance than a fixed pre-trained pipeline. 

table 3 compares a pipelined system to one jointly trained using spigot. the second group includes sentences with much lower syntactic parsing accuracy (91.3 vs. 97.4 uas), and spigot further reduces this to 89.6. even though these changes hurt syntactic parsing accuracy, they lead to a 1.1% absolute gain in labeled f1 for semantic parsing. 

table 2 shows the bleu-{3, 4} and meteor scores of different models. our corefnqg outperforms the seq2seq baseline of du et al. (2017) by a large margin. in addition, corefnqg outperforms both seq2seq+copy models significantly, whether or not they have access to the full context. we also show in table 2 the results of the qg models trained on the training set augmented with noisy examples with predicted answer spans. there is a consistent but acceptable drop for each model on this new training set, given the inaccuracy of predicted answer spans. we see that corefnqg still outperforms the baseline models across all metrics. 

table 6 shows the performance of a topperforming system for the squad dataset (document reader (chen et al., 2017)) when applied to the development and test set portions of our generated dataset. we use the squad evaluation scripts, which calculate exact match (em) and f-1 scores.2 . performance of the neural machine reading model is reasonable. 

table 3 shows the results of our system and other state-of-the-art models on the ms-marco test set. we adopt the official evaluation metrics, including rouge-l (lin, 2004) and bleu-1 (papineni et al., 2002). as we can see, for both metrics, our single model outperforms all the other competing models with an evident margin, which is extremely hard considering the near-human peformance. if we ensemble the models trained with different random seeds and hyper-parameters, the results can be further improved and outperform the ensemble model in tan et al. (2017), especially in terms of the bleu-1. 

perplexity on the test partition is detailed in table 2. encouragingly, we see that the incorporation of character encodings and preceding context improves performance substantially, reducing perplexity by almost 10 points from lm to lm??. the inferior performance of lm??-c compared to lm?? demonstrates that our approach of processing context with recurrent networks with selective encoding is more effective than convolutional networks. the full model lm??+pm+rm, which learns stress and rhyme patterns simultaneously, also appears

table 1 summarizes the results for all models on all metrics on the test set and table 2 depicts a text example lexicalized by each model. the first thing to note in the results of the first table is that the baselines in the top two rows performed quite strong on this task, generating more than half of the referring expressions exactly as in the goldstandard. the method based on castro ferreira et al.(2016) performed statistically better than onlynames on all metrics due to its capability, albeit to a limited extent, to predict pronominal references (which onlynames obviously cannot). importantly, the three neuralreg variant models statistically outperformed the two baseline systems. they achieved bleu scores, text and referential accuracies as well as string edit distances in the range of 79.01-79.39, 28%-30%, 73%-74% and 2.25- 2.36, respectively. the results for the different decoding methods for neuralreg were similar, with the neuralreg+catt performing slightly better in terms of the bleu score, text accuracy and string edit distance. the more complex neuralreg+hieratt yielded the lowest results, eventhough the differences with the other two models

table 3 summarizes the results. inspection of the table reveals a clear pattern: all three neural models scored higher than the baselines on all metrics, with especially neuralreg+catt approaching the ratings for the original sentences, although? again ?differences between the neural models were small. concerning the size of the triple sets, we did not find any clear pattern. in comparison with the neural models, neuralreg+catt significantly outperformed the baselines in terms of fluency, whereas the other comparisons between baselines and neural models were not statistically significant. the results for the 3 different decoding methods of neuralreg

table 1 shows the top-1, 3, 5, 10, and 50 candidates retrieval accuracy results on the snap captions dataset. we see that the proposed approach significantly outperforms the baselines which use fixed candidates generation method. this result indicates that the proposed zeroshot model is capable of predicting for unseen entities as well. in addition, when visual context is available (w+c+v), the performance generally improves over the textual models (w+c), showing that visual information can provide additional contexts for disambiguation. the modality attention module also adds performance gain by re-weighting the modalities based on their informativeness. 

to characterize this aspect, we provide table 2 which shows mned performance with varying quality of embeddings as follows: kb embeddings learned from 1m knowledge graph entities (same as in the main experiments), from 10k subset of entities (less triplets to train with in eq.3, hence lower quality), and random embeddings (poorest) while all the other parameters are kept the same. it can be seen that the performance notably drops with lower quality of kb embeddings. when kb embeddings are replaced by random embeddings, the network effectively prevents the contextual zeroshot matching to kb entities and relies only on lexical similarities, achieving the poorest performance. 

we report the performance of several variants of xnet on the validation set in table 1. we also compare them against the lead baseline and pointernet. interestingly, all the variants of xnet significantly outperform lead and pointernet. when the title (title), image captions (caption) and the first sentence (fs) are used separately as additional information, xnet performs best with title as its external information. the performance with title and caption is better than that with fs. we also tried possible combinations of title, caption and fs. all xnet models are superior to the ones without any external information. xnet performs best when title and caption are jointly used as external information (55.4%, 21.8%, 11.8%, 7.5%, and 49.2% for r1, r2, r3, r4, and rl respectively). i. it is better than the the lead baseline by 3.7 points on average and than pointernet by 1.8 points on average, indicating that external information is useful to identify the gist of the document. 

table 4 gives the results for the test sets of newsqa and wikiqa, and the original validation sets of squad and msmarco. our first observation is that xnet outperforms paircnn, supporting our claim that it is beneficial to read the whole document in order to make decisions, instead of only observing each candidate in isolation. secondly, we can observe that isf is indeed a strong baseline that outperforms xnet. indeed, we observe that xnet+ outperforms all baselines except for compaggr. our ensemble model lrxnet can ultimately surpass compaggr on majority of the datasets. using it as a hard constraint, with xnettopk, does not achieve the best result. it is worth noting that the improvement gained by lrxnet over the state-of-the-art follows a pattern. for the squad dataset, the results are comparable (less than 1%). however, the improvement for wikiqa reaches ?3% and then the gap shrinks again for newsqa, with an improvement of ?1%. this could be explained by the fact that each sample of the squad is a paragraph, compared to an article summary for wikiqa, and to an entire article for newsqa. interestingly, our model lags behind compaggr on the msmarco dataset. this can be observed by the fact that xnet and paircnn obtain comparable results. compaggr performs better because comparing each candidate independently is a better strategy. 

we therefore use the same experimental setup as srivastava and sutton (2017) (learning rate, momentum, batch size, and number of epochs) and find the same general patterns as they reported (see table 1 and supplementary material): our model returns more coherent topics than lda, but at the cost of worse perplexity. sage, by contrast, attains very high levels of sparsity, but at the cost of worse perplexity and coherence than lda. as expected, the nvdm produces relatively low perplexity, but very poor coherence, due to its lack of constraints on ƒæ. adding regularization to encourage sparse topics has a similar effect as in sage, leading to worse perplexity and coherence, but it does create sparse topics. interestingly, initializing the encoder with pretrained word2vec embeddings, and not updating them returned a model with the best internal coherence of any model we considered for imdb and yahoo answers, and the second-best for 20 newsgroups. 

table 4 shows span detection results on the development set. we report results for the span-based models at two threshold values tau : tau = 0.5, and tau = tau* maximizing f1. the span-based model significantly improves over the bio model in both precision and recall, although the difference is less pronounced under iou matching. 

table 5 shows the results for question generation on the development set. the sequential model exact match accuracy is significantly higher, while word-level accuracy is roughly comparable, reflecting the fact that the local model learns the slot-level posteriors. 

table 4 presents the results on chinese test set. even though we use the same parameters as for english, our model also outperforms the best reported results by 0.3% (syntax-aware) and 0.6% (syntax-agnostic) in f1 scores. 

table 5 shows the results from our syntax-aware model with lower order argument pruning. compared to the best previous model, our system still yields an increment in recall by more than 1%, leading to improvements in f1 score. 

table 9 reports the performance of existing models7 in term of sem-f1/las ratio on conll2009 english test set. interestingly, even though our system has significantly lower scores than others by 3.8% las in syntactic components, we obtain the highest results both on sem-f1 and

for reference, we include in table 1 baseline results obtained using mle, and our implementation of mle with entropy regularization (mle + lambda h) (pereyra et al., 2017), as well as the raml approach of norouzi et al. (2016) which corresponds to sequence-level smoothing based on the hamming reward and sampling replacements from the full vocabulary (seq, hamming, v). we observe that entropy smoothing is not able to improve performance much over mle for the model without attention, and even deteriorates for the attention model. we also report the performances of token-level smoothing, where the promotion of rare tokens boosted the scores in both attentive and nonattentive models. for sequence-level smoothing, choosing a taskrelevant reward with importance sampling yielded better results than plain hamming distance. moreover, we used the two smoothing schemes (tok-seq) and achieved the best results with cider as a reward for sequence-level smoothing combined with a token-level smoothing that promotes rare tokens improving cider from 93.59 (mle) to 99.92 for the model without attention, and improving from 101.63 to 103.81 with attention. 

table 3 shows evaluation results, where we also include two naive baselines of constant predictions: with the mean and median of the training data. for both datasets, rmse and mae were too sensitive to extreme errors to allow drawing safe conclusions, particularly for the scientific dataset, where both metrics were in the order of 109. mdae can be of some use, as 50% of the errors are absolutely smaller than that. along percentage metrics, mog achieved the best mape in both datasets (18% and 54% better that the second best) and was the only model to perform better than the median baseline for the clinical data. however, it had the worst mdape, which means that mog mainly reduced larger percentage errors. the d-rnn model came third and second in the clinical and scientific datasets, respectively. in the latter it achieved the best mdape, i.e.it was effective at reducing errors for 50% of the numbers. the combination model did not perform better than its constituents. 

table 1 summarizes our results. according to (marelli et al., 2014), we compute three evaluation metrics: pearson r, spearman rho and mean squared error (mse). we compare our attention models against the original tree-lstm (tai et al., 2015), instantiated on both constituency trees and dependency trees. we also compare earlier baselines with our models, and the best results are in bold. it is witnessed that the progressive-attn mechanism combined with constituency tree-lstm is overall the strongest contender, but pa failed to yield any performance gain on dependency tree-lstm in either dataset. 

table 2 summarizes our results where best results are highlighted in bold within each category. for this task, we considered four standard evaluation metrics: accuracy, f1-score, precision and recall. the progressive-attn + constituency tree-lstm model still exhibits the best performance by a small margin, but the progressive-attn mechanism works surprisingly well on the linear bi-lstm. 

table 6 shows that, when testing on a new unseen type, the more similar it is to the seen types, the better performance is achieved. 

table 7 shows the performance. to further demonstrate the effectiveness of zero-shot learning in our framework and its impact in saving human annotation effort, we used the supervised lstm approach for comparison. by contrast, without any annotated mentions on the 23 unseen test event types in its training set, our transfer learning approach achieved performance comparable to that of the lstm, which was trained on 3,000 sentences5 with 500 annotated event mentions. 

the overall comparison results with the baselines are shown in table 2 with average f1 scores and standard deviations over three random splits. clearly, the results for aspect terms (as) transfer are much lower than opinion terms (op) transfer, which indicate that the aspect terms are usually quite different across domains, whereas the opinion terms could be more common and similar. on this behalf, our proposed model shows clear advantage over other baselines for this more difficult transfer problem. specifically, we achieve 6.77%, 5.88%, 10.55% improvement over the bestperforming baselines for aspect extraction in r¨l,l¨d and d¨l,  respectively. 

the results are reported in table 4 for the best performing generative and ranking models, in both the no persona and self persona categories, 100 dialogues each. we also evaluate the scores of human performance by replacing the chatbot with a human (another turker). finally, and importantly, we compare our models trained on persona-chat with chit-chat models trained with the twitter and opensubtitles datasets (2009 and 2018 versions) instead, following vinyals and le (2015). firstly, we see a difference in fluency, engagingness and consistency between all personachat models and the models trained on opensubtitles and twitter. 

table 3 illustrates the impact of low-rank multimodal fusion on the training and testing speeds compared with tfn model. on an nvidia quadro k4200 gpu, lmf trains with an average frequency of 1134.82 ips (data point inferences per second) while the tfn model trains at an average of 340.74 ips. 

table 1 shows the performance of variants of our approach for the task. this is quite surprising, since our sequential-cg model in this case is trained on bag-of-lemma representations, and only needs sentence segmentation, tokenization and lemmatization for preprocessing. on the other hand, approaches such as narrative-chains require parsing and eventrecognition, while approaches such as gensim require learning word embeddings on large text corpora for training. further, we note that predicting the ending without normalizing for the probability of the words in the ending results in significantly weaker performance, as expected. we train another variant of sequential-cg with the sentence-level sentiment annotation (from stanford corenlp) also added as a feature. this does not improve performance, consistent with findings in mostafazadeh et al. (2016). we also experiment with a variant where we perform brown clustering (brown et al., 1992) of words in the unlabeled stories (k = 500 clusters), and include cluster-annotations as features for training the method. doing this explicitly incorporates lexical similarity into the model, leading to a small improvement in performance. finally, a mixture model consisting of the sequential-cg and a unigram language model leads to a further improvement in performance. 

the results from table 4 reveal that average attention combination performs best among all the decoding strategies on rdd newspapers and tcp books datasets. it reduces the cer of single input decoding by 41.5% for ocrfd lines in rdd newspapers and 9.76% for tcp books. the comparison between two hierarchical attention combination strategies shows that averaging evidence from each input works better than a weighted summation mechanism. flat attention combination, which merges all the inputs into a long sequence when computing the strength of each encoder hidden state, obtains the worst performance in terms of both cer and wer. 

table 5 presents the results for our model trained in different training settings as well as the baseline language model reranking (lmr) and majority vote methods. multiple input decoding performs better than single input decoding for every training setting, and the model trained in supervised mode with multi-input decoding achieves the best performance. the majority vote baseline, which works only on more than two inputs, performs worst on both the tcp books and rdd newspapers. our proposed unsupervised framework seq2seq-noisy and seq2seqboots achieves performance comparable with the supervised model via multi-input decoding on the rdd newspaper dataset. the performance of seq2seq-noisy is worse on the tcp books than the rdd newspapers, since those old books contain the character long s 6, which is formerly used where s occurred in the middle or at the beginning of a word. nonetheless, by removing the factor of long s, i.e., replacing the long s in the ground truth with f, seq2seq-noisy could achieve a cer of 0.062 for single-input decoding and 0.058 for multi-input decoding on the tcp books. both seq2seq-syn and seq2seq-boots work better on the rdd newspapers than the tcp books dataset. 

we compare our model with the baselines using perplexity metric?lower perplexity means the better prediction. table 1 summarizes the result. the 3rd row shows that adding type as a simple feature does not guarantee a significant performance improvement while our proposed method significantly outperforms both baselines and achieves 52.2% improvement with respect to baseline in terms of perplexity. 

table 2 shows that adding type as simple features does not guarantee a significant performance improvement while our proposed method significantly outperforms both forward and backward lstm baselines. our approach with backward lstm has 40.3% better perplexity than original backward lstm and forward has 63.14% lower (i.e., better) perplexity than original forward lstm. with respect to slp-core performance, our model is 22.06% better in perplexity. 

table 7 analyzes the impact of newcomer friendliness. opposite from what is done in section 5.2.2,we only evaluate on testing examples where at least a ground-truth paper is a newcomer. the table shows that newcomer friendly approaches are superior to unfriendly ones. 

table 2 summarizes the performance. surprisingly, semaxis - the simplest approach - outperforms others on both standard english and twitter datasets across all measures. 

table 1 shows the results of the first experiment. we can see that taxi has the lowest f1 a while hypenet performs the worst in f1 e. both taxi and hypenet f1 a and f1 e are lower than 30. hypenet+mst outperforms hypenet in both f1 a and f1 e, because it considers the global taxonomy structure, although the two phases are performed independently. taxorl (re) uses exactly the same input as hypenet+mst and yet achieves significantly better performance, which demonstrates the superiority of combining the phases of hypernymy detection and hypernymy organization. also, we found that presuming a shared root embedding for all taxonomies can be inappropriate if they are from different domains, which explains why taxorl (nr) performs better than taxorl (re). finally, after we add the frequency and generality features (taxorl (nr) + fg), our approach outperforms bansal et al. (2014), even if a much smaller corpus is used. 

table 2 shows the experimental results of lexicon term sentiment classification. our dse method can achieve competitive performance among all the methods. compared with sswe, our dse is still competitive because both of them consider the sentiment information in the embeddings. our dse model outperforms other methods which do not consider sentiments such as yang, embeddingcat and embeddingall. 

table 2 shows the results reported by kiela et al. (2015) on the bergsma500 dataset, along with results using our image crawl method (section 3.2) on bergsma500fs vocabulary. on all five languages, our dataset performs better than that of kiela et al. (2015). we also evaluate the identical model on our full data set, which contains 8,500 words, covering all parts of speech and the full range of concreteness ratings. the top-1 accuracy of the model is 23% on our more realistic and challenging data set, versus

table 2 contains the results of this task for the large treebanks. because dozat et al. (2017) won the challenge for the majority of the languages, we first compare our results with the performance of their system. our model outperforms dozat et al. (2017) in 32 of the 54 treebanks with 13 ties. our model tends to produce better results, especially for morphologically rich languages (e.g. slaviclanguages), whereas dozat et al (2017) showed higher performance in 10 languages in particular english, greek, brazilian portuguese and estonian. 

table 3 shows the results of our model in comparison to the results reported in state-ofthe-art literature. our model significantly outperforms these systems, with an absolute difference of 0.32% in accuracy, which corresponds to a rrie of 12%. 

table 5 shows that separately optimized models are significantly more accurate on average than jointly optimized models. 

table 8 reports, for a few morphological rich languages, the part-of-speech tagging performance of different strategies to gather the characters when creating initial word encodings. the table also contains a column with results for our reimplementation of dozat et al.(2017). the performance is quite different per language. e.g., for latin, the outputs of the forward and backward lstms of the last character scored highest. 

table 4 shows the developmentset performance of our models as compared with baseline systems. mst considers non-projective structures, and thus enjoys a theoretical advantage over projective mh 3, especially for the most non-projective languages. we also observe that mh 4 recovers more short dependencies than 1ec, while 1ec is better at longer-distance ones. in comparison to mh 4-two, the richer feature representation of mh 4-hybrid helps in all our languages. interestingly, mh 4 and mh 3 react differently to switching from global to greedy models. mh 4 covers more structures than mh 3, and is naturally more capable in the global case, even when the feature functions are the same (mh 4-two). however, its greedy version is outperformed by mh 3. 

table 3 shows overall performances of the two sequential models on development data. from the results, we can clearly see that the introduction of neural structure pushes up the scores exceptionally. the reason is that our lstm-crf model not only benefits from the linear weighted combination of local characteristics like ordinary crf models, but also has the ability to integrate more contextual information, especially long-distance information. it confirms lstm-based modelsfgreat superiority in sequence labeling problems. further more, we find that the difference among the four kinds of representations is not so obvious. the most performing one with lstm-crf model is interspace, but the advantage is narrow. pre3 uses a larger window length to incorporate richer contextual tokens, but at the same time, the searching space for decoding grows larger. in general, experiments with pos tags show higher scores as more syntactic clues are incorporated. 

table 6 presents detailed results of the in-parsing models on test data. compared with the stateof-the-art, the first-order model performs a little worse while the second-order model achieves a remarkable score. the four bold numbers in the table intuitively elicits the conclusion that integrating an empty edge and its sibling overt edges is necessary to boost the performance. comparing results regarding ec types, we can find that op and t benefit most from the parsing information, the f1 score increasing by about ten points, more markedly than other types. 

table 5 shows the empirical results. the first-row, gsingleh is the baseline targetside parser trained on the train data. the second-row gsingle (hetero)h refers to the source-side heterogeneous parser trained on train-hit and evaluated on the target-side test data. since the similarity between the two guidelines is high, as discussed in section 2.2, the source-side parser achieves even higher uas by 0.21 (76.20 ? 75.99) than the baseline target-side parser trained on the small-scale train data. in the third row, gmulti-taskh is the targetside parser trained on train & train-hit with the multi-task learning approach. it significantly outperforms the baseline parser by 4.30 (74.51 ? 70.21) in las. in the fourth row, gsingle (large)h is the basic parser trained on the large-scale converted train-hit (homogeneous). we can see that the single parser trained on the converted data significantly outperforms the parser in the multi-task learning approach by 1.32 (75.83 ?74.51) in las. 

we first describe the results of the different models when evaluated against the expert annotations we collect on 500 samples (4). since the annotators had a low agreement on a single best, we evaluate against the union of the best annotations (b1 ∪ b2 in table 2) and against the intersection of the valid annotations (v1 ∩ v2 in table 2). among non-neural baselines, we find that the bag-of-ngrams baseline performs slightly better than random but worse than all the other models. the community qa baseline, on the other hand, performs better than the neural baseline (neural (p, q)), both of which are trained without using the answers. the neural baselines with answers (neural(p, q, a) and neural(p, a)) outperform the neural baseline without answers (neural(p, q)), showing that answer helps in selecting the right question. more importantly, evpi outperforms the neural (p, q, a) baseline across most metrics. the last column in table 2 shows the results when evaluated against the original question paired with the post. the bag-of-ngrams baseline performs similar to random, unlike when evaluated against human judgments. the community qa baseline again outperforms neural(p, q) model and comes very close to the neural (p, a) model. as before, the neural baselines that make use of the answer outperform the one that does not use the answer and the evpi model performs significantly better than neural(p, q, a). 

as shown in table 1, with an equal number of parameters, the r-rntn with f mapping outperforms the s-rnn with a bigger hidden layer. although m-rnns have been successfully employed in character-level language models with small vocabularies, they are seldom used in wordlevel models. the poor results shown in table 1 could explain why.2. for fixed hidden layer sizes, r-rntns yield significant improvements to s-rnns, grus, and lstms, confirming the advantages of distinct representations. 

table 1 shows the results for the trustpilot dataset. observe that the disparity for the baseline tagger accuracy (the delta column), for age is larger than for sex, consistent with the results of hovy and sogaard (2015). our adv method leads to a sizeable reduction in the difference in accuracy across both sex and age, showing our model is capturing the bias signal less and more robust to the tagging task. moreover, our method leads to a substantial improvement in accuracy across all the test cases. 

table 2 shows the results for the aave heldout domain. note that adv also significantly outperforms the baseline across the three heldout domains. 

table 4 presents results on the second dataset for the best models identified on the first dataset. the lda-frames yielded the best results with our approach performing comparably in terms of the f1-score. we attribute the low performance of the triframes method based on cw clustering to its hard partitioning output, whereas the evaluation dataset contains fuzzy clusters. 

table 2 shows performance as a function of the number of rnn units with a fixed unit size. we find good performance across the board (there is no catastrophic collapse in results) however when using 16 units we do outperform other models substantially. 

table 3 shows that typedm and typecomplex dominate across all data sets. e by itself is understandably weak, and dm+e does not lift it much. each typed model improves upon the corresponding base model on all measures, underscoring the value of type compatibility scores.3 . 

table 1 illustrates the performance of our proin comparison with sptree sysposed model tem miwa and bansal (2016) on ace 2005. we use the same data split with sptree to compare with their model. the baseline corresponds to a model that classifies relations by using only the representations of entities in a target pair. as it can be observed from the table, the baseline model achieves the lowest f1 score between the proposed models. by incorporating attention we can further improve the performance by 1.3 percent point (pp). the addition of 2-length walks further improves performance (0.9 pp). the best results among the proposed models are achieved for maximum 4-length walks. by using up-to 8-length walks the performance drops almost by 2 pp. 

finally, we show the performance of the proposed model as a function of the number of entities in a sentence. results in table 2 reveal that for multi-pair sentences the model performs significantly better compared to the no-walks models, proving the effectiveness of the method. additionally, it is observed that for more entity pairs, longer walks seem to be required. however, very long walks result to reduced performance (l = 8). 

the performances of the seed selection methods are presented in table 2. for the hits-based and hits+k-means-based methods, we display the p@50 with three types of graph representation as shown in section 4.2. we use random seed selection as the baseline for comparison. as table 2 shows, the random method achieved a precision of 0.75. the hits-based  method p@50 when using graph1 and graph3 are confirmed to be better than when using graph2. the kmeans-based seed selection method provides the best average p@50 with a performance of 0.96. the hits+k-means-based method performs better than using only the hits strategy, while the lsa-based and nmf-based methods have a comparable performance. 

table 3 shows the ranking results using mean average precision (map) and precision at k as the metrics. accumulative scores (f1 and f3) generally do better. 

table 1 shows results on germeval using the official metric (metric 1) for the best performing systems. within the challenge, the exb (hanig et al., 2015) ensemble classifier achieved the best result with an f1 score of 76.38, followed by the rnn-based method from ukp (reimers et al., 2014) with 75.09. germaner achieves high precision, but cannot compete in terms of recall. our bilstm with wikipedia word embeddings, scores highest (79.99) and outperforms the shared task winner exb significantly, based on a bootstrap

the results in table 5 show significant improvements for the conll dataset but performance drops for germeval. performance on lft increases from 69.62 to 74.33 and on onb from 73.31 to 78.56. 

table 2 shows the average precision, recall, f1-measure, and auc. the classifiers trained on the linguistic features, while performing near the baselines on the first three measures, substantially outperform the baselines on auc, with all three yielding values over 0.8. 

table 4 lists the performance of them on ca_translated and ca8 datasets under different configurations. we can observe that on ca8 dataset, sgns representations perform better in analogical reasoning of morphological relations and ppmi representations show great advantages in semantic relations. however, table 4 shows that there is only a slight increase on ca_translated dataset with ngram features, and the accuracies in most cases decrease after integrating character features. in contrast, on ca8 dataset, the introduction of ngram and character features brings significant and consistent improvements on almost all the categories. sgns model integrating with character features even doubles the accuracy in morphological questions. 

table 5 shows that accuracies increase with the growth in corpus size, e.g .baidubaike (an online chinese encyclopedia) has a clear advantage over wikipedia. we can observe that vectors trained on news data are beneficial to geography relations, especially on people’s daily which has a focus on political news. another example is zhihu qa, an online questionanswering corpus which contains more informal data than others. with the largest size and varied domains, combinationcorpus performs much better than others in both morphological and semantic relations. 

our plain bpe baseline (table 4) outperforms the current best system on wat ja-en, an 8-model ensemble (morishita et al., 2017). 

as shown in table 2, our model significantly out-performs he et al. (2017), but falls short of tan et al. (2018). 

table 2 report evaluation results on the three data sets where ioi-global and ioi-local represent models learned with objective (17) and objective (18) respectively. we can see that both ioi-local and ioi-global outperform the best performing baseline, and improvements from ioi-local on all metrics and from ioi-global on a few metrics are statistically significant (t-test with p-value < 0.05). ioi-local is consistently better than ioi-global over all metrics on all the three data sets, demonstrating that directly supervising each block in learning can lead to a more optimal deep structure than optimizing the final matching model. 

as it is shown in table 1, we conduct an ablation study on the testset of the ubuntu corpus, where we aim to examine the effect of each part in our proposed model. firstly, we verify the effectiveness of dual multi-turn encoder by comparing baseline and dme in table 1. thanks to dual multi-turn en-coder,  dme  achieves  0.725  at r100@10 which is  0.366  better  than  the  baseline  (lowe  et  al.,2015b). the stm block makes a 10.54% improvement at r100@1. next,  we  replace  gru  with  transformer  in stm. the result in table 1 shows that using self-attention only may not be enough for representation. it  succeed  the highest results and outperforms other methods by a significant margin. 

the plain depccg already achieves higher scores than these methods, and boosts when combined with elmo (improvement of 2.73 points in terms of f1). fine-tuning the parser on genia1000 results in a mixed result, with slightly lower scores. finally, by fine-tuning on the genia ccgbank, we observe another improvement, resulting in the highest 86.52 f1 score. 

table 4 compares the performance of depccg fine-tuned on the questionbank, along with other baselines. contrary to our expectation, the plain depccg retrained on questions data performs the best, with neither elmo nor the proposed method taking any effect. 

table 1 reports the results of the proposed system in comparison to previous work. as it can be seen, our full system obtains the best published results in all cases, outperforming the previous state-of-the-art by 5-7 bleu points in all datasets and translation directions. 

so as to put our results into perspective, table 3 reports the results of different supervised systems in the same wmt 2014 test set. as it can be seen, our unsupervised system outperforms the wmt 2014 shared task winner in english-to-german, and is around 2 bleu points behind it in the other translation directions. this shows that unsupervised machine translation is already competitive with the state-of-the-art in supervised machine translation in 2014. 

table 4 shows results for korean - english, using the same configurations (1, 2 and 8) as for german - english. our results confirm that the techniques we apply are successful across datasets, and result in stronger systems than previously reported on this dataset, achieving 10.37 bleu as compared to 5.97 bleu reported by gu et al. (2018b). 

table 3 shows that the end-to-end models outper-form  the  existing  model. in particular, our proposed n-gram attention model achieves the best results in terms of precision, recall, and f1 score. our proposed model outperforms the best existing model (minie) by 33.39% and 34.78% in terms of f1 score on the wiki and geo test dataset re-spectively. as expected, the combination of the existing models with aida achieves higher f1 scores than the combination with neuralel as aida achieves a higher precision than neuralel. in this setup, our proposed model achieves 86.34% and 79.11%, while cnn achieves 81.92% and 75.82% in precision over the wiki and geo test datasets, respectively. our  proposed  n-gram  attention  model  outper-forms  the  end-to-end  models  by 15.51% and 8.38% in terms of f1 score on the wiki and geo test datasets, respectively. the transformer model also  only  yields  similar  performance  to  that  of the single attention model,  which is worse than ours.  these results indicate that our model captures  multi-word  entity  name  (in  both  datasets, 82.9% of the entities have multi-word entity name) in the input sentence better than the other models. table 3 also shows that the pre-trained embeddings improve the performance of the model in all measures. our triple classifier combined with the modified beam search boost the performance of the model. 

 table 3 reports the offline held-out set evaluations for the early stopping points selected on the dev set for all feedback modes. all models notably improve over the baseline, only using full feedback leads to the overall best model on iwslt (+0.6 bleu / -0.6 ter), but costs a massive amounts of edits (417k characters). self regulating models still achieve improvements of 0.4-0.5 bleu/ter with costs reduced up to a factor of 23. self-supervision works surprisingly well, which makes it attractive for cheap but effective unsupervised domain adaptation. it has to be noted that both weak and self-supervision worked only  well  when  targets  were  pre-computed  with the baseline model and held fixed during training. 

the right-hand side of table 1 shows the performance of our svm and the two neural methods. the results indicate that the svm setup is wellsuited for the difficulty prediction task and that it successfully generalizes to new data. 

table 4 provide the classification results of our approach when compared to those manual annotations, considering all three levels of the taxonomy. this exercise indicated that in 32 cases out of 989 operational incidents under consideration for the level 1 classification, the machine generated category were more relevant (hence correct) than those identified by the operational team. 

table 3 presents the results of human evaluation on selected methods. again, we see that the style embedding model (fu et al., 2018) is ineffective as it has a very low transfer strength, and that our method outperforms other baselines in all aspects. 

we investigate different mechanisms of integrating cross-sentence context. table 4 shows the average single model results of our sentence-level baseline compared to two different strategies of integrating cross-sentence context. the first two variants perform comparably to our sentence-level baseline and shows no notable gains from using cross-sentence context. when the  gating  mechanism  is  added,  results  improve substantially. using the gating mechanism is crucial in our crosentmodel, as it has the ability to selectively pass information through. this shows that  properly  modeling  cross-sentence  context  is essential to improve overall performance. 

table 5 shows that in terms of the number of parameters, tfn is around 511 times larger than our hffn, even under the situation where we adopt a more complex module after tensor fusion, demonstrating the high efficiency of hffn. compared to bc-lstm, hffn has about 166 times fewer parameters and the flops of hffn is over 79 times fewer than that of bclstm. moreover, bc-lstm is over 6 times faster than tfn in time complexity measured by flops and the number of parameters is over 3 times smaller. these results demonstrate that outer product applied in tfn results in heavy computational complexity and a substantial number of parameters  compared with  other methods  such as  bc-lstm, while hffn can avoid these two problems and is even more efficient than other approaches adopting low-complexity fusion methods. 

table 4 shows the results of different model variants. we observe that +message passing-a and +message passing-d contribute to the performance gains the most, which demonstrates the effectiveness of the proposed message passing mechanism. we  also  observe  that  simply  adding  document-level tasks (+ds/dd) with parameter sharing only marginally improves the performance of imn ?d. however, +message passing-d is still help-ful with considerable performance gains, showingthat aspect-level tasks can benefit from knowingpredictions of the relevant document-level tasks. 

in addition, when compare the results in table 7 and table 3, we observe that imn -d and imn consistently yield better f1-i scores on all datasets in table 3, when opinion term extraction is also considered. 

table 3 shows the results of aspect term extraction only. comparing with it, doer achieves new state-of-the-art scores. as the table shows, doer achieves better performance than doer*, which indicates the interaction between ate and asc can yield better performance for ate than only conduct a single task. 

in table 5 we compare the results of ims when trained on different corpora. as one can see, onesec achieves the best results on all when compared to automatic and semi-automatic approaches, and ranks second only with respect to semcor. interestingly enough, onesec beats its manual competitor on semeval-2013 by 1 point and on semeval-2015 by 4.7 points, an impressive result considering that onesec does not involve any human intervention during the generation of the corpus. in table 5 we also report the statistical significance between onesec and its competitors on the all dataset by juxtaposing a † symbol next to the score. moreover, our approach outperforms train-o-matic, our direct competitor, on all the datasets, with the highest increment of 3.7 points on semeval-2007, while scoring almost 2 points higher than tom overall. 

as table 7 shows, the neural predictor significantly outperforms heuristic-based baseline in predicting supporting evidence, which indicates the potential of re models in joint relation and supporting evidence prediction. 

table 5 shows results on atis and our split version of snips. we now have four tasks: atis, snips-location, snips-music, and snips-creative. for the models introduced in this paper, we define two task groups: atis and snips-location as one group, and snips-music and snips-creative as another. our models, which use these groups, generally outperform the other mtl models (parallel[univ] and parallel[univ+task]); especially the serial mtl architectures perform well. 

table 6 shows the results of the single-domain model and the mtl models on the alexa dataset. serial+highway+swap yields the best mean intent accuracy. parallel+univ+group+task and serial+highway show statistically indistinguishable results. for slot filling, all mtl architectures achieve competitive results on mean slot f1. 

cross-domain evaluation: table 3 demonstrates that the dialkg walker model can generalize to multiple domains better than the baseline approaches (train: movie & test: book / train: movie & test: music). this result indicates that our method also allows for zeroshot pruning by relations based on their proximity in the kg embeddings space, thus effective in cross-domain cases as well. 

the rightmost column of table 4 shows the results of our gear frameworks with different sentence selection thresholds. we choose the model with threshold 10^-3, which has the highest label accuracy, as our final model. when the threshold increases from 0 to 10^-3, the label accuracy increases due to less noisy information. however, when the threshold increases from 10^-3 to 10^-1, the label accuracy decreases because informative evidence is filtered out, and the model can not obtain sufficient evidence to make the right inference. 

table 7 presents the evaluations of the full pipeline. we find the test fever score of bert fine-tuning systems outperform other shared task models by nearly 1%. furthermore, our full pipeline outperforms the bert-concat baseline by 1.46% and achieves significant improvements. 

next, the transformer encoder was compared against the lstm encoder, using pre-training in both cases. the results in table 4 show that the lstm-encoder outperforms the transformer-encoder consistently in this task, when both are pre-trained. therefore, for the rest of the experiments, we only report results using the lstm-encoder. 

table 5 shows the classification performance obtained when using each feature set at the time. we measure the performance of the classifiers in terms of accuracy and f-score, which provide overall and class-specific performance assessments. compared to the majority baseline, all the feature sets demonstrate a clear improvement in the classification of counseling quality. among all feature sets, n-grams attain the best performance, followed by discourse topics and the semantic feature sets. furthermore, the combination of all the features sets achieve the best accuracy values. 

the results presented in table 1 show that the seq system outperforms the camb system on all three genres on the task of binary complex word identification. the largest performance increase for words is on the wikipedia test set (+3.60%). table 1 also shows that on the combined set of words and phrases (words+phrases) the two systems achieve similar results: the seq model beats the camb model only marginally, with the largest difference of +1.05% on the wikinews data. using the dataset statistics, we estimate that camb system achieves precision of 0.64. the seq model outperforms the camb system, achieving precision of 0.71. 

table 3 shows the results of all nine models under the semeval setting, using the matchm evaluation metric. similar to the results we showed in table 2, m3 and m4 both perform competitively and outperform the other models. 

therefore, we try to fix the embedding of the encoder and decoder on the basis of the original baseline system (baseline-fix). table 1 shows that the performance of the baseline-fix system is quite similar to that of the original baseline system. in other words, baseline-fix prevents the degradation of ubwe accuracy; however, the fixed embedding also prevents ubwe from further improving unmt training. 

table 2 presents the results. plain transfer learning already gives a boost but is still far from a satisfying quality, especially for basque®-english and azerbaijani®english. on top of that, each of our three techniques offers clear, incremental improvements in all child language pairs with a maximum of 5.1% bleu in total. 

table 5 estimates how large the vocabulary should be for the language-switching side in nmt transfer. the best results are with 10k or 20k of bpe merges, which shows that the source vocabulary should be reasonably small to maximize the transfer performance. 

glore++ improves its best f1-score from 42.7% to 45.2%, slightly outperforming the previous state-of-theart (glore, 44.7%). table 1 shows the manual evaluation results. both glore+ and glore++ get improvements over glore. glore++ obtains the best results for top 700, 900 and 1000 predictions. 

from table 1, we can see both bio tag embeddings and multi-task learning can improve the performance of the baseline model. baseline+tag can outperform the state-ofthe-art models on both the chinese and english corpus. compared to the baseline model, bio tag embeddings lead to an absolute increase of about 10% in f1-score, which indicates that bio tag embeddings are very effective. multi-task learning can yield further improvement in addition to bio tag embeddings: baseline+mtl+tag achieves the highest f1-score on both corpora. 

table 1 presents the precision, recall, and f1 score of noveltagging, multidecoder, and graphrel for both the nyt and webnlg datasets. for the nyt dataset, we see that graphrel1-hop outperforms noveltagging by 18.0%, onedecoder by 4.0%, and multidecoder by 1.3% in terms of f1.  as it acquires both sequential and regional dependency word features, graphrel1-hop performs better on both precision and recall, resulting in a higher f1 score.  with relation weighted gcn in 2nd-phase, graphrel2p, which considers interaction between name entities and relations, further surpasses multidecoder by 3.2% and yields a 1.9% improvement in comparison with graphrel1p. 

4.3 pattern-based diagnostic results . besides for improving the extraction performance, diag-nre can interpret different noise effects caused by ds via refined patterns, as table 3 shows. next, we elaborate these diagnostic results and the corresponding performance degradation of nre models from two perspectives: false negatives (fn) and false positives (fp). 

results on table 3 shows dbp15k and dwy100k. in general, mugnn significantly outperforms all baselines regarding all metrics, mainly because it reconciles the structural differences by two different schemes for kg completion and pruning, which are thus well modeled in multi-channel gnn. 

table 3 is a comparison of lstm and bert models using the gs-ec attack. it shows that the distance in embeddings space of bert can better reflect semantic similarity and contribute to more natural adversarial examples. 

both these recent methods perform extremely well on multiple natural language processing tasks. both our model and gensen fail to beat the rntn model for the sst-2 task. we see an improvement in accuracy when combining both methods' embeddings, surpassing every model in the sst paper, while being close to bertbase's performance. 

table 3 shows the results. multitask learning with msa consistently outperforms the models that use egy data only. the accuracy almost doubles in the 2k model. we also notice that the accuracy gap increases as the egy training dataset size decreases, highlighting the importance of joint modeling with msa in low-resource da settings. the adversarial adaptation results in the learning curve further show a significant increase in accuracy with decreasing training data size, compared to the multitask learning results. the model seems to be facilitating more efficient knowledgetransfer, especially for the lower-resource egy experiments. we can also observe that for the extreme low-resource setting, we can double the accuracy through adversarial multitask learning, achieving about 58% relative error reduction. 

5.3 test set evaluation table 6 contains the results for en-nl for the entire test set (3207 sentences). the dedicated nfr + nmt backoff approach outperforms all baseline systems, scoring +3.19 bleu, -3.6 ter and +1.87 meteor points compared to the best baseline (tm-smt). compared to the nmt baseline, the difference is 7.46 bleu points. the best unified nfr system (unified f3) scores only slightly worse than the approach with a dedicated nfr system and nmt backoff. both nfr systems score significantly higher than the best baseline in terms of bleu (p < 0.001). we note that the baseline smt outperforms the baseline nmt, which in turn obtains better scores than google translate on this data set. 

table 4 shows the results. similarly, the proposed attentive interactor model (without the diversity loss) outperforms all the baselines. moreover, the diversity loss further improves the performance. note that the improvement of our model on this dataset is more significant than that on the vid-sentence dataset. the reason might be that the upper bound performance of the personsentence is much higher than that of the vidsentence (77.9 for person-sentence versus 47.6 for vid-sentence on average). 

table 1 shows that our baseline models achieve strong results when compared with glue benchmark baselines (wang et al., 2018). enas models: next, table 1 shows that our enas models (for all three tasks qnli, rte, wnli) perform better or equal than the nonarchitecture search based models. cas models: next, we apply our continual architecture search (cas) approach on qnli, rte, and wnli, where we sequentially allow the model to learn qnli, rte, and wnli (in the order of decreasing dataset size, following standard transfer setup practice) and the results are as shown in table 1. we observe that even though we learn the models sequentially, we are able to maintain performance on the previously-learned qnli task in step-2 (74.1 vs. 74.2 on validation set which is statistically equal, and 73.6 vs. 73.8 on test). 

next, we consider the impact of using different combinations of f and y. table 3 shows the performance of difference configurations. overall, f + y gives excellent performance. interestingly, y on its own is only a little worse than only f, showing that target labels y are more important for learning than the domain d. 

table 2 presents the human evaluation results, from which we can draw similar conclusions. it is obvious that our approach can outperform the baselines by a large margin, especially in terms of diversity and topic-consistency. for example, the proposed model achieves improvements of 15.33% diversity score and 12.28% consistency score over the best baseline. 

as is shown in table 3, compared to the removal of the adversarial training, the model exhibits larger degradation in terms of novelty and diversity when the memory mechanism is removed. this shows that with the help of external commonsense knowledge, the source information can be enriched, leading to the outputs that are more novel and diverse. 

table 7 (top) shows results on the rotowire development set for our dynamic entity memory model (ent), the best system of wiseman et al. (2017) (ws-2017) which is an encoder-decoder model with conditional copy, the template generator (templ), our implementation of encoder-decoder model with conditional copy (ed+cc), and ncp+cc (puduppully et al., 2019). we see that ent achieves scores comparable to ncp+cc, but performs better on the metrics of rg precision, cs precision, and co. table 7 (bottom) also presents our results on mlb. ent achieves highest bleu amongst all models and highest cs recall and rg count amongst neural models. 

table 1 summarizes the results of these models. based on the selected key facts, our models achieve the scores of 20.09 bleu, 6.5130 nist, and 18.31 rouge under the vanilla seq2seq framework, and 27.34 bleu, 6.8763 nist, and 19.30 rouge under the transformer framework, which significantly outperform all the baseline models in terms of all metrics. furthermore, it shows that the implementation with the transformer can obtain higher scores than that with the vanilla seq2seq. 

table 2 summarizes the results: both gcn cited text spans and talksumm-only models, are not able to obtain better performance than abstract8. however, for the hybrid approach, where the abstract is augmented with sentences from the summaries emitted by the models, our talksumm-hybrid outperforms both gcn hybrid 2 and abstract. importantly, our model, trained on automaticallygenerated summaries, performs on par with models trained over scisummnet, in which training data was created manually. 

table 2 shows the rouge scores of our models and the baselines for the evaluation sets. with regards to toys & games and sports & outdoors, our full model (strsum + discourserank) achieves the best rouge scores among the unsupervised approaches. 

in table 2 we present intance selection results for the cnn/dm, xsum, and duc-04 datasets. our method builds representations for instances using either bert or vsm (§3.1). to ensure a thorough comparison, we experiment with selecting a mixed set of singletons and pairs (“singpairmix”) as well as selecting singletons only (“singonly”). on the cnn/dm and xsum datasets, we observe that selecting a mixed set of singletons and pairs based on bert representations (bert+singpairmix) demonstrates the most competitive results. it outperforms a number of strong baselines when evaluated on a full set of ground-truth sentences. the method also performs superiorly on identifying secondary sentences. for example, it increases recall scores for identifying secondary sentences from 33.8% to 69.8% (cnn/dm) and from 16.7% to 65.3% (xsum). further, we observe that identifying summaryworthy singletons and pairs from multi-document inputs (duc-04) appears to be more challenging than that of single-document inputs (xsum and cnn/dm). we find that the bert model performs consistently on identifying secondary sentences, and vsm yields considerable performance gain on selecting primary sentences. both bert and vsm models are trained on the cnn/dm dataset and applied to duc-04 as the latter data are only used for testing. 

table 4 reports f1 scores of rouge-1, 2, and l (lin and hovy, 2003) for all models. for bigpatent, almost all models outperform the lead-3 baseline due to the more uniform distribution of salient content in bigpatent’s input articles. among extractive models, textrank and lexrank outperform rnn-ext rl which was trained on only the first 400 words of the input, again suggesting the need for neural models to efficiently handle longer input. finally, sentrewriting, a reinforcement learning model with rouge as reward, achieves the best performance on bigpatent. 

meteor metric (n-gram overlap with synonyms) was used for automatic evaluation. the novel ngram rate9 (e.t., nn-1, nn-2, nn-3, and nn-4) was also computed to investigate the number of novel words that could be introduced by the models. table 2 and table 3 present the results and below are our observations: (i) keyphrase word graph approach (#2) is a strong baseline according to the meteor metric. in comparison, the proposed rewriter (#5) yields comparable result on the meteor metric for the giga-msc dataset but lower result for the cornell dataset. we speculate that it may be due to the difference in the ground-truth compression. 8.6% of novel unigrams exist in the ground-truth compression of the giga-msc dataset, while only 5.2% of novel unigrams exist in that of the cornell dataset, (ii) hard para.(#3), seq2seq (#4), and our rewriter (#5) significantly increase the number of novel n-grams, and the proposed rewriter (#5) seemed to be a better trade-off between the information coverage (measured by meteor) and the introduction of novel n-grams across all methods, (iii) on comparing with seq2seq (#4) and our rewriter (#5), we found that adding pseudo data helps to decrease the novel words rate and increase the meteor score on both datasets. 

table 4 shows the average ratings for informativeness and readability. from that, we found that our rewriter (rwt) significantly improved the grammaticality of compression in comparison with the keyphrase word graph approach, implying that the pseudo data may contribute to the language modeling of the decoder, thereby improving the grammaticality. 

table 2 shows the performance of our model and competing models on the leaderboard. our ensemble model of six training runs, where each model was trained with the two answer styles, achieved state-of-theart performance on both tasks in terms of rougel. in particular, for the nlg task, our single model outperformed competing models in terms of both rouge-l and bleu-1. 

table 5 shows that our single model, trained with two styles and controlled with the nqa style, pushed forward the state-of-the-art by a significant margin. the evaluation scores of the model controlled with the nlg style were low because the two styles are different. also, our model without multi-style learning (trained with only the nqa style) outperformed the baselines in terms of rouge-l. this indicates that our model architecture itself is powerful for natural language understanding in rc. 

we also report the performance on documentlevel squad in table 4 to assess our approach in single-document setting. we find our approach adapts well: the best model achieves 87.2 f1. 

table 5 shows the evidence extraction performance in the distractor setting. our model improves both precision and recall, and the improvement in precision is larger. table 5 also shows the correlation of our model about the number of evidence sentences is higher than that of the baseline. 

table 6 shows the overall results for different methods in different languages. there is a large gap between the performance of english and that of other target languages, which implies that the task of cross-lingual openqa is difficult. in the english test set, the performance of the multilingual bert model is worse than that of the monolingual bert model. in almost all target languages, however, the multilingual model achieves the best result, manifesting its ability in capturing answers for questions across various languages. when we compare documentqa to bert, although they have similar performance in english, bert consistently outperforms documentqa by a large margin in all target languages in both translate-test and translate-train settings. translate-train methods outperform translatetest methods in all cases except for documentqa in german. 

the results in table 9 verify our assumption. the performance of different languages generally decreases as the genetic distance grows. the exceptions are chinese and portuguese since the percentages of "easy" questions in them are significantly higher than those in other languages. for languages that have similar genetic distances with english (i.e. russian, ukrainian, and portuguese), the performance increases as the percentage of "easy" questions grows. 

table 3 presents parsing accuracy on the dev data when training each parser on a single-domain training data. we can see that although pb-train is much smaller than bc-train, the pb-trained parser outperforms the bc-trained parser by about 8% on pb-dev, indicating the usefulness and importance of target-domain labeled data especially when two domains are very dissimilar. however, the gap between the zx-trained parser and the bc-trained is only about 2% in las, which we believe has a two-fold reason. first, the size of zx-train is even smaller, and is only less than one third of that of pb-train. second, the bc corpus are from the people daily newspaper and probably contains novel articles, which are more similar to zx. overall, it is clear and reasonable that the parser achieves best performance on a given domain when the training data is from the same domain. 

table 5 shows the final results on the test data, which are consistent with the previous observations. first, when constrained on single-domain training data, using the target-domain data is the most effective. second, using source-domain data as extra training data is helpful, and the doemb method performs the best. third, it is extremely useful and efficient to first train elmo on very large-scale general-purpose unlabeled data and then fine-tune it on relatively small-scale targetdomain unlabeled data. 

table 3 shows model performance by entity type and the overall performance on the four tested datasets. from the table, we can observe: 1) the performance of the matching model is quite poor compared to other models. 

our method outperforms all baselines significantly, which shows the importance of using rich data. a contrast between our method and mixdata shows the effectiveness of using two different language models across domains. even through mix-data uses more data for training language models on both the source and target domains, it cannot learn a domain contrast since both sides use the same mixed data. in contrast, our model gives significantly better results by gleaning such contrast. finally, table 3 also shows a comparison with a state-of-the-art method on the 13pc and 13cg datasets (crichton et al., 2017), which leverages pos tagging for multi-task learning by using cotraining method. 

the results in table 6 indicate the competitive effect of topics on decoder attention and that on hidden states, but combining them both help our full model achieve the best performance. we also observe that pre-trained topics only bring a small boost, indicated by the close scores yielded by our model (separate train) and seq2seq-copy. 

the social media language model outperforms all baseline models, including rnns, lstms, cnns, and the linear dad and sda models. the a-cnn-lstm and the hierarchical attention model has a high recall due to its ability to better capture long term dependencies. 

table 5 shows that han-nli* is much better than the two baselines in terms of label accuracy and evidence f1 score. han-nli seem already a decent model given its much better performance than fever-base. 

table 5 shows all models' transfer performance between populations on gender. in general, all models generalize well to the respectively unseen datasets but perform best on the data they have been specifically trained for. the largest difference can be observed on the sub-1,000 author dataset pan15, where the model of ãlvarez-carmona et al. (2015) suffers a significant performance loss, and pan16, where the model of busger op vollenbroek et al.(2016) performs notably better on the celebrity data. this hypothesis is also supported by the large increase in accuracy of the baseline model after retraining for two epochs with the pan15 and pan16 training datasets, respectively. 

table 3 demonstrates the performance of the three methods. micro-f1 scores are generally better than macrof1 scores because the trivial cases like the class of good air quality are the majority of datasets with higher weights in micro-f1 scores. paqi is better than bow although bow uses the knowledge of social media. our approach significantly outperforms all baseline methods in almost all metrics. more precisely, our approach improves the air quality prediction over paqi from 6.92% to 17.71% in macro-f1 scores. 

we show in table 3 that having access to the full story provides the best performance. having no access to any of the story decreases ranking accuracy, even though the model still receives the local context window of the entity as input. the left story context model performs better, but looking at the complete story provides additional gains. 

table 2 shows the automatic results. applying our nlu model with iterative data refinement, the error rates of refined mr-text pairs yields 23.33% absolute error reduction on test set. 

human evaluation in table 3 shows that our proposed method achieves 16.69% improvement on information equivalence between mr-text pairs. these results confirm the effectiveness of our method in reducing the unaligned data noise, and the large improvement (i.e, 15.09%) on exact match when applying self-training algorithm suggests the importance of iterative data refinement. 

table 2 shows the evaluation results. the average score for overall quality is 7.6, showing that the testing set is satisfactory. 

table 4 presents the percentage of comparisons that fall into each category, along with the average and std of the e value of aso for each case (all aso results are significant with p <= 0.01). the number of comparisons that fall into case a is only 0.98%, indicating that it is rare that a decision about stochastic dominance of one algorithm can be reached when comparing dnns. 

table  4  shows  the  overall performance of schema matching on gnbusiness-test. from the table, we can see that odee-fer achieves the best f1 scores among all the methods. by  comparing nguyen  et  al.  (2015) and odee-f (p= 0.01), we can see that using continuous contextual features gives better performance than discrete  features. we can also see from the result of clustering that using  only  the  contextual  features  is  not  sufficient for odee, while combining with our neural latent variable model in odee-f can achieve strong results (p= 6×10^6). among odee models, odee-fe gives a 2% gain in f1 score  against odee-f,  which shows that the latent event type modeling is beneficial and the slot distribution relies on the latent event type. additionally, there is a 1% gain in f1 score  by  comparing odee-fer and odee-fe (p= 2×10?6), which confirms that leveraging redundancy is also beneficial in exploring which slot an entity should be assigned. 

with a target rate of 10%, the hardkuma model achieved 8.5% non-zero attention. table 3 shows that, even with so many zeros in the attention matrices, it only does about 1% worse compared to the da baseline. 

table 1 shows the performances measured in terms of bleu score. on zh-en task, transformer(base) existing systems edr (tu et al., 2017) and db (kuang et al., 2018) by 11.5 and 6.5 bleu points. with respect to bleu scores, all the proposed models consistently outperform transformer(base) by 0.96 and 1.23 bleu points. the big models (row 7-8) also achieve similar improvement by 0.73 and 0.82 bleu points on a larger parameters model. in addition, the proposed methods gain similar improvements on en-de task. 

the bottom part of table 3 lists the performances of our methods. it manifests that both teaching summary word generation and teaching attention weights are able to improve the performance over the baselines. when the summary word generation and attention weights are taught simultaneously (denoted by teaching generation+attention), the performance is further improved, surpassing the best baseline by more than two points on gigaword evaluation set and more than one point on duc2004. 

table 2 shows the result on en->sw and en>tl where we train and test on the same language pair. for query translation, psq is better than dbqt because psq uses a weighted alternative to translate query terms and does not limit to the fixed translation from the dictionary as in dbqt. for document translation, we find that both smt and nmt have a similar performance which is close to psq. in our experiments with deep relevance ranking models, we all use smt and psq because they have strong performances in both language pairs and it is fair to compare. to compare language pairs, en->tl has larger improvements over en->sw. this is because en->tl has better query translation, document translation, and query likelihood retrieval results from the baselines, and thus it enjoys more benefits from our model. we also found posit-drmm works better than the other two, suggesting term-gating is useful especially when the query translation can provide more alternatives. 

table 2 presents the results of different systems, showing that our proposed model achieves the best performance on all test language pairs under unsupervised settings. in addition, our approach is able to achieve completely comparable or even better performance than supervised systems. 

table 2 reports precision, recall and f1 scores on the training set.8 . our results show that multilingual sentence embeddings already achieve competitive performance using standard forward retrieval over cosine similarity, which is in line with schwenk (2018). both of our bidirectional retrieval strategies achieve substantial improvements over this baseline while still relying on cosine similarity, with intersection giving the best results. moreover, our proposed margin-based scoring brings large improvements when using either the distance or the ratio functions, outperforming cosine similarity by more than 10 points in all cases. the best results are achieved by ratio, which outperforms distance by 0.3-0.5 points. 

different from the lexical (jaccard) and semantic matching (wmd and sgw) baselines, bne obtains high scores in accuracy  metric (see table 3). table 3 also includes performances of other state-of-the-art baselines in biomedical name normalization, such as sieve-based (d'souza and ng, 2015), supervised semantic indexing (leaman and lu, 2016), and coherence-based neural network wright et al. (2019) approaches. when the dataset-specific annotations are utilized, even the simple exact matching rule can boost the performance of our model to surpass other baselines (see the last two rows in table 3). 

table 1 shows the results of our relational word vectors, the standard fasttext embeddings and other baselines on the two relation classification datasets (i.e. bless and diffvec). our model consistently outperforms the fasttext embeddings baseline and comparison systems, with the only exception being the precision score for diffvec. despite being completely unsupervised, it is also surprising that our model manages to outperform the knowledge-enhanced embeddings of  retrofitting  and  attract-repel  in  the  bless dataset. in general, the improvement of rwe over standard word embeddings suggests that our vectors capture relations in a way that is compatible to standard word vectors (which will be further discussed in section 6.2). 

table 2 shows the results on the mcrae feature norms dataset and qvec. in the case of the mcrae feature norms dataset, our relational word embeddings achieve the best overall results, although there is some variation for the individual features. these results suggest that attributional information is encoded well in our relational word embeddings. interestingly, our results also suggest that retrofitting and attract-repel, which use pairs of related words during training, may be too naive to capture the complex relationships proposed in these benchmarks.  in fact, they perform considerably lower than the baseline fast-text model. on the other hand, pair2vec, which we recall is the most similar to our model, yields slightly  better results  than  the fasttext  baseline, but still worse than our relational word embedding model. this is especially remarkable considering its much lower computational cost. 

table 1 shows word analogy results for three datasets. these results are shown separately in table 1 as gsem and gsyn respectively. second, we considerthe microsoft  syntactic  word  analogy  dataset, which  only  covers  syntactic  relations  and  is  re-ferred to as msr. finally, we show results for the bats analogy dataset4, which covers four categories of relations: inï¬‚ectional morphology (im), derivational morphology (dm), encyclopedic semantics (es) and lexicographic semantics (ls). the results in table 1 clearly show that our model behaves substantially differently from the baselines: for the syntactic/morphological relationships (gsyn, msr, im, dm), our model outperforms the baselines in a very substantial way. on the other hand, for the remaining, semanticallyoriented categories, the performance is less strong, with particularly weak results for gsem. for es and is, it needs to be emphasized that the results are weak for all models, which is partially due to a relatively high number of out-of-vocabulary words. 

table 7 summarizes our document classification results. it can be seen that our model outperforms all baselines, except for the techtc dataset, where the results are very close. among the baselines, interestshdp achieves the best performance. 

for understanding the improvement, we conduct an ablation test and show the result in table 2. according to table 2, we observe that the original bert cannot perform as well as the previous state-of-the-art approaches by its own. when we further add our candidate valuation method in section 2.2 to validate the candidates, its performance is significantly improved. furthermore, it is clear that our substitute candidate proposal method is much better than wordnet for candidate proposal when we compare our approach to the -w/o sp (wordnet) baseline where candidates are obtained by wordnet and validated by our validation approach. 

table 5 compares entity linking performance for different entity splits. seen entities from the training worlds are unsurprisingly the easiest to link to. for unseen entities from the training world, we observe a 5-point drop in performance. 

table 1 shows the parsing f -scores against the stanford parser. we see that, among the six most common ones, our imitation approach outperforms the prpn on four types. 

columns (i) and (ii) of table 6 show that reg (§3.4) delivers results comparable to densifier (orth) when using the same set of generic training words (gen) in lexicon induction. however, our method is more efficient - no need to compute the expensive svd after every batch update. 

table 6 shows the sentence-level phrase accuracy (spacc) and phrase error deviation (pedev) comparison on sst-5 between bi-tree-lstm and tcm, respectively. tcm outperforms bi-treelstm on all the metrics, which demonstrates that tcm gives more consistent predictions of sentiments over different phrases in a tree, compared to top-down communication. this shows the benefit of rich node communication. 

table 3 shows that the nearest neighbor baseline performs similarly to simply returning the support document which indicates that memorizing answers from the training set is insufficient. for extractive models, the oracle provides an approximate upper bound of 27.4 rouge-1. the bidaf model is the strongest (23.5), better than tfidf between the question and the support document to select sentences. however, these approaches are limited by the support document, as an oracle computed on the full web sources achieves 54.8. abstractive methods achieve higher rouge, likely because they can adapt to the domain shift between the web sources and the eli5 subreddit. in general, seq2seq models perform better than language models and the various seq2seq settings do not show large rouge differences. 

the bottom half of table 5 shows the results of ablation tests. as we can see, after removing the emotion classification term (emods-mle), the performance decreased most significantly. applying an external emotion lexicon (emods-ev) also brought performance decline, especially on emotion-w. additionally, the distinct-1/distinct-2 decreased most when using the original beam search (emods-bs), indicating that the diverse decoding can promote diversity in response generation. 

it is shown in table 6 that emods achieved the highest performance in most cases (sign test, with p-value < 0.05). specifically, for content coherence, there was no obvious difference among most models, but for emotional expression, the emods yielded a significant performance boost. as we can see from table 6, emods performed well on all categories with an overall emotion score of 0.608, while emoemb and ecm performed poorly on categories with less training data, e.g., disgust, anger and sadness. note that all emotion scores of seq2seq were the lowest, indicating that seq2seq is bad at emotional expression when generating responses. to sum up, emods can generate meaningful responses with better emotional expression, due to the fact that emods is capable of expressing the desired emotion either explicitly or implicitly. 

table 2 demonstrates performances on whether using multi-level vocabularies. we can observe that incorporating multilevel vocabularies could improve performances on almost all of the metrics. for example, enc3-dec3 (mvs) improves relative performance up to 25.73% in bleu score compared with enc3-dec3 (sv) on the weibo dataset. only on the twitter dataset, enc1-dec3 (mvs) is slightly worse than “enc1-dec3 (sv)” in the bleu score. 

as shown in table 2 only (khanpour et al., 2016; ortega and vu, 2017; lee and dernoncourt, 2016) have evaluated on more than one task, while the rest of the methods target specific one. we denote with ? models that do not have results for the task. on the dialog act mrda and swda tasks, sgnn++ outperformed deep learning methods like cnn (lee and dernoncourt, 2016), rnn (khanpour et al., 2016) and rnn with gated attention (tran et al., 2017) and reached the best results of 87.3% and 88.43% accuracy. for intent prediction, sgnn++ also improved with 0.13% 1.13% and 2.63% over the gated attention (goo et al., 2018), the joint slot and intent bilstm model (hakkani-tur et al., 2016) and the attention slot and intent rnn (liu and lane, 2016) on the atis task. on customer feedback, sgnn++ reached better performance than logistic regression models (elfardy et al., 2017; dzendzik et al., 2017). overall, sgnn++ achieves impressive results given the small memory footprint and the fact that it did not rely on pre-trained word embeddings like (hakkani-tur et al., 2016; liu and lane, 2016) and used the same architecture and model parameters across all tasks and languages. 

table 1 shows our main experimental results, with baselines shown in the top and our models at the bottom. the results show that our model (ours) outperforms competitive baselines on various evaluation metrics. 

conversations: table 4 presents results on the metrics defined in section 4.3. there are three regions of performance. first, the baseline has consistently low scores since it forms a single conversation containing all messages. second, elsner and charniak (2008) and lowe et al. (2017) perform similarly, with one doing better on vi and the other on 1-1, though elsner and charniak (2008) do consistently better across the exact conversation extraction metrics. third, our methods do best, with x10 vote best in all cases except precision, where the intersect approach is much better. 

dataset variations: table 5 shows results for the feedforward model with several modifications to the training set, designed to test corpus design decisions. removing context does not substantially impact results. decreasing the data size to match elsner and charniak (2008)’s training set leads to worse results, both if the sentences are from diverse contexts (3rd row), and if they are from just two contexts (bottom row). we also see a substantial increase in the standard deviation when only two samples are used, indicating that performance is not robust when the data is not widely sampled. 

results on test query is summarized in table 2. from the results, we can see that the rnn-based encoder is better than the cnn-based encoder on test query consistently on all metrics. 

we also implement the rnn-based and cnn-based encoders for the query representation for comparison. table 4 shows the results on 5,000 test queries by comparing the predicted response sentence function with its annotated groundtrue response sentence function. we can observe that encoding query sentence functions is useful to improve the performance for both cnn-based and rnn-based encoders. 

table 2 illustrates the results evaluated on the downstream tasks. hice outperforms the baselines in all the settings. compared to the best baseline `a la carte, the relative improvements are 12.4%, 2.9% and 5.1% for rare-ner, bioner, and twitter pos, respectively. as a result, all the systems perform worse on rare-ner than bio-ner, while hice reaches the largest improvement than all the other baselines. 

table 5 shows the absolute percentage improvement in classification performance when using each diachronic embedding compared to a classifier without diachronic embeddings. the rcnn also benefits from diachronic embeddings, but to a lesser extent, with an improvement on 4 of the 6 datasets. comparing the different methods for constructing diachronic embeddings, we find that our proposed subword method works the best on average for both classifiers. 

table 1 shows bleu scores of our approach on 3 iwslt translation tasks along with reported results from previous work. our approach achieves state-of-the-art or comparable results on all datasets. 

table 5 shows conll scores and the lea f1 values of the participating systems in the conll2012 shared task (closed task with predicted syntax and mentions) based on both maximum and minimum span evaluations. based on the results of tables 5 and  6:  (1) the use  of  minimum  spans  reduces  the  gap  between the performance on gold vs. system mentions by about two percent, (2) the use of minimum instead of  maximum  spans  results  in  a  different  ordering for some of the coreference resolvers, and (3) when gold mentions are used, there are no boundary detection errors, and consequently the results using mina are the same as those of using maximum spans. due to recognizing the same head for  distinct  overlapping  mentions,  the  scores  using the head of gold mentions are not the same as using their maximum span, which in turn indicates mina is suited better for detecting minimum spans compared to head words. 

table 3 presents the results on event coreference. our joint model outperforms all the baseines with a gap of 10.5 conll f1 points from the last published results (kcp), while surpassing our strong lemma baseline by 3 points. 

this section compares our proposed model with the current state-of-the-art models for idrr. in particular, table 2 shows the performance of the models for the multi-class classification settings (i.e., 4-way and 11-way with pdtb-lin and pdtb-ji) on the corresponding test sets. the  first  observation  from  these  tables  is  that the proposed model is significantly better than the model in (bai and zhao, 2018) over all the dataset settings (with p <0.05) with large performance gap. second, the proposed model achieves the state-of-the-art performance on the multi-class classification settings (i.e.,  table 2) and two set-tings  for  binary  classification  (i.e.,comparison and expansion). the performance gaps between the  proposed  method  and  the  other  methods  on the  multiclass  classification  datasets  (i.e.,  table2) are large and clearly testify to the advantage ofthe proposed model for idrr. 

in order to illustrate the contribution of these terms, table 3 presents the test set performance of the proposed model when different combinations of the terms are employed for the multi-class classification settings.  as we can see from the table, the embeddings of connectives  and  relations  can  only  slightly  improve the performance of the model in (bai and zhao, 2018), necessitating the penalization terms l1, l2 and l3 to facilitate the knowledge transfer and further improve the performance. from the table, it is also clear that each penalization term is important for the proposed model as eliminating any of them would worsen the performance.  combining the three penalization terms results in the best performance for idrr in this work. 

table 3 shows performance for the following linguistic features. the rule-based model can only capture inferences involving negation (r = 0.45), while the hybrid model performs more consistently across negation, modal, and question (r âˆ¼ 0.25). both models cannot handle inferences with conditionals. 

table 3 shows the performances of all the methods in the precision of the top answer (p@1) and the mean average precision (map) (oh et al., 2013). our proposed method, ours(op), outperformed all the other methods. our starting point,i.e.,base, was already superior to the methods in the previous works. compared with base and base+addtr, neither of which used compact-answer representations or fake-representation generator f, ours(op) gave 3.4% and 2.8% improvement in p@1, respectively. it also outperformed base+cans and base+cenc, which generated compact-answer representations in away different from the proposed method, and base+enc, which trained the fake-representation generator without adversarial learning. ours (op)also outperformed all the bert-based models but an interesting point is that fake-representation generatorfboosted the performance of the bert-based models (statistically significant with p <0.01by the mcnemar’s test). 

table 2 shows the exact match and f1 scores of multiple reading comprehension models with and without data augmentation. we can see that the generated unanswerable questions can improve both specifically designed reading comprehension models and strong bert fine-tuning models, yielding 1.9 absolute f1 improvement with bertbase model and 1.7 absolute f1 improvement with bert-large model. 

table 3 shows the human evaluation results of generated unanswerable questions. we compare with the baseline method tfidf, which uses the input answerable question to retrieve similar questions towards other articles as outputs. the retrieved questions are mostly unanswerable and readable, but they are not quite relevant to the question answering pair. notice that being relevant is demonstrated to be important for data augmentation in further experiments on machine reading comprehension. here pair-to-sequence model still outperforms sequence-to-sequence model in terms of all three metrics. 

table 2 shows the comparisons to the above five baseline methods. among all methods trained without extra corpora, our approach achieves the best result across datasets. after incorporating the back-translated corpus, our method yields an additional gain of 1-3 points over (sennrich et al., 2016b) trained on the same back-translated corpus. 

table 3 shows the bleu scores on the nist chinese-english translation task. we first compare our approach with the transformer model (vaswani et al., 2017) on which our model is built. as we see, the introduction of our method to the standard backbone model (trans.-base) leads to substantial improvements across the validation and test sets. specifically, our approach achieves an average gain of 2.25 bleu points and up to 2.8 bleu points on nist03. 

table 2 shows the overall ace2005 results of all baselines and our approach. for our approach, we show the results of four settings: our approach using word embedding as its word representation rw – ∆w2v; our approach using elmo as rw ∆elm o; our approach simply concatenating [rd, rg, rw] as instance representation ∆concat . from table 2, we can see that by distilling both discrimination and generalization knowledge, our method achieves state-of-the-art performance. compared with the best feature system, ∆w2v and ∆elmo gain 2.8 and 4.6 f1-score improvements. compared to the representation learning based baselines, both ∆w2v and ∆elmo outperform all of them. notably, ∆elmo outperforms all the baselines using external resources. 

however, we find that the graph-based features (leakage and advanced) make the problem feasible on a wide range of datasets. specifically, on the datasets like quoraqp and bytedance, the leakage features are even more effective than the unlexicalized features. one exception is that on multinli, majority outperforms leakage and advanced significantly. another interesting finding is that on snli and bytedance, advanced graph-based features improve a lot over the leakage features, while on quoraqp, the difference is very small. among all the tested datasets, only msrp and sicknli are almost neutral to the leakage features. results in table 1 raise concerns about the impact of selection bias on the models and evaluation results. 

table 4 reports the results on the datasets that are not biased to the leakage pattern of quoraqp. we find that the debiased model significantly outperforms the biased model on all three datasets. this indicates that the debiased model better captures the true semantic similarities of the input sentences. from the experimental results, we can see that the proposed leakage-neutral training method is effective, as the debiased model performs significantly better with synthetic dataset, msrp and sick, showing a better generalization strength. 

table 1 compares the performance of our system with different baselines in terms of efficiency and accuracy. we note the following observations from the result table. (1) denspi outperforms the query-agnostic baseline (seo et al., 2018) by a large margin, 20.1% em and 18.5% f1. (2) denspi outperforms drqa by 3.3% em. (3) denspi is 9.2% below the current state of the art. 

mt-dnnno-fine-tune. since the mtl of mt-dnn uses all glue tasks, it is possible to directly apply mt-dnn to each glue task without finetuning. the results in table 2 show that mtdnnno-fine-tune still outperforms bertlarge consistently among all tasks but cola. our analysis shows that cola is a challenge task with much smaller in-domain data than other tasks, and its task definition and dataset are unique among all glue tasks, making it difficult to benefit from the knowledge learned from other tasks. as a result, mtl tends to underfit the cola dataset. in such a case, fine-tuning is necessary to boost the performance. as shown in table 2, the accuracy improves from 58.9% to 62.5% after finetuning, even though only a very small amount of in-domain data is available for adaptation. this, together with the fact that the fine-tuned mt-dnn significantly outperforms the fine-tuned bertlarge on cola (62.5% vs. 60.5%), reveals that the learned mt-dnn representation allows much more effective domain adaptation than the pre-trained bert representation. 

table 8 shows the performance of the two models across each difficulty level. as we expect, the models perform better when the user request is easy. both models fail on most hard and extra hard questions. 

table 4 shows the sembleu and smatch scores several recent models. in particular, we asked for the outputs of lyu (lyu and titov, 2018), gros (groschwitz et al., 2018), van nood (van noord and bos, 2017) and guo (guo and lu, 2018) to evaluate on our sembleu. for camr and jamr, we obtain their outputs by running the released systems. sembleu is mostly consistent with smatch, except for the order between guo and gros. 

results in table 1 show two observations. first, models trained on en-en, in contrast to those trained on en-cs, have higher correlation for all encoders except sp. however, when the same number of english sentences is used, models trained on bitext have greater than or equal performance across all encoders. second, sp has the best performance in the en-cs setting. 

we re-implement constituent tree-lstm (contree) of tai et al. (2015) and obtain better results than their original implementation. table 2 shows the experimental results for sentiment classification on both sst-5 and sst-2 at the sentence level (root) and all nodes (phrase). 

table 3 shows that our methods beat the baseline on every task. bcn+wg improves accuracies on all task slightly by modeling sentiment composition explicitly. the obvious promotion of bcn+lvg4 and bcn+lveg shows that explicitly modeling sentiment composition with fine-grained sentiment subtypes is useful. particularly, bcn+lveg improves the sentence level classification accurracies by 1.4 points (fine-grained) and 0.7 points (binary) compared to bcn (our implementation), respectively. to our knowledge, we achieve the best results on the sst dataset. 

table 6 shows slightly lower performance for bcn+elmo and ulmfit while bert performed much worse. 

results on quora dataset are listed in table 3. the performance of re2 is on par with the state-of-the-art on this dataset. 

table 1 summarizes the result of experiments. we can clearly see that the proposed cs-lvm architecture substantially outperforms other models based on auto-encoding. also, the semantic constraints brought additional boost in performance, achieving the new state of the art in semisupervised classification of the snli dataset. when all training data are used as labeled data (? 550k), cs-lvm also improves performance by achieving accuracy of 82.8%, compared to the supervised lstm (81.5%), lstm-ae (81.6%), lstm-vae (80.8%), deconv-vae (80.9%). 

the bleu-2 results in table 1 indicate that comet exceeds the performance of all baselines, achieving a 51% relative improvement over the top performing model of sap et al. (2019). more interesting, however, is the result of the human evaluation, where comet reported a statistically significant relative avg performance increase of 18% over the top baseline,event2in(volun). this performance increase is consistent, as well, with an improvement being observed across every relation type. in addition to the quality improvements, table 1 shows that comet produces more novel tuple objects than the baselines, as well. 

our results in table 4 indicate that even with only 10% of the available training data, the model is still able to produce generations that are coherent, adequate, and novel. using only 1% of the training data clearly diminishes the quality of the produced generations, with significantly lower observed results across both quality and novelty metrics. interestingly, we note that training the model without pretrained weights performs comparably to training with 10% of the seed tuples, quantifying the impact of using pre-trained language representations. 

our results indicate that high-quality knowledge can be generated by the model: the low perplexity scores in table 6 indicate high model confidence in its predictions, while the high classifier score (95.25%) indicates that the kb completion model of li et al. (2016) scores the generated tuples as correct in most of the cases. while adversarial generations could be responsible for this high score, a human evaluation (following the same design as for atomic) scores 91.7% of greedily decoded tuples as correct. 

we evaluate all models on wsc273 and the wnli test dataset, as well as the various subsets of wsc273, as described in section 2. the results are reported in table 1 and will be discussed next. we note that models that are fine-tuned on the wscr dataset consistently outperform their non-fine-tuned counterparts. the bert_wiki_wscr model outperforms other language models on 5 out of 6 sets that they are compared on. in comparison to the lm ensemble by trinh and le (2018), the accuracy is more consistent between associative and non-associative subsets and less affected by the switched parties. however, it remains fairly inconsistent, which is a general property of lms. 

in table 4, we show the results of different baseline models and our graph2seq model for the topic of entertainment. from the results we can see that our proposed graph2seq model beats all the baselines in both coherence and informativeness. our model receives much higher scores in coherence compared with all other baseline models. 

we observe in table 2 that clqg+parallel outperforms all the other models for hindi. 

table 4 shows the results of automatic evaluation. it should be noted that the classification accuracy for human reference is relatively low (74.7% on yelp and 43.2% on amazon); thus, we do not consider it as a valid metric for comparison. for bleu score, our method outperforms recent systems by a large margin, which shows that our outputs have higher overlap with reference sentences provided by humans. 

table 3 shows the results obtained on the cqa test split. we report our two best models that represent using human explanations (cos-e-openended) for training only and using language model explanations (cage-reasoning) during both train and test. we observe that using cos-e-open-ended during training improves the state-of-the-art by approximately 6%. on the other hand, using cage-reasoning resulted in a gain of 10% accuracy over the previous state-of-the-art. this suggests that our cos-e-open-ended and cagereasoning explanations provide far more useful information than what can be achieved through simple heuristics like using google search to find relevant snippets. 

table 4 also contains results that use only the explanation and exclude the original question from cqa denoted by ‘w/o question’. when the explanation consists of words humans selected as justification for the answer (cos-e-selected), the model was able to obtain 53% in contrast to the 85% achieved by the open-ended human explanations (cos-e-open-ended). adding the question boosts performance for cos-e-selected to 70%, again falling short of almost 90% achieved by cos-e-open-ended. we conclude then that our full, open-ended cos-e thus supply a significant source of information beyond simply directing the model towards the most useful information already in the question. 

table 6 shows the results for models fine-tuned using a combination of monolingual hindi and english, and using the cs training set (both fine-tuning on the script-corrected version of the corpus as well as the transliterated version). 

results presented in table 7 show that the domain adaptation approach further boosts f1 by 1 point to 79 (t-test, p<0.5) and roc auc by 0.012. however, simply pooling the data actually hurts predictive performance leading to a drop of more than 2 points in f1. 

the automatic evaluation scores are presented in table 1. for abstractive sentence summarization, we report the rouge f1 scores compared with baselines and previous unsupervised methods. our method outperforms commonly used prefix baselines for this task which take the first 75 characters or 8 words of the source as a summary. our system achieves comparable results to wang and lee (2018) a system based on both gans and reinforcement training. in table 1, we also list scores of the state-of-the-art supervised model, an attention based seq-to-seq model of our own implementation, as well as the oracle scores of our method obtained by choosing the best summary among all finished hypothesis from beam search. 

table 3 reports the comparison results of our two models (on ae-110k and ae-650k datasets) and three baselines. it is observed that our models are consistently ranked the best over all competing baselines. 

the extensive results of our proposed model with comparisons to the state-of-the-art baselines techniques are reported in table 1. our proposed model outperforms the state-of-the-art baselines techniques by fair margins in terms of precision, recall and f1-score for all the datasets. on all the three datasets, we can visualize from table 1 that, the multi-task framework with its sharing scheme can help in boost the performance of the system. we observe the performance improvement of 5.89, 6.52 and 2.09 f1-score points on twitter, cadec, and medline dataset, respectively. 

the experimental results are shown in table 2. from the results, we can see that the mined rules alone do not perform well. however, by learning from the data automatically labeled by these rules, all four versions of rinante achieves better performances than rinante (no rule). moreover, the improvement over rinante (no rule) can be especially significant on se14-l and se15-r. we can also see from table 2 that the rules mined with our rule mining algorithm performs much better than double propagation. 

table 1 shows the results on the conll 2003 dataset and ontonotes 5.0 dataset respectively. hscrfs using gazetteer-enhanced sub-tagger outperform the baselines, achieving comparable results with those of more complex or larger models on conll 2003 and new state-of-the-art results on ontonotes 5.0. 

besides lstm, there are a few other methods of producing the sentence representation. table 6 compares the experimental results of these methods. the bag-of-tags method simply computes the average of all the pos tag embeddings and has the lowest accuracy, showing that the word order is informative for sentence encoding in d-ndmv. the anchored words method replaces the pos tag embddings used in the neural network of the neural dmv with the corresponding hidden vectors produced by a lstm on top of the input sentence, which leads to better accuracy than bag-of-tags but is still worse than lstm. replacing lstm with bi-lstm or attention-based lstm also does not lead to better performance, probably because these models are more powerful and hence more likely to result in degeneration and overfitting. 

table 1 shows the f1 score for the nested ner. when comparing the results for the nested ner in the baseline models (without the contextual word embeddings) to the previous results in literature, we see that lstm-crf reaches comparable, but suboptimal results in three out of four nested ne corpora, while seq2seq clearly outperforms all the known methods by a wide margin. we hypothesize that seq2seq, although more complex (the system must predict multiple labels per token, including the special label), is more suitable for more complex corpora. the gain is most visible in ace-2004 and ace-2005, which contain extremely long named entities and the level of “nestedness” is greater than in the other nested corpora. 

table 2 compares single-paradigm models against their double-paradigm mtl versions. on average, mtl models with auxiliary losses achieve the best performance for both parsing abstractions. they gain 1.05 f1 points on average in comparison with the single model for constituency parsing, and 0.62 uas and 0.15 las points for dependency parsing. in comparison to the single-paradigm mtl models, the average gain is smaller: 0.05 f1 points for constituency parsing, and 0.09 uas and 0.21 las points for dependency parsing. 

 table 1 shows both automatic and human evaluation results. paml achieve consistently better results in term of dialogue consistency in both automatic and human evaluation. the latter also shows that all the experimental settings have comparable fluency scores, where instead perplexity and bleu score are lower in paml. this confirms that these measures are not correlated to human judgment (liu et al., 2016). 

results in table 1 show that while the utterance-based ha network is on par with established baselines, the proposed turn-based ha model obtains more gains, achieving the best em and f1 scores. 

table 4 shows the results of our system on the dbpedia and ag news datasets. using the same model without any tuning, we managed to obtain competitive results again compared to previous stateof-the-art systems. 

table 2 shows that our proposed mtn is able to generalize to the visually grounded dialogue setting. it is interesting that our generative model outperforms other retrieval-based approaches in ndcg without any task-specific fine-tuning. 

table 3 shows the evaluation results. our system with kernel transition module outperforms all other systems in terms of all metrics on both two tasks, expect for r20@3 where the system with pmi transition performs best. the kernel approach can predict the next keywords more precisely. in the task of response selection, our systems that are augmented with predicted keywords significantly outperform the base retrieval approach, showing predicted keywords are helpful for better retrieving responses by capturing coarsegrained information of the next utterances. interestingly, the system with random transition has a close performance to the base retrieval model, indicating that the erroneous keywords can be ignored by the system after training. 

in table 3 we show our results for all tasks of raganato et al. (2017a)’s evaluation framework. the first noteworthy result we obtained was that simply replicating peters et al. (2018)’s method for wsd using bert instead of elmo, we were able to significantly, and consistently, surpass the performance of all previous works. when using our method (lmms), performance still improves significantly over the previous impressive results (+1.9 f1 on all, +3.4 f1 on semeval 2013). interestingly, we found that our method using elmo embeddings didn’t outperform elmo k-nn with mfs fallback, suggesting that it’s necessary to achieve a minimum competence level of embeddings from sense annotations (and glosses) before the inferred sense embeddings become more useful than mfs. 

table 6 shows the results of clustering on wsi semeval-2010 dataset. wordctx2sense outperforms (arora et al., 2018) and (mu et al., 2017) on both f-score and v-measure scores by a considerable margin. we observe similar improvements on the makesense-2016 dataset. 

table 2 shows de-identification performance results for the non-private de-identification classifier in comparison to the state of the art. the results are average values out of five experiment runs. when trained on the raw i2b2 2014 data, our models achieve f1 scores that are comparable to dernoncourt et al. results. the casing feature improves glove by 0.4 percentage points. 

results are shown in table 2. our work outperforms or matches existing published results that do not rely on ensembling. 

table 3 shows that adding wpl on hidden states can help improve performance slightly but not as good as adding it on word embeddings. in practice, we also observe that the value of wpl tends to vanish when using wpl on hidden states, which is presumably caused by the fact that lstms have sequence information, making the optimization of wpl trivial. we also observe that adding wpl to both the encoder and decoder brings the largest improvement. 

the results in table 7 show that it is better to use smaller number of classes for each cluster instead of using a cluster with a large number of classes. 

results table 3 shows the performance of unsupervised paraphrase generation. in the first row of table 3, simply copying the original sentences yields the highest bleu-ref, but is meaningless as it has a bleu-ori score of 100. we see that dss-vae outperforms the cgmh and the original vae in bleu-ref. especially, dss-vae achieves a closer bleu-ref compared with supervised paraphrase methods (gupta et al., 2018). in other words, the plain vae and cgmh are “inadmissible,” meaning that dssvae simultaneously outperforms them in both bleu-ori and bleu-ref, indicating that dssvae outperforms previous state-of-the-art methods in unsupervised paraphrase generation. 

the automatic results of four generation models are shown in table 2. we have the following observations: (1) three models based on our proposed framework do not have obvious performance difference in terms of bleu. meanwhile, all of them can largely outperform the seq2seq+sentimod baseline which does not follow our framework. thus it shows the effectiveness of the proposed framework. (2) hm senticons which measures the performance of sentiment analyzer is marginally consistent with the i-o senticons and sentiment which measure the performance of sentimental generator. 

the automatic and human evaluation results of four generation models are shown in table 2 and table 3 respectively. we have the following observations: (1) three models based on our proposed framework do not have obvious performance difference in terms of bleu, coherency, and fluency. meanwhile, all of them can largely outperform the seq2seq+sentimod baseline which does not follow our framework. 

table 3 shows a comparison between our models and comparative models. whereas dress-ls has a higher sari score because it directly optimizes sari using reinforcement learning, our models achieved the best bleu scores across styles and domains. 

table 2 shows the results of human evaluation on the chinese-to-english task. we asked two human evaluators who can read both chinese and english to evaluate the ﬂuency and adequacy of the translations generated by mle, mle + cp, mle + data, and clone. we find that clone significantly improves the adequacy over all baselines. 

as can be seen in table 2,  degree (tf-idf) is very close to  textrank (tf-idf).   . due to space limitations,  we  only  show  comparisons  between degree and  textrank with  tf-idf,  however, we  observed  similar  trends  across  sentence  rep-resentations.  these  results  indicate  that  considering  global  structure  does  not  make  a  difference  when  selecting  salient  sentences  for  nyt and  cnn/daily  mail,  possibly  due  to  the  fact that news articles in these datasets are relatively short (see table 1). the results in table 2 further show that pacsum substantially outperforms textrank across sentence representations, directly confirming our assumption that position information is beneficial for determining sentence centrality in news single-document summarization. 

table 3 presents various ablation studies for the document-level model on the development set. deep sentence representations when combined with multi-attention bring improvements over shallow representations (+3.68 exact-f1).  using alignments as features and as a way of highlighting where to copy from yields further performance gains both in terms of exact and partial f1. the best performing variant is deep-copy  which  combines  supervised  attention  with copying.   . 

table 4 shows our results on the test set (see the appendix for an example of model output); we compare the best performing drts parser (deepcopy) against two baselines which rely on our sentence-level parser (docsent and doctree). the drts parser, which has a global view of the document, outperforms variants which construct document representations by aggregating individually parsed sentences. 

table 5 shows classifier performance metrics for such samples. the importance weighting approach slightly outperforms the baseline classifier. replacing identity words with a special tokens, on the other hand, hurts the performance on the main task. hence, the information pertaining to identity terms is not completely lost for our method, but come at a cost. 

table 7 lists the experimental results. we find that the model obtained better performance for numerals distorted by more than 50%, with more confusion in the range below that. furthermore, according to the micro and macro-averaged f1 scores, the performance is similar among the three different cases (i.e., overstated, understated, and correct). 

table 6 shows the results obtained using the multimodal model for different sets of input features. the model that uses all the input features available leads to the best results, improving significantly over the text-only and video-only methods. 

table 1 summarizes the segmentation results on the widely used br-phono corpus, comparing it to a variety of baselines. unigram dp, bigram hdp, lstm suprisal and hmlstm refer to the benchmark models explained in ¤6. the ablated versions of our model show that without the lexicon (-memory), without the expected length penalty (-length), and without either, our model fails to discover good segmentations. 

table 2 summarizes results on the br-text (orthographic brent corpus) and chinese corpora. here we observe a similar pattern, with the snlm outperforming the baseline models, despite the tasks being quite different from each other and from the br-phono task. 

table 4 summarizes the results of the language modeling experiments. again, we see that snlm outperforms the bayesian models and a character lstm. 

experimental results on val v1.0 are shown in table 1. “-d” denotes that a discriminative decoder is used. with only one reasoning step, our redan model already achieves better performance than coatt, which is the previous best-performing model. using two or three reasoning steps further increases the performance. we also report results on an ensemble of 4 redan-d models. significant improvement was observed, boosting ndcg from 59.32 to 60.53, and mrr from 64.21 to 65.30. 

table 1 shows the performance of these baselines. we observe that the image-only models perform poorly on covr we because they are unable to identify them from the image pixels alone. on the other hand, the labels-only baseline and the proposal of lu et al.(2018) has high performance across all three metrics. 

table 3 shows the human evaluation results. they are consistent with the automatic evaluation results when it comes to the preference of humans towards the deliberation-based setups, but show a more positive outlook regarding the addition of visual information (del+obj over del) for french. 

cmasw has only one writer, which is trained on both normal and abnormal findings. table 1 shows that cmasw can achieve competitive performances to the state-of-the-art methods. cmasnw, aw is a simple concatenation of two single agent models cmasnw and cmasaw, where cmasnw is trained only on normal findings and cmasaw is trained only on abnormal findings. at test time, the final paragraph of cmasnw, aw is simply a concatenation of normal and abnormal findings generated by cmasnw and cmasaw respectively. surprisingly, cmasnw, aw performs worse than cmasw on the cx-chr dataset. as evidently shown in table 1, cmas-il achieves higher scores than cmasnw, aw, directly proving the importance of communication between agents and thus the importance of pl. finally, it can be observed from table 1 that cmas-rl consistently outperforms cmas-il on all metrics, which demonstrates the effectiveness of reinforcement learning. 

table 2 shows the results. the lstm using text-only input outperforms all other baselines. it improves all six aspects for stories by arel, and improves “focus” and “human-like” aspects for stories by glac. these results demonstrate that a relatively small set of human edits can be used to boost the story quality of an existing large vist model. table 2 also suggests that the quality of a post-edited story is heavily decided by its pre-edited version. even after editing by human editors, arel’s stories still do not achieve the quality of pre-edited stories by glac. the inefficacy of image features and transformer model might be caused by the small size of vist-edit. it also requires further research to develop a post-editing model in a multimodal context. 

