table 4 show the results in the raw setting. from the results, we can see that in both settings: (i) kale-pre and kale-joint outperform (or at least perform as well as) the other methods which use triples alone on almost all the test sets, demonstrating the superiority of incorporating logical rules. (ii) on the test-i sets which contain triples beyond the scope of pure logical inference, kale-joint performs significantly better than kale-pre. on these sets kale-joint can still beat all the baselines by a significant margin in most cases, while kale-pre can hardly outperform kale-trip. it demonstrates the capability of the joint embedding scenario to learn more predictive embeddings, through which we can make better predictions even beyond the scope of pure logical inference. (iii) on the test-ii sets which contain directly inferable triples, kale-pre can easily beat all the baselines (even kale-joint). 

table 6 shows the performance comparison for end-to-end event co-reference. results on tac-kbp show that “ssed + supervisedextend” achieves similar performance to the tac top ranking system while the proposed msep event co-reference module helps to outperform supervised methods on b 3 and ceafe metrics. 

the influence of the number of roles on the perplexity is shown in table 3 and the final results are shown in table 2. from the tables we can see several important findings. standalone ltlm performs worse than mkn on both languages, however their combination leads to dramatic improvements compared with other lms. best results are achieved by 4- gram mkn interpolated with 1000 roles ltlm and the deterministic inference. the perplexity was improved by approximately 46% on english and 49% on czech compared with standalone mkn. the deterministic inference outperformed the nondeterministic one in all cases. ltlm also significantly outperformed stlm where the syntactic dependency trees were provided as a prior knowledge. the joint learning of syntax and semantics of a sentence proved to be more suitable for predicting the words. 

table 5 shows results on the development set for bicond, compared to the best unidirectional encoding model, tweetcondtar and the baseline model concat, split by tweets that contain the target and those that do not. all three models perform well when the target is mentioned in the tweet, but less so when the targets are not mentioned explicitly. in the case where the target is mentioned in the tweet, biconditional encoding outperforms unidirectional encoding and unidirectional encoding outperforms concat. this shows that conditional encoding is able to learn useful dependencies between the tweets and the targets. 

as results in table 2 show, our supervised s2s baseline model performs slightly better than the comparable model by dong and lapata (2016). the semi-supervised seq4 model with the additional generated queries improves on it further. 

results in table 4 suggest that the svm-rbf baseline system performs poorly and its results are on par with a majority class baseline (not reported in detail). both deep learning models significantly outperform the baseline, yielding macro-f1 score about 0.35. the attention-based model performs better than simple blstm in two classes (c5 and c6), but the overall macro-f1 score is not significantly better. 

table 2 shows the perplexity evaluation on the ptb test set. as a first observation, we can clearly see that the proposed approach outperforms all other models for all configurations, in particular, rnn and lstm. more particularly, we can conclude that lsrc, with an embedding size of 100, achieves a better performance than all other models while reducing the number of parameters by ≈ 29% and ≈ 17% compared to rnn and lstm, respectively. increasing the embedding size to 200, which is used by the other models, improves significantly the performance with a resulting nop comparable to lstm. the significance of the improvements obtained here over lstm were confirmed through a statistical significance t-test, which led to p-values  ≤ 10^−10 for a significance level of 5% and 0.01%, respectively. the results of the deep models in table 2 also show that adding a single non-recurrent hidden layer to lsrc can significantly improve the performance. in fact, the additional layer bridges the gap between the lsrc models with an embedding size of 100 and 200, respectively. the resulting architectures outperform the other deep recurrent models with a significant reduction of the number of parameters(for the embedding size 100), and without usage of dropout regularization, lp and maxout units or gradient control techniques compared to the deep rnn4 (d-rnn). 

table 5 reports perplexity on training and development sets, the labeled f1-score on wsj section 23, and the tree edit distance (ted) of various systems. when decoding with beam size 10, the four new neural parsers can generate wellformed trees for almost all the 2416 sentences in the wsj section 23. this makes ted a robust metric to evaluate the overall performance of each parser. table 5 reports the average ted per sentence. though biased, this still reflects the overall performance: we achieve around 80 f1 with nmt encoding vectors, much higher than with the e2e and pe2pe encoding vectors (below 60). 

table 1 shows the evaluation results on the test dataset for several variants of our pairwise neural network architecture. adding both appropriateness and relatedness interactions (full network) yields an improvement of another two map points absolute (to 54.51), which shows that appropriateness features encode information that is complementary to the information modeled by relevance and relatedness. 

table 3 shows the results. without interaction between the knowledge and neural network learning, the pipelined method yields inferior results. we can see that both models performs poorly, achieving the accuracy of only 68.6% for the knowledge component, similar to the accuracy achieved by the “opt-joint” method. 

as seen in table 3 our approach shows competitive performance on the other four datasets. the results we obtain on these datasets exhibit the reliability of our approach in modeling non-nominal word senses. 

table 2 shows the resulting feature spaces of the composed vectors. 

table 8 shows the values for precision, recall, and f1 of the extratreeclassifier. in brief, the introduced approach detects requirements in usergenerated content with an average f1-score of 91%. 

the basic subreddit statistics are shown in table 1. we report the random policy performances and heuristic upper bound performances (averaged over 10,000 episodes) in table 2 and table 3.3 the upper bound performances are obtained using stabilized karma scores and offline constructed tree structure. 

table 3 summarizes the results of experiments on infoboxqa. the performance of the baselines indicates that unigram bag-of-words models are not sufficiently expressive for matching, tri-cnn makes use of larger semantic units through its multiple channels. the attention mechanism and the combination of an rnn and cnn in attn1511 achieves better results than rnn, but still performs slightly worse than the cnn model with weightsharing. our tri-cnn model built on top of this weight-sharing architecture achieves the best performance. 

results. as can be seen from table 8 traino-matic enabled ims to perform better than the best participating system (manion and sainudiin, 2014, sudoku) in all three settings (all domains, maths & computer and biomedicine). its performance was in fact, 1 to 3 points higher, with a 6-point peak on maths & computer in spanish and on biomedicine in italian. this demonstrates the ability of train-o-matic to enable supervised wsd systems to surpass state-of-theart knowledge-based wsd approaches in lowresourced languages without relying on manually curated data for training. 

table 2 shows the performance of the match lstm and bidaf models against all four adversaries. addsent made average f1 score across the four models fall from 75.7% to 31.3%. addany was even more effective, making average f1 score fall to 6.7%. addonesent retained much of the effectiveness of addsent, despite being model independent. finally, addcommon caused average f1 score to fall to46.1%, despite only adding common words. 

table 3 shows the comparison results based on this evaluation protocol (two-part evaluation ii). we can see from tables 2 and 3 that our proposed approach performs much better than the baseline methods over all three metrics. 

table 6 summarizes the results for different systems, including the baseline cnn model without using context information (this baseline uses pretrained embeddings and speaker change feature), and four different ways of using context: (a) predicted da information (posterior probabilities) is combined with the current sentence's cnn-based representation; (b) applying a blstm on top of the sentence cnn representation; (c) hierarchical cnn that combines the current sentence's cnn representation with its neighbors; (d) sequence decoding by combining cnn posteriors with da transition scores. from the results, we can see the positive effect when context information is used. comparing representing context information via the da labels of the previous utterances vs using the hierarchical cnn or rnn model, we see there is not much difference. 

table 3 summarizes the results of the mmrnbased semantic parsing systems and other strong baselines. compared to the pipeline approach (mmrn-pipeline), the joint learning framework (mmrn-joint) improves significantly, reaching 68.1% f1. mmrn-joint outperforms rein-force  and  its  variant,  reinforce+,  which renormalizes the probabilities of the sampled candidate  sequences. its result is also better than the state-of-the-art stagg system. comparing to stagg, note that yih et al.(2016) did not jointly train the entity linker and semantic parser together, but they did improve the results by taking the top 10 predictions of their entity linking system for re-ranking parses. our algorithm further allows to update the entity linker with the labels for semantic parsing and shows superior performance. 

table 5 compares our sfv performance against previous reported scores on judging each response as true or false. we can see that our approach advances state-of-the-art methods. 

table 2 shows the results on the ace datasets, and these are our main results. the highest results (f1-score) and those results that are not significantly different from the highest results are highlighted in bold (based on bootstrap resampling test (koehn, 2004), where p > 0.01). for ace datasets, we make comparisons with the two versions of the linear-chain crf baseline: lcrf (single) which does not support overlapping mentions at all and lcrf (multiple) which does not support overlapping mentions of the same type, as well as our implementation of the mention hypergraph baseline (lu and roth, 2015). 

table 5 details the performance of our method on the three categories at the span and token level. we observe significant improvement by using ulm+graphinterp and  ulm+graphfeat over  best  semeval  and  our  best  supervised  system on all three categories at both token and span levels. this is in part because task is much less frequent than the other types. an analysis of confusion patterns show that the most frequent type confusions are between process and material. however, we observe that ulm+graphfeat* can greatly reduce the confusion, with 3.5% relative improvement of processand 3.6% relative improvement of process over ulm+graphinterp on token level. 

in table 1 we present linking accuracy for our models that vary in the information they use. we see that the model that only encodes the contextinformation, model c (l = ltext) consistently performs better than picking the entity with the highest prior probability from crosswikis, indicating that the model is able to utilize the context across datasets. on incorporating the description with context (model cd) we see improvement in the performance on ace-2005, but slight decrease in conll, suggesting the entity descriptions are not extremely useful for the latter (it contains rare entities, many short and incomplete sentences, and specific entities as annotations for metonymic mentions, as also observed by ling et al.(2015)). on introducing the entity type-aware loss in model ct to the context-only model, we see significantly improved results for all datasets, demonstrating that explicitly modeling fine-grained types helps learning a better context encoder and, in turn, typeaware entity representations. combining descriptions with this model (model cdt) shows further gains in accuracy indicating that our model is able to exploit complementary information from the two sources. finally, on introducing explicit entity-type encoding, model cdte performs the best on two of the four datasets. 

table 2 depicts experiment results. compared to the best systems in semeval challenge, min achieves 3.0% and 1.1% absolute gains on d1 and d2 respectively. besides, our min outperforms wdemb, a strong crf-based system benefiting from several kinds of useful word embeddings, by 2.1% on d1. with memory interactions and consideration of sentimental sentence, our min boosts the performance of vanilla bi-directional lstm (+2.0% and +1.7% respectively). min also outperforms the state-of-the-art rncrf on each dataset suggesting that memory interactions can be an alternative strategy instead of syntactic parsing. 

table 4 shows the performance of the baseline. first, we observed that around 76% of gold concepts are covered by the extraction (step 1+2), while the top 25 concepts (step 5) only contain 17% of the gold concepts. second, while also 17% of gold concepts are contained in the final maps (step 6), scores for strict proposition matching are low, indicating a poor performance of the relation extraction (step 3). the propagation of these errors along the pipeline contributes to overall low scores. 

table 3 shows the model performances of the training set that is annotated once, and the training set of the merged annotation. furthermore, the difference is consistent with table 2: the "ab-merge" model has a similar precision as the "once" model, but it has a much higher recall. 

the results in table 4 demonstrate that compared to the baseline bigru, adding the reconstruction loss ("vsl-g, no vr") yields only 0.1 improvement for both twitter and ner. although adding hierarchical structure further improves performance, the improvements are small (+0.1 and +0.2 for twitter and ner respectively). for vsl-gg-hier, variational regularization accounts for relatively large differences of 0.6 for twitter and 0.5 for ner. 

table 6 shows the name tagging performance for each language using the original monolingual embeddings and multilingual embeddings. for all languages, the multilingual embeddings learned from our approach significantly outperform those learned from multicca and multicluster, which shows the effectiveness of our approach. more importantly, the multilingual embeddings learned from our approach also outperform original monolingual embeddings, which demonstrates that by projecting multiple languages into one common space, the monolingual embedding quality can be further improved. 

the experimental results of our model and baseline models on chinese-english machine translation datasets are depicted in table 2. rnnsearch, deliberation network and abdnmt have 83.99m, 125.16m and 122.86m parameters, respectively. and the parameter size of our {2,3,4,5}-pass decoder and adaptive multi-pass decoder are about 87.81m and 96.01m, respectively. {2,3,4,5}-pass decoders perform the left-to-right decoding by the multipass decoder with a fixed number of decoding passes. in contrast to the related machine translation systems, our fixed number-pass decoder significantly outperforms moses and rnnsearch by 7.53 and 1.05 bleu points at least, as table 2 presents. more importantly, our proposed multipass decoder obtains much better performance with an increase of only 3.82m parameters over rnnsearch. as a comparison with deliberation network involves two-pass decoding, the multipass decoder has a minimum increase of 0.24 bleu score. nevertheless, our multi-pass decoder proves its effectiveness due to the less parameters consumption of 37.35m in contrast to deliberation network. specifically, the multi-pass decoder with decoding depth 5 achieves the best performance with 38.64 bleu, while the one with decoding depth 3 performs the worst among the decoding depth set with 38.55 bleu. as shown in table 2, the proposed adaptive multi-pass decoder obtains an improvement about 0.41 to 0.5 bleu on average over the {2,3,4,5}-pass decoder, which demonstrates the effectiveness of the policy network. specifically, the adaptive multi-pass decoder outperforms the multi-pass decoder with a fixed decoding depth by 0.69, 0.71, 0.68 and 0.45 bleu scores on nist03, nist04, nist05 and nist06 datasets at most. in contrast to the moses, rnnsearch, deliberation network and abdnmt, the adaptive multi-pass decoder has the corresponding improvement about 8.03, 1.55, 0.74 and 0.34 bleu points, respectively. more importantly, our adaptive multi-pass decoder outperforms abdnmt, deliberation network model with a decrease of 26.85m, 29.15m parameters. 

table 1 shows the results. lm improves wordby-word baselines consistently in all four tasks, giving at least +3% bleu. when our denoising model is applied on top of it, we have additional gain around +3% bleu. note that our methods do not involve any decoding steps to generate pseudo-parallel training data, but still perform better than unsupervised mt systems that rely on repetitive back-translations (artetxe et al. 2018; lample et al. 2018) by up to +3.9% bleu. 

in table 2 we report the disambiguation accuracy on semtest, a new state-of-the-art result. in both the unsupervised and supervised disambiguation settings, the best performance is achieved by using all three features, v`, vr and vi. as summarized in table 2, our unsupervised method achieves a 2.4% improvement over stateof-art (hovy et al., 2011). denoting left, right and interplay features by `, r, i respectively, table 2 and 3 report our experimental results using only subset combinations of these features on the two disambiguation tasks. 

experimental results for relational triples, instanceof triples, and subclassof triples are shown in table 2, table 3, and table 4 respectively. in table 3 and table 4, a rising arrow means performance of this model have a promotion from yago39k to m-yago39k and a down arrow means a drop. transc can find a balance between them and all triples achieve a good performance. (2) on yago39k, transc outperforms other models in subclassof triple classification. (3) on m-yago39k, transc outperforms previous work in both instanceof triple classification and subclassof triple classification, which indicates that transc can handle the transitivity of isa relations much better than other models. (4) after comparing experimental results in yago39k and m-yago39k, we can find that most previous work’s performance suffers a big drop in instanceof triple classification and a small drop in subclassof triple classification. (5) in transc, nearly all performances have a significant promotion from yago39k to myago39k. both instanceof-subclassof transitivity and subclassof-subclassof transitivity are solved well in transc. 

in table 1 we also show recall, computed per-category and averaged over categories, at the last round of bootstrapping and similarly find the scatterplot outperforms the list. the recall performance in table 1 also confirms the performance gains of the scatterplot. 

table 3 lists the results of different taggers evaluated on npschat. based on the results, we can see that our method could achieve the best accuracy (94.0%), which was significantly better than 90.8% (forsyth, 2007). our method also outperformed the ark tagger, which applies various external corpus and features, e.g., brown clustering, ptb, freebase lists of celebrities, and video games. 

table 2 shows the translation results. it is clear that the proposed models significantly outperform the baselines in all cases, although there are considerable differences among different variations. the three baselines (rows 1, 2, and 4) differ regarding the training data used. “separate-recs⇒(+dps)” (row 3) is the best model reported in wang et al. (2018), which we employed as another strong baseline. the baseline trained on the dpp-annotated data (“baseline (+dpps)”, row 4) outperforms the other two counterparts, indicating that the error propagation problem does affect the performance of translating dps. using our shared reconstructor (row 5) not only outperforms the corresponding baseline (row 4), but also surpasses its separate reconstructor counterpart (row 3). introducing a joint prediction objective (row 6) can achieve a further improvement of +0.61 bleu points. among the variations of shared reconstructors (rows 6-8), we found that an interaction attention from encoder to decoder (row 7) achieves the best performance, which is +3.45 bleu points better than our baseline (row 4) and +1.45 bleu points better than the best result reported by wang et al. (2018) (row 3). 

table 1 shows the results on sentiment task. while all embedding models significantly improve over the baseline (‘none’), ρgcm-skip and bicvm perform the best. 

in table 4 we compare the p@10, r@10, and macro-f1 measures across all three groups (the union of s, f, and z) on the mimic iii dataset. we find that r@10 is nearly equivalent to the r@10 on the frequent group in table 3. furthermore, we find that acnn outperforms zagcnn in p@10 by almost 4%. however, we also find that acnn outperforms our methods with respect to macro-f1. 

the experimental results evaluated by automatic metrics on the opensubtitles and the reddit datasets are shown in table 1 and 2, respectively. it is observed that both cvae-based models and our proposed models outperform seq2seq by a large margin, showing the effectiveness of adding variational latent variable for response generation. our model with or without the sbow auxiliary loss outperforms cvae as observed by the significant boost in semantic relevance-oriented metrics (embedding similarities and ruber score) and diversity-oriented metrics. since adding the auxiliary loss could alleviate the model collapse problem, we found that cvae model with the bow auxiliary loss outperforms our basic model without auxiliary loss, especially on the diversity metrics. when adding the proposed sbow auxiliary loss into our model, we found that our generated responses have shown better diversity compared to those generated by cvae+bow loss. 

table 3 reports the results for individual tasks when additional tasks are included in the learning objective function. we observe that performance improves with each added task in the objective. for example, entity recognition (65.7) benefits from both coreference resolution (67.5) and relation extraction (66.8). relation extraction (37.9) significantly benefits when multi-tasked with coreference resolution (7.1% relative improvement). coreference resolution benefits when multitasked with relation extraction, with 4.9% relative improvement. 

results for token-level tasks are shown in table 4, whereas table 5 displays results for sentence-level tasks. expectedly, our neural architectures substantially outperform the traditional machine learning baselines on all tasks. for the three sentence-level tasks, the hierarchical architecture outperforms the simple model only when classifying sentences by summary relevance (src). 

table 3 reports the results of both automatic and human evaluations. we analyze the results from the following aspects. as illustrated in table 3, cvae outperforms all baselines significantly in terms of distinctness. with diversified terms, the aesthetics scores also confirm that cvae can generate poems that correspond to better user experiences. although mem-as2s can generate a rather high distinctness score, it requires a more complicated structure in learning and generating poems. recall that thematic consistency and term diversity are usually mutually exclusive, cvae produces the worst result in thematic consistency, which is confirmed in table 3 by the similarity score in automatic evaluation and the consistency score in human evaluation. following observations are made in this investigation, which confirm that adversarial learning is an effective add-on to existing models for thematics control, without affecting other aspects. when the discriminator is introduced, cvae and s2s model are capable of generating thematically consistent poems, as illustrated by the similarity and meaning scores in table 3. the bleu results also confirm that the discriminator can improve the overlapping between generated poems and the ground truth, which serves as thematic consistent cases. this is confirmed in the results, e.g., for distinctness, cvae-d and gan are comparable to cvae and s2s. overall, the cvae-d model substantially outperforms all other models in all metrics. 

the comparison of our method with various baselines and state-of-the-art methods is provided in table 2 for vqa 1.0 and table 3 for vqg-coco dataset. in both the tables, the first block consists of the current stateof-the-art methods on that dataset and the second contains the baselines. we observe that for the vqa dataset we achieve an improvement of 8% in bleu and 7% in meteor metric scores over the baselines, whereas for vqg-coco dataset this is 15% for both the metrics. we improve over the previous state-of-the-art (yang et al., 2015) for vqa dataset by around 6% in bleu score and 10% in meteor score. 

results on the test set are shown in table 5. our method much outperforms all the compared systems, which verifies the effectiveness of each component of our model. 

results table 2 reports the bleu scores on newstest2014 and newstest2017, the perplexity on the validation set, and the accuracy on long-range dependencies. transformer achieves the highest accuracy on this task and the highest bleu scores on both newstest2014 and newstest2017.". compared to rnns2s, convs2s has slightly better results regarding bleu scores, but a much lower accuracy on long-range dependencies. the rnn-bideep model achieves distinctly better bleu scores and a higher accuracy on long-range dependencies. however, it still cannot outperform transformers on any of the tasks. 

results table 5 gives the performance of all the architectures, including the perplexity on validation sets, the bleu scores on newstest, and the accuracy on contrawsd. transformers distinctly outperform rnns2s and convs2s models on de→en and de→fr. moreover, the transformer model on de→en also achieves higher accuracy than uedin-wmt17, although the bleu score on newstest2017 is 1.4 lower than uedin-wmt17. for de→en, rnns2s and convs2s have the same bleu score on newstest2014, convs2s has a higher score on newstest2017. however, the wsd accuracy of convs2s is 1.7% lower than rnns2s. for de→fr, convs2s achieves slightly better results on both bleu scores and accuracy than rnns2s. the transformer model strongly outperforms the other architectures on this wsd task, with a gap of 4–8 percentage points. the results (in table 5) show that transrnn performs better than rnns2s, but worse than the pure transformer, both in terms of bleu and wsd accuracy. 

table 3 shows the translation results. when sru is incorporated into the architecture, both the 4-layer and 5-layer model outperform the transformer base model. for instance, our 5layer model obtains an average improvement of 0.7 test bleu score and an improvement of 0.5 bleu score by comparing the best results of each model achieved across three runs. sru also exhibits more stable performance, with smaller variance over 3 runs. finally, adding sru does not affect the parallelization or speed of transformer â€“ the 4-layer model exhibits 10% speed improvement, while the 5-layer model is only 5% slower compared to the base model. 

table 4 shows that wme consistently matches or outperforms other unsupervised and supervised methods except the sif method. indeed, compared with st and nbow, wme improves pearson’s scores substantially by 10% to 33% as a result of the consideration of word alignments and the use of tf-idf weighting scheme. tf-idf also improves over these two methods but is slightly worse than our method, indicating the importance of taking into account the alignments between the words. sif method is a strong baseline for textual similarity tasks but wme still can beat it on sts’12 and achieve close performance in other cases. interestingly, wme is on a par with three supervised methods rnn, lstm(no), and lstm(o.g.) in most cases. the final remarks stem from the fact that, wme can gain significantly benefit from the supervised word embeddings similar to sif, both showing strong performance on psl. 

table 2 presents results for our binary classification task. the bert classifier has higher f1-score and precision than other classifiers. the bertsa model after three epochs only achieves an f1score of 0.491, which confirms the difference between sentiment analysis and our good/bad task, i.e., even if the segment has positive sentiment, it might be not suitable as a justification. 

table 4 shows the best fluency is achieved for enc-dec. 

table 3 presents the performance of our baselines. our best baseline (baseline 5) significantly outperforms the standard pipeline approach (baseline 1) in both the text and link evaluation. however, the performance of baseline 5 is well below the human performance. 

nll consists of the reconstruction loss (reconstruction) and the kl-divergence (kl-div.). as can be seen from table 1, with almost the same kl-divergence, dir-vhred achieves the lowest nll on both datasets, implying better performance compared with vhred. dir-vhred also has the lower reconstruction loss, which indicates that dirichlet prior is better than the gaussian prior for reconstructing the responses. 

table 2 lists the standard pearson correlation coefficients r between the system predictions and the sts gold-standard scores. we report the best scores achieved by paraphrastic embeddings (wieting et al., 2017) (text only) and the vse models in the previous section. our models achieve the best performance and the pre-trained word embeddings are preferred. 

table 1 shows the results. compared with wm18, our rgb model outperforms under all conditions. as we have stated, our model is a generalization of their approach. the more complex transformation matrix in our rgb model is able to learn more information, such as the effects of covariance between color channels, and thus achieves a better performance than wm18. note that our reimplementation of the original wm18 system lead to significantly better performance.5. according to the cosine similarity, the hsv model is superior for most test conditions (confirming our hypothesis about simpler modifier behaviour in this space). however for delta-e, the rgb model and ensemble perform better. accordingly the hsv model does worse under this metric, as it more directly models the direction of color modifiers, but as a consequence this leads to errors in its length predictions. 

table 2 shows the results for experiment 2 (using dundee gaze data and ptb dependencies). when looking at the dev set scores, the most discriminative gaze feature is mean fix dur that increases las by +0.17 and first fix dur by +0.14. on the other hand, evaluation on the test set shows that the most informative gaze features are from the context group learned as multiple auxiliary tasks (context feats aux) and they improve the las score by +0.18, followed by fix prob and w−1 fix prob with +0.13, total fix dur with +0.12 and late feats aux with +0.10. results from both datasets suggest that grouping gaze features and treating them as multiple auxiliary tasks can improve the model’s learning. 

table 6 shows results for one run of our best method across different numbers of labels per post (1 to 5). the best results are observed for values 2 to 4, suggesting that our approach performs better on multi-label samples. 

table 3 shows the results of the evaluation of the baseline models and our proposed method in dialog generation. in the evaluation of perplexity, transformer has much lower perplexity (18.0) compared to redial (28.1), and kbrd can reach the best performance in perplexity. this demonstrates the power of transformer in modeling natural language. in the evaluation of diversity, we find that the models based on transformer significantly outperform redial from the results of distinct 3-gram and 4-gram. besides, it can be found that kbrd has a clear advantage in diversity over the baseline transformer. this shows that our model can generate more diverse contents without decreasing fluency. 

table 1 report the results of automatic evaluation on the question response generation task. we can see that s2s-temp outperforms all baseline models in terms of all metrics, and the improvements are statistically significant (t-test with p-value< 0.01). the results demonstrate that when only limited pairs are available, s2stemp can effectively leverage unpaired data to enhance the quality of response generation. although lacking fine-grained check, from the comparison among s2s-temp-none, s2s-temp-50%, and s2s-temp, we can conclude that the performance of s2s-temp improves with more unpaired data. moreover, without unpaired data, our model is even worse than cvae since the structured templates cannot be accurately estimated from such a few data, and as long as half of the unpaired data are available, the model outperforms the baseline models on most metrics. s2s-temp is better than s2s-tempmle, indicating that the adversarial learning approach can indeed enhance the relevance of responses regarding to messages. 

table 2 shows performance comparison on the test set. compared to recommendation-only models, our prediction predict modules show significant improvements over the recommendation baselines on both per-turn and per-chat recommendations: 52% on turn@1 and 34% on turn@3. chat scores are always higher than turn, indicating that recommendations get better as more dialogue context is provided. the decide module yields additional improvements over the predict model in both generation and recommendation, with 67.6% decision accuracy, suggesting that the supervised signal of decisions to speak or recommend can contribute to better overall representations. in generation, our proposed models show comparable performance as the ir baseline models (e.g., bert-ranker). the +decide model improves on the f1 generation score because it learns when to predict the templated recommendation utterance. as expected, +plan slightly hurts most metrics of supervised evaluation, because it optimizes a different objective (the game objective), which might not systematically align with the supervised metrics. 

table 3 shows the test-set results. in general, the algorithms we introduce again outperform the nostruct baseline. in contrast to the crowdlabeled experiments, ap (slightly) outperformed the other algorithms.17 . 

table 8 shows the comparison of the bilstm-crf models with different feature embeddings. the results indicate that the semantic role feature is the most effective for negative focus detection, which confirms our analysis above. in addition, the performance of the system with all features is lower than that only using the semantic role feature. it might be that other features cannot capture more effective information than the semantic role features. 

table 4 reports our results on the multinli dataset. adin significantly outperforms esim, a strong baseline on the both test sets. an ensemble of adin models also achieve competitive result on the multinli dataset. 

table 3 reports the comparison of memory consumption and edges numbers between text-gcn and our model. results show that our model has a significant advantage in memory consumption. 

table 5 also contains an ablation study for the sumfocus in which the topic did make a significant difference. this suggests that it may be easier to use the topic in a symbolic model and a model may benefit from combining symbolic and continuous representations. 

table  4  summarizes  our  results  on  the  xsum dataset. extractive models here perform poorly as corroborated by the low performance of the lead baseline  (which  simply  selects  the  leading  sentence from the document), and the oracle (which selects a single-best sentence in each document) in table  4. the second block in table 4 presents the results of various abstractive models taken from narayan et al. (2018a) and also includes our own abstractive transformer baseline. in the third block we show the results of our bert summarizers which again are superior to all previously reported models (by a wide margin). 

table 3 shows the performance comparison between our work and previous work on the ontonotes english dataset. without the lstm layers (i.e., l = 0), the proposed model with dependency information significantly improves the ner performance with more than 2 points in f1 compared to the baseline bilstmcrf (l = 0), which demonstrate the effective ness of dependencies for the ner task. our best performing bilstm-crf baseline (with glove) achieves a f1 score of 87.78 which is better than or on par with previous works (chiu and nichols, 2016; li et al., 2017; ghaddar and langlais, 2018) with extra features. this baseline also outperforms the cnn-based models (strubell et al., 2017; li et al., 2017). the bilstm-gcn-crf model outperforms the bilstm-crf model but achieves inferior performance compared to the proposed dglstm-crf model. overall, the 2-layer dglstmcrf model significantly (with p < 0.01) outperforms the best bilstm-crf baseline and the bilstm-gcn-crf model. as we can see from the table, increasing the number of layers (e.g., l = 3) does not give us further improvements for both bilstm-crf and dglstm-crf because such third-order information (e.g., the relationship among a words parent, its grandparent, and greatgrandparent) does not play an important role in indicating the presence of named entities. 

table 8 shows the performance (las) of the dependency parser on four languages (i.e., ontonotes english, ontonotes chinese, catalan and spanish) and the performance of dglstm-crf against the best performing bilstm-crf with elmo. dglstmcrf even with predicted dependencies is able to consistently outperform the bilstm-crf on four languages. however, the performance is still worse than the dglstm-crf with gold dependencies, especially on the catalan and spanish. 

table 2 shows the imdb test-set results of our best rationale-biased methods and their non-rationale counterparts (chosen based on dev set performance), as well as all of the baselines. our bag-of-words models, rb-bow-proto and rb-bow-svm, which combine rationale-biasing with pre-trained word embeddings, outperform all other baselines by a large margin of up to more than 20 absolute accuracy points for training sets of size 20 or smaller. on these particularly small datasets, the other rationale-aware baselines, rasvm and ra-cnn, do not perform as well. for larger training sets of size 60 and more, all systems perform better, as expected. in particular, the more complex cnn and bert-based models have significantly improved performance with more supervision. both ra-cnn and our rb-bert, which combine pre-training with rationales, benefit substantially from rationale supervision, with up to more than 30 absolute accuracy points improvement. in absolute terms, our bert models outperform their cnn counterparts and every other baseline in this training size range. 

the experiment results are listed in table 3. we can see that our proposed induction networks achieves best classification performances on all of the four experiments. our model also outperforms the latest optimization-based method-snail. 

table 2 illustrates the bleu scores of flowseq and baselines with advanced decoding methods such as iterative refinement, iwd and npd rescoring. the first block in table 2 includes the baseline results from autoregressive transformer. for flowseq, npd obtains better results than iwd, showing that flowseq still falls behind auto-regressive transformer on model data distributions. comparing with cmlm (ghazvininejad et al., 2019) with 10 iterations of refinement, which is a contemporaneous work that achieves state-of-the-art translation performance, flowseq obtains competitive performance on both wmt2014 and wmt2016 corpora, with only slight degradation in translation quality. 

table 2 depicts the outcome of the evaluation of the combined model. while edam performs lower than the state-ofthe-art (f1-score 0.76 vs 0.69), from existing approaches edam provides the only explainable model. 

table 7 presents the mfc, lstm, and elmo results for each domain, on the subset of 15,000 conversations annotated at both the turn and sentence levels. in general for both granularities turn and sentence, both lstm, and elmo outperform mfc significantly across all domains. relative to the lstm, we find that elmo obtains a modest increase in ic accuracy of 0.41 to 2.20 f1 points and a significant increase in sl f1 score on all domains. 

in table 2 we report the results, that we compute as the average of ten runs with random parameter initialization. ling+random never improves over ling. we find that both pv and n2v user representations lead to an improvement over ling. n2v vectors are especially effective for the stance detection task, where ling+n2v outperforms ling+pv, while for hate speech the performance

table 3 summarizes the average results over 10 runs, where the top 2 results are highlighted in bold. the baseline results on snips-nlu are taken from xia et al. (2018). the results show that recapsnet-zs outperforms all the baselines, demonstrating its superiority in tackling zero-shot intent classification. we can also see that recapsnet-zs performs better than either recapsnet-zs-dim or recapsnetzs-tm, which shows the effectiveness of both of the dimensional attention mechanism and the transformation matrix construction method. 

table 7 shows the accuracy scores over different types of attacks in the sst-2 dataset. disp and dispg denotes the recovery performance with our estimator and goundtruth tokens, respectively. more specifically, the accuracy of dispg presents the upperbound performance gained by the embedding estimator. the experimental results demonstrate the robustness of the embedding estimator while the estimated embeddings only slightly lower the accuracy of disp. 

we first analyse the performance of our main model (eog) using different pre-trained word embeddings. table 3 shows the performance difference between domain-specific (pubmed) (chiu et al., 2016), general-domain (glove) (pennington et al., 2014) and randomly initialized (random) word embeddings. as observed, our proposed model performs consistently with both in-domain and out-of-domain pre-trained word embeddings. the low performance of random embeddings is due to the small size of the dataset, which results in lower quality embeddings. 

as shown in table2, cvae(qt) incorporates question type information, and slightly improves the performance compared with cvae, because it could help generate questions with reasonable type. furthermore, answer information could further improve the overall performance. first, rl-cvae and a-cvae perform fairly well with lower perplexities, because they could generate fluent questions. second, they can obtain higher rubg and ruba scores, showing that taking advantage of coherence between question and answer could further enhance semantic coherence between post and question. our rl-cvae performs better than a-cvae. 

table 3 shows that in both hard-label chain crf and soft-label chain crf, smoothing decoding gives a prediction accuracy 0.04% higher than viterbi decoding. 

table 1 shows that hybrid ori significantly decreases the difference of association between two genders as indicated by mweat_diff and p-value. other methods only show marginal mitigation effects in terms of p-value. we also find that the performance of our mitigation methods on word similarity is largely preserved as indicated by the pearson correlation scores. 

as shown in table 2, there are three variants with ablation: “– phase i” takes out splitnet and performs phase ii on word level; “– phase ii” performs random guess in the recombination process for testing; and “– rl” only contains pre-training. the symacc drops from about 55% to 40% by ablating phase i, and to 23% by ablating phase ii. their poor performances indicate both of the two phases are indispensable. “– rl” also performs worse, which again demonstrates the rationality of applying rl. as shown in table 2 and figure 4, star learns better and faster than the variants due to the reasonable reward design. 

table 4 shows the results. cbow does not perform well on either datasets. on the cb data, the heuristics based on linguistic generalizations is a strong baseline, performing better than mnlib. we gain a lot of performance with supervision from cb only (cbb), which aligns with mccoy et al.’s observation that bert performs very well when trained with in-domain data. the best results are obtained by mnli+cbb on cb and by mnlib on multinli, but still lag behind human performance. while mnli+cbb gives the best performance on cb, it does not perform well on multinli. this is in line with liu et al. (2019) who found that fine-tuning on datasets that test for a specific linguistic phenomenon decrease the performance on the original dataset. 

experimental results are reported in table 3. from table 3 we draw the following conclusions. first, sentence-level sentiment classifiers (textcnn and lstm) achieve competitive results on semeval14 restaurant review dataset but perform poorly on mams datasets. this verifies that mams datasets can alleviate the task degeneration problem encountered in restaurant dataset for absa. second, most advanced and complex abas methods, which achieve impressive results on restaurant dataset, perform poorly on the mams-small dataset. this verifies that mams (small) is more challenging than semeval-14 restaurant review dataset. third, attention based models without effectively modeling word order (e.g., memnet and aen) perform worst on mams since they lose word order information and cannot identify which part of context describes the given aspect. fourth, capsnet outperforms non-bert baselines on 4 of 6 datasets, showing the potential of applying capsule networks to aspect-based sentiment analysis task. in addition, capsnet-bert performs significantly better than other models including bert, indicating that combining capsule network and bert can obtain additional improvement compared to vanilla bert. 

finally, table 6 shows results for nominal and verbal predicates as well as for different (gold) role labels. in comparison to mate-tools, we can see that pathlstm improves precision for all argument types of nominal predicates. for verbal predicates, improvements can be observed in terms of recall of proto-agent (a0) and protopatient (a1) roles, with slight gains in precision for the a2 role. overall, pathlstm does slightly worse with respect to modifier roles, which it labels with higher precision but at the cost of recall. 

table 1 summarizes the knowledge base (kb) completion results on the nci-pid test. the rows compare our compositional learning approach all-paths+nodes with prior approaches. the comparison of the two columns demonstrates the impact when text is jointly embedded with kb. our compositional learning approach significantly outperforms all other approaches in both evaluation metrics (map and hits@10). moreover, jointly embedding text and kb led to substantial improvement, compared to embedding kb only. finally, modeling nodes in the paths offers significant gains (allpaths+node gains 3 points in map over allpaths). 

table 8 shows the results on english-german translation. our approach still significantly outperforms mle and achieves comparable results with state-of-the-art systems even though luong et al. (2015a) used a much deeper neural network. 

table 2 shows the accuracies of all baseline systems on the development sets. for wsj all three algorithms achieve similar results which shows that marmot is a competitive baseline. 

table 5 shows the mrr on the ret task for code-nn and ret-ir, averaged over 20 runs for c# and sql. code-nn outperforms the baseline by about 16% for c# and sql. 

the phonological features are important separation criteria as evidenced by the drop in performance when they are excluded from the experimental setup (table 5). specifically, using all features except phonological features is equivalent to using phonological features alone (about f = 79% in both cases) and slightly worse that using all name-intrinsic features (about f = 80%). 

table 4 shows the results obtained by different rnns and the window based neural network (nn). in this case rnn models are giving better results than the nn model for both the tasks. in particular performance of bi-lstm models are best than others in both the tasks. we observe that for the task a, rnn models obtained 1.2% to 3% improvement in f1-score than the baseline nn performance. similarly 2.55% to 4% improvement in f1-score are observed for the task b, with bi-lstm model obtaining more than 4% improvement. 

in order to compare performance, the averaged results obtained between 400-500 training dialogues are shown in the first section of table 1 along with one standard error. for the 400-500 interval, the subj, off-line rnn and on-line gp systems achieved comparable results without statistical differences. the results of continuing training on the subj and on-line gp systems from 500 to 850 training dialogues are also shown. 

table 4 summarizes the chinese dependency parsing results. again, our work is competitive with the state-of-the-art greedy parsers. 

table 4 presents our evaluation results on both development and test sets. our system (single model) can achieve 70.0% exact match and 79.0% f1 scores on the test set, which surpasses all the published results and can match the top performance on the squad leaderboard at the time of writing. 

table 2 shows the average performance on multi-source parsing by combining 3 to 4 languages for geo and 2 to 3 languages for atis. indeed, we observe that system combination at the model level is able to give better performance on average (up to 4.29% on geo) than doing so at the output level. combining at the word level and sentence level shows comparable performance on both datasets. it can be seen that the benefit is more apparent when we include english in the system combination. 

as shown in table 1, for the resource-rich setting, the single models (bpe2bpe, bpe2tree) perform similarly in terms of bleu on newstest2015. on newstest2016 we witness an advantage to the bpe2tree model. 

results in table 3 show that using dswe gains significant improvements (one-tailed t-test with p<0.05) over using glove or word2vec, on both accuracy and macro f1. furthermore, using dswe achieves better performance across all relations on the f1 score, especially for minority relations (temp, comp and cont). overall, our dswe can effectively incorporate discourse information in explicit data, and thus benefits implicit discourse relation recognition. 

results in table 4 show the superiority of our method. although liu2016 performs slightly better on macro f1, it uses the additional labeled rst-dt corpus. overall, our method can effectively utilize massive explicit data, and thus is more powerful than baselines. 

table 3 shows the averaged results from the two analysts, clearly our chatbot has a better performance (better on 37.64% of the 878 questions and worse on 18.84%). 

to analyze the benefit of our proposed word embedding variant, table 2 shows the results that were obtained when we use standard word embedding models. in particular, we show results for the standard glove model, skipgram and the continuous bag of words (cbow) model. as can be observed, our variant leads to better results than the original glove model, even for the baselines. the difference is particularly noticeable for diffvec. 

table 1 shows the performance of swap-net, state-of-the-art baselines nn and summarunner and other baselines, using rouge recall with summary length of 75 bytes, on the entire daily mail test set. the performance of swap-net is comparable to that of summarunner and better than nn and other baselines. 

table 5 lists the experimental results. our models (gen and gen+adv) outperformed the previous models. furthermore, the proposed model with adversarial training (gen+adv) was significantly better than the supervised model (gen). 

table 3 shows performance comparisons of our ilp systems with other event coreference resolution approaches including the recent joint learning approach (lu and ng, 2017) which is the best performing model on the kbp 2016 corpus. for both datasets, the full discourse structure augmented model achieved superior performance compared to the local classifier based system. the improvement is observed across all metrics with average f1 gain of 3.1 for kbp 2016 and 2.17 for kbp 2017. our ilp based system also outperforms the previous best model on the kbp 2016 corpus (lu and ng, 2017) consistently using all the evaluation metrics, with an overall improvement of 1.21 based on the average f1 scores. in table 3, we also report the f1 scores when we increasingly add each type of structure in the ilp baseline. among different scoring metrics, all structures positively contributed to the muc and blanc scores for kbp 2016 corpus. however, subevent based constraints slightly reduced the f1 scores on kbp 2017 corpus. 

table 3 shows the result on the development set. we found that, the path embedding was effective for pa, and the string match was effective for cr. the sentence distance for both cr and pa was effective for news, but not for web since the web evaluation corpus consists of three-sentence documents. 

table 5 shows the evaluation results on the conll-2014 dataset. without using the nonpublic training data from lang-8.com, our single model obtains 50.04 f0.5, larlgely outperforming the other seq2seq models and only inferior to camb17 (amu16 based) and nus17. when we build our approach on top of amu16 (i.e., we take amu16fs outputs as the input to our gec system to edit on top of its outputs), we achieve 53.30 f0.5 score. with introducing the non-public training data, our single and ensemble system obtain 52.72 and 54.51 f0.5 score respectively, which is a stateof-the-art result7 on conll-2014 dataset. 

the first two columns of table 3 show the mrr results under exact match and partial match conditions. the first 3 rows show the results for the baseline systems, and the remaining rows show results for our activity profile (ap) semi-supervised learning method. we show results for 5 variations of the algorithm: ap uses algorithm 1, and the others use algorithm 2 with different activity similarity measures: ap+al (location profile similarity), ap+ao (overlap similarity), ap+ae (embedding similarity), and ap+al+e (location profiles plus embeddings). table 3 shows that our ap algorithm outperforms all 3 baseline methods. when adding activity similarity into the algorithm, we find that al slightly improves performance, but ao and ae do not. however, we also tried combining them and obtained improved results by using al and ae together, yielding an mrrp score of 0.42. to gain more insight about the behavior of the models, table 3 also shows results for the topranked 1, 2, and 3 answers. these results show that our ap method produces more correct answers at the top of the list than the baseline methods. 

table 6 provides averages of the classification and similarity results, along with the results of selected tasks (snli, sick-e). the de models are generally worse, most likely due to the higher oov rate and overall simplicity of the training sentences. on cs, we see a clear pattern that more heads hurt the performance. the de set has more variations to consider but the results are less conclusive. for the similarity results, it is worth noting that cs-attn-attn performs very well with 1 attention head but fails miserably with more heads. otherwise, the relation to the number of heads is less clear. table 6 also provides our measurements based on sentence paraphrases. for paraphrase retrieval (nn), we found cosine distance to work better than l2 distance. this evaluation seems less stable and discerning than the previous two, but we can again confirm the victory of infersent followed by our nonattentive cs models. cs and de models are no longer clearly separated. 

table 4 shows these calculations for the 15 relation types for which the performance gap was highest and which had at least 15 instances in each of the aa and wh tweet sets, along with the corresponding calculation under the ark tagger model. the amount by which the performance gap is reduced from the first setting to the second setting is also reported. of the 15 relations shown, the gap was reduced for 14, and 7 saw a reduction of at least 10%. 

table 3 shows the performance of the baseline, which is blstm-crf with sentences as input only, and our proposed models on both datasets. our final model blstm-crf + visual attention + gate, which has visual attention component and modulation gate, obtains the best f1 scores on both datasets. all the models get better scores on twitter dataset than on snap dataset, because the average length of the sentences in snap dataset (8.1 tokens) is much smaller than that of twitter dataset (16.0 tokens), which means there is much less contextual information in snap dataset. 

table 4 shows development results, including model ablation studies. removing previous instructions (-previous instructions) or both states (-current and initial state) reduces performance across all domains. removing only the initial state (-initial state) or the current state (-current state) shows mixed results across the domains. providing access to both initial and current states increases performance for alchemy, but reduces performance on the other domains. in our development and test results we use a single architecture across the three domains, the full approach, which has the highest interactive-level accuracy when averaged across the three domains (62.7 5utts). 

table 4 demonstrates the improvement of the joint framework over individual components. the temporal only baseline is the improved temporal extraction system for which the joint inference with causal links has not been applied. the causal only baseline is to use s c (?) alone for the prediction of each pair. note that the causal accuracy column in table 4 was evaluated only on gold causal pairs. in the proposed joint system, the temporal and causal scores were added up for all event pairs. the temporal performance got strictly better in precision, recall, and f1, and the causal performance also got improved by a large margin from 70.5% to 77.3%, indicating that temporal signals and causal signals are helpful to each other. the second part of table 4 shows that if gold relations were used, how well each component would possibly perform. when using gold temporal relations, causal accuracy went up to 91.9%. first, this number is much higher than the 77.3% on line 3, so there is still room for improvement. 

table 3 shows the results of the analogy test using our reddit20m in comparison with google300d, which is the google news embedding used in previous sections. as one can expect, reddit20m shows worse performance than google300d. however, the four categories (capital-common-countries, capital-world, currency, and city-in-state denoted by world), which require some general knowledge on the world, drive the 10% decrease in overall accuracy. other categories show comparable or even better accuracy. for example, reddit20m outperforms google300d by 4.52% in the family category. by contrast, for the categories for testing grammar (denoted by gram1-9), reddit20m shows comparable performances with google300d (70.21 vs. 73.4). 

table 3 shows the character error rates (cers), where the baseline model showed 2.6% on wsj and 7.8% on csj. 

table 4 shows the results. our models tend to produce significantly better results than the winners of the conll 2017 shared task (i.e., 1.8% absolute improvement on average, corresponding to a rrie of 21.20%). the only cases for which this is not true are again languages that require significant segmentation efforts (i.e., hebrew, chinese, vietnamese and japanese) or when the task was trivial. as expected, our reimplementation of dozat et al. (2017) tends to significantly outperform the winners of the conll 2017 shared task. however, in general, our models still obtain better results, outperforming dozat et al. on 43 of the 54 treebanks, with an absolute difference of 0.42% on average. 

table 2 shows the performance obtained by the different models with and without pos tags. overall, our attention model wp outperforms all other models in 10 out of 14 scenarios (combinations of datasets and whether or not pos tags are used). importantly, our model outperforms the regular lstm model without introducing additional parameters to the model, which highlights the advantage of wpfs attention-based pooling method. for all models listed in table 2, we find that including pos tags benefits the detection of adverbial presupposition triggers in gigaword and ptb datasets. 

table 2 presents meteor (denkowski and lavie, 2014) and rouge-l (lin, 2004) scores for each method, where we can see score gains on both metrics from the editing mechanism. table 2 shows that the human judges strongly favor the abstracts from our ed(2) method. 

table 1 shows the automatic and manual evaluation results for both the baseline and our models. in manual evaluation, among baselines, wizard transformer and rnn without knowledge have the highest fluency of 1.62 and wizard obtains the highest knowledge relevance of 0.47 while transformer without knowledge gets the highest context coherence of 0.67. for all models, ite+ckad obtains the highest fluency of 1.68 and ite+dd has the highest knowledge relevance of 0.56 and highest context coherence of 0.90. in automatic evaluation, our proposed model has lower perplexity and higher bleu scores than baselines. for bleu, hred with knowledge obtains the highest bleu score of 0.77 among the baselines. and ite+dd gets 0.95 bleu score, which is the highest among all the models. for perplexity, wizard transformer obtains the lowest perplexity of 70.30 among baseline models and ite+dd has remarkably lower perplexity of 15.11 than all the other models. 

table 1 shows the accuracy of all the parsers. comparing the simple and extended architectures we see that dropping the structural features does not hurt the performance, neither for transition-based nor graph-based parsers. 

table 3 shows the comparison results. from table 3, we observe that imn−d is able to significantly outperform other baselines on f1-i. imn further boosts the performance and outperforms the best f1-i results from the baselines by 2.29%, 1.77%, and 2.61% on d1, d2, and d3. specifically, for ae (f1-a and f1-o), imn -d performs the best in most cases. for as (acc-s and f1-s), imn outperforms other methods by large margins. pipeline, imn -d, and the pipeline methods with dtrans also perform reasonably well on this task, outperforming other baselines by moderate margins. we also show the results of imn -d and imn when only the general-purpose embeddings (without domain-specific embeddings) are used for initialization. they are denoted as imn -d/imn wo de. imn wo de performs only marginally below imn. this indicates that the knowledge captured by domain-specific embeddings could be similar to that captured by joint training of the imn−d is more affected document-level tasks.  imn ?d is  more  affected without domain-specific embeddings, while it still outperforms  all  other  baselines  except  decnn-dtrans. decnn-dtrans  is  a  very  strong  base-line as it exploits additional knowledge from larger corpora for both tasks.  imn?dwo de is compet-itive with decnn-dtrans even without utilizing additional  knowledge,  which  suggests  the  effectiveness of the proposed network structure. 

table 3 shows the ofever scores of our model and models from other teams. after running the same model proposed by hanselowski et al. (2018), we find our ofever score is slightly lower, which may due to the random factors. 

table 2 shows evaluation results of our proposed approaches along with existing supervised and unsupervised alternatives. this explains why they achieve higher bleu and reduced worddifference scores. the st system did not converge for our dataset after significant number of epochs which affected the performance metrics. other supervised systems such as sbmt and nts achieve better content reduction as shown through sari, bleu and fe-diff scores; this is expected. however, it is still a good sign that the scores for the unsupervised system unts are not far from the supervised skylines. the higher word-diff scores for the unsupervised system also indicate that it is able to perform content reduction (a form of syntactic simplification), which is crucial to ts. finally, it is worth noting that aiding the system with a very small amount of labeled data can also benefit our unsupervised pipeline, as suggested by the scores for the unts+10k system. 

table 1 shows the unlabeled f1 scores for our models and various baselines. all models soundly outperform right branching baselines, and we find that the neural pcfg/compound pcfg are strong models for grammar induction. in particular the compound pcfg outperforms other models by an appreciable margin on both english and chinese. 

table 2 reports results on the baseline model and our models under different beta. nist 06 is used as dev set to select the best models, and nist 2002, 2003, 2004, 2005 and 2008 datasets are used as test sets. there are some interesting observations. first, combing textual and phonetic information improves the performance of translation. second, the phonetic information plays a very important role in translation. even when beta = 0.95, that is, most weights are put on phonetic embedding, the performance is still very good. in fact, our best bleu score (48.91), is achieved when beta = 0.95. however, word embedding is still important. in fact, when we use only phonetic information (beta = 1.0), the performance degrades, almost the same as baseline (only using textual information). 

table 4 presents the performance of retrained embeddings and a random embedding baseline on magnitude and numeration tests. there is no significant difference in performance between num and all variants, suggesting that seeing more numerals during training does not necessarily result in better representations. all models capture an approximate notion of magnitude (high performance on bc-mag), but do not capture numeration. across models, fasttext variants fare best. 

table 6 shows hate speech detection results. training with only synthetic text after thresholding and stratified sampling outperforms training with only gold-tagged text by 4% f1, and using both gold and synthetic text gives a f1 boost of 6% beyond using gold alone. remarkably, synthetic text alone outperforms gold text, because gold text has high class imbalance, leading to poorer prediction. 

table 3 shows the performances of different approaches to asc-qa. from this table, we can see that all the three state-of-the-art asc approaches, i.e., ram, gcae and s-lstm, perform better than lstm. besides, both the attention based approaches ram and s-lstm achieve comparable or better performance than gcae. the two qa matching approaches, i.e., bidaf and hmn could achieve comparable performance with the three state-of-the-art asc approaches, and mamc even beats all of them. furthermore, our rban w/o raws approach (i.e., without considering aspect information) performs consistently better than mamc. besides, itâ€™s interesting to notice that rban w/o a2q (i.e., without question vector sq) performs much better than rban w/o q2a (i.e., without answer vector sa). 

we use two datasets consisting of (question, logical form) pairs: webquestionssp (yih et al., 2016) (a standard dataset for semantic parsing, where the logical form is in sparql), and wikisql (zhong et al., 2017) (where the logical form is sql). we evaluate metrics at both a corpus level (to indicate how natural output questions are) and at a per-sentence level (to demonstrate how well output questions exactly match the gold question). accordingly, we compare our scratchpad mechanism against three baselines: (1) seq2seq, (2) copynet and (3) coverage, a method introduced by tu et al. (2016) that aims to solve attention-related problems. from table 2 it is clear that our approach, scratchpad outperforms all baselines on all the metrics. 

the evaluation was performed in the ehealthkd corpus (piad-morffis et al., 2019a), using the training and development collections for training and the test collection for evaluation. after 60 generations, the best performing pipeline (actually found in generation 18) achieved a f1 score of 0.754 in scenario 1 of the ehealth-kd challenge. this represents a 1% absolute improvement from the top result presented in the ehealth-kd challenge, and a 4.2% absolute improvement over the average result of the top 3 alternatives (f1 = 0.711). table 1 shows this result in a comparison with the rest of the approaches presented in section 3. 

table 7 reports p@1, p@3, p@5, and p@10 for each leaderboard (i.e., tdm triple). the macro average p@1 and p@3 are 0.70 and 0.67, respectively, which is encouraging. 

table 3 shows that the system obtains superior results in the hate speech dataset and yields competitive results on the kaggle data in comparison to some sate-of-the-art baseline systems. 

table 3 compares the performance of our embedding with word2gm. we notice that word2sense embeddings with µ = k(cid:48) (denoted word2sense-full in the table), i.e., with no truncation, yields the best results. 

table 5 shows that w ord2sense is competitive with the best interpretable embeddings. 

table 6 shows deep transfer active learning performance in dblp-acm with varying sampling strategies. we can observe that high-confidence sampling and the partition mechanism contribute to high and stable performance as well as good precision-recall balance. notice that there is a huge jump in recall by adding partition while precision stays the same (row 4 to row 3). 

table 6 presents the average ser rates for each model, where lower rates mean fewer mistakes (indicated by ?). amazingly, overall, table 6 results show the ser is extremely low, even while achieving a large amount of stylistic variation. naturally, base, with no access to style information, has the best (lowest) ser. but we note that there is not a large increase in ser as more information is added – even for the most difficult setting, +style, the models make an error on less than 10% of the slots in a given mr, on average. 

table 1 (top) presents results of models trained and tested on the synthetic multi-aspect dataset. all aspect-aware models beat both baselines by a large margin. for classical summarization, the lead-3 baseline remains a challenge to beat even by state-of-the-art systems, and also on multiaspect documents we observe that, unlike our systems, pg-net performs worse than lead-3. unsurprisingly, the extractive aspect-aware models outperform their abstractive counterparts in terms of rouge, and the decoder attention distributions are more amenable to extraction than encoder attention scores. overall, our structured models enable both abstractive and extractive aspectaware summarization at a quality clearly exceeding structure-agnostic baselines. 

table 4 summarizes the experimental results. the bigru model beats the other models with a micro-averaged f1 score of 80.16%, and the gru-capsule model performs the best with a macro-averaged f1 score of 64.71%. the rnn-based models outperform the cnn-based models in both the general nn framework and the capsule network framework. the results account for the importance of the order of the context in market comments when inserting numerical information. further evidence supporting this statement is that the crnn model obtains a higher performance than the cnn model does. 

cnn-sc achieves an accuracy of 90.0%, outperforming cnn-r by a large margin. additionally, sentence content is four times as fast to train as the computationally-expensive reconstruction objective. the second row of table 3 shows that both approaches suffer a drop in downstream accuracy when pre-trained on out-of-domain data. interestingly, cnn-sc still performs best, indicating that sentence content is more suitable for downstream classification. 

table 4 shows the comparison between our model and state-of-the-art visual dialog models. by using a diverse set of ensembles, redan+ outperforms the state of the art method, dan (kottur et al., 2018), by a significant margin, lifting ndcg from 59.36% to 64.47%. 

