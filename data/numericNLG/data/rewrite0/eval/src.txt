table 4 shows link prediction results on the test-i, test-ii, and test-all sets of fb122 and wn18 (raw setting). mrr of transh 0.220 is 0.218. med of transh 29.0 is 29.0. hits@3% of transh 25.7 is 25.0. hits@5% of transh 32.4 is 31.3. hits@10% of transh 40.6 is 39.2. mrr of transh 0.296 is 0.297. med of transh 5.0 is 6.0. hits@3% of transh 40.0 is 37.5. hits@5% of transh 50.8 is 48.5. hits@10% of transh 57.8 is 56.3. mrr of transh 0.262 is 0.249. med of transh 10.0 is 12.0. hits@3% of transh 33.6 is 31.9. hits@5% of transh 42.5 is 40.7. hits@10% of transh 50.0 is 48.6. mrr of transr 0.220 is 0.219. med of transr 29.0 is 31.0. hits@3% of transr 25.7 is 24.7. hits@5% of transr 32.4 is 30.8. hits@10% of transr 40.6 is 38.9. mrr of transr 0.296 is 0.273. med of transr 5.0 is 9.0. hits@3% of transr 40.0 is 32.4. hits@5% of transr 50.8 is 42.8. hits@10% of transr 57.8 is 51.6. mrr of transr 0.262 is 0.261. med of transr 10.0 is 15.0. hits@3% of transr 33.6 is 28.9. hits@5% of transr 42.5 is 37.4. hits@10% of transr 50.0 is 45.9. mrr of kale-trip 0.220 is 0.201. med of kale-trip 29.0 is 25.0. hits@3% of kale-trip 25.7 is 23.9. hits@5% of kale-trip 32.4 is 31.6. hits@10% of kale-trip 40.6 is 40.1. mrr of kale-trip 0.296 is 0.309. med of kale-trip 5.0 is 5.0. hits@3% of kale-trip 40.0 is 40.9. hits@5% of kale-trip 50.8 is 51.3. hits@10% of kale-trip 57.8 is 58.0. mrr of kale-trip 0.262 is 0.261. med of kale-trip 10.0 is 11.0. hits@3% of kale-trip 33.6 is 33.3. hits@5% of kale-trip 42.5 is 42.4. hits@10% of kale-trip 50.0 is 50.0. mrr of kale-pre 0.220 is 0.203. med of kale-pre 29.0 is 25.0. hits@3% of kale-pre 25.7 is 24.1. hits@5% of kale-pre 32.4 is 31.7. hits@10% of kale-pre 40.6 is 40.2. mrr of kale-pre 0.296 is 0.368. med of kale-pre 5.0 is 4.0. hits@3% of kale-pre 40.0 is 47.3. hits@5% of kale-pre 50.8 is 55.4. hits@10% of kale-pre 57.8 is 61.4. mrr of kale-pre 0.262 is 0.294. med of kale-pre 10.0 is 9.0. hits@3% of kale-pre 33.6 is 36.9. hits@5% of kale-pre 42.5 is 44.8. hits@10% of kale-pre 50.0 is 51.9. mrr of kale-joint 0.220 is 0.229. med of kale-joint 29.0 is 21.0. hits@3% of kale-joint 25.7 is 26.3. hits@5% of kale-joint 32.4 is 33.8. hits@10% of kale-joint 40.6 is 42.2. mrr of kale-joint 0.296 is 0.357. med of kale-joint 5.0 is 4.0. hits@3% of kale-joint 40.0 is 44.0. hits@5% of kale-joint 50.8 is 53.0. hits@10% of kale-joint 57.8 is 59.3. mrr of kale-joint 0.262 is 0.299. med of kale-joint 10.0 is 9.0. hits@3% of kale-joint 33.6 is 36.1. hits@5% of kale-joint 42.5 is 44.3. hits@10% of kale-joint 50.0 is 51.6. mrr of transe 0.220 is 0.248. med of transe 29.0 is 4.0. hits@3% of transe 25.7 is 40.9. hits@5% of transe 32.4 is 60.6. hits@10% of transe 40.6 is 77.0. mrr of transe 0.296 is 0.363. med of transe 5.0 is 3.0. hits@3% of transe 40.0 is 59.4. hits@5% of transe 50.8 is 70.8. hits@10% of transe 57.8 is 81.4. mrr of transe 0.262 is 0.331. med of transe 10.0 is 3.0. hits@3% of transe 33.6 is 54.3. hits@5% of transe 42.5 is 67.9. hits@10% of transe 50.0 is 80.2. mrr of transh 0.220 is 0.242. med of transh 29.0 is 4.0. hits@3% of transh 25.7 is 39.2. hits@5% of transh 32.4 is 60.1. hits@10% of transh 40.6 is 75.9. mrr of transh 0.296 is 0.482. med of transh 5.0 is 2.0. hits@3% of transh 40.0 is 63.5. hits@5% of transh 50.8 is 70.8. hits@10% of transh 57.8 is 79.3. mrr of transh 0.262 is 0.415. med of transh 10.0 is 3.0. hits@3% of transh 33.6 is 56.7. hits@5% of transh 42.5 is 67.8. hits@10% of transh 50.0 is 78.3. mrr of transr 0.220 is 0.240. med of transr 29.0 is 4.0. hits@3% of transr 25.7 is 40.1. hits@5% of transr 32.4 is 57.7. hits@10% of transr 40.6 is 71.6. mrr of transr 0.296 is 0.449. med of transr 5.0 is 3.0. hits@3% of transr 40.0 is 55.7. hits@5% of transr 50.8 is 64.5. hits@10% of transr 57.8 is 74.3. mrr of transr 0.262 is 0.391. med of transr 10.0 is 3.0. hits@3% of transr 33.6 is 51.3. hits@5% of transr 42.5 is 62.6. hits@10% of transr 50.0 is 73.5. mrr of kale-trip 0.220 is 0.250. med of kale-trip 29.0 is 4.0. hits@3% of kale-trip 25.7 is 40.6. hits@5% of kale-trip 32.4 is 62.3. hits@10% of kale-trip 40.6 is 78.1. mrr of kale-trip 0.296 is 0.393. med of kale-trip 5.0 is 2.0. hits@3% of kale-trip 40.0 is 61.9. hits@5% of kale-trip 50.8 is 71.2. hits@10% of kale-trip 57.8 is 80.6. mrr of kale-trip 0.262 is 0.353. med of kale-trip 10.0 is 3.0. hits@3% of kale-trip 33.6 is 56.0. hits@5% of kale-trip 42.5 is 68.7. hits@10% of kale-trip 50.0 is 79.9. mrr of kale-pre 0.220 is 0.248. med of kale-pre 29.0 is 4.0. hits@3% of kale-pre 25.7 is 40.4. hits@5% of kale-pre 32.4 is 61.5. hits@10% of kale-pre 40.6 is 78.2. mrr of kale-pre 0.296 is 0.451. med of kale-pre 5.0 is 3.0. hits@3% of kale-pre 40.0 is 69.6. hits@5% of kale-pre 50.8 is 77.5. hits@10% of kale-pre 57.8 is 85.3. mrr of kale-pre 0.262 is 0.395. med of kale-pre 10.0 is 3.0. hits@3% of kale-pre 33.6 is 61.4. hits@5% of kale-pre 42.5 is 73.0. hits@10% of kale-pre 50.0 is 83.3. mrr of kale-joint 0.220 is 0.260. med of kale-joint 29.0 is 4.0. hits@3% of kale-joint 25.7 is 43.6. hits@5% of kale-joint 32.4 is 64.1. hits@10% of kale-joint 40.6 is 79.2. mrr of kale-joint 0.296 is 0.563. med of kale-joint 5.0 is 2.0. hits@3% of kale-joint 40.0 is 67.6. hits@5% of kale-joint 50.8 is 73.8. hits@10% of kale-joint 57.8 is 81.0. mrr of kale-joint 0.262 is 0.478. med of kale-joint 10.0 is 2.0. hits@3% of kale-joint 33.6 is 60.9. hits@5% of kale-joint 42.5 is 71.1. hits@10% of kale-joint 50.0 is 80.5.
table 6 shows event co-reference end-to-end results. muc of ssed + supervisedextend muc is 47.1. b3 of ssed + supervisedextend b3 is 59.9. ceafe of ssed + supervisedextend ceafe is 58.7. blanc of ssed + supervisedextend blanc is 44.4. avg of ssed + supervisedextend avg is 52.5. muc of ssed + msep-corefesa+aug+know muc is 42.1. b3 of ssed + msep-corefesa+aug+know b3 is 60.3. ceafe of ssed + msep-corefesa+aug+know ceafe is 59.0. blanc of ssed + msep-corefesa+aug+know blanc is 44.1. avg of ssed + msep-corefesa+aug+know avg is 51.4. muc of msep-emd + msep-corefesa+aug+know muc is 40.2. b3 of msep-emd + msep-corefesa+aug+know b3 is 58.6. ceafe of msep-emd + msep-corefesa+aug+know ceafe is 57.4. blanc of msep-emd + msep-corefesa+aug+know blanc is 43.8. avg of msep-emd + msep-corefesa+aug+know avg is 50.0. muc of tac-top muc is —. b3 of tac-top b3 is —. ceafe of tac-top ceafe is —. blanc of tac-top blanc is —. avg of tac-top avg is 39.1. muc of ssed + supervisedextend muc is 34.9. b3 of ssed + supervisedextend b3 is 44.2. ceafe of ssed + supervisedextend ceafe is 39.6. blanc of ssed + supervisedextend blanc is 37.1. avg of ssed + supervisedextend avg is 39.0. muc of ssed + msep-corefesa+aug+know muc is 33.1. b3 of ssed + msep-corefesa+aug+know b3 is 44.6. ceafe of ssed + msep-corefesa+aug+know ceafe is 39.7. blanc of ssed + msep-corefesa+aug+know blanc is 36.8. avg of ssed + msep-corefesa+aug+know avg is 38.5. muc of msep-emd + msep-corefesa+aug+know muc is 30.2. b3 of msep-emd + msep-corefesa+aug+know b3 is 43.9. ceafe of msep-emd + msep-corefesa+aug+know ceafe is 38.7. blanc of msep-emd + msep-corefesa+aug+know blanc is 35.7. avg of msep-emd + msep-corefesa+aug+know avg is 37.1.
table 3 shows perplexity results on the test data for ltlms and stlms with different number of roles. perplexity of stlm 10 is 408.5. perplexity of stlm 20 is 335.2. perplexity of stlm 50 is 261.7. perplexity of stlm 100 is 212.6. perplexity of stlm 200 is 178.9. perplexity of stlm 500 is 137.8. perplexity of stlm 1000 is 113.7. perplexity of stlm 10 is 992.7. perplexity of stlm 20 is 764.2. perplexity of stlm 50 is 556.4. perplexity of stlm 100 is 451.0. perplexity of stlm 200 is 365.9. perplexity of stlm 500 is 265.7. perplexity of stlm 1000 is 211.0. perplexity of non-det. ltlm 10 is 329.5. perplexity of non-det. ltlm 20 is 215.1. perplexity of non-det. ltlm 50 is 160.4. perplexity of non-det. ltlm 100 is 126.5. perplexity of non-det. ltlm 200 is 105.6. perplexity of non-det. ltlm 500 is 86.7. perplexity of non-det. ltlm 1000 is 78.4. perplexity of non-det. ltlm 10 is 851.0. perplexity of non-det. ltlm 20 is 536.6. perplexity of non-det. ltlm 50 is 367.4. perplexity of non-det. ltlm 100 is 292.6. perplexity of non-det. ltlm 200 is 235.2. perplexity of non-det. ltlm 500 is 186.1. perplexity of non-det. ltlm 1000 is 157.6. perplexity of det. ltlm 10 is 252.4. perplexity of det. ltlm 20 is 166.4. perplexity of det. ltlm 50 is 115.3. perplexity of det. ltlm 100 is 92.0. perplexity of det. ltlm 200 is 75.4. perplexity of det. ltlm 500 is 60.9. perplexity of det. ltlm 1000 is 54.2. perplexity of det. ltlm 10 is 708.5. perplexity of det. ltlm 20 is 390.2. perplexity of det. ltlm 50 is 267.8. perplexity of det. ltlm 100 is 213.2. perplexity of det. ltlm 200 is 167.9. perplexity of det. ltlm 500 is 133.5. perplexity of det. ltlm 1000 is 111.1. perplexity of 4-gram mkn + stlm 10 is 42.7. perplexity of 4-gram mkn + stlm 20 is 41.6. perplexity of 4-gram mkn + stlm 50 is 39.9. perplexity of 4-gram mkn + stlm 100 is 37.9. perplexity of 4-gram mkn + stlm 200 is 36.3. perplexity of 4-gram mkn + stlm 500 is 34.9. perplexity of 4-gram mkn + stlm 1000 is 33.6. perplexity of 4-gram mkn + stlm 10 is 67.5. perplexity of 4-gram mkn + stlm 20 is 65.1. perplexity of 4-gram mkn + stlm 50 is 61.4. perplexity of 4-gram mkn + stlm 100 is 58.3. perplexity of 4-gram mkn + stlm 200 is 55.5. perplexity of 4-gram mkn + stlm 500 is 52.4. perplexity of 4-gram mkn + stlm 1000 is 50.1. perplexity of 4-gram mkn + non-det. ltlm 10 is 41.1. perplexity of 4-gram mkn + non-det. ltlm 20 is 38.0. perplexity of 4-gram mkn + non-det. ltlm 50 is 35.2. perplexity of 4-gram mkn + non-det. ltlm 100 is 32.7. perplexity of 4-gram mkn + non-det. ltlm 200 is 30.7. perplexity of 4-gram mkn + non-det. ltlm 500 is 28.9. perplexity of 4-gram mkn + non-det. ltlm 1000 is 27.8. perplexity of 4-gram mkn + non-det. ltlm 10 is 65.8. perplexity of 4-gram mkn + non-det. ltlm 20 is 59.4. perplexity of 4-gram mkn + non-det. ltlm 50 is 55.1. perplexity of 4-gram mkn + non-det. ltlm 100 is 51.1. perplexity of 4-gram mkn + non-det. ltlm 200 is 47.5. perplexity of 4-gram mkn + non-det. ltlm 500 is 43.7. perplexity of 4-gram mkn + non-det. ltlm 1000 is 41.3. perplexity of 4-gram mkn + det. ltlm 10 is 39.9. perplexity of 4-gram mkn + det. ltlm 20 is 36.4. perplexity of 4-gram mkn + det. ltlm 50 is 32.8. perplexity of 4-gram mkn + det. ltlm 100 is 30.3. perplexity of 4-gram mkn + det. ltlm 200 is 28.1. perplexity of 4-gram mkn + det. ltlm 500 is 26.0. perplexity of 4-gram mkn + det. ltlm 1000 is 24.9. perplexity of 4-gram mkn + det. ltlm 10 is 64.4. perplexity of 4-gram mkn + det. ltlm 20 is 56.1. perplexity of 4-gram mkn + det. ltlm 50 is 51.5. perplexity of 4-gram mkn + det. ltlm 100 is 47.3. perplexity of 4-gram mkn + det. ltlm 200 is 43.4. perplexity of 4-gram mkn + det. ltlm 500 is 39.9. perplexity of 4-gram mkn + det. ltlm 1000 is 37.2.
table 5 shows results for the unseen target stance detection development setup for tweets containing the target vs tweets not containing the target. p of favor p is 0.3153. r of favor r is 0.6214. f1 of favor f1 is 0.4183. p of against p is 0.7438. r of against r is 0.4630. f1 of against f1 is 0.5707. f1 of macro f1 is 0.4945. p of favor p is 0.0450. r of favor r is 0.6429. f1 of favor f1 is 0.0841. p of against p is 0.4793. r of against r is 0.4265. f1 of against f1 is 0.4514. f1 of macro f1 is 0.2677. p of favor p is 0.3529. r of favor r is 0.2330. f1 of favor f1 is 0.2807. p of against p is 0.7254. r of against r is 0.8327. f1 of against f1 is 0.7754. f1 of macro f1 is 0.5280. p of favor p is 0.0441. r of favor r is 0.2143. f1 of favor f1 is 0.0732. p of against p is 0.4663. r of against r is 0.5588. f1 of against f1 is 0.5084. f1 of macro f1 is 0.2908. p of favor p is 0.3585. r of favor r is 0.3689. f1 of favor f1 is 0.3636. p of against p is 0.7393. r of against r is 0.7393. f1 of against f1 is 0.7393. f1 of macro f1 is 0.5515. p of favor p is 0.0938. r of favor r is 0.4286. f1 of favor f1 is 0.1538. p of against p is 0.5846. r of against r is 0.2794. f1 of against f1 is 0.3781. f1 of macro f1 is 0.2660.
table 2 shows non-neural and neural model results on geoquery using the train/test split from (zettlemoyer and collins, 2005). accuracy of zettlemoyer and collins (2005) accuracy is 79.3. accuracy of zettlemoyer and collins (2007) accuracy is 86.1. accuracy of liang et al. (2013) accuracy is 87.9. accuracy of kwiatkowski et al. (2011) accuracy is 88.6. accuracy of zhao and huang (2014) accuracy is 88.9. accuracy of kwiatkowski et al. (2013) accuracy is 89.0. accuracy of dong and lapata (2016) accuracy is 84.6. accuracy of jia and liang (2016)6 accuracy is 89.3. accuracy of s2s accuracy is 86.5. accuracy of seq4 accuracy is 87.3.
table 4 shows results for experiment 2. p of svm-rbf p is 0.351. r of svm-rbf r is 0.023. f1 of svm-rbf f1 is 0.044. p of svm-rbf p is 0.394. r of svm-rbf r is 0.083. f1 of svm-rbf f1 is 0.137. p of svm-rbf p is 0.446. r of svm-rbf r is 0.918. f1 of svm-rbf f1 is 0.600. m-f1 of svm-rbf m-f1 is 0.260. c.i. of svm-rbf c.i. is 0.014. p of blstm p is 0.265. r of blstm r is 0.600. f1 of blstm f1 is 0.368. p of blstm p is 0.376. r of blstm r is 0.229. f1 of blstm f1 is 0.285. p of blstm p is 0.479. r of blstm r is 0.301. f1 of blstm f1 is 0.370. m-f1 of blstm m-f1 is 0.341. c.i. of blstm c.i. is 0.015. p of blstm/att/cnn p is 0.270. r of blstm/att/cnn r is 0.625. f1 of blstm/att/cnn f1 is 0.378. p of blstm/att/cnn p is 0.421. r of blstm/att/cnn r is 0.247. f1 of blstm/att/cnn f1 is 0.311. p of blstm/att/cnn p is 0.484. r of blstm/att/cnn r is 0.301. f1 of blstm/att/cnn f1 is 0.371. m-f1 of blstm/att/cnn m-f1 is 0.353. c.i. of blstm/att/cnn c.i. is 0.015.
table 2 shows lms performance on the ptb test set. perplexity of n-1= model is 1. perplexity of n-1= model is 2. perplexity of n-1= model is 4. perplexity of n-1= model+kn5 is 1. perplexity of n-1= model+kn5 is 2. perplexity of n-1= model+kn5 is 4. nop of n-1= nop is 4. perplexity of kn model is 186. perplexity of kn model is 148. perplexity of kn model is 141. perplexity of kn+cache model is 168. perplexity of kn+cache model is 134. perplexity of kn+cache model is 129. perplexity of ffnn model is 176. perplexity of ffnn model is 131. perplexity of ffnn model is 119. perplexity of ffnn model+kn5 is 132. perplexity of ffnn model+kn5 is 116. perplexity of ffnn model+kn5 is 107. nop of ffnn nop is 6.32m. perplexity of fofe model is 123. perplexity of fofe model is 111. perplexity of fofe model is 112. perplexity of fofe model+kn5 is 108. perplexity of fofe model+kn5 is 100. perplexity of fofe model+kn5 is 101. nop of fofe nop is 6.32m. perplexity of rnn model is 117. perplexity of rnn model is 117. perplexity of rnn model is 117. perplexity of rnn model+kn5 is 104. perplexity of rnn model+kn5 is 104. perplexity of rnn model+kn5 is 104. nop of rnn nop is 8.16m. perplexity of lstm (1l) model is 113. perplexity of lstm (1l) model is 113. perplexity of lstm (1l) model is 113. perplexity of lstm (1l) model+kn5 is 99. perplexity of lstm (1l) model+kn5 is 99. perplexity of lstm (1l) model+kn5 is 99. nop of lstm (1l) nop is 6.96m. perplexity of lsrc (100) model is 109. perplexity of lsrc (100) model is 109. perplexity of lsrc (100) model is 109. perplexity of lsrc (100) model+kn5 is 96. perplexity of lsrc (100) model+kn5 is 96. perplexity of lsrc (100) model+kn5 is 96. nop of lsrc (100) nop is 5.81m. perplexity of lsrc (200) model is 104. perplexity of lsrc (200) model is 104. perplexity of lsrc (200) model is 104. perplexity of lsrc (200) model+kn5 is 94. perplexity of lsrc (200) model+kn5 is 94. perplexity of lsrc (200) model+kn5 is 94. nop of lsrc (200) nop is 7.0m. perplexity of ffnn model is 176. perplexity of ffnn model is 129. perplexity of ffnn model is 114. perplexity of ffnn model+kn5 is 132. perplexity of ffnn model+kn5 is 114. perplexity of ffnn model+kn5 is 102. nop of ffnn nop is 6.96m. perplexity of fofe model is 116. perplexity of fofe model is 108. perplexity of fofe model is 109. perplexity of fofe model+kn5 is 104. perplexity of fofe model+kn5 is 98. perplexity of fofe model+kn5 is 97. nop of fofe nop is 6.96m. perplexity of d-lstm (2l) model is 110. perplexity of d-lstm (2l) model is 110. perplexity of d-lstm (2l) model is 110. perplexity of d-lstm (2l) model+kn5 is 97. perplexity of d-lstm (2l) model+kn5 is 97. perplexity of d-lstm (2l) model+kn5 is 97. nop of d-lstm (2l) nop is 8.42m. perplexity of d-rnn4 (3l) model is 107.5. perplexity of d-rnn4 (3l) model is 107.5. perplexity of d-rnn4 (3l) model is 107.5. perplexity of d-rnn4 (3l) model+kn5 is nr. perplexity of d-rnn4 (3l) model+kn5 is nr. perplexity of d-rnn4 (3l) model+kn5 is nr. nop of d-rnn4 (3l) nop is 6.16m. perplexity of d-lsrc (100) model is 103. perplexity of d-lsrc (100) model is 103. perplexity of d-lsrc (100) model is 103. perplexity of d-lsrc (100) model+kn5 is 93. perplexity of d-lsrc (100) model+kn5 is 93. perplexity of d-lsrc (100) model+kn5 is 93. nop of d-lsrc (100) nop is 5.97m. perplexity of d-lsrc (200) model is 102. perplexity of d-lsrc (200) model is 102. perplexity of d-lsrc (200) model is 102. perplexity of d-lsrc (200) model+kn5 is 92. perplexity of d-lsrc (200) model+kn5 is 92. perplexity of d-lsrc (200) model+kn5 is 92. nop of d-lsrc (200) nop is 7.16m.
table 5 shows perplexity, labeled f1-score, and tree edit distance (ted) of various systems. perplexity on train of pe2pe2p perplexity on train is 1.83. perplexity on wsj 22 of pe2pe2p perplexity on wsj 22 is 1.92. labeled f1 on wsj23 of pe2pe2p labeled f1 on wsj23 is 46.64. # evalb-trees (out of 2416) of pe2pe2p # evalb-trees (out of 2416) is 818. average ted per sentence of pe2pe2p average ted per sentence is 34.43. # well-formed trees (out of 2416) of pe2pe2p # well-formed trees (out of 2416) is 2416. perplexity on train of e2e2p perplexity on train is 1.69. perplexity on wsj 22 of e2e2p perplexity on wsj 22 is 1.77. labeled f1 on wsj23 of e2e2p labeled f1 on wsj23 is 59.35. # evalb-trees (out of 2416) of e2e2p # evalb-trees (out of 2416) is 796. average ted per sentence of e2e2p average ted per sentence is 31.25. # well-formed trees (out of 2416) of e2e2p # well-formed trees (out of 2416) is 2416. perplexity on train of e2g2p perplexity on train is 1.39. perplexity on wsj 22 of e2g2p perplexity on wsj 22 is 1.41. labeled f1 on wsj23 of e2g2p labeled f1 on wsj23 is 80.34. # evalb-trees (out of 2416) of e2g2p # evalb-trees (out of 2416) is 974. average ted per sentence of e2g2p average ted per sentence is 17.11. # well-formed trees (out of 2416) of e2g2p # well-formed trees (out of 2416) is 2340. perplexity on train of e2f2p perplexity on train is 1.36. perplexity on wsj 22 of e2f2p perplexity on wsj 22 is 1.38. labeled f1 on wsj23 of e2f2p labeled f1 on wsj23 is 79.27. # evalb-trees (out of 2416) of e2f2p # evalb-trees (out of 2416) is 1093. average ted per sentence of e2f2p average ted per sentence is 17.77. # well-formed trees (out of 2416) of e2f2p # well-formed trees (out of 2416) is 2415. perplexity on train of e2p perplexity on train is 1.11. perplexity on wsj 22 of e2p perplexity on wsj 22 is 1.18. labeled f1 on wsj23 of e2p labeled f1 on wsj23 is 89.61. # evalb-trees (out of 2416) of e2p # evalb-trees (out of 2416) is 2362. average ted per sentence of e2p average ted per sentence is 11.50. # well-formed trees (out of 2416) of e2p # well-formed trees (out of 2416) is 2415.
table 1 shows results on the answer ranking task of our full nn vs. map of - map is 21.78. avgrec of - avgrec is 20.66. mrr of - mrr is 22.59. map of + appropriateness map is 30.94. avgrec of + appropriateness avgrec is 29.86. mrr of + appropriateness mrr is 35.02. map of + relatedness map is 52.43. avgrec of + relatedness avgrec is 57.05. mrr of + relatedness mrr is 60.14. map of - map is 54.51. avgrec of - avgrec is 60.93. mrr of - mrr is 62.94. map of - map is 15.01. avgrec of - avgrec is 11.44. mrr of - mrr is 15.19. map of - map is 40.36. avgrec of - avgrec is 45.97. mrr of - mrr is 45.83.
table 3 shows comparisons between our mutual distillation (rows 4-5) and other knowledge optimization methods, on sst2. accuracy (%) of cnn (kim, 2014) accuracy (%) is 86.6. accuracy (%) of opt-joint accuracy (%) is 86.9 (68.8). accuracy (%) of opt-knwl-pipeline accuracy (%) is 86.7 (70.4). accuracy (%) of opt-joint-iterative-p accuracy (%) is 86.9. accuracy (%) of opt-joint-iterative-q accuracy (%) is 87.6 (68.6). accuracy (%) of mutual-p accuracy (%) is 87.2. accuracy (%) of mutual-q accuracy (%) is 88.0 (72.5).
table 3 shows pearson (r × 100) and spearman (ρ × 100) correlation scores on four standard word similarity benchmarks. r of iacobacci et al. (2015) r is ?. rho of iacobacci et al. (2015) rho is 80.5. r of iacobacci et al. (2015) r is ?. rho of iacobacci et al. (2015) rho is 66.5. r of deconf r is 78.0. rho of deconf rho is 78.6. r of deconf r is 72.3. rho of deconf rho is 73.2. r of faruqui et al. (2015) r is ?. rho of faruqui et al. (2015) rho is 75.9. r of faruqui et al. (2015) r is ?. rho of faruqui et al. (2015) rho is 73.7. r of pilehvar and navigli (2015) r is 61.7. rho of pilehvar and navigli (2015) rho is 66.6. r of pilehvar and navigli (2015) r is ?. rho of pilehvar and navigli (2015) rho is ?. r of deconf r is 90.5. rho of deconf rho is 89.6. r of deconf r is 77.2. rho of deconf rho is 76.1. r of iacobacci et al. (2015) r is ?. rho of iacobacci et al. (2015) rho is 87.1. r of iacobacci et al. (2015) r is ?. rho of iacobacci et al. (2015) rho is 73.2. r of faruqui et al. (2015) r is ?. rho of faruqui et al. (2015) rho is 84.2. r of faruqui et al. (2015) r is ?. rho of faruqui et al. (2015) rho is 76.7. r of pilehvar and navigli (2015) r is 80.2. rho of pilehvar and navigli (2015) rho is 84.3. r of pilehvar and navigli (2015) r is ?. rho of pilehvar and navigli (2015) rho is ?. r of deconf r is 81.6. rho of deconf rho is 75.2. r of deconf r is 58.0. rho of deconf rho is 55.9. r of pilehvar and navigli (2015) r is 79.0. rho of pilehvar and navigli (2015) rho is 71.0. r of pilehvar and navigli (2015) r is ?. rho of pilehvar and navigli (2015) rho is ?. r of iacobacci et al. (2015) r is ?. rho of iacobacci et al. (2015) rho is 63.9. r of iacobacci et al. (2015) r is ?. rho of iacobacci et al. (2015) rho is 34.3. r of deconf r is 54.2. rho of deconf rho is 51.7. r of deconf r is 45.4. rho of deconf rho is 44.2. r of pilehvar and navigli (2015) r is 43.4. rho of pilehvar and navigli (2015) rho is 43.6. r of pilehvar and navigli (2015) r is ?. rho of pilehvar and navigli (2015) rho is ?.
table 2 shows comparison of composition by union and composition by intersection. co-occurrence count of :shoes co-occurrence count is 1. co-occurrence count of amod:clean co-occurrence count is 1. co-occurrence count of dobj:bought co-occurrence count is 1. co-occurrence count of dobj:folded co-occurrence count is 2. co-occurrence count of dobj:folded co-occurrence count is 2. co-occurrence count of dobj:like co-occurrence count is 1. co-occurrence count of dobj:nsubj:we co-occurrence count is 2. co-occurrence count of dobj:nsubj:we co-occurrence count is 2.
table 8 shows evaluation results for the on-/off-topic classification with the extratreeclassifier precision of off-topic info precision is 0.94. recall of off-topic info recall is 0.85. f1 of off-topic info f1 is 0.89. precision of requirements precision is 0.89. recall of requirements recall is 0.96. f1 of requirements f1 is 0.93. precision of avg. precision is 0.92. recall of avg. recall is 0.91. f1 of avg. f1 is 0.91.
table 2 shows mean and standard deviation of random and upperbound performance (with n = 10, k = 3) across different subreddits. mean of askscience random is 321.3 (7.0). mean of askscience upper bound is 2109.0 (16.5). mean of askmen random is 132.4 (0.7). mean of askmen upper bound is 651.4 (2.8). mean of todayilearned random is 390.3 (5.7). mean of todayilearned upper bound is 2679.6 (30.1). mean of worldnews random is 205.8 (4.5). mean of worldnews upper bound is 1853.4 (44.4). mean of nfl random is 237.1 (1.4. mean of nfl upper bound is 1338.2 (13.2).
table 3 shows results of five-fold cross validation. map of tf-idf avg is 0.503. map of tf-idf sd is 0.004. mrr of tf-idf avg is 0.501. mrr of tf-idf sd is 0.065. map of bm-25 avg is 0.531. map of bm-25 sd is 0.007. mrr of bm-25 avg is 0.532. mrr of bm-25 sd is 0.056. map of avg avg is 0.593. map of avg sd is 0.021. mrr of avg avg is 0.609. mrr of avg sd is 0.042. map of rnn avg is 0.685. map of rnn sd is 0.024. mrr of rnn avg is 0.674. mrr of rnn sd is 0.028. map of attn1511 avg is 0.772. map of attn1511 sd is 0.016. mrr of attn1511 avg is 0.771. mrr of attn1511 sd is 0.014. map of cnn avg is 0.757. map of cnn sd is 0.015. mrr of cnn avg is 0.754. mrr of cnn sd is 0.024. map of tri-cnn avg is 0.806. map of tri-cnn sd is 0.014. mrr of tri-cnn avg is 0.781. mrr of tri-cnn sd is 0.025.
table 8 shows performance comparison between t-o-m and semeval-2015’s best sudoku run. f1 of all f1 is 56.6. p of all p is 65.1. r of all r is 55.6. f1 of all f1 is 59.9. f1 of computers & math f1 is 46.6. p of computers & math p is 52.7. r of computers & math r is 43.3. f1 of computers & math f1 is 47.6. f1 of biomedicine f1 is 65.9. p of biomedicine p is 76.6. r of biomedicine r is 67.6. f1 of biomedicine f1 is 71.8. f1 of all f1 is 56.3. p of all p is 61.3. r of all r is 54.8. f1 of all f1 is 57.9. f1 of computers & math f1 is 42.4. p of computers & math p is 53.3. r of computers & math r is 44.4. f1 of computers & math f1 is 48.5. f1 of biomedicine f1 is 65.5. p of biomedicine p is 71.8. r of biomedicine r is 65.5. f1 of biomedicine f1 is 68.5.
table 2 shows adversarial evaluation on the matchlstm and bidaf systems. f1 of original single is 71.4. f1 of original ens. is 75.4. f1 of original single is 75.5. f1 of original ens. is 80. f1 of addsent single is 27.3. f1 of addsent ens. is 29.4. f1 of addsent single is 34.3. f1 of addsent ens. is 34.2. f1 of addonesent single is 39. f1 of addonesent ens. is 41.8. f1 of addonesent single is 45.7. f1 of addonesent ens. is 46.9. f1 of addany single is 7.6. f1 of addany ens. is 11.7. f1 of addany single is 4.8. f1 of addany ens. is 2.7. f1 of addcommon single is 38.9. f1 of addcommon ens. is 51. f1 of addcommon single is 41.7. f1 of addcommon ens. is 52.6.
table 3 shows comparison results on two-part evaluation ii r-1 of lead r-1 is 0.3985. r-2 of lead r-2 is 0.11888. r-su4 of lead r-su4 is 0.16209. r-1 of coverage r-1 is 0.39957. r-2 of coverage r-2 is 0.1161. r-su4 of coverage r-su4 is 0.15753. r-1 of textrank r-1 is 0.41132. r-2 of textrank r-2 is 0.12045. r-su4 of textrank r-su4 is 0.16317. r-1 of centroid r-1 is 0.40071. r-2 of centroid r-2 is 0.11772. r-su4 of centroid r-su4 is 0.15859. r-1 of ilp r-1 is 0.40795. r-2 of ilp r-2 is 0.12149. r-su4 of ilp r-su4 is 0.1635. r-1 of clustercmrw r-1 is 0.41379. r-2 of clustercmrw r-2 is 0.12769. r-su4 of clustercmrw r-su4 is 0.16935. r-1 of submodular r-1 is 0.40677. r-2 of submodular r-2 is 0.11903. r-su4 of submodular r-su4 is 0.16163. r-1 of sendivrank r-1 is 0.4021. r-2 of sendivrank r-2 is 0.12704. r-su4 of sendivrank r-su4 is 0.17001. r-1 of our approach r-1 is 0.42207. r-2 of our approach r-2 is 0.14401. r-su4 of our approach r-su4 is 0.18392.
table 6 shows da classification results (%) using different systems. accuracy of cnn baseline no context set 1 is 74.73. accuracy of cnn baseline no context set 2 is 77.12. accuracy of cnn + da predictions set 1 is 76.73. accuracy of cnn + da predictions set 2 is 79.9. accuracy of cnn + rnn/blstm set 1 is 76.91. accuracy of cnn + rnn/blstm set 2 is 79.7. accuracy of cnn + cnn set 1 is 77.15. accuracy of cnn + cnn set 2 is 79.74. accuracy of cnn prob + da transition set 1 is 76.7. accuracy of cnn prob + da transition set 2 is 79.69.
table 3 shows implicit supervision: semantic parsing. avg. f1 of mmrn-pipeline avg. f1 is 62.5. p of mmrn-pipeline p is 85.6. r of mmrn-pipeline r is 77.5. f1 of mmrn-pipeline f1 is 81.3. avg. f1 of mmrn-joint avg. f1 is 68.1. p of mmrn-joint p is 89.3. r of mmrn-joint r is 78.9. f1 of mmrn-joint f1 is 83.7. avg. f1 of reinforce avg. f1 is 62.9. p of reinforce p is 87.5. r of reinforce r is 76.6. f1 of reinforce f1 is 81.7. avg. f1 of reinforce+ avg. f1 is 66.7. p of reinforce+ p is 91.1. r of reinforce+ r is 76.9. f1 of reinforce+ f1 is 83.4. avg. f1 of stagg avg. f1 is 66.8.
table 5 shows overall performance for sfv: all the baseline systems are from yu et al. precision (%) of random precision (%) is 28.64. recall (%) of random recall (%) is 50.48. f-score (%) of random f-score (%) is 36.54. precision (%) of voting precision (%) is 42.16. recall (%) of voting recall (%) is 70.18. f-score (%) of voting f-score (%) is 52.68. precision (%) of linguistic indicators precision (%) is 50.24. recall (%) of linguistic indicators recall (%) is 70.69. f-score (%) of linguistic indicators f-score (%) is 58.73. precision (%) of svm precision (%) is 56.59. recall (%) of svm recall (%) is 48.72. f-score (%) of svm f-score (%) is 52.36. precision (%) of mtm (yu et al., 2014a) precision (%) is 53.94. recall (%) of mtm (yu et al., 2014a) recall (%) is 72.11. f-score (%) of mtm (yu et al., 2014a) f-score (%) is 61.72. precision (%) of - precision (%) is 70.46. recall (%) of - recall (%) is 64.07. f-score (%) of - f-score (%) is 67.11.
table 2 shows main results (on ace). p of lcrf (single) p is 70.6. r of lcrf (single) r is 41.7. f1 of lcrf (single) f1 is 52.5. w/s of lcrf (single) w/s is 40.2. p of lcrf (single) p is 66. r of lcrf (single) r is 45. f1 of lcrf (single) f1 is 53.5. w/s of lcrf (single) w/s is 41.2. p of lcrf (single) p is 66.2. r of lcrf (single) r is 47.7. f1 of lcrf (single) f1 is 55.4. p of lcrf (single) p is 62.1. r of lcrf (single) r is 48.9. f1 of lcrf (single) f1 is 54.7. p of lcrf (multiple) p is 78.6. r of lcrf (multiple) r is 44.5. f1 of lcrf (multiple) f1 is 56.9. w/s of lcrf (multiple) w/s is 119.4. p of lcrf (multiple) p is 76.2. r of lcrf (multiple) r is 46.8. f1 of lcrf (multiple) f1 is 58. w/s of lcrf (multiple) w/s is 118.7. p of lcrf (multiple) p is 69.9. r of lcrf (multiple) r is 55.1. f1 of lcrf (multiple) f1 is 61.6. p of lcrf (multiple) p is 66.5. r of lcrf (multiple) r is 55.3. f1 of lcrf (multiple) f1 is 60.4. p of lu and roth (2015) p is 81.2. r of lu and roth (2015) r is 45.9. f1 of lu and roth (2015) f1 is 58.6. w/s of lu and roth (2015) w/s is 472.5. p of lu and roth (2015) p is 78.6. r of lu and roth (2015) r is 46.9. f1 of lu and roth (2015) f1 is 58.7. w/s of lu and roth (2015) w/s is 516.6. p of lu and roth (2015) p is 72.5. r of lu and roth (2015) r is 55.7. f1 of lu and roth (2015) f1 is 63. p of lu and roth (2015) p is 66.3. r of lu and roth (2015) r is 57.3. f1 of lu and roth (2015) f1 is 61.5. p of this work (state) p is 78. r of this work (state) r is 51.2. f1 of this work (state) f1 is 61.8. w/s of this work (state) w/s is 50.5. p of this work (state) p is 75.3. r of this work (state) r is 51.7. f1 of this work (state) f1 is 61.3. w/s of this work (state) w/s is 52.1. p of this work (state) p is 71.2. r of this work (state) r is 58. f1 of this work (state) f1 is 64. p of this work (state) p is 67.6. r of this work (state) r is 58.4. f1 of this work (state) f1 is 62.7. p of this work (edge) p is 79.5. r of this work (edge) r is 51.1. f1 of this work (edge) f1 is 62.2. w/s of this work (edge) w/s is 251.5. p of this work (edge) p is 75.5. r of this work (edge) r is 51.7. f1 of this work (edge) f1 is 61.3. w/s of this work (edge) w/s is 253.3. p of this work (edge) p is 72.7. r of this work (edge) r is 58. f1 of this work (edge) f1 is 64.5. p of this work (edge) p is 69.1. r of this work (edge) r is 58.1. f1 of this work (edge) f1 is 63.1.
table 5 shows f1 score results on the test set for different categories: t indicates task, p indicates process, m is material and k is keyword identification (subtask a). f1 of best semeval t is 19. f1 of best semeval p is 44. f1 of best semeval m is 48. f1 of best semeval k is 55. f1 of supervised t is 13.3. f1 of supervised p is 40.5. f1 of supervised m is 43.7. f1 of supervised k is 52.1. f1 of ulm+graphinterp t is 17. f1 of ulm+graphinterp p is 45.4. f1 of ulm+graphinterp m is 49.4. f1 of ulm+graphinterp k is 56.9. f1 of ulm+graphfeat t is 17.2. f1 of ulm+graphfeat p is 46.5. f1 of ulm+graphfeat m is 50.7. f1 of ulm+graphfeat k is 57.6. f1 of supervised t is 29.6. f1 of supervised p is 56. f1 of supervised m is 59.3. f1 of supervised k is 70.8. f1 of ulm+graphinterp t is 40. f1 of ulm+graphinterp p is 60.7. f1 of ulm+graphinterp m is 61.2. f1 of ulm+graphinterp k is 77. f1 of ulm+graphfeat t is 40.1. f1 of ulm+graphfeat p is 62.8. f1 of ulm+graphfeat m is 63.4. f1 of ulm+graphfeat k is 78.1.
table 1 shows entity linking performance: accuracy of existing systems, and variations of our model on gold mentions. accuracy of plato(sup) test is 79.7. accuracy of plato(semi-sup) test is 86.4. accuracy of aida* test is 81.8. accuracy of berkcnn:sparse* test is 74.9. accuracy of berkcnn:sparse* - is 83.6. accuracy of berkcnn:sparse* - is 81.5. accuracy of berkcnn:cnn* test is 81.2. accuracy of berkcnn:cnn* dev is 86.91. accuracy of berkcnn:cnn* - is 84.5. accuracy of berkcnn:cnn* - is 75.7. accuracy of berkcnn:full* test is 85.5. accuracy of berkcnn:full* - is 89.9. accuracy of berkcnn:full* - is 82.2. accuracy of priors test is 68.5. accuracy of priors dev is 70.9. accuracy of priors - is 81.1. accuracy of priors - is 78.1. accuracy of model c test is 81.4. accuracy of model c dev is 83.4. accuracy of model c - is 83.7. accuracy of model c - is 86.1. accuracy of model cd test is 81. accuracy of model cd dev is 83.2. accuracy of model cd - is 85.8. accuracy of model cd - is 86.1. accuracy of model ct test is 82.3. accuracy of model ct dev is 83.9. accuracy of model ct - is 86.5. accuracy of model ct - is 88.2. accuracy of model cdt test is 82.5. accuracy of model cdt dev is 85.6. accuracy of model cdt - is 86.8. accuracy of model cdt - is 88. accuracy of model cdte test is 82.9. accuracy of model cdte dev is 84.9. accuracy of model cdte - is 85.6. accuracy of model cdte - is 89.
table 2 shows experiment results f1 of crf d1 is 74.01%. f1 of crf d2 is 69.56%. f1 of semi-crf d1 is 68.75%. f1 of semi-crf d2 is 66.35%. f1 of ihs_rd d1 is 74.55%. f1 of ihs_rd d2 is  -. f1 of dlirec d1 is 73.78%. f1 of dlirec d2 is  -. f1 of nlangp d1 is  -. f1 of nlangp d2 is 72.34%. f1 of aueb d1 is  -. f1 of aueb d2 is 70.44%. f1 of wdemb d1 is 75.16%. f1 of wdemb d2 is  -. f1 of lstm d1 is 75.25%. f1 of lstm d2 is 71.26%. f1 of rncrf d1 is 77.26%. f1 of rncrf d2 is 69.74%. f1 of our work d1 is 77.58%. f1 of our work d2 is 73.44%.
table 4 shows baseline performance on test set. pr of strict match pr is 0.0006. re of strict match re is 0.0026. f1 of strict match f1 is 0.001. pr of meteor pr is 0.1512. re of meteor re is 0.1949. f1 of meteor f1 is 0.17. pr of rouge-2 pr is 0.0603. re of rouge-2 re is 0.1798. f1 of rouge-2 f1 is 0.0891.
table 3 shows the annotation quality’s impact on model performance. avg. prec of once avg. prec is 79.3. avg. rec of once avg. rec is 69.1. avg. f1 of once avg. f1 is 73.9. avg. prec of ab-merge avg. prec is 78.1. avg. rec of ab-merge avg. rec is 76.5. avg. f1 of ab-merge avg. f1 is 77.3.
table 4 shows results on twitter and ner dev sets. acc. of bigru baseline acc. is 90.8. f1 of bigru baseline f1 is 87.6. acc. of vsl-g acc. is 91.1. no vr of vsl-g no vr is 90.9. f1 of vsl-g f1 is 87.8. no vr of vsl-g no vr is 87.7. acc. of vsl-gg-flat acc. is 91.4. no vr of vsl-gg-flat no vr is 90.9. f1 of vsl-gg-flat f1 is 88.0. no vr of vsl-gg-flat no vr is 87.8. acc. of vsl-gg-hier acc. is 91.6. no vr of vsl-gg-hier no vr is 91.0. f1 of vsl-gg-hier f1 is 88.4. no vr of vsl-gg-hier no vr is 87.9.
table 6 shows comparison on monolingual embedding quality: name tagging performance (f-score, %) using monolingual embedding and multilingual embeddings. f-score of amh - is 52.0. f-score of amh - is 50.6. f-score of amh - is 53.4. f-score of amh w is 52.4. f-score of amh w+n+c+l is 55.8. f-score of tig - is 78.2. f-score of tig - is 78.4. f-score of tig - is 76.4. f-score of tig w is 77.9. f-score of tig w+n+c+l is 78.5. f-score of uig - is 63.3. f-score of uig - is 59.6. f-score of uig - is 60.1. f-score of uig w is 61.9. f-score of uig w+n+c+l is 65.2. f-score of tur - is 62.9. f-score of tur - is 47.7. f-score of tur - is 54.0. f-score of tur w is 59.3. f-score of tur w+n+c+l is 64.9.
table 2 shows evaluation of the nist chinese-english translation task. bleu of moses mt02 is 33.79. bleu of moses mt03 is 30.86. bleu of moses mt04 is 32.71. bleu of moses mt05 is 30.02. bleu of moses mt06 is 30.49. bleu of moses ave. is 31.02. #params of rnnsearch #params is 83.99m. speed of rnnsearch speed is 87. bleu of rnnsearch mt02 is 39.68. bleu of rnnsearch mt03 is 36.51. bleu of rnnsearch mt04 is 40.20. bleu of rnnsearch mt05 is 36.87. bleu of rnnsearch mt06 is 36.43. bleu of rnnsearch ave. is 37.50. #params of deliberation network #params is 125.16m. speed of deliberation network speed is 162. bleu of deliberation network mt02 is 40.98. bleu of deliberation network mt03 is 37.82. bleu of deliberation network mt04 is 40.56. bleu of deliberation network mt05 is 37.67. bleu of deliberation network mt06 is 37.20. bleu of deliberation network ave. is 38.31. #params of abdnmt #params is 122.86m. speed of abdnmt speed is 132. bleu of abdnmt mt02 is 41.12. bleu of abdnmt mt03 is 38.01. bleu of abdnmt mt04 is 41.20. bleu of abdnmt mt05 is 38.07. bleu of abdnmt mt06 is 37.59. bleu of abdnmt ave. is 38.71. #params of 2-pass decoder #params is 87.81m. speed of 2-pass decoder speed is 160. bleu of 2-pass decoder mt02 is 41.18. bleu of 2-pass decoder mt03 is 37.76. bleu of 2-pass decoder mt04 is 41.06. bleu of 2-pass decoder mt05 is 38.02. bleu of 2-pass decoder mt06 is 37.41. bleu of 2-pass decoder ave. is 38.56. #params of 3-pass decoder #params is 87.81m. speed of 3-pass decoder speed is 245. bleu of 3-pass decoder mt02 is 41.28.41.05. bleu of 3-pass decoder mt03 is 37.99. bleu of 3-pass decoder mt04 is 40.72. bleu of 3-pass decoder mt05 is 37.86. bleu of 3-pass decoder mt06 is 37.63. bleu of 3-pass decoder ave. is 38.55. #params of 4-pass decoder #params is 87.81m. speed of 4-pass decoder speed is 293. bleu of 4-pass decoder mt02 is 41.05. bleu of 4-pass decoder mt03 is 37.86. bleu of 4-pass decoder mt04 is 40.87. bleu of 4-pass decoder mt05 is 38.18. bleu of 4-pass decoder mt06 is 37.57. bleu of 4-pass decoder ave. is 38.62. #params of 5-pass decoder #params is 87.81m. speed of 5-pass decoder speed is 322. bleu of 5-pass decoder mt02 is 40.88. bleu of 5-pass decoder mt03 is 37.70. bleu of 5-pass decoder mt04 is 40.84. bleu of 5-pass decoder mt05 is 38.06. bleu of 5-pass decoder mt06 is 37.97. bleu of 5-pass decoder ave. is 38.64. #params of adaptive multi-pass decoder #params is 96.01m. speed of adaptive multi-pass decoder speed is 180. bleu of adaptive multi-pass decoder mt02 is 41.42. bleu of adaptive multi-pass decoder mt03 is 38.39. bleu of adaptive multi-pass decoder mt04 is 41.43. bleu of adaptive multi-pass decoder mt05 is 38.54. bleu of adaptive multi-pass decoder mt06 is 37.86. bleu of adaptive multi-pass decoder ave. is 39.05.
table 1 shows translation results on german↔english newstest2016 and french↔english newstest2014. bleu [%] of - bleu [%] is 11.1. bleu [%] of - bleu [%] is 6.7. bleu [%] of - bleu [%] is 10.6. bleu [%] of - bleu [%] is 7.8. bleu [%] of - bleu [%] is 14.5. bleu [%] of - bleu [%] is 9.9. bleu [%] of - bleu [%] is 13.6. bleu [%] of - bleu [%] is 10.9. bleu [%] of + denoising bleu [%] is 17.2. bleu [%] of + denoising bleu [%] is 11.0. bleu [%] of + denoising bleu [%] is 16.5. bleu [%] of + denoising bleu [%] is 13.9. bleu [%] of - bleu [%] is 13.3. bleu [%] of - bleu [%] is 9.6. bleu [%] of - bleu [%] is 14.3. bleu [%] of - bleu [%] is 15.1. bleu [%] of - bleu [%] is 15.6. bleu [%] of - bleu [%] is 15.1.
table 2 shows performance of the unsupervised psd compared with the state-of-the-art. accuracy of - accuracy is 0.56. accuracy of average accuracy is 0.555. accuracy of (l r) accuracy is 0.561. accuracy of (l i) accuracy is 0.565. accuracy of (r i) accuracy is 0.534. accuracy of (l r i) accuracy is 0.584.
table 4 shows experimental results on subclassof triple classification(%). accuracy of transe accuracy is 77.6. precision of transe precision is 72.2. recall of transe recall is 89.8. f1-score of transe f1-score is 80.0. accuracy of transe accuracy is 76.9. precision of transe precision is 72.3. recall of transe recall is 87.2. f1-score of transe f1-score is 79.0. accuracy of transh accuracy is 80.2. precision of transh precision is 76.4. recall of transh recall is 87.5. f1-score of transh f1-score is 81.5. accuracy of transh accuracy is 79.1. precision of transh precision is 72.8. recall of transh recall is 92.9. f1-score of transh f1-score is 81.6. accuracy of transr accuracy is 80.4. precision of transr precision is 74.7. recall of transr recall is 91.9. f1-score of transr f1-score is 82.4. accuracy of transr accuracy is 80.0. precision of transr precision is 73.9. recall of transr recall is 92.9. f1-score of transr f1-score is 82.3. accuracy of transd accuracy is 75.9. precision of transd precision is 70.6. recall of transd recall is 88.8. f1-score of transd f1-score is 78.7. accuracy of transd accuracy is 76.1. precision of transd precision is 70.7. recall of transd recall is 89.0. f1-score of transd f1-score is 78.8. accuracy of hole accuracy is 70.5. precision of hole precision is 73.9. recall of hole recall is 63.3. f1-score of hole f1-score is 68.2. accuracy of hole accuracy is 66.6. precision of hole precision is 72.3. recall of hole recall is 53.7. f1-score of hole f1-score is 61.7. accuracy of distmult accuracy is 61.9. precision of distmult precision is 68.7. recall of distmult recall is 43.7. f1-score of distmult f1-score is 53.4. accuracy of distmult accuracy is 60.7. precision of distmult precision is 71.7. recall of distmult recall is 35.5. f1-score of distmult f1-score is 47.7. accuracy of complex accuracy is 61.6. precision of complex precision is 71.5. recall of complex recall is 38.6. f1-score of complex f1-score is 50.1. accuracy of complex accuracy is 59.8. precision of complex precision is 65.6. recall of complex recall is 41.4. f1-score of complex f1-score is 50.7. accuracy of transc (unif) accuracy is 82.9. precision of transc (unif) precision is 77.1. recall of transc (unif) recall is 93.7. f1-score of transc (unif) f1-score is 84.6. accuracy of transc (unif) accuracy is 83.0. precision of transc (unif) precision is 77.5. recall of transc (unif) recall is 93.1. f1-score of transc (unif) f1-score is 84.7. accuracy of transc (bern) accuracy is 83.7. precision of transc (bern) precision is 78.1. recall of transc (bern) recall is 93.9. f1-score of transc (bern) f1-score is 85.2. accuracy of transc (bern) accuracy is 84.4. precision of transc (bern) precision is 80.7. recall of transc (bern) recall is 90.4. f1-score of transc (bern) f1-score is 85.3.
table 1 shows we show performance in terms of recall, computed per-category and averaged across categories, for the baseline, list and scatterplot interfaces. recall of bootstrapping scatterplot is 88.8. recall of bootstrapping list is 87.2. recall of bootstrapping baseline is 85.1. recall of extrapolation scatterplot is 84.7. recall of extrapolation list is 83.5. recall of extrapolation baseline is 78.6.
table 3 shows accuracy comparison of different methods on npschat corpus. acc. of forsyth (2007) acc. is 90.8%. acc. of ark tagger acc. is 93.4%±0.3%. acc. of gui et al. (2017) acc. is 94.1%. acc. of bi-lstm(irc) acc. is 90.3%. acc. of bi-lstm(wsj + irc) acc. is 93.2%. acc. of dcnn acc. is 94.0%.
table 2 shows evaluation of translation performance for chinese–english. # params of baseline - is 86.7m. speed of baseline train is 1.60k. speed of baseline decode is 15.23. bleu of baseline - is 31.80. # params of baseline (+dps) - is 86.7m. speed of baseline (+dps) train is 1.59k. speed of baseline (+dps) decode is 15.20. bleu of baseline (+dps) - is 32.67. # params of separate-recs⇒(+dps) - is +73.8m. speed of separate-recs⇒(+dps) train is 0.57k. speed of separate-recs⇒(+dps) decode is 12.00. bleu of separate-recs⇒(+dps) - is 35.08. # params of baseline (+dpps) - is 86.7m. speed of baseline (+dpps) train is 1.54k. speed of baseline (+dpps) decode is 15.19. bleu of baseline (+dpps) - is 33.18. # params of shared-recindependent⇒(+dpps) - is +86.6m. speed of shared-recindependent⇒(+dpps) train is 0.52k. speed of shared-recindependent⇒(+dpps) decode is 11.87. bleu of shared-recindependent⇒(+dpps) - is 35.27†‡. # params of shared-recindependent⇒(+dpps) + joint prediction - is +87.9m. speed of shared-recindependent⇒(+dpps) + joint prediction train is 0.51k. speed of shared-recindependent⇒(+dpps) + joint prediction decode is 11.88. bleu of shared-recindependent⇒(+dpps) + joint prediction - is 35.88†‡. # params of shared-recenc→dec⇒(+dpps) + joint prediction - is +91.9m. speed of shared-recenc→dec⇒(+dpps) + joint prediction train is 0.48k. speed of shared-recenc→dec⇒(+dpps) + joint prediction decode is 11.84. bleu of shared-recenc→dec⇒(+dpps) + joint prediction - is 36.53†‡. # params of shared-recdec→enc⇒(+dpps) + joint prediction - is +89.9m. speed of shared-recdec→enc⇒(+dpps) + joint prediction train is 0.49k. speed of shared-recdec→enc⇒(+dpps) + joint prediction decode is 11.85. bleu of shared-recdec→enc⇒(+dpps) + joint prediction - is 35.99†‡.
table 1 shows the performance of different pre-trained embeddings on sentiment (f1 score) and pos tasks (accuracy). f1 score of none f1 score is 54.4(1.3). f1 score of none f1 score is 64.5(0.6). f1 score of none f1 score is 61.4(1.0). accuracy of none accuracy is 84.5(0.3). accuracy of none accuracy is 74.0(0.7). f1 score of bicca f1 score is 57.6(3.0). f1 score of bicca f1 score is 64.6(1.0). f1 score of bicca f1 score is 59.5(1.8). accuracy of bicca accuracy is 84.7(0.8). accuracy of bicca accuracy is 75.0(1.8). f1 score of bicvm f1 score is 64.3(1.3). f1 score of bicvm f1 score is 66.8(1.0)). f1 score of bicvm f1 score is 61.9(1.0). accuracy of bicvm accuracy is 82.0(0.5). accuracy of bicvm accuracy is 70.6(1.7). f1 score of biskip f1 score is 61.5(1.7). f1 score of biskip f1 score is 66.6(0.9). f1 score of biskip f1 score is 63.9(1.2). accuracy of biskip accuracy is 84.4(0.7). accuracy of biskip accuracy is 73.8(0.9). f1 score of χ-gcm-skip f1 score is 62.0(1.9). f1 score of χ-gcm-skip f1 score is 67.4(1.3). f1 score of χ-gcm-skip f1 score is 63.2(1.5). accuracy of χ-gcm-skip accuracy is 84.8(0.6). accuracy of χ-gcm-skip accuracy is 74.0(0.6). f1 score of ρ-gcm-skip f1 score is 64.6(2.0). f1 score of ρ-gcm-skip f1 score is 67.7(1.4). f1 score of ρ-gcm-skip f1 score is 63.8(2.2). accuracy of ρ-gcm-skip accuracy is 84.9(0.7). accuracy of ρ-gcm-skip accuracy is 75.3(1.7).
table 4 shows p@k, r@k, and macro-f1 results over all labels (the union of s, f, and z). p@10 of cnn p@10 is 0.562. r@10 of cnn r@10 is 0.407. macro-f1 of cnn macro-f1 is 0.028. p@10 of acnn p@10 is 0.624. r@10 of acnn r@10 is 0.452. macro-f1 of acnn macro-f1 is 0.068. p@10 of match-cnn p@10 is 0.561. r@10 of match-cnn r@10 is 0.415. macro-f1 of match-cnn macro-f1 is 0.033. p@10 of zacnn p@10 is 0.577. r@10 of zacnn r@10 is 0.429. macro-f1 of zacnn macro-f1 is 0.037. p@10 of zagcnn p@10 is 0.587. r@10 of zagcnn r@10 is 0.439. macro-f1 of zagcnn macro-f1 is 0.038.
table 1 shows experimental results on the opensubtitles dataset. embedding similarity of ground truth emba is 1.000. embedding similarity of ground truth embe is 1.000. embedding similarity of ground truth embg is 1.000. ruber of ground truth rubg is 0.872. ruber of ground truth ruba is 0.881. diversity of ground truth dist1 is 0.091. diversity of ground truth dist2 is 0.423. diversity of ground truth entropy is 11.886. embedding similarity of seq2seq emba is 0.572. embedding similarity of seq2seq embe is 0.493. embedding similarity of seq2seq embg is 0.487. ruber of seq2seq rubg is 0.441. ruber of seq2seq ruba is 0.462. diversity of seq2seq dist1 is 0.015. diversity of seq2seq dist2 is 0.053. diversity of seq2seq entropy is 6.730. embedding similarity of cvae emba is 0.639. embedding similarity of cvae embe is 0.531. embedding similarity of cvae embg is 0.578. ruber of cvae rubg is 0.562. ruber of cvae ruba is 0.580. diversity of cvae dist1 is 0.026. diversity of cvae dist2 is 0.102. diversity of cvae entropy is 8.215. embedding similarity of cvae+bow loss emba is 0.659. embedding similarity of cvae+bow loss embe is 0.530. embedding similarity of cvae+bow loss embg is 0.526. ruber of cvae+bow loss rubg is 0.602. ruber of cvae+bow loss ruba is 0.597. diversity of cvae+bow loss dist1 is 0.041. diversity of cvae+bow loss dist2 is 0.302. diversity of cvae+bow loss entropy is 9.519. embedding similarity of ours (without sbow) emba is 0.678. embedding similarity of ours (without sbow) embe is 0.520. embedding similarity of ours (without sbow) embg is 0.563. ruber of ours (without sbow) rubg is 0.591. ruber of ours (without sbow) ruba is 0.604. diversity of ours (without sbow) dist1 is 0.031. diversity of ours (without sbow) dist2 is 0.259. diversity of ours (without sbow) entropy is 8.815. embedding similarity of ours emba is 0.714. embedding similarity of ours embe is 0.582. embedding similarity of ours embg is 0.642. ruber of ours rubg is 0.635. ruber of ours ruba is 0.642. diversity of ours dist1 is 0.053. diversity of ours dist2 is 0.404. diversity of ours entropy is 10.976.
table 3 shows ablation study for multitask learning on scierc development set. entity rec. of multi task (sciie) entity rec. is 68.1. relation of multi task (sciie) relation is 39.5. coref. of multi task (sciie) coref. is 58.0. entity rec. of single task entity rec. is 65.7. relation of single task relation is 37.9. coref. of single task coref. is 55.3. relation of single task+entity rec. relation is 38.9. coref. of single task+entity rec. coref. is 57.1. entity rec. of single task+relation entity rec. is 66.8. coref. of single task+relation coref. is 57.6. entity rec. of single task+coreference entity rec. is 67.5. relation of single task+coreference relation is 39.5.
table 5 shows single-task results for sentence-level tasks (macro-averaged f1 scores). f1 of svmtfidf drc is 34.0. f1 of svmtfidf sac is 10.3. f1 of svmtfidf src is 22.2. f1 of svmembeddings drc is 25.7. f1 of svmembeddings sac is 08.5. f1 of svmembeddings src is 19.3. f1 of neural: simple drc is 44.1. f1 of neural: simple sac is 20.5. f1 of neural: simple src is 31.5. f1 of neural: hierarchical drc is 42.6. f1 of neural: hierarchical sac is 19.1. f1 of neural: hierarchical src is 33.2.
table 3 shows results of automatic and human evaluations. bleu-1 of s2s bleu-1 is 13.8. bleu-2 of s2s bleu-2 is 2.48. sim of s2s sim is 14.7. dist-1 of s2s dist-1 is 2.50. dist-2 of s2s dist-2 is 16.2. dist-3 of s2s dist-3 is 34.9. dist-4 of s2s dist-4 is 50.0. con. of s2s con. is 1.79. flu. of s2s flu. is 1.84. mea. of s2s mea. is 1.71. poe. of s2s poe. is 1.60. ovr. of s2s ovr. is 1.74. bleu-1 of as2s bleu-1 is 15.5. bleu-2 of as2s bleu-2 is 2.59. sim of as2s sim is 14.8. dist-1 of as2s dist-1 is 2.30. dist-2 of as2s dist-2 is 15.2. dist-3 of as2s dist-3 is 31.4. dist-4 of as2s dist-4 is 44.3. con. of as2s con. is 1.92. flu. of as2s flu. is 1.71. mea. of as2s mea. is 1.80. poe. of as2s poe. is 1.74. ovr. of as2s ovr. is 1.79. bleu-1 of key-as2s bleu-1 is 15.8. bleu-2 of key-as2s bleu-2 is 1.92. sim of key-as2s sim is 19.8. dist-1 of key-as2s dist-1 is 3.00. dist-2 of key-as2s dist-2 is 16.3. dist-3 of key-as2s dist-3 is 33.0. dist-4 of key-as2s dist-4 is 45.6. con. of key-as2s con. is 2.21. flu. of key-as2s flu. is 2.15. mea. of key-as2s mea. is 1.92. poe. of key-as2s poe. is 2.23. ovr. of key-as2s ovr. is 2.13. bleu-1 of mem-as2s bleu-1 is 16.0. bleu-2 of mem-as2s bleu-2 is 1.48. sim of mem-as2s sim is 22.0. dist-1 of mem-as2s dist-1 is 3.40. dist-2 of mem-as2s dist-2 is 51.4. dist-3 of mem-as2s dist-3 is 87.9. dist-4 of mem-as2s dist-4 is 96.8. con. of mem-as2s con. is 1.70. flu. of mem-as2s flu. is 2.23. mea. of mem-as2s mea. is 2.09. poe. of mem-as2s poe. is 2.89. ovr. of mem-as2s ovr. is 2.23. bleu-1 of gan bleu-1 is 17.7. bleu-2 of gan bleu-2 is 2.54. sim of gan sim is 22.5. dist-1 of gan dist-1 is 2.50. dist-2 of gan dist-2 is 16.8. dist-3 of gan dist-3 is 35.3. dist-4 of gan dist-4 is 49.6. con. of gan con. is 2.36. flu. of gan flu. is 2.08. mea. of gan mea. is 2.01. poe. of gan poe. is 2.08. ovr. of gan ovr. is 2.13. bleu-1 of cvae bleu-1 is 17.0. bleu-2 of cvae bleu-2 is 1.73. sim of cvae sim is 13.7. dist-1 of cvae dist-1 is 4.70. dist-2 of cvae dist-2 is 52.3. dist-3 of cvae dist-3 is 90.6. dist-4 of cvae dist-4 is 99.0. con. of cvae con. is 1.69. flu. of cvae flu. is 2.16. mea. of cvae mea. is 2.14. poe. of cvae poe. is 2.58. ovr. of cvae ovr. is 2.14. bleu-1 of cvae-key bleu-1 is 16.4. bleu-2 of cvae-key bleu-2 is 1.83. sim of cvae-key sim is 31.0. dist-1 of cvae-key dist-1 is 4.31. dist-2 of cvae-key dist-2 is 43.0. dist-3 of cvae-key dist-3 is 80.6. dist-4 of cvae-key dist-4 is 95.8. con. of cvae-key con. is 1.83. flu. of cvae-key flu. is 2.29. mea. of cvae-key mea. is 2.08. poe. of cvae-key poe. is 2.53. ovr. of cvae-key ovr. is 2.18. bleu-1 of cvae-d bleu-1 is 18.1. bleu-2 of cvae-d bleu-2 is 2.85. sim of cvae-d sim is 36.3. dist-1 of cvae-d dist-1 is 5.20. dist-2 of cvae-d dist-2 is 59.2. dist-3 of cvae-d dist-3 is 94.2. dist-4 of cvae-d dist-4 is 99.8. con. of cvae-d con. is 2.58. flu. of cvae-d flu. is 2.35. mea. of cvae-d mea. is 2.34. poe. of cvae-d poe. is 2.96. ovr. of cvae-d ovr. is 2.56.
table 2 shows state-of-the-art comparison on vqa-1.0 dataset. bleu1 of sample (yang 2015) bleu1 is 38.8. meteor of sample (yang 2015) meteor is 12.7. rouge of sample (yang 2015) rouge is 34.2. cider of sample (yang 2015) cider is 13.3. bleu1 of max (yang 2015) bleu1 is 59.4. meteor of max (yang 2015) meteor is 17.8. rouge of max (yang 2015) rouge is 49.3. cider of max (yang 2015) cider is 33.1. bleu1 of image only bleu1 is 56.6. meteor of image only meteor is 15.1. rouge of image only rouge is 40.0. cider of image only cider is 31.0. bleu1 of caption only bleu1 is 57.1. meteor of caption only meteor is 15.5. rouge of caption only rouge is 36.6. cider of caption only cider is 30.5. bleu1 of mdn-attention bleu1 is 60.7. meteor of mdn-attention meteor is 16.7. rouge of mdn-attention rouge is 49.8. cider of mdn-attention cider is 33.6. bleu1 of mdn-hadamard bleu1 is 61.7. meteor of mdn-hadamard meteor is 16.7. rouge of mdn-hadamard rouge is 50.1. cider of mdn-hadamard cider is 29.3. bleu1 of mdn-addition bleu1 is 61.7. meteor of mdn-addition meteor is 18.3. rouge of mdn-addition rouge is 50.4. cider of mdn-addition cider is 42.6. bleu1 of mdn-joint (ours) bleu1 is 65.1. meteor of mdn-joint (ours) meteor is 22.7. rouge of mdn-joint (ours) rouge is 52.0. cider of mdn-joint (ours) cider is 33.1.
table 5 shows results of adding different components of our method in terms of rouge-1, rouge-2, rouge-l, strcom (equation 1) and strcov (equation 2) scores. r-1 of hierarchical-b. r-1 is 34.95. r-2 of hierarchical-b. r-2 is 14.79. r-l of hierarchical-b. r-l is 32.68. strcom of hierarchical-b. strcom is 0.22. strcov of hierarchical-b. strcov is 0.31. r-1 of +strcom r-1 is 37.03. r-2 of +strcom r-2 is 16.21. r-l of +strcom r-l is 34.44. strcom of +strcom strcom is 0.64. strcov of +strcom strcov is 0.71. r-1 of +strcov r-1 is 39.52. r-2 of +strcov r-2 is 17.12. r-l of +strcov r-l is 36.44. strcom of +strcov strcom is 0.65. strcov of +strcov strcov is 0.87. r-1 of +hierd r-1 is 40.30. r-2 of +hierd r-2 is 18.02. r-l of +hierd r-l is 37.36. strcom of +hierd strcom is 0.68. strcov of +hierd strcov is 0.93.
table 2 shows the results of different nmt models, including the bleu scores on newstest2014 and newstest2017, the perplexity on the validation set, and the accuracy of long-range dependencies. bleu 2014 of rnns2s bleu 2014 is 23.3. bleu 2017 of rnns2s bleu 2017 is 25.1. ppl of rnns2s ppl is 6.1. acc (%) of rnns2s acc (%) is 95.1. bleu 2014 of convs2s bleu 2014 is 23.9. bleu 2017 of convs2s bleu 2017 is 25.2. ppl of convs2s ppl is 7.0. acc (%) of convs2s acc (%) is 84.9. bleu 2014 of transformer bleu 2014 is 26.7. bleu 2017 of transformer bleu 2017 is 27.5. ppl of transformer ppl is 4.5. acc (%) of transformer acc (%) is 97.1. bleu 2014 of rnn-bideep bleu 2014 is 24.7. bleu 2017 of rnn-bideep bleu 2017 is 26.1. ppl of rnn-bideep ppl is 5.7. acc (%) of rnn-bideep acc (%) is 96.3.
table 5 shows the results of different architectures on newstest sets and contrawsd. ppl of rnns2s ppl is 5.7. bleu 2014 of rnns2s 2014 is 29.1. bleu 2017 of rnns2s 2017 is 30.1. acc (%) of rnns2s acc(%) is 84.0. ppl of rnns2s ppl is 7.06. bleu 2012 of rnns2s 2012 is 16.4. acc (%) of rnns2s acc(%) is 72.2. ppl of convs2s ppl is 6.3. bleu 2014 of convs2s 2014 is 29.1. bleu 2017 of convs2s 2017 is 30.4. acc (%) of convs2s acc(%) is 82.3. ppl of convs2s ppl is 7.93. bleu 2012 of convs2s 2012 is 16.8. acc (%) of convs2s acc(%) is 72.7. ppl of transformer ppl is 4.3. bleu 2014 of transformer 2014 is 32.7. bleu 2017 of transformer 2017 is 33.7. acc (%) of transformer acc(%) is 90.3. ppl of transformer ppl is 4.9. bleu 2012 of transformer 2012 is 18.7. acc (%) of transformer acc(%) is 76.7. bleu 2017 of uedin-wmt17 2017 is 35.1. acc (%) of uedin-wmt17 acc(%) is 87.9. ppl of transrnn ppl is 5.2. bleu 2014 of transrnn 2014 is 30.5. bleu 2017 of transrnn 2017 is 31.9. acc (%) of transrnn acc(%) is 86.1. ppl of transrnn ppl is 6.3. bleu 2012 of transrnn 2012 is 17.6. acc (%) of transrnn acc(%) is 74.2.
table 3 shows english!german translation results (section 4.3). size of 6 - is 76m. bleu score of 6 valid is 26.6±0.2 (26.9). bleu score of 6 test is 27.6±0.2 (27.9). speed (toks/sec) of 6 - is 20k. hours per epoch of 6 - is 2. size of 4 - is 79m. bleu score of 4 valid is 26.7±0.1 (26.8). bleu score of 4 test is 27.8±0.1 (28.3). speed (toks/sec) of 4 - is 22k. hours per epoch of 4 - is 1.8. size of 5 - is 90m. bleu score of 5 valid is 27.1±0.0 (27.2). bleu score of 5 test is 28.3±0.1 (28.4). speed (toks/sec) of 5 - is 19k. hours per epoch of 5 - is 2.1.
table 4 shows pearson’s scores of wme against other unsupervised, semi-supervised, and supervised methods on 22 textual similarity tasks. similarity of sts’12 pp is 58.7. similarity of sts’12  dan is 56.0. similarity of sts’12 rnn is 48.1. similarity of sts’12 irnn is 58.4. similarity of sts’12 lstm(no) is 51.0. similarity of sts’12 lstm(o.g.) is 46.4. similarity of sts’12 st is 30.8. similarity of sts’12 nbow is 52.5. similarity of sts’12 tf-idf is 58.7. similarity of sts’12 sif is 56.2. similarity of sts’12 wme is 60.6. similarity of sts’12 sif is 59.5. similarity of sts’12 wme is 62.8. similarity of sts’13 pp is 55.8. similarity of sts’13  dan is 54.2. similarity of sts’13 rnn is 44.7. similarity of sts’13 irnn is 56.7. similarity of sts’13 lstm(no) is 45.2. similarity of sts’13 lstm(o.g.) is 41.5. similarity of sts’13 st is 24.8. similarity of sts’13 nbow is 42.3. similarity of sts’13 tf-idf is 52.1. similarity of sts’13 sif is 56.6. similarity of sts’13 wme is 54.5. similarity of sts’13 sif is 61.8. similarity of sts’13 wme is 56.3. similarity of sts’14 pp is 70.9. similarity of sts’14  dan is 69.5. similarity of sts’14 rnn is 57.7. similarity of sts’14 irnn is 70.9. similarity of sts’14 lstm(no) is 59.8. similarity of sts’14 lstm(o.g.) is 51.5. similarity of sts’14 st is 31.4. similarity of sts’14 nbow is 54.2. similarity of sts’14 tf-idf is 63.8. similarity of sts’14 sif is 68.5. similarity of sts’14 wme is 65.5. similarity of sts’14 sif is 73.5. similarity of sts’14 wme is 68.0. similarity of sts’15 pp is 75.8. similarity of sts’15  dan is 72.7. similarity of sts’15 rnn is 57.2. similarity of sts’15 irnn is 75.6. similarity of sts’15 lstm(no) is 63.9. similarity of sts’15 lstm(o.g.) is 56.0. similarity of sts’15 st is 31.0. similarity of sts’15 nbow is 52.7. similarity of sts’15 tf-idf is 60.6. similarity of sts’15 sif is 71.7. similarity of sts’15 wme is 61.8. similarity of sts’15 sif is 76.3. similarity of sts’15 wme is 64.2. similarity of sick’14 pp is 71.6. similarity of sick’14  dan is 70.7. similarity of sick’14 rnn is 61.2. similarity of sick’14 irnn is 71.2. similarity of sick’14 lstm(no) is 63.9. similarity of sick’14 lstm(o.g.) is 59.0. similarity of sick’14 st is 49.8. similarity of sick’14 nbow is 65.9. similarity of sick’14 tf-idf is 69.4. similarity of sick’14 sif is 72.2. similarity of sick’14 wme is 68.0. similarity of sick’14 sif is 72.9. similarity of sick’14 wme is 68.1. similarity of twitter’15 pp is 52.9. similarity of twitter’15  dan is 53.7. similarity of twitter’15 rnn is 45.1. similarity of twitter’15 irnn is 52.9. similarity of twitter’15 lstm(no) is 47.6. similarity of twitter’15 lstm(o.g.) is 36.1. similarity of twitter’15 st is 24.7. similarity of twitter’15 nbow is 30.3. similarity of twitter’15 tf-idf is 33.8. similarity of twitter’15 sif is 48.0. similarity of twitter’15 wme is 41.6. similarity of twitter’15 sif is 49.0. similarity of twitter’15 wme is 47.4.
table 2 shows performance for classifying review segments as good or bad for recommendation justification. f1 of bow-xgboost f1 is 0.559. recall of bow-xgboost recall is 0.679. precision of bow-xgboost precision is 0.475. f1 of cnn f1 is 0.644. recall of cnn recall is 0.596. precision of cnn precision is 0.7. f1 of lstm-maxpool f1 is 0.675. recall of lstm-maxpool recall is 0.703. precision of lstm-maxpool precision is 0.65. f1 of bert f1 is 0.747. recall of bert recall is 0.7. precision of bert precision is 0.8. f1 of bert-sa (one epoch) f1 is 0.481. recall of bert-sa (one epoch) recall is 0.975. precision of bert-sa (one epoch) precision is 0.32. f1 of bert-sa (three epoch) f1 is 0.491. recall of bert-sa (three epoch) recall is 1. precision of bert-sa (three epoch) precision is 0.325.
table 4 shows human evaluation on ﬂuency, intraconsistency and inter-diversity of content selection on duc 2004. fluency of reference fluency is 0.96. fluency of enc-dec fluency is 0.83. fluency of bo.up. fluency is 0.46. intra-consistency of bo.up. intra-consistency is 0.48. inter-diversity of bo.up. inter-diversity is 0.61. fluency of ss fluency is 0.27. intra-consistency of ss intra-consistency is 0.41. inter-diversity of ss inter-diversity is 0.54. fluency of rs fluency is 0.78. intra-consistency of rs intra-consistency is 0.39. inter-diversity of rs inter-diversity is 0.47. fluency of vrs fluency is 0.74. intra-consistency of vrs intra-consistency is 0.72. inter-diversity of vrs inter-diversity is 0.87.
table 3 shows the performance of our baseline approaches is well below human performance. p of baseline 1 p is 0.44. r of baseline 1 r is 0.64. f1 of baseline 1 f1 is 0.52. p of baseline 1 p is 0.31. r of baseline 1 r is 0.26. f1 of baseline 1 f1 is 0.28. p of baseline 2 p is 0.49. r of baseline 2 r is 0.64. f1 of baseline 2 f1 is 0.55. p of baseline 2 p is 0.37. r of baseline 2 r is 0.32. f1 of baseline 2 f1 is 0.34. p of baseline 3 p is 0.47. r of baseline 3 r is 0.66. f1 of baseline 3 f1 is 0.55. p of baseline 3 p is 0.35. r of baseline 3 r is 0.37. f1 of baseline 3 f1 is 0.36. p of baseline 4 p is 0.6. r of baseline 4 r is 0.65. f1 of baseline 4 f1 is 0.62. p of baseline 4 p is 0.51. r of baseline 4 r is 0.48. f1 of baseline 4 f1 is 0.49. p of baseline 5 p is 0.68. r of baseline 5 r is 0.7. f1 of baseline 5 f1 is 0.69. p of baseline 5 p is 0.53. r of baseline 5 r is 0.48. f1 of baseline 5 f1 is 0.5. p of human p is 0.88. r of human r is 0.88. f1 of human f1 is 0.88. p of human p is 0.81. r of human r is 0.84. f1 of human f1 is 0.82.
table 1 shows results of nll on ubuntu and movie datasets with different models. loss of hred nll is 3.844. loss of vhred nll is ≤ 4.132. loss of vhred reconstruction is 3.765. loss of vhred kl-div. is 0.367. loss of dir-vhred nll is ≤ 3.999. loss of dir-vhred reconstruction is 3.614. loss of dir-vhred kl-div. is 0.385. loss of hred nll is 3.944. loss of vhred nll is ≤ 4.233. loss of vhred reconstruction is 3.904. loss of vhred kl-div. is 0.33. loss of dir-vhred nll is ≤ 4.073. loss of dir-vhred reconstruction is 3.741. loss of dir-vhred kl-div. is 0.332.
table 2 shows results on the image and video datasets of semantic textual similarity task. r of sts baseline ms-vid (2012) is 29.9. r of sts baseline pascal (2014) is 51.3. r of sts baseline pascal (2015) is 60.4. r of sts best system ms-vid (2012) is 86.3. r of sts best system pascal (2014) is 83.4. r of sts best system pascal (2015) is 86.4. r of gran (wieting et al. 2017) ms-vid (2012) is 83.7. r of gran (wieting et al. 2017) pascal (2014) is 84.5. r of gran (wieting et al. 2017) pascal (2015) is 85.0. r of vse (kiros et al. 2014) ms-vid (2012) is 80.6. r of vse (kiros et al. 2014) pascal (2014) is 82.7. r of vse (kiros et al. 2014) pascal (2015) is 89.6. r of oe (vendrov et al. 2015) ms-vid (2012) is 82.2. r of oe (vendrov et al. 2015) pascal (2014) is 84.1. r of oe (vendrov et al. 2015) pascal (2015) is 90.8. r of dan (nam et al. 2017) ms-vid (2012) is 84.1. r of dan (nam et al. 2017) pascal (2014) is 84.3. r of dan (nam et al. 2017) pascal (2015) is 90.8. r of vse++ (faghri et al. 2018) ms-vid (2012) is 84.5. r of vse++ (faghri et al. 2018) pascal (2014) is 84.8. r of vse++ (faghri et al. 2018) pascal (2015) is 91.2. r of scan (lee et al. 2018) ms-vid (2012) is 84.0. r of scan (lee et al. 2018) pascal (2014) is 83.9. r of scan (lee et al. 2018) pascal (2015) is 90.7. r of pivot (gella et al. 2017) ms-vid (2012) is 84.6. r of pivot (gella et al. 2017) pascal (2014) is 84.5. r of pivot (gella et al. 2017) pascal (2015) is 91.5. r of ours (w/ random) ms-vid (2012) is 85.8. r of ours (w/ random) pascal (2014) is 87.8. r of ours (w/ random) pascal (2015) is 91.5. r of ours (w/ fasttext) ms-vid (2012) is 86.2. r of ours (w/ fasttext) pascal (2014) is 88.3. r of ours (w/ fasttext) pascal (2015) is 91.8. r of ours (w/ bert) ms-vid (2012) is 86.4. r of ours (w/ bert) pascal (2014) is 88.0. r of ours (w/ bert) pascal (2015) is 91.7.
table 1 shows average cosine similarity score and delta-e distance over 5 runs. cosine similarity ± sd  of seen pairings rgb is 0.954±0.001. cosine similarity ± sd  of seen pairings wm18 hsv is 0.953±0.000 0.934±0.089. cosine similarity ± sd of seen pairings ensemble is 0.954±0.0. cosine similarity ± sd  of seen pairings wm18* is 0.68. cosine similarity ± sd  of seen pairings rgb is 3.121±0.027. delta-e ± sd  of seen pairings wm18 hsv is 3.188±0.062 5.380±4.846. delta-e ± sd  of seen pairings ensemble is 4.093±0.1. delta-e ± sd of seen pairings wm18* is 6.1. cosine similarity ± sd  of unseen pairings rgb is 0.799±0.044. cosine similarity ± sd  of unseen pairings wm18 hsv is 0.771±0.032 0.843±0.144. cosine similarity ± sd of unseen pairings ensemble is 0.797±0.0. cosine similarity ± sd  of unseen pairings wm18* is 0.68. cosine similarity ± sd  of unseen pairings rgb is 6.454±0.233. delta-e ± sd  of unseen pairings wm18 hsv is 6.825±0.093 11.701±3.358. delta-e ± sd  of unseen pairings ensemble is 5.873±0.0. delta-e ± sd of unseen pairings wm18* is 7.9. cosine similarity ± sd  of unseen ref. color rgb is 0.781±0.015. cosine similarity ± sd  of unseen ref. color wm18 hsv is 0.767±0.010 0.945±0.019. cosine similarity ± sd of unseen ref. color ensemble is 0.804±0.0. cosine similarity ± sd  of unseen ref. color wm18* is 0.4. cosine similarity ± sd  of unseen ref. color rgb is 7.456±0.184. delta-e ± sd  of unseen ref. color wm18 hsv is 7.658±0.363 10.429±2.523. delta-e ± sd  of unseen ref. color ensemble is 7.171±0.0. delta-e ± sd of unseen ref. color wm18* is 11.4. cosine similarity ± sd  of unseen modifiers rgb is 0.633±0.042. cosine similarity ± sd  of unseen modifiers wm18 hsv is 0.637±0.032 0.724±0.131. cosine similarity ± sd of unseen modifiers ensemble is 0.629±0.0. cosine similarity ± sd  of unseen modifiers wm18* is 0.41. cosine similarity ± sd  of unseen modifiers rgb is 13.288±1.082. delta-e ± sd  of unseen modifiers wm18 hsv is 13.891±1.077 14.183±5.175. delta-e ± sd  of unseen modifiers ensemble is 10.927±0.0. delta-e ± sd of unseen modifiers wm18* is 10.5. cosine similarity ± sd  of fully unseen rgb is 0.370±0.029. cosine similarity ± sd  of fully unseen wm18 hsv is 0.358±0.038 0.919±0.026. cosine similarity ± sd of fully unseen ensemble is 0.445±0.0. cosine similarity ± sd  of fully unseen wm18* is -0.21. cosine similarity ± sd  of fully unseen rgb is 13.859±0.874. delta-e ± sd  of fully unseen wm18 hsv is 14.516±0.587 12.432±2.170. delta-e ± sd  of fully unseen ensemble is 13.448±0.0. delta-e ± sd of fully unseen wm18* is 15.9. cosine similarity ± sd  of overall rgb is 0.858±0.006. cosine similarity ± sd  of overall wm18 hsv is 0.856±0.003 0.911±0.057. cosine similarity ± sd of overall ensemble is 0.868±0.0. cosine similarity ± sd  of overall wm18* is 0.65. cosine similarity ± sd  of overall rgb is 5.412±0.169. delta-e ± sd  of overall wm18 hsv is 5.595±0.128 7.487±3.940. delta-e ± sd  of overall ensemble is 5.777±0.0. delta-e ± sd of overall wm18* is 6.8.
table 2 shows results for dependency parsing evaluated on ptb treebank with gaze features as auxiliary task(s) learned from the disjoint dataset: dundee treebank. uas of baseline uas is 93.98. las of baseline las is 91.67. uas of baseline uas is 93.86. las of baseline las is 91.80. uas of total fix dur uas is 93.94. las of total fix dur las is 91.60. uas of total fix dur uas is 93.99. las of total fix dur las is 91.92. uas of mean fix dur uas is 94.12. las of mean fix dur las is 91.84. uas of mean fix dur uas is 93.95. las of mean fix dur las is 91.82. uas of n fix uas is 93.97. las of n fix las is 91.70. uas of n fix uas is 93.91. las of n fix las is 91.87. uas of fix prob uas is 93.98. las of fix prob las is 91.71. uas of fix prob uas is 93.99. las of fix prob las is 91.93. uas of basic feats aux uas is 94.00. las of basic feats aux las is 91.69. uas of basic feats aux uas is 93.84. las of basic feats aux las is 91.81. uas of first fix dur uas is 94.07. las of first fix dur las is 91.81. uas of first fix dur uas is 93.87. las of first fix dur las is 91.80. uas of first pass dur uas is 93.93. las of first pass dur las is 91.58. uas of first pass dur uas is 93.79. las of first pass dur las is 91.70. uas of early feats aux uas is 94.04. las of early feats aux las is 91.78. uas of early feats aux uas is 93.96. las of early feats aux las is 91.88. uas of n re-fix uas is 94.01. las of n re-fix las is 91.69. uas of n re-fix uas is 93.87. las of n re-fix las is 91.79. uas of reread prob uas is 94.03. las of reread prob las is 91.74. uas of reread prob uas is 93.98. las of reread prob las is 91.89. uas of late feats aux uas is 93.98. las of late feats aux las is 91.58. uas of late feats aux uas is 93.92. las of late feats aux las is 91.90. uas of w − 1 fix prob uas is 94.02. las of w − 1 fix prob las is 91.65. uas of w − 1 fix prob uas is 93.95. las of w − 1 fix prob las is 91.93. uas of w + 1 fix prob uas is 93.88. las of w + 1 fix prob las is 91.61. uas of w + 1 fix prob uas is 93.89. las of w + 1 fix prob las is 91.82. uas of w − 1 fix dur uas is 94.06. las of w − 1 fix dur las is 91.65. uas of w − 1 fix dur uas is 93.86. las of w − 1 fix dur las is 91.83. uas of w + 1 fix dur uas is 93.91. las of w + 1 fix dur las is 91.69. uas of w + 1 fix dur uas is 93.89. las of w + 1 fix dur las is 91.84. uas of context feats aux uas is 93.93. las of context feats aux las is 91.63. uas of context feats aux uas is 94.01. las of context feats aux las is 91.98.
table 6 shows performance variation across #labels per post fi of 1 fi is 0.729. fmacro of 1 fmacro is 0.527. acci of 1 acci is 0.632. fmicro of 1 fmicro is 0.637. fi of 2 fi is 0.754. fmacro of 2 fmacro is 0.675. acci of 2 acci is 0.631. fmicro of 2 fmicro is 0.722. fi of 3 fi is 0.781. fmacro of 3 fmacro is 0.721. acci of 3 acci is 0.652. fmicro of 3 fmicro is 0.764. fi of 4 fi is 0.743. fmacro of 4 fmacro is 0.722. acci of 4 acci is 0.604. fmicro of 4 fmicro is 0.735. fi of 5 fi is 0.739. fmacro of 5 fmacro is 0.592. acci of 5 acci is 0.592. fmicro of 5 fmicro is 0.735.
table 3 shows automatic and human evaluation of dialog generation. ppl of redial ppl is 28.1. dist-3 of redial dist-3 is 0.11. dist-4 of redial dist-4 is 0.13. cstc of redial cstc is 1.73. ppl of transformer ppl is 18.0. dist-3 of transformer dist-3 is 0.27. dist-4 of transformer dist-4 is 0.39. ppl of kbrd ppl is 17.9. dist-3 of kbrd dist-3 is 0.30. dist-4 of kbrd dist-4 is 0.45. cstc of kbrd cstc is 1.99.
table 1 shows automatic evaluation results for the task of question response generation. bleu-1 of seq2seq bleu-1 is 0.037. rouge-l of seq2seq rouge-l is 0.111. average of seq2seq average is 0.656. extreme of seq2seq extreme is 0.438. greedy of seq2seq greedy is 0.456. bleu-1 of cvae bleu-1 is 0.094. rouge-l of cvae rouge-l is 0.088. average of cvae average is 0.685. extreme of cvae extreme is 0.414. greedy of cvae greedy is 0.422. bleu-1 of htd bleu-1 is 0.073. rouge-l of htd rouge-l is 0.103. average of htd average is 0.647. extreme of htd extreme is 0.425. greedy of htd greedy is 0.439. bleu-1 of s2s-temp-mle bleu-1 is 0.097. rouge-l of s2s-temp-mle rouge-l is 0.119. average of s2s-temp-mle average is 0.699. extreme of s2s-temp-mle extreme is 0.438. greedy of s2s-temp-mle greedy is 0.457. bleu-1 of s2s-temp-none bleu-1 is 0.069. rouge-l of s2s-temp-none rouge-l is 0.092. average of s2s-temp-none average is 0.677. extreme of s2s-temp-none extreme is 0.429. greedy of s2s-temp-none greedy is 0.416. bleu-1 of s2s-temp-50% bleu-1 is 0.091. rouge-l of s2s-temp-50% rouge-l is 0.113. average of s2s-temp-50% average is 0.702. extreme of s2s-temp-50% extreme is 0.442. greedy of s2s-temp-50% greedy is 0.461. bleu-1 of s2s-temp bleu-1 is 0.102. rouge-l of s2s-temp rouge-l is 0.128. average of s2s-temp average is 0.710. extreme of s2s-temp extreme is 0.451. greedy of s2s-temp greedy is 0.469.
table 2 shows evaluation on supervised models. f1 of tfidf-ranker f1 is 32.5. bleu of tfidf-ranker bleu is 27.8. f1 of bert-ranker f1 is 38.3. bleu of bert-ranker bleu is 23.9. f1 of random recc. f1 is 3.6. bleu of random recc. bleu is 0.1. turn@1 of random recc. turn@1 is 21.3. turn@3 of random recc. turn@3 is 59.2. chat@1 of random recc. chat@1 is 23.1. chat@3 of random recc. chat@3 is 62.2. f1 of bert recc. f1 is 16.5. bleu of bert recc. bleu is 0.2. turn@1 of bert recc. turn@1 is 25.5. turn@3 of bert recc. turn@3 is 66.3. chat@1 of bert recc. chat@1 is 26.4. chat@3 of bert recc. chat@3 is 68.3. f1 of generate f1 is 39.5. bleu of generate bleu is 26.0. f1 of +predict f1 is 40.2. bleu of +predict bleu is 26.4. turn@1 of +predict turn@1 is 76.4. turn@3 of +predict turn@3 is 96.9. chat@1 of +predict chat@1 is 75.7. chat@3 of +predict chat@3 is 97.0. f1 of +decide f1 is 41.0. bleu of +decide bleu is 27.4. turn@1 of +decide turn@1 is 77.8. turn@3 of +decide turn@3 is 97.1. chat@1 of +decide chat@1 is 78.2. chat@3 of +decide chat@3 is 97.7. acc of +decide acc is 67.6. f1 of +plan f1 is 40.9. bleu of +plan bleu is 26.8. turn@1 of +plan turn@1 is 76.3. turn@3 of +plan turn@3 is 95.7. chat@1 of +plan chat@1 is 77.5. chat@3 of +plan chat@3 is 97.6. acc of +plan acc is 53.6.
table 3 shows performance on the organically-multimodal data; values within 1% of best-in-column are bolded. auc of random auc is 49.4. p@1 of random p@1 is 17.8. p@5 of random p@5 is 16.7. auc of random auc is 49.8. p@1 of random p@1 is 6.3. p@5 of random p@5 is 6.8. auc of obj detect auc is 58.7. p@1 of obj detect p@1 is 25.1. p@5 of obj detect p@5 is 21.5. auc of obj detect auc is 53.4. p@1 of obj detect p@1 is 17.9. p@5 of obj detect p@5 is 11.8. auc of nostruct auc is 60.5. p@1 of nostruct p@1 is 33.8. p@5 of nostruct p@5 is 27. auc of nostruct auc is 57. p@1 of nostruct p@1 is 13.3. p@5 of nostruct p@5 is 11.8. auc of dc auc is 63.5. p@1 of dc p@1 is 38.3. p@5 of dc p@5 is 30.6. auc of dc auc is 59.3. p@1 of dc p@1 is 20.8. p@5 of dc p@5 is 16.1. auc of tk auc is 67.9. p@1 of tk p@1 is 44. p@5 of tk p@5 is 35.8. auc of tk auc is 60.5. p@1 of tk p@1 is 21.2. p@5 of tk p@5 is 16. auc of tk+1/2k auc is 68.1. p@1 of tk+1/2k p@1 is 44.5. p@5 of tk+1/2k p@5 is 35.4. auc of tk+1/2k auc is 56. p@1 of tk+1/2k p@1 is 14.1. p@5 of tk+1/2k p@5 is 12.5. auc of ap auc is 69.3. p@1 of ap p@1 is 47.3. p@5 of ap p@5 is 37.3. auc of ap auc is 61.8. p@1 of ap p@1 is 22.5. p@5 of ap p@5 is 17.2. auc of ap+1/2k auc is 68.7. p@1 of ap+1/2k p@1 is 47.2. p@5 of ap+1/2k p@5 is 36.2. auc of ap+1/2k auc is 59.4. p@1 of ap+1/2k p@1 is 21.6. p@5 of ap+1/2k p@5 is 15.3.
table 8 shows performance comparison for different features with the bilstm-crf model. acc of word acc is 67.28. acc of word+chunking label acc is 67.84. acc of word+dependency label acc is 68.12. acc of word+pos tag acc is 68.26. acc of word+semantic role acc is 68.82. acc of all features acc is 68.54.
table 4 shows comparison with previous models on the multinli dataset. test accuracy of esim (chen et al., 2016) matched is 72.3. test accuracy of esim (chen et al., 2016) mismatched is 72.1. test accuracy of diin (gong et al., 2017) matched is 78.8. test accuracy of diin (gong et al., 2017) mismatched is 77.8. test accuracy of af-dmn (duan et al., 2018) matched is 76.9. test accuracy of af-dmn (duan et al., 2018) mismatched is 76.3. test accuracy of cafe (tay et al., 2018) matched is 78.7. test accuracy of cafe (tay et al., 2018) mismatched is 77.9. test accuracy of mwan (tan et al., 2018) matched is 78.5. test accuracy of mwan (tan et al., 2018) mismatched is 77.7. test accuracy of adin (ours) matched is 78.8. test accuracy of adin (ours) mismatched is 77.9. test accuracy of diin (gong et al., 2017) matched is 80. test accuracy of diin (gong et al., 2017) mismatched is 78.7. test accuracy of cafe (tay et al., 2018) matched is 80.2. test accuracy of cafe (tay et al., 2018) mismatched is 79. test accuracy of mwan (tan et al., 2018) matched is 79.8. test accuracy of mwan (tan et al., 2018) mismatched is 79.4. test accuracy of adin (ours) matched is 80.3. test accuracy of adin (ours) mismatched is 79.6.
table 3 shows comparison of memory consuming. memory of r8  text-gcn is  9979m (2841760). memory of r8 our model is 954m(250623). memory of r52  text-gcn is  8699m (3574162). memory of r52 our model is 951m(316669). memory of ohsumed  text-gcn is 13510m (6867490). memory of ohsumed our model is 1167m (419583).
table 5 shows an evaluation using the recall variant of rouge of the different extractive preprocessing steps. r1-r of heuristic labels r1-r is 80.25. r2-r of heuristic labels r2-r is 35.75. rl-r of heuristic labels rl-r is 55.11. r1-r of lead-200 r1-r is 60.89. r2-r of lead-200 r2-r is 22.31. rl-r of lead-200 rl-r is 44.97. r1-r of bm25 r1-r is 64.19. r2-r of bm25 r2-r is 25.04. rl-r of bm25 rl-r is 56.24. r1-r of sumfocus r1-r is 61.79. r2-r of sumfocus r2-r is 22.77. rl-r of sumfocus rl-r is 54.15. r1-r of sumfocus -topic r1-r is 60.95. r2-r of sumfocus -topic r2-r is 22.38. rl-r of sumfocus -topic rl-r is 53.61. r1-r of sumfocus -context r1-r is 61.03. r2-r of sumfocus -context r2-r is 22.08. rl-r of sumfocus -context rl-r is 53.32. r1-r of sumfocus -topic,-context r1-r is 59.11. r2-r of sumfocus -topic,-context r2-r is 20.62. rl-r of sumfocus -topic,-context rl-r is 51.93. r1-r of contentselector r1-r is 66.27. r2-r of contentselector r2-r is 27.59. rl-r of contentselector rl-r is 58.25. r1-r of contentselector -topic r1-r is 66.32. r2-r of contentselector -topic r2-r is 27.63. rl-r of contentselector -topic rl-r is 58.32. r1-r of contentselector -context r1-r is 63.86. r2-r of contentselector -context r2-r is 24.48. rl-r of contentselector -context rl-r is 55.8. r1-r of contentselector -topic,-context r1-r is 63.11. r2-r of contentselector -topic,-context r2-r is 23.76. rl-r of contentselector -topic,-context rl-r is 55.09.
table 4 shows rouge f1 results on the xsum test set. r1 of oracle.1 r1 is 29.79. r2 of oracle.1 r2 is 8.81. rl of oracle.1 rl is 22.66. r1 of lead r1 is 16.30. r2 of lead r2 is  1.60. rl of lead rl is 11.95. r1 of ptgen (see et al., 2017) r1 is 29.70. r2 of ptgen (see et al., 2017) r2 is  9.21. rl of ptgen (see et al., 2017) rl is 23.24. r1 of ptgen+cov (see et al., 2017) r1 is 28.10. r2 of ptgen+cov (see et al., 2017) r2 is  8.02. rl of ptgen+cov (see et al., 2017) rl is 21.72. r1 of tconvs2s (narayan et al., 2018a) r1 is 31.89. r2 of tconvs2s (narayan et al., 2018a) r2 is  11.54. rl of tconvs2s (narayan et al., 2018a) rl is 25.75. r1 of transformerabs r1 is 29.41. r2 of transformerabs r2 is  9.77. rl of transformerabs rl is 23.01. r1 of bertsumabs r1 is 38.76. r2 of bertsumabs r2 is  16.33. rl of bertsumabs rl is 31.15. r1 of bertsumextabs r1 is 38.81. r2 of bertsumextabs r2 is  16.50. rl of bertsumextabs rl is 31.27.
table 3 shows performance comparison on the ontonotes 5.0 english dataset. prec. of chiu and nichols (2016) prec. is 86.04. rec. of chiu and nichols (2016) rec. is 86.53. f1 of chiu and nichols (2016) f1 is 86.28. prec. of li et al. (2017) prec. is 88. rec. of li et al. (2017) rec. is 86.5. f1 of li et al. (2017) f1 is 87.21. f1 of ghaddar and langlais (2018) f1 is 87.95. f1 of strubell et al. (2017) f1 is 86.84. prec. of bilstm-crf (l = 0) prec. is 82.03. rec. of bilstm-crf (l = 0) rec. is 80.78. f1 of bilstm-crf (l = 0) f1 is 81.4. prec. of bilstm-crf (l = 1) prec. is 87.21. rec. of bilstm-crf (l = 1) rec. is 86.93. f1 of bilstm-crf (l = 1) f1 is 87.07. prec. of bilstm-crf (l = 2) prec. is 87.89. rec. of bilstm-crf (l = 2) rec. is 87.68. f1 of bilstm-crf (l = 2) f1 is 87.78. prec. of bilstm-crf (l = 3) prec. is 87.81. rec. of bilstm-crf (l = 3) rec. is 87.5. f1 of bilstm-crf (l = 3) f1 is 87.65. prec. of bilstm-gcn-crf prec. is 88.3. rec. of bilstm-gcn-crf rec. is 88.06. f1 of bilstm-gcn-crf f1 is 88.18. prec. of dglstm-crf (l = 0) prec. is 85.31. rec. of dglstm-crf (l = 0) rec. is 82.19. f1 of dglstm-crf (l = 0) f1 is 84.09. prec. of dglstm-crf (l = 1) prec. is 88.78. rec. of dglstm-crf (l = 1) rec. is 87.29. f1 of dglstm-crf (l = 1) f1 is 88.03. prec. of dglstm-crf (l = 2) prec. is 88.53. rec. of dglstm-crf (l = 2) rec. is 88.5. f1 of dglstm-crf (l = 2) f1 is 88.52. prec. of dglstm-crf (l = 3) prec. is 87.59. rec. of dglstm-crf (l = 3) rec. is 88.93. f1 of dglstm-crf (l = 3) f1 is 88.25.
table 8 shows f1 performance of dglstm-crf with predicted dependencies against the best performing bilstm-crf. f1 of bilstm-crf english is 88.98. f1 of bilstm-crf chinese is 79.2. f1 of bilstm-crf catalan is 78.46. f1 of bilstm-crf spanish is 80.59. f1 of dglstm-crf (predicted) english is 89.64. f1 of dglstm-crf (predicted) chinese is 79.59. f1 of dglstm-crf (predicted) catalan is 82.37. f1 of dglstm-crf (predicted) spanish is 83.92. f1 of dglstm-crf (gold) english is 89.88. f1 of dglstm-crf (gold) chinese is 79.92. f1 of dglstm-crf (gold) catalan is 84.22. f1 of dglstm-crf (gold) spanish is 87.56.
table 2 shows imdb test set results. accuracy of rb-bow-proto 2 is 53.6. accuracy of rb-bow-proto 6 is 63.6. accuracy of rb-bow-proto 10 is 68.2. accuracy of rb-bow-proto 20 is 75.9. accuracy of rb-bow-proto 60 is 80.9. accuracy of rb-bow-proto 200 is 82.5. accuracy of rb-bow-proto 400 is 82.6. accuracy of rb-bow-svm 2 is 53.6. accuracy of rb-bow-svm 6 is 63.5. accuracy of rb-bow-svm 10 is 67.0. accuracy of rb-bow-svm 20 is 73.4. accuracy of rb-bow-svm 60 is 80.2. accuracy of rb-bow-svm 200 is 79.7. accuracy of rb-bow-svm 400 is 80.2. accuracy of bow-proto 2 is 50.8. accuracy of bow-proto 6 is 55.6. accuracy of bow-proto 10 is 58.5. accuracy of bow-proto 20 is 62.5. accuracy of bow-proto 60 is 70.2. accuracy of bow-proto 200 is 71.3. accuracy of bow-proto 400 is 71.8. accuracy of rb-wavg-bert 2 is 51.8. accuracy of rb-wavg-bert 6 is 53.2. accuracy of rb-wavg-bert 10 is 52.3. accuracy of rb-wavg-bert 20 is 58.4. accuracy of rb-wavg-bert 60 is 81.6. accuracy of rb-wavg-bert 200 is 90.9. accuracy of rb-wavg-bert 400 is 92.2. accuracy of avg-bert 2 is 50.9. accuracy of avg-bert 6 is 53.5. accuracy of avg-bert 10 is 52.9. accuracy of avg-bert 20 is 58.5. accuracy of avg-bert 60 is 70.7. accuracy of avg-bert 200 is 83.8. accuracy of avg-bert 400 is 86.2. accuracy of ra-svm 2 is 50.6. accuracy of ra-svm 6 is 54. accuracy of ra-svm 10 is 52.7. accuracy of ra-svm 20 is 59.1. accuracy of ra-svm 60 is 70.2. accuracy of ra-svm 200 is 78.3. accuracy of ra-svm 400 is 81.5. accuracy of svm 2 is 50.7. accuracy of svm 6 is 52.5. accuracy of svm 10 is 52.1. accuracy of svm 20 is 54.9. accuracy of svm 60 is 62.5. accuracy of svm 200 is 71.2. accuracy of svm 400 is 75.8. accuracy of ra-cnn 2 is *. accuracy of ra-cnn 6 is *. accuracy of ra-cnn 10 is 52.6. accuracy of ra-cnn 20 is 59.8. accuracy of ra-cnn 60 is 79.9. accuracy of ra-cnn 200 is 86.2. accuracy of ra-cnn 400 is 87.3. accuracy of cnn 2 is *. accuracy of cnn 6 is *. accuracy of cnn 10 is 52.6. accuracy of cnn 20 is 54. accuracy of cnn 60 is 62. accuracy of cnn 200 is 79.1. accuracy of cnn 400 is 83.5. accuracy of ulmfit 2 is *. accuracy of ulmfit 6 is *. accuracy of ulmfit 10 is *. accuracy of ulmfit 20 is 54.7. accuracy of ulmfit 60 is 62. accuracy of ulmfit 200 is 71.3. accuracy of ulmfit 400 is 78.8.
table 3 shows comparison of mean accuracy (%) on odic 5-way acc. of matching networks (vinyals et al., 2016) 5-shot is 82.54±0.12. 5-way acc. of matching networks (vinyals et al., 2016) 10-shot is 84.63±0.08. 10-way acc. of matching networks (vinyals et al., 2016) 5-shot is 73.64±0.15. 10-way acc. of matching networks (vinyals et al., 2016) 10-shot is 76.72±0.07. 5-way acc. of prototypical networks (snell et al., 2017) 5-shot is 81.82±0.08. 5-way acc. of prototypical networks (snell et al., 2017) 10-shot is 85.83±0.06. 10-way acc. of prototypical networks (snell et al., 2017) 5-shot is 73.31±0.14. 10-way acc. of prototypical networks (snell et al., 2017) 10-shot is 75.97±0.11. 5-way acc. of graph network (garcia and bruna, 2017) 5-shot is 84.15±0.16. 5-way acc. of graph network (garcia and bruna, 2017) 10-shot is 87.24±0.09. 10-way acc. of graph network (garcia and bruna, 2017) 5-shot is 75.58±0.12. 10-way acc. of graph network (garcia and bruna, 2017) 10-shot is 78.27±0.10. 5-way acc. of relation network (sung et al., 2018) 5-shot is 84.41±0.14. 5-way acc. of relation network (sung et al., 2018) 10-shot is 86.93±0.15. 10-way acc. of relation network (sung et al., 2018) 5-shot is 75.28±0.13. 10-way acc. of relation network (sung et al., 2018) 10-shot is 78.61±0.06. 5-way acc. of snail (mishra et al., 2018) 5-shot is 84.62±0.16. 5-way acc. of snail (mishra et al., 2018) 10-shot is 87.31±0.11. 10-way acc. of snail (mishra et al., 2018) 5-shot is 75.74±0.07. 10-way acc. of snail (mishra et al., 2018) 10-shot is 79.26±0.09. 5-way acc. of induction networks (ours) 5-shot is 87.16±0.09. 5-way acc. of induction networks (ours) 10-shot is 88.49±0.17. 10-way acc. of induction networks (ours) 5-shot is 78.27±0.14. 10-way acc. of induction networks (ours) 10-shot is 81.64±0.08.
table 2 shows bleu scores on two wmt datasets of models using advanced decoding methods. bleu of transformer-base en-de is 27.3. bleu of our implementation en-de is 27.16. bleu of our implementation de-en is 31.44. bleu of our implementation en-ro is 32.92. bleu of our implementation ro-en is 33.09. bleu of cmlm-base (refinement 4) en-de is 22.06. bleu of cmlm-base (refinement 4) en-ro is 30.89. bleu of cmlm-base (refinement 10) en-de is 24.65. bleu of cmlm-base (refinement 10) en-ro is 32.53. bleu of flowseq-base (iwd n = 15) en-de is 20.2. bleu of flowseq-base (iwd n = 15) de-en is 24.63. bleu of flowseq-base (iwd n = 15) en-ro is 30.61. bleu of flowseq-base (iwd n = 15) ro-en is 31.5. bleu of flowseq-base (npd n = 15) en-de is 20.81. bleu of flowseq-base (npd n = 15) de-en is 25.76. bleu of flowseq-base (npd n = 15) en-ro is 31.38. bleu of flowseq-base (npd n = 15) ro-en is 32.01. bleu of flowseq-base (npd n = 30) en-de is 21.15. bleu of flowseq-base (npd n = 30) de-en is 26.04. bleu of flowseq-base (npd n = 30) en-ro is 31.74. bleu of flowseq-base (npd n = 30) ro-en is 32.45. bleu of flowseq-large (iwd n = 15) en-de is 22.94. bleu of flowseq-large (iwd n = 15) de-en is 27.16. bleu of flowseq-large (iwd n = 15) en-ro is 31.08. bleu of flowseq-large (iwd n = 15) ro-en is 32.03. bleu of flowseq-large (npd n = 15) en-de is 23.14. bleu of flowseq-large (npd n = 15) de-en is 27.71. bleu of flowseq-large (npd n = 15) en-ro is 31.97. bleu of flowseq-large (npd n = 15) ro-en is 32.46. bleu of flowseq-large (npd n = 30) en-de is 23.64. bleu of flowseq-large (npd n = 30) de-en is 28.29. bleu of flowseq-large (npd n = 30) en-ro is 32.35. bleu of flowseq-large (npd n = 30) ro-en is 32.91. bleu of nat-ir (refinement 10) en-de is 21.61. bleu of nat-ir (refinement 10) de-en is 25.48. bleu of nat-ir (refinement 10) en-ro is 29.32. bleu of nat-ir (refinement 10) ro-en is 30.19. bleu of nat w/ ft (npd n = 10) en-de is 18.66. bleu of nat w/ ft (npd n = 10) de-en is 22.42. bleu of nat w/ ft (npd n = 10) en-ro is 29.02. bleu of nat w/ ft (npd n = 10) ro-en is 31.44. bleu of nat-reg (npd n = 9) en-de is 24.61. bleu of nat-reg (npd n = 9) de-en is 28.9. bleu of lv nar (refinement 4) en-de is 24.2. bleu of cmlm-small (refinement 10) en-de is 25.51. bleu of cmlm-small (refinement 10) de-en is 29.47. bleu of cmlm-small (refinement 10) en-ro is 31.65. bleu of cmlm-small (refinement 10) ro-en is 32.27. bleu of cmlm-base (refinement 10) en-de is 26.92. bleu of cmlm-base (refinement 10) de-en is 30.86. bleu of cmlm-base (refinement 10) en-ro is 32.42. bleu of cmlm-base (refinement 10) ro-en is 33.06. bleu of flowseq-base (iwd n = 15) en-de is 22.49. bleu of flowseq-base (iwd n = 15) de-en is 27.4. bleu of flowseq-base (iwd n = 15) en-ro is 30.59. bleu of flowseq-base (iwd n = 15) ro-en is 31.58. bleu of flowseq-base (npd n = 15) en-de is 23.08. bleu of flowseq-base (npd n = 15) de-en is 28.07. bleu of flowseq-base (npd n = 15) en-ro is 31.35. bleu of flowseq-base (npd n = 15) ro-en is 32.11. bleu of flowseq-base (npd n = 30) en-de is 23.48. bleu of flowseq-base (npd n = 30) de-en is 28.4. bleu of flowseq-base (npd n = 30) en-ro is 31.75. bleu of flowseq-base (npd n = 30) ro-en is 32.49. bleu of flowseq-large (iwd n = 15) en-de is 24.7. bleu of flowseq-large (iwd n = 15) de-en is 29.44. bleu of flowseq-large (iwd n = 15) en-ro is 31.02. bleu of flowseq-large (iwd n = 15) ro-en is 31.97. bleu of flowseq-large (npd n = 15) en-de is 25.03. bleu of flowseq-large (npd n = 15) de-en is 30.48. bleu of flowseq-large (npd n = 15) en-ro is 31.89. bleu of flowseq-large (npd n = 15) ro-en is 32.43. bleu of flowseq-large (npd n = 30) en-de is 25.31. bleu of flowseq-large (npd n = 30) de-en is 30.68. bleu of flowseq-large (npd n = 30) en-ro is 32.2. bleu of flowseq-large (npd n = 30) ro-en is 32.84.
table 2 shows performance comparison (recall) against a random sample of categorized triples. recall of dbm sensory is 0.49. recall of dbm logical is 0.33. recall of dbm relative is 0.4. recall of dbm absolute is 0.41. recall of dbm essential is 0.29. recall of dbm incidental is 0.46. recall of ckg sensory is 0.28. recall of ckg logical is 0.5. recall of ckg relative is 0.2. recall of ckg absolute is 0.44. recall of ckg essential is 0.58. recall of ckg incidental is 0.31. recall of vfm sensory is 0.13. recall of vfm logical is 0.11. recall of vfm relative is 0.05. recall of vfm absolute is 0.14. recall of vfm essential is 0.1. recall of vfm incidental is 0.13. recall of edam(dbm+ckg+vfm) sensory is 0.62. recall of edam(dbm+ckg+vfm) logical is 0.63. recall of edam(dbm+ckg+vfm) relative is 0.5. recall of edam(dbm+ckg+vfm) absolute is 0.65. recall of edam(dbm+ckg+vfm) essential is 0.68. recall of edam(dbm+ckg+vfm) incidental is 0.6. recall of edam gain sensory is 27%. recall of edam gain logical is 26%. recall of edam gain relative is 25%. recall of edam gain absolute is 48%. recall of edam gain essential is 17%. recall of edam gain incidental is 30%.
table 7 shows dialogue act (da), intent class (ic), and slot labeling (sl) f1 scores by domain for the majority class, lstm, and elmo baselines on data annotated at the sentence (s) and turn (t) level. f1 of s da is 60.57. f1 of s ic is 33.69. f1 of s sl is 38.71. f1 of s da is 57.14. f1 of s ic is 25.42. f1 of s sl is 61.92. f1 of s da is 51.73. f1 of s ic is 37.37. f1 of s sl is 34.07. f1 of s da is 56.87. f1 of s ic is 38.37. f1 of s sl is 53.75. f1 of s da is 57.02. f1 of s ic is 30.42. f1 of s sl is 82.06. f1 of s da is 58.14. f1 of s ic is 33.32. f1 of s sl is 53.96. f1 of s da is 97.2. f1 of s ic is 90.84. f1 of s sl is 74.16. f1 of s da is 90.4. f1 of s ic is 86.09. f1 of s sl is 72.93. f1 of s da is 93.9. f1 of s ic is 90.06. f1 of s sl is 69.09. f1 of s da is 94.73. f1 of s ic is 93.3. f1 of s sl is 75.27. f1 of s da is 94.27. f1 of s ic is 92.35. f1 of s sl is 90.84. f1 of s da is 93.22. f1 of s ic is 90.95. f1 of s sl is 69.48. f1 of s da is 97.32. f1 of s ic is 91.88. f1 of s sl is 86.55. f1 of s da is 91.03. f1 of s ic is 87.95. f1 of s sl is 77.51. f1 of s da is 94.07. f1 of s ic is 91.15. f1 of s sl is 77.36. f1 of s da is 94.63. f1 of s ic is 94.27. f1 of s sl is 88.45. f1 of s da is 94.27. f1 of s ic is 93.32. f1 of s sl is 93.99. f1 of s da is 93.66. f1 of s ic is 92.25. f1 of s sl is 76.04. f1 of t da is 33.04. f1 of t ic is 32.79. f1 of t sl is 37.73. f1 of t da is 33.07. f1 of t ic is 25.33. f1 of t sl is 61.84. f1 of t da is 36.52. f1 of t ic is 38.16. f1 of t sl is 34.31. f1 of t da is 36.39. f1 of t ic is 39.42. f1 of t sl is 54.66. f1 of t da is 29.9. f1 of t ic is 31.82. f1 of t sl is 78.83. f1 of t da is 36.79. f1 of t ic is 33.78. f1 of t sl is 54.84. f1 of t da is 84.25. f1 of t ic is 89.15. f1 of t sl is 75.78. f1 of t da is 66.41. f1 of t ic is 87.35. f1 of t sl is 73.57. f1 of t da is 76.19. f1 of t ic is 92.3. f1 of t sl is 70.92. f1 of t da is 75.37. f1 of t ic is 94.75. f1 of t sl is 76.84. f1 of t da is 77.94. f1 of t ic is 94.35. f1 of t sl is 87.33. f1 of t da is 83.32. f1 of t ic is 89.78. f1 of t sl is 72.34. f1 of t da is 84.04. f1 of t ic is 89.99. f1 of t sl is 85.64. f1 of t da is 65.69. f1 of t ic is 88.96. f1 of t sl is 79.63. f1 of t da is 76.29. f1 of t ic is 94.5. f1 of t sl is 79.47. f1 of t da is 75.34. f1 of t ic is 95.39. f1 of t sl is 89.51. f1 of t da is 77.81. f1 of t ic is 94.76. f1 of t sl is 91.48. f1 of t da is 82.97. f1 of t ic is 90.85. f1 of t sl is 76.48.
table 2 shows results for all the models on the three datasets in our experiment. accuracy of frequency sentiment is 0.332. accuracy of frequency stance is 0.397. accuracy of frequency hate is 0.057. accuracy of ling sentiment is 0.676. accuracy of ling stance is 0.569. accuracy of ling hate is 0.624. accuracy of ling+random sentiment is 0.657. accuracy of ling+random stance is 0.571. accuracy of ling+random hate is 0.600. accuracy of ling+pv sentiment is 0.671. accuracy of ling+pv stance is 0.601*. accuracy of ling+pv hate is 0.667*. accuracy of ling+n2v sentiment is 0.672. accuracy of ling+n2v stance is 0.629*. accuracy of ling+n2v hate is 0.656*. accuracy of ling+gat sentiment is 0.666. accuracy of ling+gat stance is 0.640*. accuracy of ling+gat hate is 0.674*.
table 3 shows results of zero-shot intent classification. acc of devise acc is 0.7447. f1 of devise f1 is 0.7446. acc of devise acc is 0.5456. f1 of devise f1 is 0.3875. acc of cmt acc is 0.7396. f1 of cmt f1 is 0.7206. acc of cmt acc is 0.4452. f1 of cmt f1 is 0.4245. acc of cdssm acc is 0.7588. f1 of cdssm f1 is 0.758. acc of cdssm acc is 0.4308. f1 of cdssm f1 is 0.3765. acc of zero-shot dnn acc is 0.7165. f1 of zero-shot dnn f1 is 0.7116. acc of zero-shot dnn acc is 0.4615. f1 of zero-shot dnn f1 is 0.3897. acc of intentcapsnet acc is 0.7752. f1 of intentcapsnet f1 is 0.775. acc of intentcapsnet acc is 0.4864. f1 of intentcapsnet f1 is 0.4227. acc of recapsnet-zs-dim acc is 0.7868. f1 of recapsnet-zs-dim f1 is 0.7859. acc of recapsnet-zs-dim acc is 0.5005. f1 of recapsnet-zs-dim f1 is 0.4501. acc of recapsnet-zs-tm acc is 0.786. f1 of recapsnet-zs-tm f1 is 0.7837. acc of recapsnet-zs-tm acc is 0.5315. f1 of recapsnet-zs-tm f1 is 0.463. acc of recapsnet-zs acc is 0.7996. f1 of recapsnet-zs f1 is 0.798. acc of recapsnet-zs acc is 0.5418. f1 of recapsnet-zs f1 is 0.4769.
table 7 shows the performance of disp using ground-truth and recovered tokens over different types of attacks in sst-2. accuracy of disp g insertion is 0.8773. accuracy of disp g delete is 0.8681. accuracy of disp g swap is 0.8796. accuracy of disp g random is 0.797. accuracy of disp g embed is 0.7924. accuracy of disp g overall is 0.8429. accuracy of disp insertion is 0.8278. accuracy of disp delete is 0.8278. accuracy of disp swap is 0.8301. accuracy of disp random is 0.7773. accuracy of disp embed is 0.7784. accuracy of disp overall is 0.8083.
table 3 shows performance of eog on the cdr test set with different pre-trained word embeddings. f1 (%) of eog (pubmed) overall is 63.62. f1 (%) of eog (pubmed) intra is 68.25. f1 (%) of eog (pubmed) inter is 50.94. f1 (%) of eog (glove) overall is 63.01. f1 (%) of eog (glove) intra is 67.52. f1 (%) of eog (glove) inter is 50.26. f1 (%) of eog (random) overall is 61.41. f1 (%) of eog (random) intra is 66.8. f1 (%) of eog (random) inter is 46.51.
table 2 shows evaluation result of question generation. ruba of seq2seq ruba is 0.614. rubg of seq2seq rubg is 0.574. dist2 of seq2seq dist2 is 0.008. ppl of seq2seq ppl is 63.02. ruba of cvae ruba is 0.682. rubg of cvae rubg is 0.649. dist2 of cvae dist2 is 0.112. ppl of cvae ppl is 20.39. ruba of std ruba is 0.658. rubg of std rubg is 0.613. dist2 of std dist2 is 0.010. ppl of std ppl is 28.75. ruba of htd ruba is 0.689. rubg of htd rubg is 0.654. dist2 of htd dist2 is 0.114. ppl of htd ppl is 26.02. ruba of cvae (qt) ruba is 0.688. rubg of cvae (qt) rubg is 0.652. dist2 of cvae (qt) dist2 is 0.114. ppl of cvae (qt) ppl is 20.03. ruba of a-cvae ruba is 0.715. rubg of a-cvae rubg is 0.661. dist2 of a-cvae dist2 is 0.123. ppl of a-cvae ppl is 19.51. ruba of rl-cvae ruba is 0.720. rubg of rl-cvae rubg is 0.668. dist2 of rl-cvae dist2 is 0.185. ppl of rl-cvae ppl is 16.93.
table 3 shows decoding algorithms’ impact on performance. accuracy of viterbi (map) hl-ccrf is 72.26. accuracy of viterbi (map) sl-ccrf is 74.69. accuracy of smoothing hl-ccrf is 72.30. accuracy of smoothing sl-ccrf is 74.73.
table 1 shows analyses on spanish and french monolingual embeddings before and after bias mitigation. mweat–diff of original mweat–diff is 3.6918. mweat–p-value of original mweat–p-value is 0.0000. mweat–diff of original mweat–diff is 2.3437. mweat–p-value of original mweat–p-value is 0.0000. word similarity of original word similarity is 0.7392. word similarity of original word similarity is 0.7294. mweat–diff of shift_ori mweat–diff is 0.3090. mweat–p-value of shift_ori mweat–p-value is 0.1130. mweat–diff of shift_ori mweat–diff is 0.2446. mweat–p-value of shift_ori mweat–p-value is 0.1470. word similarity of shift_ori word similarity is 0.7363. word similarity of shift_ori word similarity is 0.7218. mweat–diff of shift_en mweat–diff is 0.3324. mweat–p-value of shift_en mweat–p-value is 0.0010. mweat–diff of shift_en mweat–diff is 0.3882. mweat–p-value of shift_en mweat–p-value is 0.0010. word similarity of shift_en word similarity is 0.7359. word similarity of shift_en word similarity is 0.7218. mweat–diff of de-align mweat–diff is 3.5748. mweat–p-value of de-align mweat–p-value is 0.0010. mweat–diff of de-align mweat–diff is 2.3436. mweat–p-value of de-align mweat–p-value is 0.0020. word similarity of de-align word similarity is 0.7392. word similarity of de-align word similarity is 0.7156. mweat–diff of hybrid_ori mweat–diff is 0.3090. mweat–p-value of hybrid_ori mweat–p-value is 0.7330. mweat–diff of hybrid_ori mweat–diff is 0.2446. mweat–p-value of hybrid_ori mweat–p-value is 0.5290. word similarity of hybrid_ori word similarity is 0.7358. word similarity of hybrid_ori word similarity is 0.7218. mweat–diff of hybrid_en mweat–diff is 2.2494. mweat–p-value of hybrid_en mweat–p-value is 0.0020. mweat–diff of hybrid_en mweat–diff is 1.1758. mweat–p-value of hybrid_en mweat–p-value is 0.0910. word similarity of hybrid_en word similarity is 0.7356. word similarity of hybrid_en word similarity is 0.7218.
table 2 shows variant results on followup dev set. symacc (%) of star symacc (%) is 55.38. bleu (%) of star bleu (%) is 67.62. symacc (%) of – phase i symacc (%) is 40.63. bleu (%) of – phase i bleu (%) is 61.82. symacc (%) of – phase ii symacc (%) is 23.12. bleu (%) of – phase ii bleu (%) is 48.65. symacc (%) of – rl symacc (%) is 41.25. bleu (%) of – rl bleu (%) is 60.19. symacc (%) of + basic reward symacc (%) is 43.13. bleu (%) of + basic reward bleu (%) is 58.48. symacc (%) of + oracle reward symacc (%) is 45.20. bleu (%) of + oracle reward bleu (%) is 63.04. symacc (%) of + uniform reward symacc (%) is 53.40. bleu (%) of + uniform reward bleu (%) is 66.93.
table 4 shows performance on the cb test set and the multinli dev set. acc. of cbow acc. is 69.2. f1 of cbow f1 is 47.6. acc. of cbow acc. is 71.2. f1 of cbow f1 is 71.2. acc. of mnlib acc. is 77.6. f1 of mnlib f1 is 66.7. acc. of mnlib acc. is 83.6. f1 of mnlib f1 is 83.6. acc. of heuristics acc. is 81.2. f1 of heuristics f1 is 71.3. acc. of cbb acc. is 85.2. f1 of cbb f1 is 81.2. acc. of cbb acc. is 41.1. f1 of cbb f1 is 30.6. acc. of mnli+cbb acc. is 91.2. f1 of mnli+cbb f1 is 85.3. acc. of mnli+cbb acc. is 72.3. f1 of mnli+cbb f1 is 74.4. acc. of human acc. is 98.9. f1 of human f1 is 95.8. acc. of human acc. is 92.0/92.8.
table 3 shows experimental results on two mams datasets and semeval-2014 restaurant review dataset for both atsa and acsa subtasks. accuracy of textcnn mams is 52.694. accuracy of textcnn mams-small is 51.736. accuracy of textcnn restaurant is 75.928. accuracy of textcnn mams is 48.856. accuracy of textcnn mams-small is 48.814. accuracy of textcnn restaurant is 81.418. accuracy of lstm mams is 52.486. accuracy of lstm mams-small is 50.134. accuracy of lstm restaurant is 75.552. accuracy of lstm mams is 48.546. accuracy of lstm mams-small is 48.346. accuracy of lstm restaurant is 82.076. accuracy of td lstm mams is 74.596. accuracy of td lstm mams-small is 73.582. accuracy of td lstm restaurant is 75.63. accuracy of at lstm mams is 77.558. accuracy of at lstm mams-small is 73.028. accuracy of at lstm restaurant is 76.2. accuracy of at lstm mams is 66.436. accuracy of at lstm mams-small is 65.128. accuracy of at lstm restaurant is 83.1. accuracy of atae lstm mams is 77.054. accuracy of atae lstm mams-small is 71.708. accuracy of atae lstm restaurant is 77.2. accuracy of atae lstm mams is 70.634. accuracy of atae lstm mams-small is 66.814. accuracy of atae lstm restaurant is 84. accuracy of bilstm+attn mams is 76.166. accuracy of bilstm+attn mams-small is 70.72. accuracy of bilstm+attn restaurant is 77.356. accuracy of bilstm+attn mams is 66.304. accuracy of bilstm+attn mams-small is 66.262. accuracy of bilstm+attn restaurant is 80.514. accuracy of ian mams is 76.602. accuracy of ian mams-small is 71.168. accuracy of ian restaurant is 78.6. accuracy of aoa lstm mams is 77.26. accuracy of aoa lstm mams-small is 72.29. accuracy of aoa lstm restaurant is 81.2. accuracy of aen mams is 66.722. accuracy of aen mams-small is 61.706. accuracy of aen restaurant is 80.98. accuracy of memnet mams is 64.568. accuracy of memnet mams-small is 62.694. accuracy of memnet restaurant is 80.95. accuracy of memnet mams is 63.288. accuracy of memnet mams-small is 62.818. accuracy of memnet restaurant is 77.862. accuracy of gcae mams is 77.588. accuracy of gcae mams-small is 73.246. accuracy of gcae restaurant is 77.28. accuracy of gcae mams is 72.098. accuracy of gcae mams-small is 66.968. accuracy of gcae restaurant is 79.35. accuracy of capsnet mams is 79.776. accuracy of capsnet mams-small is 73.86. accuracy of capsnet restaurant is 80.786. accuracy of capsnet mams is 73.986. accuracy of capsnet mams-small is 67.128. accuracy of capsnet restaurant is 83.554. accuracy of bert mams is 82.218. accuracy of bert mams-small is 79.44. accuracy of bert restaurant is 84.46. accuracy of bert mams is 78.292. accuracy of bert mams-small is 75.316. accuracy of bert restaurant is 90.442. accuracy of capsnet-bert mams is 83.391. accuracy of capsnet-bert mams-small is 80.91. accuracy of capsnet-bert restaurant is 85.934. accuracy of capsnet-bert mams is 79.461. accuracy of capsnet-bert mams-small is 76.366. accuracy of capsnet-bert restaurant is 91.375. accuracy of capsnet-dr mams is 79.434. accuracy of capsnet-dr mams-small is 71.398. accuracy of capsnet-dr restaurant is 77.768. accuracy of capsnet-dr mams is 69.036. accuracy of capsnet-dr mams-small is 65.638. accuracy of capsnet-dr restaurant is 81.904. accuracy of capsnet-bert-dr mams is 82.97. accuracy of capsnet-bert-dr mams-small is 80.092. accuracy of capsnet-bert-dr restaurant is 84.646. accuracy of capsnet-bert-dr mams is 79.132. accuracy of capsnet-bert-dr mams-small is 75.634. accuracy of capsnet-bert-dr restaurant is 90.736.
table 6 shows results by word category and role label. p (%) of verb / a0 p (%) is 90.8. r (%) of verb / a0 r (%) is 89.2. p (%) of verb / a0 p (%) is −0.4. r (%) of verb / a0 r (%) is +1.8. p (%) of verb / a1 p (%) is 91.0. r (%) of verb / a1 r (%) is 91.9. p (%) of verb / a1 p (%) is +0.0. r (%) of verb / a1 r (%) is +1.1. p (%) of verb / a2 p (%) is 84.3. r (%) of verb / a2 r (%) is 76.9. p (%) of verb / a2 p (%) is +1.5. r (%) of verb / a2 r (%) is +0.0. p (%) of verb / am p (%) is 82.2. r (%) of verb / am r (%) is 72.4. p (%) of verb / am p (%) is +2.9. r (%) of verb / am r (%) is −2.0. p (%) of noun / a0 p (%) is 86.9. r (%) of noun / a0 r (%) is 78.2. p (%) of noun / a0 p (%) is +0.8. r (%) of noun / a0 r (%) is +3.3. p (%) of noun / a1 p (%) is 87.5. r (%) of noun / a1 r (%) is 84.4. p (%) of noun / a1 p (%) is +2.6. r (%) of noun / a1 r (%) is +2.2. p (%) of noun / a2 p (%) is 82.4. r (%) of noun / a2 r (%) is 76.8. p (%) of noun / a2 p (%) is +1.0. r (%) of noun / a2 r (%) is +2.1. p (%) of noun / am p (%) is 79.5. r (%) of noun / am r (%) is 69.2. p (%) of noun / am p (%) is +0.9. r (%) of noun / am r (%) is −2.8.
table 1 shows kb completion results on nci-pid test: comparison of our compositional learning approach (all-paths+nodes) with baseline systems. map of bilinear-diag (guu et al., 2015) d=100 map is 12.48. hits@10 of bilinear-diag (guu et al., 2015) d=100 hits@10 is 19.66. map of bilinear-diag (guu et al., 2015) d=100 map is 12.48. hits@10 of bilinear-diag (guu et al., 2015) d=100 hits@10 is 19.66. map of bilinear-diag d=100 map is 28.56. hits@10 of bilinear-diag d=100 hits@10 is 39.92. map of bilinear-diag d=100 map is 28.56. hits@10 of bilinear-diag d=100 hits@10 is 39.92. map of bilinear-diag d=2000 map is 30.16. hits@10 of bilinear-diag d=2000 hits@10 is 42.51. map of bilinear-diag d=2000 map is 30.16. hits@10 of bilinear-diag d=2000 hits@10 is 42.51. map of +guu et al. (2015) d=100 orig. map is 23.2. hits@10 of +guu et al. (2015) d=100 orig. hits@10 is 34.84. map of +guu et al. (2015) d=100 orig. map is 23.2. hits@10 of +guu et al. (2015) d=100 orig. hits@10 is 24.84. map of +guu et al. (2015) d=100 reimpl. map is 29.13. hits@10 of +guu et al. (2015) d=100 reimpl. hits@10 is 40.59. map of +guu et al. (2015) d=100 reimpl. map is 30.25. hits@10 of +guu et al. (2015) d=100 reimpl. hits@10 is 41.45. map of pruned-paths d=100 c=1000 map is 32.31. hits@10 of pruned-paths d=100 c=1000 hits@10 is 43.16. map of pruned-paths d=100 c=1000 map is 36.42. hits@10 of pruned-paths d=100 c=1000 hits@10 is 48.22. map of pruned-paths d=100 c=100 map is 32.31. hits@10 of pruned-paths d=100 c=100 hits@10 is 43.16. map of pruned-paths d=100 c=100 map is 36.79. hits@10 of pruned-paths d=100 c=100 hits@10 is 48.27. map of pruned-paths d=100 c=1 map is 32.31. hits@10 of pruned-paths d=100 c=1 hits@10 is 43.16. map of pruned-paths d=100 c=1 map is 37.03. hits@10 of pruned-paths d=100 c=1 hits@10 is 48.26. map of all-paths d=100 map is 32.31. hits@10 of all-paths d=100 hits@10 is 43.16. map of all-paths d=100 map is 36.24. hits@10 of all-paths d=100 hits@10 is 48.6. map of all-paths+nodes d=100 map is 33.92. hits@10 of all-paths+nodes d=100 hits@10 is 45.96. map of all-paths+nodes d=100 map is 39.31. hits@10 of all-paths+nodes d=100 hits@10 is 52.53.
table 8 shows comparison with previous work on english-german translation. bleu of mle bleu is 16.46. bleu of mle bleu is 18.97. bleu of mle bleu is 19.4. bleu of mle bleu is 20.9. bleu of mle bleu is 16.45. bleu of mrt bleu is 18.02. bleu of mrt bleu is 20.45.
table 2 shows results (f1) for baselines for sentence boundary detection on dev sets. f1 of opennlp wsj is 98.09. f1 of corenlp wsj is 98.6. f1 of marmot wsj is 98.21. f1 of marmot switchboard is 71.78.
table 5 shows mrr for the ret task. mrr of ret-ir mrr is 0.42 ± 0.02 (0.44 ± 0.01). mrr of code-nn mrr is 0.58 ± 0.01 (0.66 ± 0.02). mrr of ret-ir mrr is 0.28 ±0.01(0.4 ± 0.01). mrr of code-nn mrr is 0.44 ± 0.01 (0.54 ± 0.02).
table 5 shows performance of j48 for different feature settings rec. of without domain features rec. is 0.803. prec. of without domain features prec. is 0.801. f-score of without domain features f-score is 0.802. rec. of only domain features rec. is 0.725. prec. of only domain features prec. is 0.699. f-score of only domain features f-score is 0.667. rec. of only phonological features rec. is 0.790. prec. of only phonological features prec. is 0.786. f-score of only phonological features f-score is 0.787. rec. of without poetic features rec. is 0.836. prec. of without poetic features prec. is 0.832. f-score of without poetic features f-score is 0.833. rec. of without consonance feature rec. is 0.823. prec. of without consonance feature prec. is 0.820. f-score of without consonance feature f-score is 0.821. rec. of without emotions features rec. is 0.814. prec. of without emotions features prec. is 0.810. f-score of without emotions features f-score is 0.811. rec. of without phonological features rec. is 0.798. prec. of without phonological features prec. is 0.792. f-score of without phonological features f-score is 0.793. rec. of without social features rec. is 0.807. prec. of without social features prec. is 0.803. f-score of without social features f-score is 0.804. rec. of all features rec. is 0.824. prec. of all features prec. is 0.822. f-score of all features f-score is 0.823.
table 4 shows performance of various models using 50 dimensional we features. precision of nn+we precision is 81.86. recall of nn+we recall is 76.82. f1 score of nn+we f1 score is 79.26. precision of nn+we precision is 80.32. recall of nn+we recall is 73.58. f1 score of nn+we f1 score is 76.81. precision of bi-rnn+we precision is 84.14. recall of bi-rnn+we recall is 77.46. f1 score of bi-rnn+we f1 score is 80.67. precision of bi-rnn+we precision is 82.49. recall of bi-rnn+we recall is 73.58. f1 score of bi-rnn+we f1 score is 77.78. precision of bi-gru+we precision is 84.51. recall of bi-gru+we recall is 78.23. f1 score of bi-gru+we f1 score is 81.25. precision of bi-gru+we precision is 82.32. recall of bi-gru+we recall is 75.16. f1 score of bi-gru+we f1 score is 78.58. precision of bi-lstm+we precision is 85.13. recall of bi-lstm+we recall is 77.72. f1 score of bi-lstm+we f1 score is 81.26. precision of bi-lstm+we precision is 84.87. recall of bi-lstm+we recall is 74.11. f1 score of bi-lstm+we f1 score is 79.13. precision of nn+we precision is 65.33. recall of nn+we recall is 56.43. f1 score of nn+we f1 score is 60.55. precision of nn+we precision is 64.23. recall of nn+we recall is 57.14. f1 score of nn+we f1 score is 60.48. precision of bi-rnn+we precision is 63.62. recall of bi-rnn+we recall is 56.84. f1 score of bi-rnn+we f1 score is 60.04. precision of bi-rnn+we precision is 67.47. recall of bi-rnn+we recall is 57.50. f1 score of bi-rnn+we f1 score is 62.09. precision of bi-gru+we precision is 66.42. recall of bi-gru+we recall is 57.41. f1 score of bi-gru+we f1 score is 61.59. precision of bi-gru+we precision is 68.25. recall of bi-gru+we recall is 58.58. f1 score of bi-gru+we f1 score is 63.05. precision of bi-lstm+we precision is 67.48. recall of bi-lstm+we recall is 58.01. f1 score of bi-lstm+we f1 score is 62.39. precision of bi-lstm+we precision is 68.97. recall of bi-lstm+we recall is 58.25. f1 score of bi-lstm+we f1 score is 63.16.
table 1 shows subjective evaluation of the obj=subj, off-line rnn, subj and on-line gp system during different stages of on-line policy learning. subjective (%) of obj=subj subjective (%) is 85.0 ± 2.1. subjective (%) of off-line rnn subjective (%) is 89.0 ± 1.8. subjective (%) of subj subjective (%) is 90.7 ± 1.7. subjective (%) of on-line gp subjective (%) is 91.7 ± 1.6. subjective (%) of subj subjective (%) is 87.1 ± 1.0. subjective (%) of on-line gp subjective (%) is 90.9 ± 0.9*.
table 4 shows development and test set results for shiftreduce dependency parser on penn chinese treebank (ctb-5) using only (s1, s0, q0) position features (trained and tested with gold pos tags). uas of c & m 2014 uas is 84.0. las of c & m 2014 las is 82.4. uas of c & m 2014 uas is 83.9. las of c & m 2014 las is 82.4. uas of dyer et al. 2015 uas is 87.2. las of dyer et al. 2015 las is 85.9. uas of dyer et al. 2015 uas is 87.2. las of dyer et al. 2015 las is 85.7. uas of bi-lstm uas is 85.84. las of bi-lstm las is 85.24. uas of bi-lstm uas is 85.53. las of bi-lstm las is 84.89. uas of 2-layer bi-lstm uas is 86.13. las of 2-layer bi-lstm las is 85.51. uas of 2-layer bi-lstm uas is 86.35. las of 2-layer bi-lstm las is 85.71.
table 4 shows evaluation results on the squad dataset (single model only). em of dynamic coattention networks (xiong et al. 2016) em is 65.4. f1 of dynamic coattention networks (xiong et al. 2016) f1 is 75.6. em of dynamic coattention networks (xiong et al. 2016) em is 66.2. f1 of dynamic coattention networks (xiong et al. 2016) f1 is 75.9. em of multi-perspective matching (wang et al. 2016)† em is 66.1. f1 of multi-perspective matching (wang et al. 2016)† f1 is 75.8. em of multi-perspective matching (wang et al. 2016)† em is 65.5. f1 of multi-perspective matching (wang et al. 2016)† f1 is 75.1. em of bidaf (seo et al. 2016) em is 67.7. f1 of bidaf (seo et al. 2016) f1 is 77.3. em of bidaf (seo et al. 2016) em is 68.0. f1 of bidaf (seo et al. 2016) f1 is 77.3. em of r-net† em is n/a. f1 of r-net† f1 is n/a. em of r-net† em is 71.3. f1 of r-net† f1 is 79.7. em of drqa (our model document reader only) em is 69.5. f1 of drqa (our model document reader only) f1 is 78.8. em of drqa (our model document reader only) em is 70.0. f1 of drqa (our model document reader only) f1 is 79.0.
table 2 shows multi-source parsing results in terms of average accuracy % over 3 runs. accuracy of - en+de+el is 83.21. accuracy of - en+de+th is 82.02. accuracy of - en+el+th is 82.62. accuracy of - de+el+th is 79.64. accuracy of - en+de+el+th is 82.50. accuracy of - en+id is 82.81. accuracy of - en+zh is 82.81. accuracy of - id+zh is 78.50. accuracy of - en+id+zh is 83.11. accuracy of word en+de+el is 85.48. accuracy of word en+de+th is 86.19. accuracy of word en+el+th is 85.60. accuracy of word de+el+th is 72.14. accuracy of word en+de+el+th is 85.48. accuracy of word en+id is 83.93. accuracy of word en+zh is 82.96. accuracy of word id+zh is 76.79. accuracy of word en+id+zh is 82.22. accuracy of sentence en+de+el is 86.43. accuracy of sentence en+de+th is 85.48. accuracy of sentence en+el+th is 85.24. accuracy of sentence de+el+th is 76.43. accuracy of sentence en+de+el+th is 86.79. accuracy of sentence en+id is 83.78. accuracy of sentence en+zh is 82.96. accuracy of sentence id+zh is 77.75. accuracy of sentence en+id+zh is 83.85.
table 1 shows bleu results for the wmt16 experiment bleu of bpe2bpe newstest2015 is 27.33. bleu of bpe2bpe newstest2016 is 31.19. bleu of bpe2tree newstest2015 is 27.36. bleu of bpe2tree newstest2016 is 32.13. bleu of bpe2bpe ens. newstest2015 is 28.62. bleu of bpe2bpe ens. newstest2016 is 32.38. bleu of bpe2tree ens. newstest2015 is 28.7. bleu of bpe2tree ens. newstest2016 is 33.24.
table 3 shows results of using different word embeddings. p of +glove p is 36.00. r of +glove r is 16.36. f1 of +glove f1 is 22.50. p of +glove p is 53.97. r of +glove r is 23.45. f1 of +glove f1 is 32.69. p of +glove p is 44.90. r of +glove r is 40.29. f1 of +glove f1 is 42.47. p of +glove p is 60.47. r of +glove r is 76.21. f1 of +glove f1 is 67.43. accuracy of +glove accuracy is 55.68. macro f1 of +glove macro f1 is 41.27. p of +word2vec p is 27.03. r of +word2vec r is 18.18. f1 of +word2vec f1 is 21.74. p of +word2vec p is 50.00. r of +word2vec r is 20.00. f1 of +word2vec f1 is 28.57. p of +word2vec p is 51.81. r of +word2vec r is 36.63. f1 of +word2vec f1 is 42.92. p of +word2vec p is 60.72. r of +word2vec r is 81.60. f1 of +word2vec f1 is 69.63. accuracy of +word2vec accuracy is 57.17. macro f1 of +word2vec macro f1 is 40.71. p of +dswe p is 31.58. r of +dswe r is 21.82. f1 of +dswe f1 is 25.81. p of +dswe p is 43.00. r of +dswe r is 29.66. f1 of +dswe f1 is 35.10. p of +dswe p is 55.29. r of +dswe r is 42.12. f1 of +dswe f1 is 47.82. p of +dswe p is 63.91. r of +dswe r is 79.00. f1 of +dswe f1 is 70.66. accuracy of +dswe accuracy is 58.85. macro f1 of +dswe macro f1 is 44.84.
table 4 shows comparison with recent systems. accuracy of r&x2015 accuracy is 57.10. macro f1 of r&x2015 macro f1 is 40.50. accuracy of b&d2016 accuracy is 52.81. macro f1 of b&d2016 macro f1 is 42.27. accuracy of liu2016 accuracy is 57.27. macro f1 of liu2016 macro f1 is 44.98. accuracy of cdrr+dswe accuracy is 58.85. macro f1 of cdrr+dswe macro f1 is 44.84.
table 3 shows comparison with another chatbot. number of win number is 330. percentage of win percentage is 37.64%. number of equal number is 382. percentage of equal percentage is 43.52%. number of lose number is 165. percentage of lose percentage is 18.84%.
table 2 shows results for the relation induction task using alternative word embedding models. acc of diff acc is 90. f1 of diff f1 is 81.9. acc of diff acc is 21.2. f1 of diff f1 is 13.9. acc of diff acc is 89.8. f1 of diff f1 is 81.9. acc of diff acc is 21.7. f1 of diff f1 is 14.5. acc of diff acc is 89.9. f1 of diff f1 is 82.1. acc of diff acc is 17.4. f1 of diff f1 is 9.7. acc of conc acc is 88.9. f1 of conc f1 is 80.4. acc of conc acc is 20.2. f1 of conc f1 is 11.9. acc of conc acc is 89.2. f1 of conc f1 is 81.6. acc of conc acc is 20.5. f1 of conc f1 is 12. acc of conc acc is 89.1. f1 of conc f1 is 81.1. acc of conc acc is 16.4. f1 of conc f1 is 7.7. acc of avg acc is 89.8. f1 of avg f1 is 82.1. acc of avg acc is 21.4. f1 of avg f1 is 13.9. acc of avg acc is 90.2. f1 of avg f1 is 82.4. acc of avg acc is 21.8. f1 of avg f1 is 14.4. acc of avg acc is 89.8. f1 of avg f1 is 82.2. acc of avg acc is 17.5. f1 of avg f1 is 10. acc of r1ik acc is 89.7. f1 of r1ik f1 is 81.7. acc of r1ik acc is 20.9. f1 of r1ik f1 is 12.5. acc of r1ik acc is 89.4. f1 of r1ik f1 is 81.2. acc of r1ik acc is 21.1. f1 of r1ik f1 is 12.3. acc of r1ik acc is 89.8. f1 of r1ik f1 is 81.9. acc of r1ik acc is 17.2. f1 of r1ik f1 is 9.2. acc of r2ik acc is 90. f1 of r2ik f1 is 82.8. acc of r2ik acc is 21.2. f1 of r2ik f1 is 13.4. acc of r2ik acc is 89.1. f1 of r2ik f1 is 81.3. acc of r2ik acc is 21.1. f1 of r2ik f1 is 12.9. acc of r2ik acc is 90.2. f1 of r2ik f1 is 82.4. acc of r2ik acc is 17.7. f1 of r2ik f1 is 10. acc of r3ik acc is 90. f1 of r3ik f1 is 82.3. acc of r3ik acc is 20. f1 of r3ik f1 is 11.2. acc of r3ik acc is 89.5. f1 of r3ik f1 is 81.1. acc of r3ik acc is 20.5. f1 of r3ik f1 is 12.3. acc of r3ik acc is 89.5. f1 of r3ik f1 is 81.1. acc of r3ik acc is 17.2. f1 of r3ik f1 is 9.6. acc of r4ik acc is 90. f1 of r4ik f1 is 82.5. acc of r4ik acc is 20. f1 of r4ik f1 is 11.4. acc of r4ik acc is 88.9. f1 of r4ik f1 is 80.8. acc of r4ik acc is 20.6. f1 of r4ik f1 is 12.1. acc of r4ik acc is 90.5. f1 of r4ik f1 is 82.2. acc of r4ik acc is 17.1. f1 of r4ik f1 is 8.4.
table 1 shows performance on daily-mail test set using the limited length recall of rouge at 75 bytes. r1 of lead-3 r1 is 21.9. r2 of lead-3 r2 is 7.2. rl of lead-3 rl is 11.6. r1 of nn r1 is 22.7. r2 of nn r2 is 8.5. rl of nn rl is 12.5. r1 of summarunner-abs r1 is 23.8. r2 of summarunner-abs r2 is 9.6. rl of summarunner-abs rl is 13.3. r1 of summarunner r1 is 26.2. r2 of summarunner r2 is 10.8. rl of summarunner rl is 14.4. r1 of swap-net r1 is 26.4. r2 of swap-net r2 is 10.7. rl of swap-net rl is 14.4.
table 5 shows the results of case analysis (case) and zero anaphora resolution (zero). f-measure of ouchi+ 2015 case is 76.5. f-measure of ouchi+ 2015 zero is 42.1. f-measure of shibata+ 2016 case is 89.3. f-measure of shibata+ 2016 zero is 53.4. f-measure of gen case is 91.5. f-measure of gen zero is 56.2. f-measure of gen+adv case is 92.0ö. f-measure of gen+adv zero is 58.4ö.
table 3 shows results for event coreference resolution systems on the kbp 2016 and 2017 corpus. b3 of local classifier b3 is 51.47. ceafe of local classifier ceafe is 47.96. muc of local classifier muc is 26.29. blanc of local classifier blanc is 30.82. avg of local classifier avg is 39.13. b3 of local classifier b3 is 50.24. ceafe of local classifier ceafe is 48.47. muc of local classifier muc is 30.81. blanc of local classifier blanc is 29.94. avg of local classifier avg is 39.87. b3 of clustering b3 is 46.97. ceafe of clustering ceafe is 41.95. muc of clustering muc is 18.79. blanc of clustering blanc is 26.88. avg of clustering avg is 33.65. b3 of clustering b3 is 46.51. ceafe of clustering ceafe is 40.21. muc of clustering muc is 23.1. blanc of clustering blanc is 25.08. avg of clustering avg is 33.72. b3 of basic ilp b3 is 51.44. ceafe of basic ilp ceafe is 47.77. muc of basic ilp muc is 26.65. blanc of basic ilp blanc is 30.95. avg of basic ilp avg is 39.19. b3 of basic ilp b3 is 50.4. ceafe of basic ilp ceafe is 48.49. muc of basic ilp muc is 31.33. blanc of basic ilp blanc is 30.58. avg of basic ilp avg is 40.2. b3 of basic ilp + topic structure b3 is 51.44. ceafe of basic ilp + topic structure ceafe is 47.94. muc of basic ilp + topic structure muc is 28.86. blanc of basic ilp + topic structure blanc is 31.87. avg of basic ilp + topic structure avg is 40.03. b3 of basic ilp + topic structure b3 is 50.39. ceafe of basic ilp + topic structure ceafe is 48.23. muc of basic ilp + topic structure muc is 33.08. blanc of basic ilp + topic structure blanc is 31.26. avg of basic ilp + topic structure avg is 40.74. b3 of basic ilp + cross-chain b3 is 51.09. ceafe of basic ilp + cross-chain ceafe is 47.53. muc of basic ilp + cross-chain muc is 31.27. blanc of basic ilp + cross-chain blanc is 33.07. avg of basic ilp + cross-chain avg is 40.74. b3 of basic ilp + cross-chain b3 is 50.39. ceafe of basic ilp + cross-chain ceafe is 47.67. muc of basic ilp + cross-chain muc is 35.15. blanc of basic ilp + cross-chain blanc is 31.88. avg of basic ilp + cross-chain avg is 41.27. b3 of basic ilp + distribution b3 is 51.06. ceafe of basic ilp + distribution ceafe is 48.28. muc of basic ilp + distribution muc is 33.53. blanc of basic ilp + distribution blanc is 33.63. avg of basic ilp + distribution avg is 41.62. b3 of basic ilp + distribution b3 is 50.42. ceafe of basic ilp + distribution ceafe is 48.67. muc of basic ilp + distribution muc is 37.52. blanc of basic ilp + distribution blanc is 32.08. avg of basic ilp + distribution avg is 42.17. b3 of basic ilp + subevent b3 is 51.67. ceafe of basic ilp + subevent ceafe is 49.1. muc of basic ilp + subevent muc is 34.08. blanc of basic ilp + subevent blanc is 34.08. avg of basic ilp + subevent avg is 42.23. b3 of basic ilp + subevent b3 is 50.35. ceafe of basic ilp + subevent ceafe is 48.61. muc of basic ilp + subevent muc is 37.24. blanc of basic ilp + subevent blanc is 31.94. avg of basic ilp + subevent avg is 42.04. b3 of joint learning b3 is 50.16. ceafe of joint learning ceafe is 48.59. muc of joint learning muc is 32.41. blanc of joint learning blanc is 32.72. avg of joint learning avg is 40.97.
table 3 shows ablation study on the development set. f1 of our proposed model f1 is 0.633. delta of our proposed model delta is . f1 of our proposed model f1 is 0.613. delta of our proposed model delta is . f1 of our proposed model f1 is 0.512. delta of our proposed model delta is . f1 of our proposed model f1 is 0.361. delta of our proposed model delta is . f1 of cr - string match f1 is 0.212. delta of cr - string match delta is -0.42. f1 of cr - string match f1 is 0.184. delta of cr - string match delta is -0.429. f1 of cr - string match f1 is 0.474. delta of cr - string match delta is -0.038. f1 of cr - string match f1 is 0.348. delta of cr - string match delta is -0.013. f1 of cr - sentence distance f1 is 0.643. delta of cr - sentence distance delta is 0.011. f1 of cr - sentence distance f1 is 0.588. delta of cr - sentence distance delta is -0.025. f1 of cr - sentence distance f1 is 0.505. delta of cr - sentence distance delta is -0.007. f1 of cr - sentence distance f1 is 0.343. delta of cr - sentence distance delta is -0.018. f1 of cr - synonym dictionary f1 is 0.643. delta of cr - synonym dictionary delta is 0.01. f1 of cr - synonym dictionary f1 is 0.613. delta of cr - synonym dictionary delta is 0. f1 of cr - synonym dictionary f1 is 0.51. delta of cr - synonym dictionary delta is -0.002. f1 of cr - synonym dictionary f1 is 0.348. delta of cr - synonym dictionary delta is -0.013. f1 of pa - path embedding f1 is 0.643. delta of pa - path embedding delta is 0.01. f1 of pa - path embedding f1 is 0.625. delta of pa - path embedding delta is 0.012. f1 of pa - path embedding f1 is 0.459. delta of pa - path embedding delta is -0.054. f1 of pa - path embedding f1 is 0.268. delta of pa - path embedding delta is -0.093. f1 of pa - selectional preference f1 is 0.638. delta of pa - selectional preference delta is 0.005. f1 of pa - selectional preference f1 is 0.316. delta of pa - selectional preference delta is -0.297. f1 of pa - selectional preference f1 is 0.507. delta of pa - selectional preference delta is -0.005. f1 of pa - selectional preference f1 is 0.173. delta of pa - selectional preference delta is -0.188. f1 of pa - sentence distance f1 is 0.647. delta of pa - sentence distance delta is 0.014. f1 of pa - sentence distance f1 is 0.606. delta of pa - sentence distance delta is -0.007. f1 of pa - sentence distance f1 is 0.516. delta of pa - sentence distance delta is 0.004. f1 of pa - sentence distance f1 is 0.327. delta of pa - sentence distance delta is -0.034.
table 5 shows performance of systems on conll-2014 dataset. p of spell check p is 53.01. r of spell check r is 8.16. f0.5 of spell check f0.5 is 25.25. p of camb14 p is 39.71. r of camb14 r is 30.1. f0.5 of camb14 f0.5 is 37.33. p of camb16smt p is 45.39. r of camb16smt r is 21.82. f0.5 of camb16smt f0.5 is 37.33. f0.5 of camb16nmt f0.5 is 39.9. p of camb17 (camb16smt based) p is 51.09. r of camb17 (camb16smt based) r is 25.3. f0.5 of camb17 (camb16smt based) f0.5 is 42.44. p of camb17 (amu16 based) p is 59.88. r of camb17 (amu16 based) r is 32.16. f0.5 of camb17 (amu16 based) f0.5 is 51.08. p of amu14 p is 41.62. r of amu14 r is 21.4. f0.5 of amu14 f0.5 is 35.01. p of amu16 p is 61.27. r of amu16 r is 27.98. f0.5 of amu16 f0.5 is 49.49. p of amu16★ p is 63.52. r of amu16★ r is 30.49. f0.5 of amu16★ f0.5 is 52.21. p of cuui p is 41.78. r of cuui r is 24.88. f0.5 of cuui f0.5 is 36.79. p of vt16★ p is 60.17. r of vt16★ r is 25.64. f0.5 of vt16★ f0.5 is 47.4. p of nus14 p is 53.55. r of nus14 r is 19.14. f0.5 of nus14 f0.5 is 39.39. f0.5 of nus16 f0.5 is 44.27. p of nus17 p is 62.74. r of nus17 r is 32.96. f0.5 of nus17 f0.5 is 53.14. p of char-seq2seq p is 49.24. r of char-seq2seq r is 23.77. f0.5 of char-seq2seq f0.5 is 40.56. f0.5 of nested-seq2seq f0.5 is 45.15. f0.5 of adapt-seq2seq f0.5 is 41.37. p of dual-boost (single) p is 62.7. r of dual-boost (single) r is 27.69. f0.5 of dual-boost (single) f0.5 is 50.04. p of dual-boost (amu16 based) p is 60.57. r of dual-boost (amu16 based) r is 36.02. f0.5 of dual-boost (amu16 based) f0.5 is 53.3. p of dual-boost (single) * p is 64.47. r of dual-boost (single) * r is 30.48. f0.5 of dual-boost (single) * f0.5 is 52.72. p of dual-boost (amu16 based) * p is 61.24. r of dual-boost (amu16 based) * r is 37.86. f0.5 of dual-boost (amu16 based) * f0.5 is 54.51.
table 3 shows scores for mrr and top k results. mrre of embed mrre is 0.02. mrrp of embed mrrp is 0.09. top1 of embed top1 is 0.05. top2 of embed top2 is 0.08. top3 of embed top3 is 0.12. mrre of pmi mrre is 0.20. mrrp of pmi mrrp is 0.33. top1 of pmi top1 is 0.25. top2 of pmi top2 is 0.36. top3 of pmi top3 is 0.41. mrre of freq mrre is 0.23. mrrp of freq mrrp is 0.34. top1 of freq top1 is 0.23. top2 of freq top2 is 0.32. top3 of freq top3 is 0.40. mrre of ap mrre is 0.28. mrrp of ap mrrp is 0.38. top1 of ap top1 is 0.29. top2 of ap top2 is 0.41. top3 of ap top3 is 0.47. mrre of ap+al mrre is 0.28. mrrp of ap+al mrrp is 0.40. top1 of ap+al top1 is 0.32. top2 of ap+al top2 is 0.44. top3 of ap+al top3 is 0.49. mrre of ap+ao mrre is 0.23. mrrp of ap+ao mrrp is 0.33. top1 of ap+ao top1 is 0.24. top2 of ap+ao top2 is 0.35. top3 of ap+ao top3 is 0.43. mrre of ap+ae mrre is 0.25. mrrp of ap+ae mrrp is 0.36. top1 of ap+ae top1 is 0.28. top2 of ap+ae top2 is 0.40. top3 of ap+ae top3 is 0.47. mrre of ap+al+e mrre is 0.29. mrrp of ap+al+e mrrp is 0.42. top1 of ap+al+e top1 is 0.35. top2 of ap+al+e top2 is 0.44. top3 of ap+al+e top3 is 0.52.
table 6 shows abridged senteval and paraphrase evaluation results. accuracy of - snli is 83.7. accuracy of - sick-e is 86.4. accuracy of - avgacc is 81.7. accuracy of - avgsim is .70. accuracy of - hy-cl is 99.99. accuracy of - hy-nn is 100.0. accuracy of - hy-idb is 0.579. accuracy of - co-cl is 31.58. accuracy of - co-nn is 26.21. accuracy of - co-idb is 0.367. accuracy of - snli is 66.0. accuracy of - sick-e is 78.2. accuracy of - avgacc is 75.8. accuracy of - avgsim is .59. accuracy of - hy-cl is 99.94. accuracy of - hy-nn is 100.0. accuracy of - hy-idb is 0.654. accuracy of - co-cl is 34.28. accuracy of - co-nn is 19.72. accuracy of - co-idb is 0.352. accuracy of - snli is 70.2. accuracy of - sick-e is 82.1. accuracy of - avgacc is 74.4. accuracy of - avgsim is .60. accuracy of - hy-cl is 99.92. accuracy of - hy-nn is 100.0. accuracy of - hy-idb is 0.406. accuracy of - co-cl is 23.20. accuracy of - co-nn is 16.07. accuracy of - co-idb is 0.346. accuracy of 1 snli is 69.3. accuracy of 1 sick-e is 80.8. accuracy of 1 avgacc is 73.4. accuracy of 1 avgsim is .54. accuracy of 1 hy-cl is 99.88. accuracy of 1 hy-nn is 99.91. accuracy of 1 hy-idb is 0.347. accuracy of 1 co-cl is 21.54. accuracy of 1 co-nn is 11.50. accuracy of 1 co-idb is 0.331. accuracy of - snli is 69.2. accuracy of - sick-e is 81.1. accuracy of - avgacc is 73.2. accuracy of - avgsim is .60. accuracy of - hy-cl is 99.91. accuracy of - hy-nn is 100.0. accuracy of - hy-idb is 0.439. accuracy of - co-cl is 22.40. accuracy of - co-nn is 14.63. accuracy of - co-idb is 0.340. accuracy of - snli is 68.5. accuracy of - sick-e is 81.7. accuracy of - avgacc is 73.0. accuracy of - avgsim is .60. accuracy of - hy-cl is 99.86. accuracy of - hy-nn is 100.0. accuracy of - hy-idb is 0.447. accuracy of - co-cl is 21.76. accuracy of - co-nn is 16.34. accuracy of - co-idb is 0.348. accuracy of - snli is 67.8. accuracy of - sick-e is 79.7. accuracy of - avgacc is 72.4. accuracy of - avgsim is .50. accuracy of - hy-cl is 99.80. accuracy of - hy-nn is 99.99. accuracy of - hy-idb is 0.387. accuracy of - co-cl is 17.90. accuracy of - co-nn is 8.61. accuracy of - co-idb is 0.311. accuracy of 4 snli is 66.0. accuracy of 4 sick-e is 79.5. accuracy of 4 avgacc is 72.2. accuracy of 4 avgsim is .45. accuracy of 4 hy-cl is 99.75. accuracy of 4 hy-nn is 99.74. accuracy of 4 hy-idb is 0.287. accuracy of 4 co-cl is 14.60. accuracy of 4 co-nn is 7.54. accuracy of 4 co-idb is 0.318. accuracy of 4 snli is 65.2. accuracy of 4 sick-e is 78.0. accuracy of 4 avgacc is 71.2. accuracy of 4 avgsim is .39. accuracy of 4 hy-cl is 99.54. accuracy of 4 hy-nn is 98.98. accuracy of 4 hy-idb is 0.252. accuracy of 4 co-cl is 11.52. accuracy of 4 co-nn is 5.51. accuracy of 4 co-idb is 0.303. accuracy of 4 snli is 64.6. accuracy of 4 sick-e is 78.0. accuracy of 4 avgacc is 70.8. accuracy of 4 avgsim is .39. accuracy of 4 hy-cl is 99.26. accuracy of 4 hy-nn is 98.93. accuracy of 4 hy-idb is 0.253. accuracy of 4 co-cl is 10.84. accuracy of 4 co-nn is 5.20. accuracy of 4 co-idb is 0.299. accuracy of 8 snli is 63.2. accuracy of 8 sick-e is 76.6. accuracy of 8 avgacc is 70.0. accuracy of 8 avgsim is .36. accuracy of 8 hy-cl is 99.41. accuracy of 8 hy-nn is 98.09. accuracy of 8 hy-idb is 0.243. accuracy of 8 co-cl is 10.24. accuracy of 8 co-nn is 4.64. accuracy of 8 co-idb is 0.287. accuracy of - snli is 68.0. accuracy of - sick-e is 78.8. accuracy of - avgacc is 67.1. accuracy of - avgsim is .50. accuracy of - hy-cl is 98.42. accuracy of - hy-nn is 99.90. accuracy of - hy-idb is 0.343. accuracy of - co-cl is 21.54. accuracy of - co-nn is 15.62. accuracy of - co-idb is 0.341. accuracy of 12 snli is 65.0. accuracy of 12 sick-e is 77.4. accuracy of 12 avgacc is 66.7. accuracy of 12 avgsim is .52. accuracy of 12 hy-cl is 98.88. accuracy of 12 hy-nn is 99.91. accuracy of 12 hy-idb is 0.347. accuracy of 12 co-cl is 20.06. accuracy of 12 co-nn is 16.68. accuracy of 12 co-idb is 0.348. accuracy of 8 snli is 64.0. accuracy of 8 sick-e is 75.7. accuracy of 8 avgacc is 65.8. accuracy of 8 avgsim is .51. accuracy of 8 hy-cl is 98.11. accuracy of 8 hy-nn is 99.90. accuracy of 8 hy-idb is 0.348. accuracy of 8 co-cl is 21.64. accuracy of 8 co-nn is 17.32. accuracy of 8 co-idb is 0.349. accuracy of - snli is 65.2. accuracy of - sick-e is 77.5. accuracy of - avgacc is 65.6. accuracy of - avgsim is .48. accuracy of - hy-cl is 97.72. accuracy of - hy-nn is 99.60. accuracy of - hy-idb is 0.312. accuracy of - co-cl is 20.04. accuracy of - co-nn is 14.27. accuracy of - co-idb is 0.337. accuracy of 12 snli is 61.9. accuracy of 12 sick-e is 76.0. accuracy of 12 avgacc is 65.5. accuracy of 12 avgsim is .50. accuracy of 12 hy-cl is 97.79. accuracy of 12 hy-nn is 99.89. accuracy of 12 hy-idb is 0.360. accuracy of 12 co-cl is 20.22. accuracy of 12 co-nn is 16.10. accuracy of 12 co-idb is 0.344. accuracy of - snli is 64.7. accuracy of - sick-e is 77.0. accuracy of - avgacc is 65.3. accuracy of - avgsim is .47. accuracy of - hy-cl is 97.01. accuracy of - hy-nn is 99.30. accuracy of - hy-idb is 0.305. accuracy of - co-cl is 19.88. accuracy of - co-nn is 12.40. accuracy of - co-idb is 0.328. accuracy of 3 snli is 63.3. accuracy of 3 sick-e is 76.0. accuracy of 3 avgacc is 65.3. accuracy of 3 avgsim is .50. accuracy of 3 hy-cl is 97.81. accuracy of 3 hy-nn is 99.87. accuracy of 3 hy-idb is 0.328. accuracy of 3 co-cl is 19.74. accuracy of 3 co-nn is 16.43. accuracy of 3 co-idb is 0.343. accuracy of 1 snli is 63.8. accuracy of 1 sick-e is 76.9. accuracy of 1 avgacc is 64.8. accuracy of 1 avgsim is .50. accuracy of 1 hy-cl is 97.70. accuracy of 1 hy-nn is 99.73. accuracy of 1 hy-idb is 0.352. accuracy of 1 co-cl is 19.74. accuracy of 1 co-nn is 16.26. accuracy of 1 co-idb is 0.340. accuracy of 3 snli is 61.5. accuracy of 3 sick-e is 74.7. accuracy of 3 avgacc is 64.5. accuracy of 3 avgsim is .47. accuracy of 3 hy-cl is 97.42. accuracy of 3 hy-nn is 99.75. accuracy of 3 hy-idb is 0.314. accuracy of 3 co-cl is 17.36. accuracy of 3 co-nn is 14.35. accuracy of 3 co-idb is 0.333. accuracy of - snli is 62.6. accuracy of - sick-e is 76.2. accuracy of - avgacc is 64.5. accuracy of - avgsim is .48. accuracy of - hy-cl is 96.65. accuracy of - hy-nn is 99.70. accuracy of - hy-idb is 0.323. accuracy of - co-cl is 17.22. accuracy of - co-nn is 12.84. accuracy of - co-idb is 0.333. accuracy of 6 snli is 59.6. accuracy of 6 sick-e is 72.3. accuracy of 6 avgacc is 64.3. accuracy of 6 avgsim is .41. accuracy of 6 hy-cl is 98.05. accuracy of 6 hy-nn is 99.80. accuracy of 6 hy-idb is 0.289. accuracy of 6 co-cl is 11.90. accuracy of 6 co-nn is 10.69. accuracy of 6 co-idb is 0.327. accuracy of 3 snli is 61.4. accuracy of 3 sick-e is 72.5. accuracy of 3 avgacc is 63.9. accuracy of 3 avgsim is .49. accuracy of 3 hy-cl is 95.79. accuracy of 3 hy-nn is 99.64. accuracy of 3 hy-idb is 0.315. accuracy of 3 co-cl is 15.76. accuracy of 3 co-nn is 14.04. accuracy of 3 co-idb is 0.340. accuracy of 12 snli is 58.2. accuracy of 12 sick-e is 72.5. accuracy of 12 avgacc is 63.4. accuracy of 12 avgsim is .43. accuracy of 12 hy-cl is 97.15. accuracy of 12 hy-nn is 99.65. accuracy of 12 hy-idb is 0.283. accuracy of 12 co-cl is 12.18. accuracy of 12 co-nn is 11.97. accuracy of 12 co-idb is 0.330. accuracy of 12 snli is 59.8. accuracy of 12 sick-e is 73.9. accuracy of 12 avgacc is 63.2. accuracy of 12 avgsim is .41. accuracy of 12 hy-cl is 98.69. accuracy of 12 hy-nn is 99.77. accuracy of 12 hy-idb is 0.287. accuracy of 12 co-cl is 10.26. accuracy of 12 co-nn is 10.94. accuracy of 12 co-idb is 0.326. accuracy of 12 snli is 59.0. accuracy of 12 sick-e is 71.2. accuracy of 12 avgacc is 63.0. accuracy of 12 avgsim is .46. accuracy of 12 hy-cl is 95.82. accuracy of 12 hy-nn is 99.03. accuracy of 12 hy-idb is 0.307. accuracy of 12 co-cl is 5.66. accuracy of 12 co-nn is 14.53. accuracy of 12 co-idb is 0.339. accuracy of 6 snli is 57.5. accuracy of 6 sick-e is 70.9. accuracy of 6 avgacc is 62.6. accuracy of 6 avgsim is .40. accuracy of 6 hy-cl is 96.03. accuracy of 6 hy-nn is 99.71. accuracy of 6 hy-idb is 0.287. accuracy of 6 co-cl is 12.22. accuracy of 6 co-nn is 10.59. accuracy of 6 co-idb is 0.323. accuracy of 8 snli is 55.6. accuracy of 8 sick-e is 68.6. accuracy of 8 avgacc is 62.1. accuracy of 8 avgsim is .39. accuracy of 8 hy-cl is 95.32. accuracy of 8 hy-nn is 99.73. accuracy of 8 hy-idb is 0.275. accuracy of 8 co-cl is 10.22. accuracy of 8 co-nn is 10.58. accuracy of 8 co-idb is 0.325. accuracy of 6 snli is 59.5. accuracy of 6 sick-e is 71.0. accuracy of 6 avgacc is 61.9. accuracy of 6 avgsim is .45. accuracy of 6 hy-cl is 90.24. accuracy of 6 hy-nn is 98.44. accuracy of 6 hy-idb is 0.313. accuracy of 6 co-cl is 9.06. accuracy of 6 co-nn is 13.64. accuracy of 6 co-idb is 0.332. accuracy of 12 snli is 55.2. accuracy of 12 sick-e is 70.5. accuracy of 12 avgacc is 61.5. accuracy of 12 avgsim is .40. accuracy of 12 hy-cl is 95.16. accuracy of 12 hy-nn is 99.64. accuracy of 12 hy-idb is 0.278. accuracy of 12 co-cl is 9.62. accuracy of 12 co-nn is 10.47. accuracy of 12 co-idb is 0.323. accuracy of 12 snli is 58.2. accuracy of 12 sick-e is 68.8. accuracy of 12 avgacc is 61.1. accuracy of 12 avgsim is .46. accuracy of 12 hy-cl is 90.71. accuracy of 12 hy-nn is 98.22. accuracy of 12 hy-idb is 0.301. accuracy of 12 co-cl is 7.06. accuracy of 12 co-nn is 13.70. accuracy of 12 co-idb is 0.333. accuracy of 6 snli is 62.9. accuracy of 6 sick-e is 68.7. accuracy of 6 avgacc is 61.0. accuracy of 6 avgsim is .43. accuracy of 6 hy-cl is 98.11. accuracy of 6 hy-nn is 99.86. accuracy of 6 hy-idb is 0.358. accuracy of 6 co-cl is 20.44. accuracy of 6 co-nn is 15.57. accuracy of 6 co-idb is 0.342. accuracy of - snli is 190.6. accuracy of - sick-e is 299.4. accuracy of - avgacc is 1150.2. accuracy of - avgsim is 1224.2. accuracy of - hy-cl is . accuracy of - hy-nn is 668.5. accuracy of - hy-idb is . accuracy of - co-cl is . accuracy of - co-nn is 238.5. accuracy of - co-idb is . accuracy of - snli is 0.3. accuracy of - sick-e is 0.2. accuracy of - avgacc is 2.3. accuracy of - avgsim is 2.6. accuracy of - hy-cl is . accuracy of - hy-nn is 1.2. accuracy of - hy-idb is . accuracy of - co-cl is . accuracy of - co-nn is 0.1. accuracy of - co-idb is . accuracy of - snli is 38.8. accuracy of - sick-e is 65.0. accuracy of - avgacc is 3578.2. accuracy of - avgsim is 2010.6. accuracy of - hy-cl is . accuracy of - hy-nn is 3354.8. accuracy of - hy-idb is . accuracy of - co-cl is . accuracy of - co-nn is 86.3. accuracy of - co-idb is . accuracy of - snli is 1.5. accuracy of - sick-e is 0.7. accuracy of - avgacc is 17.8. accuracy of - avgsim is 16.2. accuracy of - hy-cl is . accuracy of - hy-nn is 19.3. accuracy of - hy-idb is . accuracy of - co-cl is . accuracy of - co-nn is 1.9. accuracy of - co-idb is .
table 4 shows recall by relation type under udpipe’s morpho-tagger and ark tagger settings (+synthetic+embeddings; (3) and (6) from table 3; §5.3). accuracy of compound aa recall is 36.4. accuracy of compound wh recall is 71.2. accuracy of compound gap (wh - aa) is 34.8. accuracy of compound aa recall is 42.4. accuracy of compound wh recall is 72.9. accuracy of compound gap (wh - aa) is 30.5. accuracy of compound reduction is 4.4. accuracy of obl:tmod aa recall is 25.0. accuracy of obl:tmod wh recall is 51.7. accuracy of obl:tmod gap (wh - aa) is 26.7. accuracy of obl:tmod aa recall is 43.8. accuracy of obl:tmod wh recall is 55.2. accuracy of obl:tmod gap (wh - aa) is 11.4. accuracy of obl:tmod reduction is 15.3. accuracy of nmod aa recall is 28.6. accuracy of nmod wh recall is 54.4. accuracy of nmod gap (wh - aa) is 25.8. accuracy of nmod aa recall is 45.7. accuracy of nmod wh recall is 51.5. accuracy of nmod gap (wh - aa) is 5.8. accuracy of nmod reduction is 20.1. accuracy of cop aa recall is 56.5. accuracy of cop wh recall is 82.1. accuracy of cop gap (wh - aa) is 25.6. accuracy of cop aa recall is 65.2. accuracy of cop wh recall is 79.1. accuracy of cop gap (wh - aa) is 13.9. accuracy of cop reduction is 11.7. accuracy of obl aa recall is 41.4. accuracy of obl wh recall is 65.4. accuracy of obl gap (wh - aa) is 24.0. accuracy of obl aa recall is 56.8. accuracy of obl wh recall is 62.5. accuracy of obl gap (wh - aa) is 5.7. accuracy of obl reduction is 18.3. accuracy of cc aa recall is 56.9. accuracy of cc wh recall is 79.0. accuracy of cc gap (wh - aa) is 22.1. accuracy of cc aa recall is 78.5. accuracy of cc wh recall is 82.7. accuracy of cc gap (wh - aa) is 4.3. accuracy of cc reduction is 17.8. accuracy of ccomp aa recall is 33.3. accuracy of ccomp wh recall is 54.2. accuracy of ccomp gap (wh - aa) is 20.8. accuracy of ccomp aa recall is 40.5. accuracy of ccomp wh recall is 54.2. accuracy of ccomp gap (wh - aa) is 13.7. accuracy of ccomp reduction is 7.1. accuracy of obj aa recall is 61.3. accuracy of obj wh recall is 81.5. accuracy of obj gap (wh - aa) is 20.2. accuracy of obj aa recall is 72.8. accuracy of obj wh recall is 83.5. accuracy of obj gap (wh - aa) is 10.7. accuracy of obj reduction is 9.5. accuracy of case aa recall is 60.5. accuracy of case wh recall is 79.8. accuracy of case gap (wh - aa) is 19.3. accuracy of case aa recall is 75.2. accuracy of case wh recall is 83.4. accuracy of case gap (wh - aa) is 8.2. accuracy of case reduction is 11.1. accuracy of det aa recall is 73.1. accuracy of det wh recall is 90.7. accuracy of det gap (wh - aa) is 17.5. accuracy of det aa recall is 83.4. accuracy of det wh recall is 92.2. accuracy of det gap (wh - aa) is 8.8. accuracy of det reduction is 8.7. accuracy of advmod aa recall is 53.8. accuracy of advmod wh recall is 71.2. accuracy of advmod gap (wh - aa) is 17.3. accuracy of advmod aa recall is 62.9. accuracy of advmod wh recall is 72.1. accuracy of advmod gap (wh - aa) is 9.1. accuracy of advmod reduction is 8.2. accuracy of advcl aa recall is 31.5. accuracy of advcl wh recall is 46.8. accuracy of advcl gap (wh - aa) is 15.3. accuracy of advcl aa recall is 25.9. accuracy of advcl wh recall is 46.8. accuracy of advcl gap (wh - aa) is 20.9. accuracy of advcl reduction is -5.6. accuracy of root aa recall is 56.4. accuracy of root wh recall is 71.6. accuracy of root gap (wh - aa) is 15.2. accuracy of root aa recall is 62.8. accuracy of root wh recall is 74.0. accuracy of root gap (wh - aa) is 11.2. accuracy of root reduction is 4.0. accuracy of xcomp aa recall is 40.0. accuracy of xcomp wh recall is 54.9. accuracy of xcomp gap (wh - aa) is 14.9. accuracy of xcomp aa recall is 51.2. accuracy of xcomp wh recall is 50.0. accuracy of xcomp gap (wh - aa) is 1.2. accuracy of xcomp reduction is 13.7. accuracy of discourse aa recall is 30.7. accuracy of discourse wh recall is 44.9. accuracy of discourse gap (wh - aa) is 14.2. accuracy of discourse aa recall is 46. accuracy of discourse wh recall is 51.4. accuracy of discourse gap (wh - aa) is 5.4. accuracy of discourse reduction is 8.8.
table 3 shows results of our models on noisy social media data. precision of blstm-crf precision is 57.71. recall of blstm-crf recall is 58.65. f1 of blstm-crf f1 is 58.18. precision of blstm-crf precision is 78.88. recall of blstm-crf recall is 77.47. f1 of blstm-crf f1 is 78.17. precision of blstm-crf + global image vector precision is 61.49. recall of blstm-crf + global image vector recall is 57.84. f1 of blstm-crf + global image vector f1 is 59.61. precision of blstm-crf + global image vector precision is 79.75. recall of blstm-crf + global image vector recall is 77.32. f1 of blstm-crf + global image vector f1 is 78.51. precision of blstm-crf + visual attention precision is 65.53. recall of blstm-crf + visual attention recall is 57.03. f1 of blstm-crf + visual attention f1 is 60.98. precision of blstm-crf + visual attention precision is 80.81. recall of blstm-crf + visual attention recall is 77.36. f1 of blstm-crf + visual attention f1 is 79.05. precision of blstm-crf + visual attention + gate precision is 66.67. recall of blstm-crf + visual attention + gate recall is 57.84. f1 of blstm-crf + visual attention + gate f1 is 61.94. precision of blstm-crf + visual attention + gate precision is 81.62. recall of blstm-crf + visual attention + gate recall is 79.90. f1 of blstm-crf + visual attention + gate f1 is 80.75.
table 4 shows development results, including model ablations. accuracy of supervised inst is 92.0. accuracy of supervised 3utts is 83.3. accuracy of supervised 5utts is 71.4. accuracy of supervised inst is 85.3. accuracy of supervised 3utts is 72.7. accuracy of supervised 5utts is 60.6. accuracy of supervised inst is 86.1. accuracy of supervised 3utts is 81.9. accuracy of supervised 5utts is 58.3. accuracy of policygradient inst is 0.0. accuracy of policygradient 3utts is 0.0. accuracy of policygradient 5utts is 0.0. accuracy of policygradient inst is 0.9. accuracy of policygradient 3utts is 1.0. accuracy of policygradient 5utts is 0.5. accuracy of policygradient inst is 85.2. accuracy of policygradient 3utts is 74.9. accuracy of policygradient 5utts is 52.3. accuracy of contextualbandit inst is 58.8. accuracy of contextualbandit 3utts is 6.9. accuracy of contextualbandit 5utts is 5.7. accuracy of contextualbandit inst is 12.0. accuracy of contextualbandit 3utts is 0.5. accuracy of contextualbandit 5utts is 1.5. accuracy of contextualbandit inst is 85.6. accuracy of contextualbandit 3utts is 78.4. accuracy of contextualbandit 5utts is 52.6. accuracy of our approach inst is 92.1. accuracy of our approach 3utts is 82.9. accuracy of our approach 5utts is 71.8. accuracy of our approach inst is 83.9. accuracy of our approach 3utts is 68.7. accuracy of our approach 5utts is 56.1. accuracy of our approach inst is 88.5. accuracy of our approach 3utts is 82.4. accuracy of our approach 5utts is 60.3. accuracy of our approach -previous instructions inst is 90.1. accuracy of our approach -previous instructions 3utts is 77.1. accuracy of our approach -previous instructions 5utts is 66.1. accuracy of our approach -previous instructions inst is 79.3. accuracy of our approach -previous instructions 3utts is 60.6. accuracy of our approach -previous instructions 5utts is 45.5. accuracy of our approach -previous instructions inst is 76.4. accuracy of our approach -previous instructions 3utts is 55.8. accuracy of our approach -previous instructions 5utts is 27.6. accuracy of our approach -current and initial state inst is 25.7. accuracy of our approach -current and initial state 3utts is 4.5. accuracy of our approach -current and initial state 5utts is 3.3. accuracy of our approach -current and initial state inst is 17.5. accuracy of our approach -current and initial state 3utts is 0.0. accuracy of our approach -current and initial state 5utts is 0.0. accuracy of our approach -current and initial state inst is 45.4. accuracy of our approach -current and initial state 3utts is 15.1. accuracy of our approach -current and initial state 5utts is 3.5. accuracy of our approach -current state inst is 89.8. accuracy of our approach -current state 3utts is 78.0. accuracy of our approach -current state 5utts is 62.9. accuracy of our approach -current state inst is 83.0. accuracy of our approach -current state 3utts is 68.7. accuracy of our approach -current state 5utts is 54.0. accuracy of our approach -current state inst is 87.6. accuracy of our approach -current state 3utts is 78.4. accuracy of our approach -current state 5utts is 60.8. accuracy of our approach -initial state inst is 81.1. accuracy of our approach -initial state 3utts is 68.6. accuracy of our approach -initial state 5utts is 42.9. accuracy of our approach -initial state inst is 82.7. accuracy of our approach -initial state 3utts is 67.7. accuracy of our approach -initial state 5utts is 57.1. accuracy of our approach -initial state inst is 88.6. accuracy of our approach -initial state 3utts is 82.9. accuracy of our approach -initial state 5utts is 63.3. accuracy of our approach (mean ± stdev) inst is 91.5 ±1.4. accuracy of our approach (mean ± stdev) 3utts is 80.4 ±2.6. accuracy of our approach (mean ± stdev) 5utts is 69.5 ±5.0. accuracy of our approach (mean ± stdev) inst is 62.9 ±17.7. accuracy of our approach (mean ± stdev) 3utts is 37.8 ±23.5. accuracy of our approach (mean ± stdev) 5utts is 29.0 ±21.1. accuracy of our approach (mean ± stdev) inst is 88.2 ±0.6. accuracy of our approach (mean ± stdev) 3utts is 80.8 ±2.8. accuracy of our approach (mean ± stdev) 5utts is 59.2 ±2.3.
table 4 shows comparison between the proposed method and existing ones, in terms of both temporal and causal performances. p of 1. temporal only p is 67.2. r of 1. temporal only r is 72.3. f1 of 1. temporal only f1 is 69.7. accuracy of 2. causal only accuracy is 70.5. p of 3. joint system p is 68.6. r of 3. joint system r is 73.8. f1 of 3. joint system f1 is 71.1. accuracy of 3. joint system accuracy is 77.3. p of 4. gold temporal p is 100. r of 4. gold temporal r is 100. f1 of 4. gold temporal f1 is 100. accuracy of 4. gold temporal accuracy is 91.9. p of 5. gold causal p is 69.3. r of 5. gold causal r is 74.4. f1 of 5. gold causal f1 is 71.8. accuracy of 5. gold causal accuracy is 100.
table 3 shows results of analogy tests, comparing 20m sample texts from reddit vs. accuracy of world reddit20m is 28.34. accuracy of world google300d is 70.2. accuracy of family reddit20m is 94.58. accuracy of family google300d is 90.06. accuracy of gram1-9 reddit20m is 70.21. accuracy of gram1-9 google300d is 73.4. accuracy of total reddit20m is 67.88. accuracy of total google300d is 77.08.
table 3 shows evaluation of unmixed speech without multi-speaker training. avg. of wsj avg. is 2.6. avg. of csj avg. is 7.8.
table 4 shows results for morphological features. accuracy of cs_cac conll winner is 90.72. accuracy of cs_cac dqm reimpl. is 94.66. accuracy of cs_cac ours is 96.41. accuracy of ru_syn. conll winner is 94.55. accuracy of ru_syn. dqm reimpl. is 96.70. accuracy of ru_syn. ours is 97.53. accuracy of cs conll winner is 93.14. accuracy of cs dqm reimpl. is 96.32. accuracy of cs ours is 97.14. accuracy of la_ittb conll winner is 94.28. accuracy of la_ittb dqm reimpl. is 96.45. accuracy of la_ittb ours is 97.12. accuracy of sl conll winner is 90.08. accuracy of sl dqm reimpl. is 95.26. accuracy of sl ours is 96.03. accuracy of ca conll winner is 97.23. accuracy of ca dqm reimpl. is 97.85. accuracy of ca ours is 98.13. accuracy of fi_ftb conll winner is 93.43. accuracy of fi_ftb dqm reimpl. is 95.96. accuracy of fi_ftb ours is 96.42. accuracy of no_bok. conll winner is 95.56. accuracy of no_bok. dqm reimpl. is 96.95. accuracy of no_bok. ours is 97.26. accuracy of grc_proiel conll winner is 90.24. accuracy of grc_proiel dqm reimpl. is 91.35. accuracy of grc_proiel ours is 92.22. accuracy of fr_sequoia conll winner is 96.10. accuracy of fr_sequoia dqm reimpl. is 96.62. accuracy of fr_sequoia ours is 97.62. accuracy of la_proiel conll winner is 89.22. accuracy of la_proiel dqm reimpl. is 91.52. accuracy of la_proiel ours is 92.35. accuracy of es_ancora conll winner is 97.72. accuracy of es_ancora dqm reimpl. is 98.15. accuracy of es_ancora ours is 98.32. accuracy of da conll winner is 94.83. accuracy of da dqm reimpl. is 96.62. accuracy of da ours is 96.94. accuracy of fi conll winner is 92.43. accuracy of fi dqm reimpl. is 94.29. accuracy of fi ours is 94.83. accuracy of sv conll winner is 95.15. accuracy of sv dqm reimpl. is 96.52. accuracy of sv ours is 96.84. accuracy of pt conll winner is 94.62. accuracy of pt dqm reimpl. is 95.89. accuracy of pt ours is 96.27. accuracy of grc conll winner is 88.00. accuracy of grc dqm reimpl. is 90.39. accuracy of grc ours is 91.13. accuracy of no_nyn. conll winner is 95.25. accuracy of no_nyn. dqm reimpl. is 96.79. accuracy of no_nyn. ours is 97.08. accuracy of de conll winner is 93.11. accuracy of de dqm reimpl. is 89.78. accuracy of de ours is 90.70. accuracy of ru conll winner is 87.27. accuracy of ru dqm reimpl. is 91.99. accuracy of ru ours is 92.69. accuracy of hi conll winner is 91.03. accuracy of hi dqm reimpl. is 90.72. accuracy of hi ours is 91.78. accuracy of cu conll winner is 88.90. accuracy of cu dqm reimpl. is 88.93. accuracy of cu ours is 89.82. accuracy of fa conll winner is 96.34. accuracy of fa dqm reimpl. is 97.23. accuracy of fa ours is 97.45. accuracy of tr conll winner is 87.03. accuracy of tr dqm reimpl. is 89.39. accuracy of tr ours is 90.21. accuracy of en_partut conll winner is 92.69. accuracy of en_partut dqm reimpl. is 93.93. accuracy of en_partut ours is 94.40. accuracy of sk conll winner is 81.23. accuracy of sk dqm reimpl. is 87.54. accuracy of sk ours is 88.48. accuracy of eu conll winner is 89.57. accuracy of eu dqm reimpl. is 92.48. accuracy of eu ours is 83.04. accuracy of pt_br conll winner is 99.73. accuracy of pt_br dqm reimpl. is 99.73. accuracy of pt_br ours is 99.75. accuracy of es conll winner is 96.34. accuracy of es dqm reimpl. is 96.42. accuracy of es ours is 96.68. accuracy of ko conll winner is 99.41. accuracy of ko dqm reimpl. is 99.44. accuracy of ko ours is 99.48. accuracy of ar conll winner is 87.15. accuracy of ar dqm reimpl. is 85.45. accuracy of ar ours is 88.29. accuracy of it conll winner is 97.37. accuracy of it dqm reimpl. is 97.72. accuracy of it ours is 97.86. accuracy of nl_lassy conll winner is 97.55. accuracy of nl_lassy dqm reimpl. is 98.04. accuracy of nl_lassy ours is 98.15. accuracy of nl conll winner is 90.04. accuracy of nl dqm reimpl. is 92.06. accuracy of nl ours is 92.47. accuracy of pl conll winner is 86.53. accuracy of pl dqm reimpl. is 91.71. accuracy of pl ours is 92.14. accuracy of ur conll winner is 81.03. accuracy of ur dqm reimpl. is 83.16. accuracy of ur ours is 84.02. accuracy of bg conll winner is 96.47. accuracy of bg dqm reimpl. is 97.71. accuracy of bg ours is 97.82. accuracy of hr conll winner is 85.82. accuracy of hr dqm reimpl. is 90.64. accuracy of hr ours is 91.50. accuracy of he conll winner is 85.06. accuracy of he dqm reimpl. is 79.34. accuracy of he ours is 79.76. accuracy of et conll winner is 84.62. accuracy of et dqm reimpl. is 88.18. accuracy of et ours is 88.25. accuracy of zh conll winner is 92.90. accuracy of zh dqm reimpl. is 87.67. accuracy of zh ours is 87.74. accuracy of vi conll winner is 86.92. accuracy of vi dqm reimpl. is 82.23. accuracy of vi ours is 82.30. accuracy of ja conll winner is 96.84. accuracy of ja dqm reimpl. is 89.65. accuracy of ja ours is 89.66. accuracy of en_lines conll winner is 99.96. accuracy of en_lines dqm reimpl. is 99.99. accuracy of en_lines ours is 99.99. accuracy of fr conll winner is 96.12. accuracy of fr dqm reimpl. is 95.98. accuracy of fr ours is 95.98. accuracy of gl conll winner is 99.78. accuracy of gl dqm reimpl. is 99.72. accuracy of gl ours is 99.72. accuracy of id conll winner is 99.55. accuracy of id dqm reimpl. is 99.50. accuracy of id ours is 99.50. accuracy of ro conll winner is 96.24. accuracy of ro dqm reimpl. is 97.26. accuracy of ro ours is 97.26. accuracy of sv_lines conll winner is 99.98. accuracy of sv_lines dqm reimpl. is 99.98. accuracy of sv_lines ours is 99.98. accuracy of cs_cltt conll winner is 87.88. accuracy of cs_cltt dqm reimpl. is 90.41. accuracy of cs_cltt ours is 90.36. accuracy of lv conll winner is 84.14. accuracy of lv dqm reimpl. is 87.00. accuracy of lv ours is 86.92. accuracy of el conll winner is 91.37. accuracy of el dqm reimpl. is 94.00. accuracy of el ours is 93.92. accuracy of hu conll winner is 72.61. accuracy of hu dqm reimpl. is 82.67. accuracy of hu ours is 82.44. accuracy of en conll winner is 94.49. accuracy of en dqm reimpl. is 95.93. accuracy of en ours is 95.71. accuracy of macro-avg conll winner is 91.51. accuracy of macro-avg dqm reimpl. is 92.89. accuracy of macro-avg ours is 93.31.
table 2 shows performance of various models, including our weighted-pooled lstm (wp). accuracy of - all adverbs is 51.66. accuracy of - all adverbs is 50.24. accuracy of - also is 50.32. accuracy of - still is 50.29. accuracy of - again is 50.25. accuracy of - too is 65.06. accuracy of - yet is 50.19. accuracy of + pos all adverbs is 52.81. accuracy of + pos all adverbs is 53.65. accuracy of + pos also is 52.00. accuracy of + pos still is 56.36. accuracy of + pos again is 59.49. accuracy of + pos too is 69.77. accuracy of + pos yet is 61.05. accuracy of - pos all adverbs is 54.47. accuracy of - pos all adverbs is 52.86. accuracy of - pos also is 56.07. accuracy of - pos still is 55.29. accuracy of - pos again is 58.60. accuracy of - pos too is 67.60. accuracy of - pos yet is 58.60. accuracy of + pos all adverbs is 58.84. accuracy of + pos all adverbs is 59.12. accuracy of + pos also is 61.53. accuracy of + pos still is 59.54. accuracy of + pos again is 60.26. accuracy of + pos too is 67.53. accuracy of + pos yet is 59.69. accuracy of - pos all adverbs is 62.16. accuracy of - pos all adverbs is 57.21. accuracy of - pos also is 59.76. accuracy of - pos still is 56.95. accuracy of - pos again is 57.28. accuracy of - pos too is 67.84. accuracy of - pos yet is 56.53. accuracy of + pos all adverbs is 74.23. accuracy of + pos all adverbs is 60.58. accuracy of + pos also is 81.48. accuracy of + pos still is 60.72. accuracy of + pos again is 61.81. accuracy of + pos too is 69.70. accuracy of + pos yet is 59.13. accuracy of - pos all adverbs is 73.18. accuracy of - pos all adverbs is 58.86. accuracy of - pos also is 81.16. accuracy of - pos still is 58.97. accuracy of - pos again is 59.93. accuracy of - pos too is 68.32. accuracy of - pos yet is 55.71. accuracy of + pos all adverbs is 76.09. accuracy of + pos all adverbs is 60.62. accuracy of + pos also is 82.42. accuracy of + pos still is 61.00. accuracy of + pos again is 61.59. accuracy of + pos too is 69.38. accuracy of + pos yet is 57.68. accuracy of - pos all adverbs is 74.84. accuracy of - pos all adverbs is 58.87. accuracy of - pos also is 81.64. accuracy of - pos still is 59.03. accuracy of - pos again is 58.49. accuracy of - pos too is 68.37. accuracy of - pos yet is 56.68.
table 2 shows method comparison (%). meteor of lstm-lm meteor is 8.7. rouge-l of lstm-lm rouge-l is 15.1. preference of lstm-lm preference is 0. meteor of seq2seq meteor is 13.5. rouge-l of seq2seq rouge-l is 19.2. preference of seq2seq preference is 22. meteor of ed(1) meteor is 13.3. rouge-l of ed(1) rouge-l is 20.3. preference of ed(1) preference is 30. meteor of ed(2) meteor is 14.0. rouge-l of ed(2) rouge-l is 19.8. preference of ed(2) preference is 48.
table 1 shows automatic evaluation and manual evaluation results for baselines and our proposed models. ppl of seq2seq without knowledge ppl is 80.93. bleu(%) of seq2seq without knowledge bleu(%) is 0.38. fluency of seq2seq without knowledge fluency is 1.62. knowledge relevance of seq2seq without knowledge knowledge relevance is 0.18. context coherence of seq2seq without knowledge context coherence is 0.54. ppl of hred without knowledge ppl is 80.84. bleu(%) of hred without knowledge bleu(%) is 0.43. fluency of hred without knowledge fluency is 1.25. knowledge relevance of hred without knowledge knowledge relevance is 0.18. context coherence of hred without knowledge context coherence is 0.3. ppl of transformer without knowledge ppl is 87.32. bleu(%) of transformer without knowledge bleu(%) is 0.36. fluency of transformer without knowledge fluency is 1.6. knowledge relevance of transformer without knowledge knowledge relevance is 0.29. context coherence of transformer without knowledge context coherence is 0.67. ppl of seq2seq (+knowledge) ppl is 78.47. bleu(%) of seq2seq (+knowledge) bleu(%) is 0.39. fluency of seq2seq (+knowledge) fluency is 1.5. knowledge relevance of seq2seq (+knowledge) knowledge relevance is 0.22. context coherence of seq2seq (+knowledge) context coherence is 0.61. ppl of hred (+knowledge) ppl is 79.12. bleu(%) of hred (+knowledge) bleu(%) is 0.77. fluency of hred (+knowledge) fluency is 1.56. knowledge relevance of hred (+knowledge) knowledge relevance is 0.35. context coherence of hred (+knowledge) context coherence is 0.47. ppl of wizard transformer ppl is 70.3. bleu(%) of wizard transformer bleu(%) is 0.66. fluency of wizard transformer fluency is 1.62. knowledge relevance of wizard transformer knowledge relevance is 0.47. context coherence of wizard transformer context coherence is 0.56. ppl of ite+dd (ours) ppl is 15.11. bleu(%) of ite+dd (ours) bleu(%) is 0.95. fluency of ite+dd (ours) fluency is 1.67. knowledge relevance of ite+dd (ours) knowledge relevance is 0.56. context coherence of ite+dd (ours) context coherence is 0.9. ppl of ite+ckad (ours) ppl is 64.97. bleu(%) of ite+ckad (ours) bleu(%) is 0.86. fluency of ite+ckad (ours) fluency is 1.68. knowledge relevance of ite+ckad (ours) knowledge relevance is 0.5. context coherence of ite+ckad (ours) context coherence is 0.82. ppl of kat (ours) ppl is 65.36. bleu(%) of kat (ours) bleu(%) is 0.58. fluency of kat (ours) fluency is 1.58. knowledge relevance of kat (ours) knowledge relevance is 0.33. context coherence of kat (ours) context coherence is 0.78.
table 1 shows average (from six runs) parsing results (las) on test sets. accuracy of tbmin avg. is 76.43. accuracy of tbmin en-ptb is 90.25. accuracy of tbmin ar is 76.22. accuracy of tbmin en is 81.85†. accuracy of tbmin fi is 72.51†. accuracy of tbmin grc is 71.92†. accuracy of tbmin he is 79.41†. accuracy of tbmin ko is 64.39. accuracy of tbmin ru is 74.35†. accuracy of tbmin sv is 80.11†. accuracy of tbmin zh is 73.28†. accuracy of tbext avg. is 75.56. accuracy of tbext en-ptb is 90.25. accuracy of tbext ar is 75.77. accuracy of tbext en is 80.5. accuracy of tbext fi is 71.47. accuracy of tbext grc is 70.32. accuracy of tbext he is 78.62. accuracy of tbext ko is 63.88. accuracy of tbext ru is 73.82. accuracy of tbext sv is 78.8. accuracy of tbext zh is 72.17. accuracy of gbmin avg. is 77.74. accuracy of gbmin en-ptb is 91.4. accuracy of gbmin ar is 77.25. accuracy of gbmin en is 82.53. accuracy of gbmin fi is 74.37. accuracy of gbmin grc is 73.48. accuracy of gbmin he is 80.83. accuracy of gbmin ko is 65.47. accuracy of gbmin ru is 76.43. accuracy of gbmin sv is 81.22. accuracy of gbmin zh is 74.47. accuracy of gbsibl avg. is 77.89. accuracy of gbsibl en-ptb is 91.59. accuracy of gbsibl ar is 77.21. accuracy of gbsibl en is 82.65. accuracy of gbsibl fi is 74.44. accuracy of gbsibl grc is 73.2. accuracy of gbsibl he is 81.03. accuracy of gbsibl ko is 65.61. accuracy of gbsibl ru is 76.79†. accuracy of gbsibl sv is 81.42. accuracy of gbsibl zh is 74.95†.
table 3 shows model comparison. f1-a of cmla-alstm f1-a is 82.45. f1-o of cmla-alstm f1-o is 82.67. acc-s of cmla-alstm acc-s is 77.46. f1-s of cmla-alstm f1-s is 68.7. f1-i of cmla-alstm f1-i is 63.87. f1-a of cmla-alstm f1-a is 76.8. f1-o of cmla-alstm f1-o is 77.33. acc-s of cmla-alstm acc-s is 70.25. f1-s of cmla-alstm f1-s is 66.67. f1-i of cmla-alstm f1-i is 53.68. f1-a of cmla-alstm f1-a is 68.55. f1-o of cmla-alstm f1-o is 71.07. acc-s of cmla-alstm acc-s is 81.03. f1-s of cmla-alstm f1-s is 58.91. f1-i of cmla-alstm f1-i is 54.79. f1-a of cmla-dtrans f1-a is 82.45. f1-o of cmla-dtrans f1-o is 82.67. acc-s of cmla-dtrans acc-s is 79.58. f1-s of cmla-dtrans f1-s is 72.23. f1-i of cmla-dtrans f1-i is 65.34. f1-a of cmla-dtrans f1-a is 76.8. f1-o of cmla-dtrans f1-o is 77.33. acc-s of cmla-dtrans acc-s is 72.38. f1-s of cmla-dtrans f1-s is 69.52. f1-i of cmla-dtrans f1-i is 55.56. f1-a of cmla-dtrans f1-a is 68.55. f1-o of cmla-dtrans f1-o is 71.07. acc-s of cmla-dtrans acc-s is 82.27. f1-s of cmla-dtrans f1-s is 66.45. f1-i of cmla-dtrans f1-i is 56.09. f1-a of decnn-alstm f1-a is 83.94. f1-o of decnn-alstm f1-o is 85.6. acc-s of decnn-alstm acc-s is 77.79. f1-s of decnn-alstm f1-s is 68.5. f1-i of decnn-alstm f1-i is 65.26. f1-a of decnn-alstm f1-a is 78.38. f1-o of decnn-alstm f1-o is 78.81. acc-s of decnn-alstm acc-s is 70.46. f1-s of decnn-alstm f1-s is 66.78. f1-i of decnn-alstm f1-i is 55.05. f1-a of decnn-alstm f1-a is 68.32. f1-o of decnn-alstm f1-o is 71.22. acc-s of decnn-alstm acc-s is 80.32. f1-s of decnn-alstm f1-s is 57.25. f1-i of decnn-alstm f1-i is 55.1. f1-a of decnn-dtrans f1-a is 83.94. f1-o of decnn-dtrans f1-o is 85.6. acc-s of decnn-dtrans acc-s is 80.04. f1-s of decnn-dtrans f1-s is 73.31. f1-i of decnn-dtrans f1-i is 67.25. f1-a of decnn-dtrans f1-a is 78.38. f1-o of decnn-dtrans f1-o is 78.81. acc-s of decnn-dtrans acc-s is 73.1. f1-s of decnn-dtrans f1-s is 70.63. f1-i of decnn-dtrans f1-i is 56.6. f1-a of decnn-dtrans f1-a is 68.32. f1-o of decnn-dtrans f1-o is 71.22. acc-s of decnn-dtrans acc-s is 82.65. f1-s of decnn-dtrans f1-s is 69.58. f1-i of decnn-dtrans f1-i is 56.28. f1-a of pipeline f1-a is 83.94. f1-o of pipeline f1-o is 85.6. acc-s of pipeline acc-s is 79.56. f1-s of pipeline f1-s is 69.59. f1-i of pipeline f1-i is 66.53. f1-a of pipeline f1-a is 78.38. f1-o of pipeline f1-o is 78.81. acc-s of pipeline acc-s is 72.29. f1-s of pipeline f1-s is 68.12. f1-i of pipeline f1-i is 56.02. f1-a of pipeline f1-a is 68.32. f1-o of pipeline f1-o is 71.22. acc-s of pipeline acc-s is 82.27. f1-s of pipeline f1-s is 59.53. f1-i of pipeline f1-i is 55.96. f1-a of mnn f1-a is 83.05. f1-o of mnn f1-o is 84.55. acc-s of mnn acc-s is 77.17. f1-s of mnn f1-s is 68.45. f1-i of mnn f1-i is 63.87. f1-a of mnn f1-a is 76.94. f1-o of mnn f1-o is 77.77. acc-s of mnn acc-s is 70.4. f1-s of mnn f1-s is 65.98. f1-i of mnn f1-i is 53.8. f1-a of mnn f1-a is 70.24. f1-o of mnn f1-o is 69.38. acc-s of mnn acc-s is 80.79. f1-s of mnn f1-s is 57.9. f1-i of mnn f1-i is 56.57. f1-a of inabsa f1-a is 83.92. f1-o of inabsa f1-o is 84.97. acc-s of inabsa acc-s is 79.68. f1-s of inabsa f1-s is 68.38. f1-i of inabsa f1-i is 66.6. f1-a of inabsa f1-a is 77.34. f1-o of inabsa f1-o is 76.62. acc-s of inabsa acc-s is 72.3. f1-s of inabsa f1-s is 68.24. f1-i of inabsa f1-i is 55.88. f1-a of inabsa f1-a is 69.4. f1-o of inabsa f1-o is 71.43. acc-s of inabsa acc-s is 82.56. f1-s of inabsa f1-s is 58.81. f1-i of inabsa f1-i is 57.38. f1-a of imn -d wo de f1-a is 83.95. f1-o of imn -d wo de f1-o is 85.21. acc-s of imn -d wo de acc-s is 79.65. f1-s of imn -d wo de f1-s is 69.32. f1-i of imn -d wo de f1-i is 66.96. f1-a of imn -d wo de f1-a is 76.96. f1-o of imn -d wo de f1-o is 76.85. acc-s of imn -d wo de acc-s is 72.89. f1-s of imn -d wo de f1-s is 67.26. f1-i of imn -d wo de f1-i is 56.25. f1-a of imn -d wo de f1-a is 69.23. f1-o of imn -d wo de f1-o is 68.39. acc-s of imn -d wo de acc-s is 81.64. f1-s of imn -d wo de f1-s is 57.51. f1-i of imn -d wo de f1-i is 56.8. f1-a of imn -d f1-a is 84.01. f1-o of imn -d f1-o is 85.64. acc-s of imn -d acc-s is 81.56. f1-s of imn -d f1-s is 71.9. f1-i of imn -d f1-i is 68.32. f1-a of imn -d f1-a is 78.46. f1-o of imn -d f1-o is 78.14. acc-s of imn -d acc-s is 73.21. f1-s of imn -d f1-s is 69.92. f1-i of imn -d f1-i is 57.66. f1-a of imn -d f1-a is 69.8. f1-o of imn -d f1-o is 72.11. acc-s of imn -d acc-s is 83.38. f1-s of imn -d f1-s is 60.65. f1-i of imn -d f1-i is 57.91. f1-a of imn wo de f1-a is 83.5. f1-o of imn wo de f1-o is 84.62. acc-s of imn wo de acc-s is 83.17. f1-s of imn wo de f1-s is 73.44. f1-i of imn wo de f1-i is 69.11. f1-a of imn wo de f1-a is 76.87. f1-o of imn wo de f1-o is 77.04. acc-s of imn wo de acc-s is 74.31. f1-s of imn wo de f1-s is 70.76. f1-i of imn wo de f1-i is 57.04. f1-a of imn wo de f1-a is 68.23. f1-o of imn wo de f1-o is 70.09. acc-s of imn wo de acc-s is 85.9. f1-s of imn wo de f1-s is 71.67. f1-i of imn wo de f1-i is 58.82. f1-a of imn f1-a is 83.33. f1-o of imn f1-o is 85.61. acc-s of imn acc-s is 83.89. f1-s of imn f1-s is 75.66. f1-i of imn f1-i is 69.54. f1-a of imn f1-a is 77.96. f1-o of imn f1-o is 77.51. acc-s of imn acc-s is 75.36. f1-s of imn f1-s is 72.02. f1-i of imn f1-i is 58.37. f1-a of imn f1-a is 70.04. f1-o of imn f1-o is 71.94. acc-s of imn acc-s is 85.64. f1-s of imn f1-s is 71.76. f1-i of imn f1-i is 59.18.
table 3 shows document retrieval evaluation on dev set (%). ofever of athene ofever is 93.55. ofever of unc nlp ofever is 92.82. ofever of our model ofever is 93.33.
table 2 shows comparison of evaluation metrics for proposed systems (unts), unsupervised baseline (unmt,usmt, and st) and existing supervised and the unsupervised lexical simplification system lightls. fe-diff of unts+10k fe-diff is 10.45. sari of unts+10k sari is 35.29. bleu of unts+10k bleu is 76.13. word-diff of unts+10k word-diff is 2.38. fe-diff of unts fe-diff is 11.15. sari of unts sari is 33.8. bleu of unts bleu is 74.24. word-diff of unts word-diff is 3.55. fe-diff of unmt fe-diff is 6.6. sari of unmt sari is 33.72. bleu of unmt bleu is 70.84. word-diff of unmt word-diff is 0.74. fe-diff of usmt fe-diff is 13.84. sari of usmt sari is 32.11. bleu of usmt bleu is 87.36. word-diff of usmt word-diff is -0.01. fe-diff of st fe-diff is 54.38. sari of st sari is 14.97. bleu of st bleu is 0.73. word-diff of st word-diff is 5.61. fe-diff of nts fe-diff is 5.37. sari of nts sari is 36.1. bleu of nts bleu is 79.38. word-diff of nts word-diff is 2.73. fe-diff of sbmt fe-diff is 17.68. sari of sbmt sari is 38.59. bleu of sbmt bleu is 73.62. word-diff of sbmt word-diff is -0.84. fe-diff of pbsmt fe-diff is 9.14. sari of pbsmt sari is 34.07. bleu of pbsmt bleu is 67.79. word-diff of pbsmt word-diff is 2.26. fe-diff of lightls fe-diff is 3.01. sari of lightls sari is 34.96. bleu of lightls bleu is 83.54. word-diff of lightls word-diff is -0.02.
table 1 shows unlabeled sentence-level f1 scores on ptb and ctb test sets. f1 of prpn (shen et al., 2018) mean is 37.4. f1 of prpn (shen et al., 2018) max is 38.1. f1 of (shen et al., 2019) mean is 47.7. f1 of (shen et al., 2019) max is 49.4. f1 of urnng  (kim et al., 2019) max is 4540.00%. f1 of (drozdov et al., 2019) max is 5890.00%. f1 of left branching mean is 8.7. f1 of left branching max is 8.7. f1 of left branching mean is 9.7. f1 of left branching max is 9.7. f1 of right branching mean is 39.5. f1 of right branching max is 39.5. f1 of right branching mean is 20. f1 of right branching max is 20. f1 of random trees mean is 19.2. f1 of random trees max is 19.5. f1 of random trees mean is 1570.00%. f1 of random trees max is 1600.00%. f1 of prpn (tuned) mean is 47.3. f1 of prpn (tuned) max is 47.9. f1 of prpn (tuned) mean is 30.4. f1 of prpn (tuned) max is 31.5. f1 of on (tuned) mean is 48.1. f1 of on (tuned) max is 50. f1 of on (tuned) mean is 25.4. f1 of on (tuned) max is 25.7. f1 of neural pcfg mean is 50.8. f1 of neural pcfg max is 52.6. f1 of neural pcfg mean is 25.7. f1 of neural pcfg max is 29.5. f1 of compound pcfg mean is 55.2. f1 of compound pcfg max is 60.1. f1 of compound pcfg mean is 36. f1 of compound pcfg max is 39.8. f1 of oracle trees mean is 84.3. f1 of oracle trees max is 84.3. f1 of oracle trees mean is 81.1. f1 of oracle trees max is 81.1.
table 2 shows translation results on nist mandarin-english test sets bleu of transformer-base nist06 (dev set) is 45.97. bleu of transformer-base nist02 is 47.4. bleu of transformer-base nist03 is 46.01. bleu of transformer-base nist04 is 47.25. bleu of transformer-base nist08 is 41.71. bleu of beta = 0.2 nist06 (dev set) is 47.14. bleu of beta = 0.2 nist02 is 48.63. bleu of beta = 0.2 nist03 is 47.82. bleu of beta = 0.2 nist04 is 48.63. bleu of beta = 0.2 nist08 is 43.77. bleu of beta = 0.4 nist06 (dev set) is 48.56. bleu of beta = 0.4 nist02 is 49.41. bleu of beta = 0.4 nist03 is 48.73. bleu of beta = 0.4 nist04 is 50.53. bleu of beta = 0.4 nist08 is 45.16. bleu of beta = 0.6 nist06 (dev set) is 48.32. bleu of beta = 0.6 nist02 is 48.83. bleu of beta = 0.6 nist03 is 48.82. bleu of beta = 0.6 nist04 is 49.86. bleu of beta = 0.6 nist08 is 44.17. bleu of beta = 0.8 nist06 (dev set) is 48.15. bleu of beta = 0.8 nist02 is 49.42. bleu of beta = 0.8 nist03 is 49.44. bleu of beta = 0.8 nist04 is 49.98. bleu of beta = 0.8 nist08 is 44.86. bleu of beta = 0.95 nist06 (dev set) is 48.91. bleu of beta = 0.95 nist02 is 49.33. bleu of beta = 0.95 nist03 is 50.46. bleu of beta = 0.95 nist04 is 50.57. bleu of beta = 0.95 nist08 is 44.83. bleu of beta = 1.0 nist06 (dev set) is 45.6. bleu of beta = 1.0 nist02 is 47.04. bleu of beta = 1.0 nist03 is 46.42. bleu of beta = 1.0 nist04 is 47.65. bleu of beta = 1.0 nist08 is 40.27.
table 4 shows performance (% accuracy) of various (retrained) embedding models on magnitude and numeration tests. accuracy of random ova-mag is 0. accuracy of random sc-mag is 49.62. accuracy of random bc-mag is 49.71. accuracy of random ova-num is 2.31. accuracy of random sc-num is 47.69. accuracy of random bc-num is 53.68. accuracy of glove-num ova-mag is 0.01. accuracy of glove-num sc-mag is 49.47. accuracy of glove-num bc-mag is 72.76. accuracy of glove-num ova-num is 0. accuracy of glove-num sc-num is 50. accuracy of glove-num bc-num is 19.85. accuracy of glove-all ova-mag is 0.01. accuracy of glove-all sc-mag is 49.08. accuracy of glove-all bc-mag is 74.02. accuracy of glove-all ova-num is 0. accuracy of glove-all sc-num is 46.15. accuracy of glove-all bc-num is 19.85. accuracy of fasttext-num ova-mag is 0.09. accuracy of fasttext-num sc-mag is 51.05. accuracy of fasttext-num bc-mag is 96.69. accuracy of fasttext-num ova-num is 1.54. accuracy of fasttext-num sc-num is 54.62. accuracy of fasttext-num bc-num is 58.09. accuracy of fasttext-all ova-mag is 0.09. accuracy of fasttext-all sc-mag is 51.16. accuracy of fasttext-all bc-mag is 97.9. accuracy of fasttext-all ova-num is 0. accuracy of fasttext-all sc-num is 46.92. accuracy of fasttext-all bc-num is 61.03. accuracy of word2vec-num ova-mag is 0.02. accuracy of word2vec-num sc-mag is 50.12. accuracy of word2vec-num bc-mag is 93.55. accuracy of word2vec-num ova-num is 0.77. accuracy of word2vec-num sc-num is 44.62. accuracy of word2vec-num bc-num is 33.82. accuracy of word2vec-all ova-mag is 0.02. accuracy of word2vec-all sc-mag is 49.37. accuracy of word2vec-all bc-mag is 94.2. accuracy of word2vec-all ova-num is 0. accuracy of word2vec-all sc-num is 54.62. accuracy of word2vec-all bc-num is 34.56.
table 6 shows hate speech results (3-fold cross val.). prec of only synthetic prec is 0.58 (0.63). recall of only synthetic recall is 0.60 (0.63). f-score of only synthetic f-score is 0.51 (0.52). prec of synthetic +gold prec is 0.59 (0.60). recall of synthetic +gold recall is 0.63 (0.63). f-score of synthetic +gold f-score is 0.53 (0.54). prec of gold prec is 0.4. recall of gold recall is 0.62. f-score of gold f-score is 0.48.
table 3 shows performances of all the approaches to two sub-tasks, i.e., term-level and category-level asc-qa. f1 of lstm (wang et al.,2016) f1 is 0.571. acc. of lstm (wang et al.,2016) acc. is 0.757. f1 of lstm (wang et al.,2016) f1 is 0.582. acc. of lstm (wang et al.,2016) acc. is 0.771. f1 of lstm (wang et al.,2016) f1 is 0.534. acc. of lstm (wang et al.,2016) acc. is 0.756. f1 of lstm (wang et al.,2016) f1 is 0.528. acc. of lstm (wang et al.,2016) acc. is 0.773. f1 of lstm (wang et al.,2016) f1 is 0.493. acc. of lstm (wang et al.,2016) acc. is 0.739. f1 of lstm (wang et al.,2016) f1 is 0.522. acc. of lstm (wang et al.,2016) acc. is 0.752. f1 of ram (chen et al.,2017) f1 is 0.605. acc. of ram (chen et al.,2017) acc. is 0.782. f1 of ram (chen et al.,2017) f1 is 0.614. acc. of ram (chen et al.,2017) acc. is 0.805. f1 of ram (chen et al.,2017) f1 is 0.557. acc. of ram (chen et al.,2017) acc. is 0.788. f1 of ram (chen et al.,2017) f1 is 0.561. acc. of ram (chen et al.,2017) acc. is 0.795. f1 of ram (chen et al.,2017) f1 is 0.519. acc. of ram (chen et al.,2017) acc. is 0.762. f1 of ram (chen et al.,2017) f1 is 0.579. acc. of ram (chen et al.,2017) acc. is 0.792. f1 of gcae (xue and li, 2018) f1 is 0.617. acc. of gcae (xue and li, 2018) acc. is 0.779. f1 of gcae (xue and li, 2018) f1 is 0.623. acc. of gcae (xue and li, 2018) acc. is 0.819. f1 of gcae (xue and li, 2018) f1 is 0.57. acc. of gcae (xue and li, 2018) acc. is 0.781. f1 of gcae (xue and li, 2018) f1 is 0.59. acc. of gcae (xue and li, 2018) acc. is 0.787. f1 of gcae (xue and li, 2018) f1 is 0.514. acc. of gcae (xue and li, 2018) acc. is 0.791. f1 of gcae (xue and li, 2018) f1 is 0.576. acc. of gcae (xue and li, 2018) acc. is 0.788. f1 of s-lstm (wang and lu, 2018) f1 is 0.615. acc. of s-lstm (wang and lu, 2018) acc. is 0.824. f1 of s-lstm (wang and lu, 2018) f1 is 0.623. acc. of s-lstm (wang and lu, 2018) acc. is 0.821. f1 of s-lstm (wang and lu, 2018) f1 is 0.569. acc. of s-lstm (wang and lu, 2018) acc. is 0.794. f1 of s-lstm (wang and lu, 2018) f1 is 0.587. acc. of s-lstm (wang and lu, 2018) acc. is 0.828. f1 of s-lstm (wang and lu, 2018) f1 is 0.522. acc. of s-lstm (wang and lu, 2018) acc. is 0.788. f1 of s-lstm (wang and lu, 2018) f1 is 0.581. acc. of s-lstm (wang and lu, 2018) acc. is 0.801. f1 of bidaf (seo et al., 2016) f1 is 0.613. acc. of bidaf (seo et al., 2016) acc. is 0.815. f1 of bidaf (seo et al., 2016) f1 is 0.618. acc. of bidaf (seo et al., 2016) acc. is 0.813. f1 of bidaf (seo et al., 2016) f1 is 0.558. acc. of bidaf (seo et al., 2016) acc. is 0.809. f1 of bidaf (seo et al., 2016) f1 is 0.592. acc. of bidaf (seo et al., 2016) acc. is 0.83. f1 of bidaf (seo et al., 2016) f1 is 0.515. acc. of bidaf (seo et al., 2016) acc. is 0.788. f1 of bidaf (seo et al., 2016) f1 is 0.571. acc. of bidaf (seo et al., 2016) acc. is 0.787. f1 of hmn (shen et al., 2018a) f1 is 0.607. acc. of hmn (shen et al., 2018a) acc. is 0.817. f1 of hmn (shen et al., 2018a) f1 is 0.615. acc. of hmn (shen et al., 2018a) acc. is 0.821. f1 of hmn (shen et al., 2018a) f1 is 0.561. acc. of hmn (shen et al., 2018a) acc. is 0.802. f1 of hmn (shen et al., 2018a) f1 is 0.606. acc. of hmn (shen et al., 2018a) acc. is 0.827. f1 of hmn (shen et al., 2018a) f1 is 0.512. acc. of hmn (shen et al., 2018a) acc. is 0.798. f1 of hmn (shen et al., 2018a) f1 is 0.579. acc. of hmn (shen et al., 2018a) acc. is 0.804. f1 of mamc (yin et al., 2017) f1 is 0.621. acc. of mamc (yin et al., 2017) acc. is 0.825. f1 of mamc (yin et al., 2017) f1 is 0.629. acc. of mamc (yin et al., 2017) acc. is 0.823. f1 of mamc (yin et al., 2017) f1 is 0.562. acc. of mamc (yin et al., 2017) acc. is 0.815. f1 of mamc (yin et al., 2017) f1 is 0.612. acc. of mamc (yin et al., 2017) acc. is 0.837. f1 of mamc (yin et al., 2017) f1 is 0.524. acc. of mamc (yin et al., 2017) acc. is 0.794. f1 of mamc (yin et al., 2017) f1 is 0.582. acc. of mamc (yin et al., 2017) acc. is 0.805. f1 of rban w/o raws f1 is 0.623. acc. of rban w/o raws acc. is 0.826. f1 of rban w/o raws f1 is 0.633. acc. of rban w/o raws acc. is 0.827. f1 of rban w/o raws f1 is 0.578. acc. of rban w/o raws acc. is 0.817. f1 of rban w/o raws f1 is 0.616. acc. of rban w/o raws acc. is 0.839. f1 of rban w/o raws f1 is 0.532. acc. of rban w/o raws acc. is 0.804. f1 of rban w/o raws f1 is 0.591. acc. of rban w/o raws acc. is 0.813. f1 of rban w/o q2a f1 is 0.595. acc. of rban w/o q2a acc. is 0.788. f1 of rban w/o q2a f1 is 0.614. acc. of rban w/o q2a acc. is 0.817. f1 of rban w/o q2a f1 is 0.569. acc. of rban w/o q2a acc. is 0.779. f1 of rban w/o q2a f1 is 0.578. acc. of rban w/o q2a acc. is 0.814. f1 of rban w/o q2a f1 is 0.514. acc. of rban w/o q2a acc. is 0.788. f1 of rban w/o q2a f1 is 0.569. acc. of rban w/o q2a acc. is 0.782. f1 of rban w/o a2q f1 is 0.623. acc. of rban w/o a2q acc. is 0.837. f1 of rban w/o a2q f1 is 0.639. acc. of rban w/o a2q acc. is 0.834. f1 of rban w/o a2q f1 is 0.588. acc. of rban w/o a2q acc. is 0.821. f1 of rban w/o a2q f1 is 0.617. acc. of rban w/o a2q acc. is 0.845. f1 of rban w/o a2q f1 is 0.536. acc. of rban w/o a2q acc. is 0.815. f1 of rban w/o a2q f1 is 0.603. acc. of rban w/o a2q acc. is 0.826. f1 of rban f1 is 0.648. acc. of rban acc. is 0.856. f1 of rban f1 is 0.662. acc. of rban acc. is 0.855. f1 of rban f1 is 0.616. acc. of rban acc. is 0.833. f1 of rban f1 is 0.634. acc. of rban acc. is 0.869. f1 of rban f1 is 0.557. acc. of rban acc. is 0.833. f1 of rban f1 is 0.625. acc. of rban acc. is 0.839.
table 2 shows methods allowing the model to keep track of past attention (coverage, scratchpad) significantly improve performance when combined with a copy mechanism. bleu of baseline bleu is 7.51. meteor of baseline meteor is 23.9. rouge-l of baseline rouge-l is 47.1. bleu of baseline bleu is 17.96. meteor of baseline meteor is 22.9. rouge-l of baseline rouge-l is 47.13. bleu of copynet bleu is 6.89. meteor of copynet meteor is 27.1. rouge-l of copynet rouge-l is 52.5. bleu of copynet bleu is 17.42. meteor of copynet meteor is 26.03. rouge-l of copynet rouge-l is 52.56. bleu of copy + coverage bleu is 14.55. meteor of copy + coverage meteor is 33.7. rouge-l of copy + coverage rouge-l is 58.9. bleu of copy + coverage bleu is 26.78. meteor of copy + coverage meteor is 30.86. rouge-l of copy + coverage rouge-l is 58.91. bleu of copy + scratchpad bleu is 15.29. meteor of copy + scratchpad meteor is 34.7. rouge-l of copy + scratchpad rouge-l is 59.5. bleu of copy + scratchpad bleu is 27.64. meteor of copy + scratchpad meteor is 31.49. rouge-l of copy + scratchpad rouge-l is 59.44. bleu of baseline bleu is 9.94. meteor of baseline meteor is 26.71. rouge-l of baseline rouge-l is 47.96. bleu of baseline bleu is 17.34. meteor of baseline meteor is 25.34. rouge-l of baseline rouge-l is 47.96. bleu of copynet bleu is 8.04. meteor of copynet meteor is 24.66. rouge-l of copynet rouge-l is 46.82. bleu of copynet bleu is 15.11. meteor of copynet meteor is 23.53. rouge-l of copynet rouge-l is 46.82. bleu of copy + coverage bleu is 15.76. meteor of copy + coverage meteor is 34.04. rouge-l of copy + coverage rouge-l is 54.94. bleu of copy + coverage bleu is 25.01. meteor of copy + coverage meteor is 32.38. rouge-l of copy + coverage rouge-l is 54.94. bleu of copy + scratchpad bleu is 16.89. meteor of copy + scratchpad meteor is 34.47. rouge-l of copy + scratchpad rouge-l is 55.69. bleu of copy + scratchpad bleu is 26.1. meteor of copy + scratchpad meteor is 32.76. rouge-l of copy + scratchpad rouge-l is 55.69.
table 1 shows comparison of approaches in the ehealth-kd challenge. f 1 of zavala et al. (2018) f 1 is 0.744. f 1 of lopez-ubeda et al. (2018) f 1 is 0.71. f 1 of palatresi and hontoria (2018) f 1 is 0.681. f 1 of suarez-paniagua et al. (2018) f 1 is 0.31. f 1 of our proposal f 1 is 0.754.
table 7 shows results of tdms-ie for ten leaderboards on arc-pdn. p@1 of dependency parsing:penn treebank:uas p@1 is 1. p@3 of dependency parsing:penn treebank:uas p@3 is 1. p@5 of dependency parsing:penn treebank:uas p@5 is 0.8. p@10 of dependency parsing:penn treebank:uas p@10 is 0.9. #correct score of dependency parsing:penn treebank:uas #correct score is 2. #wrong task of dependency parsing:penn treebank:uas #wrong task is 0. p@1 of summarization:duc 2004 task 1:rouge-2 p@1 is 0. p@3 of summarization:duc 2004 task 1:rouge-2 p@3 is 0.67. p@5 of summarization:duc 2004 task 1:rouge-2 p@5 is 0.8. p@10 of summarization:duc 2004 task 1:rouge-2 p@10 is 0.7. #correct score of summarization:duc 2004 task 1:rouge-2 #correct score is 0. #wrong task of summarization:duc 2004 task 1:rouge-2 #wrong task is 0. p@1 of word sense disambiguation:senseval 2:f1 p@1 is 0. p@3 of word sense disambiguation:senseval 2:f1 p@3 is 0. p@5 of word sense disambiguation:senseval 2:f1 p@5 is 0.1. p@10 of word sense disambiguation:senseval 2:f1 p@10 is 0.1. #correct score of word sense disambiguation:senseval 2:f1 #correct score is 0. #wrong task of word sense disambiguation:senseval 2:f1 #wrong task is 0. p@1 of word sense disambiguation:semeval 2007:f1 p@1 is 1. p@3 of word sense disambiguation:semeval 2007:f1 p@3 is 1. p@5 of word sense disambiguation:semeval 2007:f1 p@5 is 0.8. p@10 of word sense disambiguation:semeval 2007:f1 p@10 is 0.7. #correct score of word sense disambiguation:semeval 2007:f1 #correct score is 1. #wrong task of word sense disambiguation:semeval 2007:f1 #wrong task is 0. p@1 of word segmentation:chinese treebank 6:f1 p@1 is 1. p@3 of word segmentation:chinese treebank 6:f1 p@3 is 0.67. p@5 of word segmentation:chinese treebank 6:f1 p@5 is 0.4. p@10 of word segmentation:chinese treebank 6:f1 p@10 is 0.2. #correct score of word segmentation:chinese treebank 6:f1 #correct score is 0. #wrong task of word segmentation:chinese treebank 6:f1 #wrong task is 2. p@1 of word segmentation:msra:f1 p@1 is 1. p@3 of word segmentation:msra:f1 p@3 is 0.67. p@5 of word segmentation:msra:f1 p@5 is 0.6. p@10 of word segmentation:msra:f1 p@10 is 0.7. #correct score of word segmentation:msra:f1 #correct score is 2. #wrong task of word segmentation:msra:f1 #wrong task is 3. p@1 of sentiment analysis:sst-2:accuracy p@1 is 1. p@3 of sentiment analysis:sst-2:accuracy p@3 is 0.67. p@5 of sentiment analysis:sst-2:accuracy p@5 is 0.6. p@10 of sentiment analysis:sst-2:accuracy p@10 is 0.3. #correct score of sentiment analysis:sst-2:accuracy #correct score is 0. #wrong task of sentiment analysis:sst-2:accuracy #wrong task is 3. p@1 of amr parsing:ldc2014t12:f1 on all p@1 is 0. p@3 of amr parsing:ldc2014t12:f1 on all p@3 is 0.67. p@5 of amr parsing:ldc2014t12:f1 on all p@5 is 0.4. p@10 of amr parsing:ldc2014t12:f1 on all p@10 is 0.2. #correct score of amr parsing:ldc2014t12:f1 on all #correct score is 0. #wrong task of amr parsing:ldc2014t12:f1 on all #wrong task is 5. p@1 of ccg supertagging:ccgbank:accuracy p@1 is 1. p@3 of ccg supertagging:ccgbank:accuracy p@3 is 1. p@5 of ccg supertagging:ccgbank:accuracy p@5 is 1. p@10 of ccg supertagging:ccgbank:accuracy p@10 is 0.8. #correct score of ccg supertagging:ccgbank:accuracy #correct score is 0. #wrong task of ccg supertagging:ccgbank:accuracy #wrong task is 1. p@1 of machine translation:wmt 2014 en-fr:bleu p@1 is 1. p@3 of machine translation:wmt 2014 en-fr:bleu p@3 is 0.33. p@5 of machine translation:wmt 2014 en-fr:bleu p@5 is 0.2. p@10 of machine translation:wmt 2014 en-fr:bleu p@10 is 0.1. #correct score of machine translation:wmt 2014 en-fr:bleu #correct score is 0. #wrong task of machine translation:wmt 2014 en-fr:bleu #wrong task is 0. p@1 of macro-average p@1 is 0.7. p@3 of macro-average p@3 is 0.67. p@5 of macro-average p@5 is 0.57. p@10 of macro-average p@10 is 0.46.
table 3 shows the results of our system on the hate speech and kaggle datasets. p (%) of (davidson et al., 2017) p (%) is 91. r (%) of (davidson et al., 2017) r (%) is 90. f1 (%) of (davidson et al., 2017) f1 (%) is 90. auc (%) of (davidson et al., 2017) auc (%) is 87. p (%) of (founta et al., 2018) p (%) is 89. r (%) of (founta et al., 2018) r (%) is 89. f1 (%) of (founta et al., 2018) f1 (%) is 89. auc (%) of (founta et al., 2018) auc (%) is 92. p (%) of this work + chisquare50  p (%) is 89.7. r (%) of this work + chisquare50  r (%) is 90.4. f1 (%) of this work + chisquare50  f1 (%) is 90. auc (%) of this work + chisquare50  auc (%) is 92.9. p (%) of this work + chisuare100  p (%) is 90.3. r (%) of this work + chisuare100  r (%) is 92.5. f1 (%) of this work + chisuare100  f1 (%) is 91.3. auc (%) of this work + chisuare100  auc (%) is 93.7. p (%) of this work + anova50  p (%) is 89.2. r (%) of this work + anova50  r (%) is 89.6. f1 (%) of this work + anova50  f1 (%) is 89.3. auc (%) of this work + anova50  auc (%) is 92.1. p (%) of this work + anova100  p (%) is 89.8. r (%) of this work + anova100  r (%) is 89.2. f1 (%) of this work + anova100  f1 (%) is 89.4. auc (%) of this work + anova100  auc (%) is 92.4. auc (%) of leader-board   auc (%) is 98.82. auc (%) of this work + chisquare50  auc (%) is 98.05. auc (%) of this work + chisquare100  auc (%) is 98.24.
table 3 shows comparison of embeddings on word entailment. best ap of (baroni et al., 2012) best ap is 0.751. best ap of word2gm (10)-cos best ap is 0.729. best f1 of word2gm (10)-cos best f1 is 0.757. best ap of word2gm (10)-kl best ap is 0.747. best f1 of word2gm (10)-kl best f1 is 0.763. best ap of word2sense best ap is 0.751. best f1 of word2sense best f1 is 0.761. best ap of word2sense -full best ap is 0.791. best f1 of word2sense -full best f1 is 0.798.
table 5 shows comparison of embeddings on for word intrusion tasks. agreement of word2vec agreement is 0.77/0.18. precision of word2vec precision is 0.261. agreement of spowv agreement is 0.79/0.28. precision of spowv precision is 0.418. agreement of spine agreement is 0.91/0.48. precision of spine precision is 0.748. agreement of word2sense agreement is 0.891/0.589. precision of word2sense precision is 0.753.
table 6 shows low-resource performance (300 labeled examples) of different sampling strategies (dblp-acm). prec of high-confidence prec is 93.32. recall of high-confidence recall is 97.21. f1 of high-confidence f1 is 95.19±2.21. prec of partition prec is 96.14. recall of partition recall is 97.12. f1 of partition f1 is 96.61±0.57. prec of high-conf.+part. prec is 97.63. recall of high-conf.+part. recall is 97.84. f1 of high-conf.+part. f1 is 97.73±0.43. prec of top k entrop prec is 96.16. recall of top k entrop recall is 89.64. f1 of top k entrop f1 is 92.07±9.73.
table 6 shows automatic semantic evaluation (higher is better for all but ser). bleu of base bleu is 0.126. meteor of base meteor is 0.206. cider of base cider is 1.3. nist of base nist is 3.84. avg ser of base avg ser is 0.053. bleu of  +adi bleu is 0.164. meteor of  +adi meteor is 0.233. cider of  +adi cider is 1.686. nist of  +adi nist is 4.547. avg ser of  +adi avg ser is 0.063. bleu of  +sent bleu is 0.166. meteor of  +sent meteor is 0.234. cider of  +sent cider is 1.692. nist of  +sent nist is 4.477. avg ser of  +sent avg ser is 0.064. bleu of  +style bleu is 0.173. meteor of  +style meteor is 0.235. cider of  +style cider is 1.838. nist of  +style nist is 5.537. avg ser of  +style avg ser is 0.09.
table 1 shows quantitative comparison (rouge 1, 2 and l) of models on aspect-specific summarization. rouge 1 of lead-3 rouge 1 is 0.2150. rouge 2 of lead-3 rouge 2 is 0.0690. rouge l of lead-3 rouge l is 0.1410. rouge 1 of pg-net rouge 1 is 0.1757. rouge 2 of pg-net rouge 2 is 0.0472. rouge l of pg-net rouge l is 0.1594. rouge 1 of enc-attn rouge 1 is 0.2750. rouge 2 of enc-attn rouge 2 is 0.1027. rouge l of enc-attn rouge l is 0.2502. rouge 1 of dec-attn rouge 1 is 0.2734. rouge 2 of dec-attn rouge 2 is 0.1005. rouge l of dec-attn rouge l is 0.2509. rouge 1 of sf rouge 1 is 0.2802. rouge 2 of sf rouge 2 is 0.1046. rouge l of sf rouge l is 0.2536. rouge 1 of enc-attn-extract rouge 1 is 0.3033. rouge 2 of enc-attn-extract rouge 2 is 0.1092. rouge l of enc-attn-extract rouge l is 0.2732. rouge 1 of dec-attn-extract rouge 1 is 0.3326. rouge 2 of dec-attn-extract rouge 2 is 0.1379. rouge l of dec-attn-extract rouge l is 0.3026.
table 4 shows experimental results. micro-f1 of lr micro-f1 is 71.25%. macro-f1 of lr macro-f1 is 60.80%. micro-f1 of cnn micro-f1 is 77.17%. macro-f1 of cnn macro-f1 is 58.49%. micro-f1 of gru micro-f1 is 78.25%. macro-f1 of gru macro-f1 is 58.08%. micro-f1 of bigru micro-f1 is 80.16%. macro-f1 of bigru macro-f1 is 62.74%. micro-f1 of crnn micro-f1 is 78.00%. macro-f1 of crnn macro-f1 is 64.62%. micro-f1 of cnn-capsule micro-f1 is 75.89%. macro-f1 of cnn-capsule macro-f1 is 59.22%. micro-f1 of gru-capsule micro-f1 is 77.36%. macro-f1 of gru-capsule macro-f1 is 64.71%. micro-f1 of bigru-capsule micro-f1 is 77.97%. macro-f1 of bigru-capsule macro-f1 is 64.34%.
table 3 shows  yelp test accuracy (without fine-tuning).cnn-sc significantly improves over cnn-r. accuracy of on yelp cnn-r is 67.4. accuracy of on yelp cnn-sc is 90. accuracy of on wikipedia cnn-r is 61.4. accuracy of on wikipedia cnn-sc is 65.7. accuracy of wall-clock speedup cnn-r is 1x. accuracy of wall-clock speedup cnn-sc is 4x.
table 4 shows comparison of redan to state-of-the-art visual dialog models on the blind test-std v1.0 set, as reported by (†) taken from https://evalai.cloudcv.org/web/challenges/challenge-page/161/ the test server. ndcg of redan+ (diverse ens.) ndcg is 64.47. mrr of redan+ (diverse ens.) mrr is 53.73. r@1 of redan+ (diverse ens.) r@1 is 42.45. r@5 of redan+ (diverse ens.) r@5 is 64.68. r@10 of redan+ (diverse ens.) r@10 is 75.68. mean of redan+ (diverse ens.) mean is 6.63. ndcg of redan (1 dis. + 1 gen.) ndcg is 61.86. mrr of redan (1 dis. + 1 gen.) mrr is 53.13. r@1 of redan (1 dis. + 1 gen.) r@1 is 41.38. r@5 of redan (1 dis. + 1 gen.) r@5 is 66.07. r@10 of redan (1 dis. + 1 gen.) r@10 is 74.5. mean of redan (1 dis. + 1 gen.) mean is 8.91. ndcg of dan (kang et al., 2019) ndcg is 59.36. mrr of dan (kang et al., 2019) mrr is 64.92. r@1 of dan (kang et al., 2019) r@1 is 51.28. r@5 of dan (kang et al., 2019) r@5 is 81.6. r@10 of dan (kang et al., 2019) r@10 is 90.88. mean of dan (kang et al., 2019) mean is 3.92. ndcg of nmn (kottur et al., 2018) ndcg is 58.1. mrr of nmn (kottur et al., 2018) mrr is 58.8. r@1 of nmn (kottur et al., 2018) r@1 is 44.15. r@5 of nmn (kottur et al., 2018) r@5 is 76.88. r@10 of nmn (kottur et al., 2018) r@10 is 86.88. mean of nmn (kottur et al., 2018) mean is 4.81. ndcg of sync (guo et al., 2019) ndcg is 57.88. mrr of sync (guo et al., 2019) mrr is 63.42. r@1 of sync (guo et al., 2019) r@1 is 49.3. r@5 of sync (guo et al., 2019) r@5 is 80.77. r@10 of sync (guo et al., 2019) r@10 is 90.68. mean of sync (guo et al., 2019) mean is 3.97. ndcg of hacan (yang et al., 2019) ndcg is 57.17. mrr of hacan (yang et al., 2019) mrr is 64.22. r@1 of hacan (yang et al., 2019) r@1 is 50.88. r@5 of hacan (yang et al., 2019) r@5 is 80.63. r@10 of hacan (yang et al., 2019) r@10 is 89.45. mean of hacan (yang et al., 2019) mean is 4.2. ndcg of fga† ndcg is 57.13. mrr of fga† mrr is 69.25. r@1 of fga† r@1 is 55.65. r@5 of fga† r@5 is 86.73. r@10 of fga† r@10 is 94.05. mean of fga† mean is 3.14. ndcg of ustc-yth‡ ndcg is 56.47. mrr of ustc-yth‡ mrr is 61.44. r@1 of ustc-yth‡ r@1 is 47.65. r@5 of ustc-yth‡ r@5 is 78.13. r@10 of ustc-yth‡ r@10 is 87.88. mean of ustc-yth‡ mean is 4.65. ndcg of rva (niu et al., 2018) ndcg is 55.59. mrr of rva (niu et al., 2018) mrr is 63.03. r@1 of rva (niu et al., 2018) r@1 is 49.03. r@5 of rva (niu et al., 2018) r@5 is 80.4. r@10 of rva (niu et al., 2018) r@10 is 89.83. mean of rva (niu et al., 2018) mean is 4.18. ndcg of ms convai‡ ndcg is 55.35. mrr of ms convai‡ mrr is 63.27. r@1 of ms convai‡ r@1 is 49.53. r@5 of ms convai‡ r@5 is 80.4. r@10 of ms convai‡ r@10 is 89.6. mean of ms convai‡ mean is 4.15. ndcg of corefnmn (kottur et al., 2018) ndcg is 54.7. mrr of corefnmn (kottur et al., 2018) mrr is 61.5. r@1 of corefnmn (kottur et al., 2018) r@1 is 47.55. r@5 of corefnmn (kottur et al., 2018) r@5 is 78.1. r@10 of corefnmn (kottur et al., 2018) r@10 is 88.8. mean of corefnmn (kottur et al., 2018) mean is 4.4. ndcg of fga (schwartz et al., 2019) ndcg is 54.46. mrr of fga (schwartz et al., 2019) mrr is 67.25. r@1 of fga (schwartz et al., 2019) r@1 is 53.4. r@5 of fga (schwartz et al., 2019) r@5 is 85.28. r@10 of fga (schwartz et al., 2019) r@10 is 92.7. mean of fga (schwartz et al., 2019) mean is 3.54. ndcg of gnn (zheng et al., 2019) ndcg is 52.82. mrr of gnn (zheng et al., 2019) mrr is 61.37. r@1 of gnn (zheng et al., 2019) r@1 is 47.33. r@5 of gnn (zheng et al., 2019) r@5 is 77.98. r@10 of gnn (zheng et al., 2019) r@10 is 87.83. mean of gnn (zheng et al., 2019) mean is 4.57. ndcg of lf-att w/ bottom-up† ndcg is 51.63. mrr of lf-att w/ bottom-up† mrr is 60.41. r@1 of lf-att w/ bottom-up† r@1 is 46.18. r@5 of lf-att w/ bottom-up† r@5 is 77.8. r@10 of lf-att w/ bottom-up† r@10 is 87.3. mean of lf-att w/ bottom-up† mean is 4.75. ndcg of lf-att‡ ndcg is 49.76. mrr of lf-att‡ mrr is 57.07. r@1 of lf-att‡ r@1 is 42.08. r@5 of lf-att‡ r@5 is 74.83. r@10 of lf-att‡ r@10 is 85.05. mean of lf-att‡ mean is 5.41. ndcg of mn-att‡ ndcg is 49.58. mrr of mn-att‡ mrr is 56.9. r@1 of mn-att‡ r@1 is 42.43. r@5 of mn-att‡ r@5 is 74. r@10 of mn-att‡ r@10 is 84.35. mean of mn-att‡ mean is 5.59. ndcg of mn‡ ndcg is 47.5. mrr of mn‡ mrr is 55.49. r@1 of mn‡ r@1 is 40.98. r@5 of mn‡ r@5 is 72.3. r@10 of mn‡ r@10 is 83.3. mean of mn‡ mean is 5.92. ndcg of hre‡ ndcg is 45.46. mrr of hre‡ mrr is 54.16. r@1 of hre‡ r@1 is 39.93. r@5 of hre‡ r@5 is 70.45. r@10 of hre‡ r@10 is 81.5. mean of hre‡ mean is 6.41. ndcg of lf‡ ndcg is 45.31. mrr of lf‡ mrr is 55.42. r@1 of lf‡ r@1 is 40.95. r@5 of lf‡ r@5 is 72.45. r@10 of lf‡ r@10 is 82.83. mean of lf‡ mean is 5.95.
