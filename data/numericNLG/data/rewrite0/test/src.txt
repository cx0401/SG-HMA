table 5 shows link prediction results on the test-i, test-ii, and test-all sets of fb122 and wn18 (filtered setting). mrr of transe mrr is 0.296. med of transe med is 13.0. hits@3 (%) of transe hits@3 (%) is 36.0. hits@5 (%) of transe hits@5 (%) is 41.5. hits@10 (%) of transe hits@10 (%) is 48.1. mrr of transe mrr is 0.630. med of transe med is 2.0. hits@3 (%) of transe hits@3 (%) is 77.5. hits@5 (%) of transe hits@5 (%) is 82.8. hits@10 (%) of transe hits@10 (%) is 88.4. mrr of transe mrr is 0.480. med of transe med is 2.0. hits@3 (%) of transe hits@3 (%) is 58.9. hits@5 (%) of transe hits@5 (%) is 64.2. hits@10 (%) of transe hits@10 (%) is 70.2. mrr of transh mrr is 0.280. med of transh med is 15.0. hits@3 (%) of transh hits@3 (%) is 33.6. hits@5 (%) of transh hits@5 (%) is 39.1. hits@10 (%) of transh hits@10 (%) is 46.4. mrr of transh mrr is 0.606. med of transh med is 2.0. hits@3 (%) of transh hits@3 (%) is 70.1. hits@5 (%) of transh hits@5 (%) is 75.4. hits@10 (%) of transh hits@10 (%) is 82.0. mrr of transh mrr is 0.460. med of transh med is 3.0. hits@3 (%) of transh hits@3 (%) is 53.7. hits@5 (%) of transh hits@5 (%) is 59.1. hits@10 (%) of transh hits@10 (%) is 66.0. mrr of transr mrr is 0.283. med of transr med is 16.0. hits@3 (%) of transr hits@3 (%) is 33.4. hits@5 (%) of transr hits@5 (%) is 39.2. hits@10 (%) of transr hits@10 (%) is 46.0. mrr of transr mrr is 0.499. med of transr med is 2.0. hits@3 (%) of transr hits@3 (%) is 57.0. hits@5 (%) of transr hits@5 (%) is 63.2. hits@10 (%) of transr hits@10 (%) is 70.1. mrr of transr mrr is 0.401. med of transr med is 5.0. hits@3 (%) of transr hits@3 (%) is 46.4. hits@5 (%) of transr hits@5 (%) is 52.4. hits@10 (%) of transr hits@10 (%) is 59.3. mrr of kale-trip mrr is 0.299. med of kale-trip med is 10.0. hits@3 (%) of kale-trip hits@3 (%) is 36.6. hits@5 (%) of kale-trip hits@5 (%) is 42.9. hits@10 (%) of kale-trip hits@10 (%) is 50.2. mrr of kale-trip mrr is 0.650. med of kale-trip med is 2.0. hits@3 (%) of kale-trip hits@3 (%) is 79.0. hits@5 (%) of kale-trip hits@5 (%) is 83.4. hits@10 (%) of kale-trip hits@10 (%) is 88.7. mrr of kale-trip mrr is 0.492. med of kale-trip med is 2.0. hits@3 (%) of kale-trip hits@3 (%) is 59.9. hits@5 (%) of kale-trip hits@5 (%) is 65.2. hits@10 (%) of kale-trip hits@10 (%) is 71.4. mrr of kale-pre mrr is 0.291. med of kale-pre med is 11.0. hits@3 (%) of kale-pre hits@3 (%) is 35.8. hits@5 (%) of kale-pre hits@5 (%) is 41.9. hits@10 (%) of kale-pre hits@10 (%) is 49.8. mrr of kale-pre mrr is 0.713. med of kale-pre med is 1.0. hits@3 (%) of kale-pre hits@3 (%) is 82.9. hits@5 (%) of kale-pre hits@5 (%) is 86.1. hits@10 (%) of kale-pre hits@10 (%) is 89.9. mrr of kale-pre mrr is 0.523. med of kale-pre med is 2.0. hits@3 (%) of kale-pre hits@3 (%) is 61.7. hits@5 (%) of kale-pre hits@5 (%) is 66.2. hits@10 (%) of kale-pre hits@10 (%) is 71.8. mrr of kale-joint mrr is 0.325. med of kale-joint med is 9.0. hits@3 (%) of kale-joint hits@3 (%) is 38.4. hits@5 (%) of kale-joint hits@5 (%) is 44.7. hits@10 (%) of kale-joint hits@10 (%) is 52.2. mrr of kale-joint mrr is 0.684. med of kale-joint med is 1.0. hits@3 (%) of kale-joint hits@3 (%) is 79.7. hits@5 (%) of kale-joint hits@5 (%) is 84.1. hits@10 (%) of kale-joint hits@10 (%) is 89.6. mrr of kale-joint mrr is 0.523. med of kale-joint med is 2.0. hits@3 (%) of kale-joint hits@3 (%) is 61.2. hits@5 (%) of kale-joint hits@5 (%) is 66.4. hits@10 (%) of kale-joint hits@10 (%) is 72.8. mrr of transe mrr is 0.306. med of transe med is 3.0. hits@3 (%) of transe hits@3 (%) is 57.4. hits@5 (%) of transe hits@5 (%) is 72.3. hits@10 (%) of transe hits@10 (%) is 80.1. mrr of transe mrr is 0.511. med of transe med is 2.0. hits@3 (%) of transe hits@3 (%) is 87.5. hits@5 (%) of transe hits@5 (%) is 95.6. hits@10 (%) of transe hits@10 (%) is 98.7. mrr of transe mrr is 0.453. med of transe med is 2.0. hits@3 (%) of transe hits@3 (%) is 79.1. hits@5 (%) of transe hits@5 (%) is 89.1. hits@10 (%) of transe hits@10 (%) is 93.6. mrr of transh mrr is 0.318. med of transh med is 3.0. hits@3 (%) of transh hits@3 (%) is 61.7. hits@5 (%) of transh hits@5 (%) is 72.4. hits@10 (%) of transh hits@10 (%) is 78.2. mrr of transh mrr is 0.653. med of transh med is 2.0. hits@3 (%) of transh hits@3 (%) is 87.1. hits@5 (%) of transh hits@5 (%) is 91.4. hits@10 (%) of transh hits@10 (%) is 94.6. mrr of transh mrr is 0.560. med of transh med is 2.0. hits@3 (%) of transh hits@3 (%) is 80.0. hits@5 (%) of transh hits@5 (%) is 86.1. hits@10 (%) of transh hits@10 (%) is 90.0. mrr of transr mrr is 0.299. med of transr med is 3.0. hits@3 (%) of transr hits@3 (%) is 56.1. hits@5 (%) of transr hits@5 (%) is 66.7. hits@10 (%) of transr hits@10 (%) is 74.5. mrr of transr mrr is 0.597. med of transr med is 2.0. hits@3 (%) of transr hits@3 (%) is 75.0. hits@5 (%) of transr hits@5 (%) is 81.7. hits@10 (%) of transr hits@10 (%) is 88.0. mrr of transr mrr is 0.514. med of transr med is 2.0. hits@3 (%) of transr hits@3 (%) is 69.7. hits@5 (%) of transr hits@5 (%) is 77.5. hits@10 (%) of transr hits@10 (%) is 84.3. mrr of kale-trip mrr is 0.322. med of kale-trip med is 3.0. hits@3 (%) of kale-trip hits@3 (%) is 61.0. hits@5 (%) of kale-trip hits@5 (%) is 73.9. hits@10 (%) of kale-trip hits@10 (%) is 80.8. mrr of kale-trip mrr is 0.555. med of kale-trip med is 2.0. hits@3 (%) of kale-trip hits@3 (%) is 90.6. hits@5 (%) of kale-trip hits@5 (%) is 96.3. hits@10 (%) of kale-trip hits@10 (%) is 98.8. mrr of kale-trip mrr is 0.490. med of kale-trip med is 2.0. hits@3 (%) of kale-trip hits@3 (%) is 82.3. hits@5 (%) of kale-trip hits@5 (%) is 90.1. hits@10 (%) of kale-trip hits@10 (%) is 93.8. mrr of kale-pre mrr is 0.322. med of kale-pre med is 3.0. hits@3 (%) of kale-pre hits@3 (%) is 60.6. hits@5 (%) of kale-pre hits@5 (%) is 74.5. hits@10 (%) of kale-pre hits@10 (%) is 81.1. mrr of kale-pre mrr is 0.612. med of kale-pre med is 2.0. hits@3 (%) of kale-pre hits@3 (%) is 96.4. hits@5 (%) of kale-pre hits@5 (%) is 98.6. hits@10 (%) of kale-pre hits@10 (%) is 99.6. mrr of kale-pre mrr is 0.532. med of kale-pre med is 2.0. hits@3 (%) of kale-pre hits@3 (%) is 86.4. hits@5 (%) of kale-pre hits@5 (%) is 91.9. hits@10 (%) of kale-pre hits@10 (%) is 94.4. mrr of kale-joint mrr is 0.338. med of kale-joint med is 3.0. hits@3 (%) of kale-joint hits@3 (%) is 65.5. hits@5 (%) of kale-joint hits@5 (%) is 76.3. hits@10 (%) of kale-joint hits@10 (%) is 82.1. mrr of kale-joint mrr is 0.787. med of kale-joint med is 1.0. hits@3 (%) of kale-joint hits@3 (%) is 93.3. hits@5 (%) of kale-joint hits@5 (%) is 95.4. hits@10 (%) of kale-joint hits@10 (%) is 97.2. mrr of kale-joint mrr is 0.662. med of kale-joint med is 2.0. hits@3 (%) of kale-joint hits@3 (%) is 85.5. hits@5 (%) of kale-joint hits@5 (%) is 90.1. hits@10 (%) of kale-joint hits@10 (%) is 93.0.
table 3 shows comparison on validation perplexity. bow, tdnn and nabs are the baseline neural compression models with different encoders in rush et al. (2015) labelled data of bag-of-word (bow) labelled data is 3.8m. perplexity of bag-of-word (bow) perplexity is 43.6. labelled data of convolutional (tdnn) labelled data is 3.8m. perplexity of convolutional (tdnn) perplexity is 35.9. labelled data of attention-based (nabs) (rush et al.,,2015) labelled data is 3.8m. perplexity of attention-based (nabs) (rush et al.,,2015) perplexity is 27.1. labelled data of forced-attention (fsc) labelled data is 3.8m. perplexity of forced-attention (fsc) perplexity is 18.6. labelled data of auto-encoding (asc+fsc1) labelled data is 3.8m. perplexity of auto-encoding (asc+fsc1) perplexity is 16.6.
table 4 shows performance on maximally covered datasets. similarity of text sl is 0.310. similarity of text men is 0.682. similarity of text sl is 0.310. similarity of text men is 0.682. similarity of text sl is 0.310. similarity of text men is 0.682. similarity of text sl is 0.310. similarity of text men is 0.682. similarity of text sl is 0.310. similarity of text men is 0.682. similarity of text sl is 0.310. similarity of text men is 0.682. similarity of visual sl is 0.340. similarity of visual men is 0.503. similarity of visual sl is 0.334. similarity of visual men is 0.513. similarity of visual sl is 0.358. similarity of visual men is 0.495. similarity of visual sl is 0.367. similarity of visual men is 0.501. similarity of visual sl is 0.342. similarity of visual men is 0.512. similarity of visual sl is 0.332. similarity of visual men is 0.494. similarity of mm sl is 0.380. similarity of mm men is 0.711. similarity of mm sl is 0.370. similarity of mm men is 0.719. similarity of mm sl is 0.379. similarity of mm men is 0.711. similarity of mm sl is 0.365. similarity of mm men is 0.716. similarity of mm sl is 0.380. similarity of mm men is 0.714. similarity of mm sl is 0.365. similarity of mm men is 0.716. similarity of visual sl is 0.325. similarity of visual men is 0.567. similarity of visual sl is 0.316. similarity of visual men is 0.554. similarity of visual sl is 0.310. similarity of visual men is 0.526. similarity of visual sl is 0.303. similarity of visual men is 0.520. similarity of visual sl is 0.304. similarity of visual men is 0.551. similarity of visual sl is 0.289. similarity of visual men is 0.507. similarity of mm sl is 0.373. similarity of mm men is 0.727. similarity of mm sl is 0.360. similarity of mm men is 0.725. similarity of mm sl is 0.364. similarity of mm men is 0.723. similarity of mm sl is 0.350. similarity of mm men is 0.724. similarity of mm sl is 0.361. similarity of mm men is 0.727. similarity of mm sl is 0.349. similarity of mm men is 0.719. similarity of visual sl is 0.234. similarity of visual men is 0.483. similarity of visual sl is 0.224. similarity of visual men is 0.441. similarity of visual sl is 0.238. similarity of visual men is 0.407. similarity of visual sl is 0.236. similarity of visual men is 0.385. similarity of visual sl is 0.243. similarity of visual men is 0.460. similarity of visual sl is 0.226. similarity of visual men is 0.385. similarity of mm sl is 0.350. similarity of mm men is 0.715. similarity of mm sl is 0.343. similarity of mm men is 0.711. similarity of mm sl is 0.347. similarity of mm men is 0.689. similarity of mm sl is 0.344. similarity of mm men is 0.703. similarity of mm sl is 0.354. similarity of mm men is 0.702. similarity of mm sl is 0.339. similarity of mm men is 0.696. similarity of visual sl is 0.313. similarity of visual men is 0.561. similarity of visual sl is 0.313. similarity of visual men is 0.561. similarity of visual sl is 0.341. similarity of visual men is 0.540. similarity of visual sl is 0.411. similarity of visual men is 0.603. similarity of visual sl is 0.404. similarity of visual men is 0.584. similarity of visual sl is 0.401. similarity of visual men is 0.578. similarity of mm sl is 0.362. similarity of mm men is 0.713. similarity of mm sl is 0.362. similarity of mm men is 0.713. similarity of mm sl is 0.373. similarity of mm men is 0.719. similarity of mm sl is 0.401. similarity of mm men is 0.731. similarity of mm sl is 0.427. similarity of mm men is 0.727. similarity of mm sl is 0.412. similarity of mm men is 0.723. similarity of visual sl is 0.018. similarity of visual men is 0.448. similarity of visual sl is 0.026. similarity of visual men is 0.376. similarity of visual sl is 0.063. similarity of visual men is 0.487. similarity of visual sl is 0.050. similarity of visual men is 0.434. similarity of visual sl is 0.125. similarity of visual men is 0.506. similarity of visual sl is 0.106. similarity of visual men is 0.451. similarity of mm sl is 0.208. similarity of mm men is 0.686. similarity of mm sl is 0.187. similarity of mm men is 0.672. similarity of mm sl is 0.243. similarity of mm men is 0.700. similarity of mm sl is 0.246. similarity of mm men is 0.696. similarity of mm sl is 0.269. similarity of mm men is 0.708. similarity of mm sl is 0.260. similarity of mm men is 0.698.
table 4 shows open-ended and multiple-choice (mc) results on vqa test set (trained on train+val set) compared with state-of-the-art: accuracy in %. see sec. 4.4. accuracy of mcb y/n is 81.2. accuracy of mcb no. is 35.1. accuracy of mcb other is 49.3. accuracy of mcb all is 60.8. accuracy of mcb all is 65.4. accuracy of mcb + genome y/n is 81.7. accuracy of mcb + genome no. is 36.6. accuracy of mcb + genome other is 51.5. accuracy of mcb + genome all is 62.3. accuracy of mcb + genome all is 66.4. accuracy of mcb + att. y/n is 82.2. accuracy of mcb + att. no. is 37.7. accuracy of mcb + att. other is 54.8. accuracy of mcb + att. all is 64.2. accuracy of mcb + att. all is 68.6. accuracy of mcb + att. + glove y/n is 82.5. accuracy of mcb + att. + glove no. is 37.6. accuracy of mcb + att. + glove other is 55.6. accuracy of mcb + att. + glove all is 64.7. accuracy of mcb + att. + glove all is 69.1. accuracy of mcb + att. + genome y/n is 81.7. accuracy of mcb + att. + genome no. is 38.2. accuracy of mcb + att. + genome other is 57.0. accuracy of mcb + att. + genome all is 65.1. accuracy of mcb + att. + genome all is 69.5. accuracy of mcb + att. + glove + genome y/n is 82.3. accuracy of mcb + att. + glove + genome no. is 37.2. accuracy of mcb + att. + glove + genome other is 57.4. accuracy of mcb + att. + glove + genome all is 65.4. accuracy of mcb + att. + glove + genome all is 69.9. accuracy of ensemble of 7 att. models y/n is 83.4. accuracy of ensemble of 7 att. models no. is 39.8. accuracy of ensemble of 7 att. models other is 58.5. accuracy of ensemble of 7 att. models all is 66.7. accuracy of ensemble of 7 att. models all is 70.2. accuracy of ensemble of 7 att. models y/n is 83.2. accuracy of ensemble of 7 att. models no. is 39.5. accuracy of ensemble of 7 att. models other is 58.0. accuracy of ensemble of 7 att. models all is 66.5. accuracy of ensemble of 7 att. models all is 70.1. accuracy of naver labs (challenge 2nd) y/n is 83.5. accuracy of naver labs (challenge 2nd) no. is 39.8. accuracy of naver labs (challenge 2nd) other is 54.8. accuracy of naver labs (challenge 2nd) all is 64.9. accuracy of naver labs (challenge 2nd) all is 69.4. accuracy of naver labs (challenge 2nd) y/n is 83.3. accuracy of naver labs (challenge 2nd) no. is 38.7. accuracy of naver labs (challenge 2nd) other is 54.6. accuracy of naver labs (challenge 2nd) all is 64.8. accuracy of naver labs (challenge 2nd) all is 69.3. accuracy of hiecoatt (lu et al. 2016) y/n is 79.7. accuracy of hiecoatt (lu et al. 2016) no. is 38.7. accuracy of hiecoatt (lu et al. 2016) other is 51.7. accuracy of hiecoatt (lu et al. 2016) all is 61.8. accuracy of hiecoatt (lu et al. 2016) all is 65.8. accuracy of hiecoatt (lu et al. 2016) all is 62.1. accuracy of hiecoatt (lu et al. 2016) all is 66.1. accuracy of dmn+ (xiong et al. 2016) y/n is 80.5. accuracy of dmn+ (xiong et al. 2016) no. is 36.8. accuracy of dmn+ (xiong et al. 2016) other is 48.3. accuracy of dmn+ (xiong et al. 2016) all is 60.3. accuracy of dmn+ (xiong et al. 2016) all is 60.4. accuracy of fda (ilievski et al. 2016) y/n is 81.1. accuracy of fda (ilievski et al. 2016) no. is 36.2. accuracy of fda (ilievski et al. 2016) other is 45.8. accuracy of fda (ilievski et al. 2016) all is 59.2. accuracy of fda (ilievski et al. 2016) all is 59.5. accuracy of d-nmn (andreas et al. 2016a) y/n is 81.1. accuracy of d-nmn (andreas et al. 2016a) no. is 38.6. accuracy of d-nmn (andreas et al. 2016a) other is 45.5. accuracy of d-nmn (andreas et al. 2016a) all is 59.4. accuracy of d-nmn (andreas et al. 2016a) all is 59.4. accuracy of ama (wu et al. 2016) y/n is 81.0. accuracy of ama (wu et al. 2016) no. is 38.4. accuracy of ama (wu et al. 2016) other is 45.2. accuracy of ama (wu et al. 2016) all is 59.2. accuracy of ama (wu et al. 2016) y/n is 81.1. accuracy of ama (wu et al. 2016) no. is 37.1. accuracy of ama (wu et al. 2016) other is 45.8. accuracy of ama (wu et al. 2016) all is 59.4. accuracy of san (yang et al. 2015) y/n is 79.3. accuracy of san (yang et al. 2015) no. is 36.6. accuracy of san (yang et al. 2015) other is 46.1. accuracy of san (yang et al. 2015) all is 58.7. accuracy of san (yang et al. 2015) all is 58.9. accuracy of nmn (andreas et al. 2016b) y/n is 81.2. accuracy of nmn (andreas et al. 2016b) no. is 38.0. accuracy of nmn (andreas et al. 2016b) other is 44.0. accuracy of nmn (andreas et al. 2016b) all is 58.6. accuracy of nmn (andreas et al. 2016b) y/n is 81.2. accuracy of nmn (andreas et al. 2016b) no. is 37.7. accuracy of nmn (andreas et al. 2016b) other is 44.0. accuracy of nmn (andreas et al. 2016b) all is 58.7. accuracy of ayn (malinowski et al. 2016) y/n is 78.4. accuracy of ayn (malinowski et al. 2016) no. is 36.4. accuracy of ayn (malinowski et al. 2016) other is 46.3. accuracy of ayn (malinowski et al. 2016) all is 58.4. accuracy of ayn (malinowski et al. 2016) y/n is 78.2. accuracy of ayn (malinowski et al. 2016) no. is 36.3. accuracy of ayn (malinowski et al. 2016) other is 46.3. accuracy of ayn (malinowski et al. 2016) all is 58.4. accuracy of smem (xu and saenko, 2016) y/n is 80.9. accuracy of smem (xu and saenko, 2016) no. is 37.3. accuracy of smem (xu and saenko, 2016) other is 43.1. accuracy of smem (xu and saenko, 2016) all is 58.0. accuracy of smem (xu and saenko, 2016) y/n is 80.9. accuracy of smem (xu and saenko, 2016) no. is 37.5. accuracy of smem (xu and saenko, 2016) other is 43.5. accuracy of smem (xu and saenko, 2016) all is 58.2. accuracy of vqa team (antol et al. 2015) y/n is 80.5. accuracy of vqa team (antol et al. 2015) no. is 36.8. accuracy of vqa team (antol et al. 2015) other is 43.1. accuracy of vqa team (antol et al. 2015) all is 57.8. accuracy of vqa team (antol et al. 2015) all is 62.7. accuracy of vqa team (antol et al. 2015) y/n is 80.6. accuracy of vqa team (antol et al. 2015) no. is 36.5. accuracy of vqa team (antol et al. 2015) other is 43.7. accuracy of vqa team (antol et al. 2015) all is 58.2. accuracy of vqa team (antol et al. 2015) all is 63.1. accuracy of dppnet (noh et al. 2015) y/n is 80.7. accuracy of dppnet (noh et al. 2015) no. is 37.2. accuracy of dppnet (noh et al. 2015) other is 41.7. accuracy of dppnet (noh et al. 2015) all is 57.2. accuracy of dppnet (noh et al. 2015) y/n is 80.3. accuracy of dppnet (noh et al. 2015) no. is 36.9. accuracy of dppnet (noh et al. 2015) other is 42.2. accuracy of dppnet (noh et al. 2015) all is 57.4. accuracy of ibowimg (zhou et al. 2015) y/n is 76.5. accuracy of ibowimg (zhou et al. 2015) no. is 35.0. accuracy of ibowimg (zhou et al. 2015) other is 42.6. accuracy of ibowimg (zhou et al. 2015) all is 55.7. accuracy of ibowimg (zhou et al. 2015) y/n is 76.8. accuracy of ibowimg (zhou et al. 2015) no. is 35.0. accuracy of ibowimg (zhou et al. 2015) other is 42.6. accuracy of ibowimg (zhou et al. 2015) all is 55.9. accuracy of ibowimg (zhou et al. 2015) all is 62.0.
table 1 shows corpus size (length in token) and system performance by genre. news* used gold trees and is not included in total. sentences of news* sentences is 100. length of news* length is 19.3. yield of news* yield is 142. precision of news* precision is 78.9. sentences of news sentences is 100. length of news length is 19.3. yield of news yield is 144. precision of news precision is 70.8. sentences of wiki sentences is 100. length of wiki length is 21.4. yield of wiki yield is 178. precision of wiki precision is 61.8. sentences of web sentences is 100. length of web length is 19.2. yield of web yield is 165. precision of web precision is 49.1. sentences of total sentences is 300. length of total length is 20.0. yield of total yield is 487. precision of total precision is 60.2.
table 3 shows results of instance-wise evaluation for anaphoric and cataphoric sets recall of single-column cnn (w/ position vec.) recall is 0.445. precision of single-column cnn (w/ position vec.) precision is 0.525. f-score of single-column cnn (w/ position vec.) f-score is 0.481. avg.p of single-column cnn (w/ position vec.) avg.p is 0.341. recall of mcnn (base) recall is 0.591. precision of mcnn (base) precision is 0.33. f-score of mcnn (base) f-score is 0.424. avg.p of mcnn (base) avg.p is 0.367. recall of mcnn (base+surfseq) recall is 0.555. precision of mcnn (base+surfseq) precision is 0.566. f-score of mcnn (base+surfseq) f-score is 0.56. avg.p of mcnn (base+surfseq) avg.p is 0.565. recall of mcnn (base+deptree) recall is 0.389. precision of mcnn (base+deptree) precision is 0.615. f-score of mcnn (base+deptree) f-score is 0.476. avg.p of mcnn (base+deptree) avg.p is 0.518. recall of mcnn (base+surfseq+deptree) recall is 0.503. precision of mcnn (base+surfseq+deptree) precision is 0.66. f-score of mcnn (base+surfseq+deptree) f-score is 0.571. avg.p of mcnn (base+surfseq+deptree) avg.p is 0.599. recall of mcnn (base+surfseq+predcontext) recall is 0.535. precision of mcnn (base+surfseq+predcontext) precision is 0.611. f-score of mcnn (base+surfseq+predcontext) f-score is 0.57. avg.p of mcnn (base+surfseq+predcontext) avg.p is 0.581. recall of mcnn (base+deptree+predcontext) recall is 0.33. precision of mcnn (base+deptree+predcontext) precision is 0.699. f-score of mcnn (base+deptree+predcontext) f-score is 0.449. avg.p of mcnn (base+deptree+predcontext) avg.p is 0.528. recall of mcnn (proposed) recall is 0.492. precision of mcnn (proposed) precision is 0.673. f-score of mcnn (proposed) f-score is 0.569. avg.p of mcnn (proposed) avg.p is 0.602. recall of single-column cnn (w/ position vec.) recall is 0.163. precision of single-column cnn (w/ position vec.) precision is 0.293. f-score of single-column cnn (w/ position vec.) f-score is 0.209. avg.p of single-column cnn (w/ position vec.) avg.p is 0.163. recall of mcnn (base) recall is 0.171. precision of mcnn (base) precision is 0.13. f-score of mcnn (base) f-score is 0.148. avg.p of mcnn (base) avg.p is 0.099. recall of mcnn (base+surfseq) recall is 0.202. precision of mcnn (base+surfseq) precision is 0.417. f-score of mcnn (base+surfseq) f-score is 0.272. avg.p of mcnn (base+surfseq) avg.p is 0.257. recall of mcnn (base+deptree) recall is 0.268. precision of mcnn (base+deptree) precision is 0.438. f-score of mcnn (base+deptree) f-score is 0.332. avg.p of mcnn (base+deptree) avg.p is 0.329. recall of mcnn (base+surfseq+deptree) recall is 0.195. precision of mcnn (base+surfseq+deptree) precision is 0.525. f-score of mcnn (base+surfseq+deptree) f-score is 0.285. avg.p of mcnn (base+surfseq+deptree) avg.p is 0.33. recall of mcnn (base+surfseq+predcontext) recall is 0.258. precision of mcnn (base+surfseq+predcontext) precision is 0.406. f-score of mcnn (base+surfseq+predcontext) f-score is 0.316. avg.p of mcnn (base+surfseq+predcontext) avg.p is 0.276. recall of mcnn (base+deptree+predcontext) recall is 0.24. precision of mcnn (base+deptree+predcontext) precision is 0.488. f-score of mcnn (base+deptree+predcontext) f-score is 0.322. avg.p of mcnn (base+deptree+predcontext) avg.p is 0.341. recall of mcnn (proposed) recall is 0.251. precision of mcnn (proposed) precision is 0.522. f-score of mcnn (proposed) f-score is 0.339. avg.p of mcnn (proposed) avg.p is 0.337.
table 3 shows bilingual lexicon induction performance from es, it, nl to en. gouws and søgaard (2015) + panlex/wikt is our reimplementation using panlex/wiktionary dictionary. all our models use panlex as the dictionary. we reported the recall at 1 and 5. the best performance is bold. rec1 of gouws and søgaard (2015) + panlex rec1 is 37.6. rec5 of gouws and søgaard (2015) + panlex rec5 is 63.6. rec1 of gouws and søgaard (2015) + panlex rec1 is 26.6. rec5 of gouws and søgaard (2015) + panlex rec5 is 56.3. rec1 of gouws and søgaard (2015) + panlex rec1 is 49.8. rec5 of gouws and søgaard (2015) + panlex rec5 is 76.0. rec1 of gouws and søgaard (2015) + panlex rec1 is 38.0. rec5 of gouws and søgaard (2015) + panlex rec5 is 65.3. rec1 of gouws and søgaard (2015) + wikt rec1 is 61.6. rec5 of gouws and søgaard (2015) + wikt rec5 is 78.9. rec1 of gouws and søgaard (2015) + wikt rec1 is 62.6. rec5 of gouws and søgaard (2015) + wikt rec5 is 81.1. rec1 of gouws and søgaard (2015) + wikt rec1 is 65.6. rec5 of gouws and søgaard (2015) + wikt rec5 is 79.7. rec1 of gouws and søgaard (2015) + wikt rec1 is 63.3. rec5 of gouws and søgaard (2015) + wikt rec5 is 79.9. rec1 of bilbowa: gouws et al. (2015) rec1 is 51.6. rec1 of bilbowa: gouws et al. (2015) rec1 is 55.7. rec1 of bilbowa: gouws et al. (2015) rec1 is 57.5. rec1 of bilbowa: gouws et al. (2015) rec1 is 54.9. rec1 of vulic and moens (2015) rec1 is 68.9. rec1 of vulic and moens (2015) rec1 is 68.3. rec1 of vulic and moens (2015) rec1 is 39.2. rec1 of vulic and moens (2015) rec1 is 58.8. rec1 of our model (random selection) rec1 is 41.1. rec5 of our model (random selection) rec5 is 62.0. rec1 of our model (random selection) rec1 is 57.4. rec5 of our model (random selection) rec5 is 75.4. rec1 of our model (random selection) rec1 is 34.3. rec5 of our model (random selection) rec5 is 55.5. rec1 of our model (random selection) rec1 is 44.3. rec5 of our model (random selection) rec5 is 64.3. rec1 of our model (em selection) rec1 is 67.3. rec5 of our model (em selection) rec5 is 79.5. rec1 of our model (em selection) rec1 is 66.8. rec5 of our model (em selection) rec5 is 82.3. rec1 of our model (em selection) rec1 is 64.7. rec5 of our model (em selection) rec5 is 82.4. rec1 of our model (em selection) rec1 is 66.3. rec5 of our model (em selection) rec5 is 81.4. rec1 of our model (em selection) + joint model rec1 is 68.0. rec5 of our model (em selection) + joint model rec5 is 80.5. rec1 of our model (em selection) + joint model rec1 is 70.5. rec5 of our model (em selection) + joint model rec5 is 83.3. rec1 of our model (em selection) + joint model rec1 is 68.8. rec5 of our model (em selection) + joint model rec5 is 84.0. rec1 of our model (em selection) + joint model rec1 is 69.1. rec5 of our model (em selection) + joint model rec5 is 82.6. rec1 of our model (em selection) + combine embeddings (δ = 0.01) rec1 is 74.7. rec5 of our model (em selection) + combine embeddings (δ = 0.01) rec5 is 85.4. rec1 of our model (em selection) + combine embeddings (δ = 0.01) rec1 is 80.8. rec5 of our model (em selection) + combine embeddings (δ = 0.01) rec5 is 90.4. rec1 of our model (em selection) + combine embeddings (δ = 0.01) rec1 is 79.1. rec5 of our model (em selection) + combine embeddings (δ = 0.01) rec5 is 90.5. rec1 of our model (em selection) + combine embeddings (δ = 0.01) rec1 is 78.2. rec5 of our model (em selection) + combine embeddings (δ = 0.01) rec5 is 88.8. rec1 of our model (em selection) + lemmatization rec1 is 74.9. rec5 of our model (em selection) + lemmatization rec5 is 86.0. rec1 of our model (em selection) + lemmatization rec1 is 81.3. rec5 of our model (em selection) + lemmatization rec5 is 91.3. rec1 of our model (em selection) + lemmatization rec1 is 79.8. rec5 of our model (em selection) + lemmatization rec5 is 91.3. rec1 of our model (em selection) + lemmatization rec1 is 78.7. rec5 of our model (em selection) + lemmatization rec5 is 89.5.
table 1 shows meteor results for different configuration of our model on stardev, startest and cartoon datasets. meteor of - stardev is 31.82. meteor of - startest is 29.16. meteor of - cartoon is 32.08. meteor of -sem stardev is 28.72. meteor of -sem startest is 25.55. meteor of -sem cartoon is 27.55. meteor of -syn stardev is 31.92. meteor of -syn startest is 29.14. meteor of -syn cartoon is 32.04.
table 2 shows accuracy results on the test sets. bold font marks the best performance for a dataset. * indicates statistical significance of improvement over lasso at p < 0.05 using micro sign test for one of our models lsi, gow and word2vec (underlined). accuracy of science - is 0.946. accuracy of science - is 0.916. accuracy of science - is 0.954. accuracy of science - is 0.954. accuracy of science lda is 0.968. accuracy of science lsi is 0.968*. accuracy of science sentence is 0.942. accuracy of science gow is 0.967. accuracy of science word2vec is 0.968*. accuracy of sports - is 0.908. accuracy of sports - is 0.907. accuracy of sports - is 0.925. accuracy of sports - is 0.92. accuracy of sports lda is 0.959. accuracy of sports lsi is 0.964*. accuracy of sports sentence is 0.966. accuracy of sports gow is 0.959*. accuracy of sports word2vec is 0.946*. accuracy of religion - is 0.894. accuracy of religion - is 0.876. accuracy of religion - is 0.895. accuracy of religion - is 0.89. accuracy of religion lda is 0.918. accuracy of religion lsi is 0.907*. accuracy of religion sentence is 0.934. accuracy of religion gow is 0.911*. accuracy of religion word2vec is 0.916*. accuracy of computer - is 0.846. accuracy of computer - is 0.843. accuracy of computer - is 0.869. accuracy of computer - is 0.856. accuracy of computer lda is 0.891. accuracy of computer lsi is 0.885*. accuracy of computer sentence is 0.904. accuracy of computer gow is 0.885*. accuracy of computer word2vec is 0.911*. accuracy of vote - is 0.606. accuracy of vote - is 0.643. accuracy of vote - is 0.616. accuracy of vote - is 0.622. accuracy of vote lda is 0.658. accuracy of vote lsi is 0.653. accuracy of vote sentence is 0.656. accuracy of vote gow is 0.640. accuracy of vote word2vec is 0.651. accuracy of movie - is 0.865. accuracy of movie - is 0.86. accuracy of movie - is 0.87. accuracy of movie - is 0.875. accuracy of movie lda is 0.900. accuracy of movie lsi is 0.895. accuracy of movie sentence is 0.895. accuracy of movie gow is 0.895. accuracy of movie word2vec is 0.890. accuracy of books - is 0.75. accuracy of books - is 0.77. accuracy of books - is 0.76. accuracy of books - is 0.78. accuracy of books lda is 0.790. accuracy of books lsi is 0.795. accuracy of books sentence is 0.785. accuracy of books gow is 0.790. accuracy of books word2vec is 0.800. accuracy of dvd - is 0.765. accuracy of dvd - is 0.735. accuracy of dvd - is 0.77. accuracy of dvd - is 0.76. accuracy of dvd lda is 0.800. accuracy of dvd lsi is 0.805*. accuracy of dvd sentence is 0.785. accuracy of dvd gow is 0.795*. accuracy of dvd word2vec is 0.795*. accuracy of electr. - is 0.79. accuracy of electr. - is 0.8. accuracy of electr. - is 0.8. accuracy of electr. - is 0.825. accuracy of electr. lda is 0.800. accuracy of electr. lsi is 0.815. accuracy of electr. sentence is 0.805. accuracy of electr. gow is 0.820. accuracy of electr. word2vec is 0.815. accuracy of kitch. - is 0.76. accuracy of kitch. - is 0.8. accuracy of kitch. - is 0.775. accuracy of kitch. - is 0.8. accuracy of kitch. lda is 0.845. accuracy of kitch. lsi is 0.860*. accuracy of kitch. sentence is 0.855. accuracy of kitch. gow is 0.840. accuracy of kitch. word2vec is 0.855*.
table 1 shows accuracy results on wikipedia and stack exchange. accuracy of bag-of-words wiki is 80.9%. accuracy of bag-of-words se is 64.6%. accuracy of linguistic features wiki is 82.6%. accuracy of linguistic features se is 65.2%. accuracy of with discovered features wiki is 83.8%. accuracy of with discovered features se is 65.7%. accuracy of cnn wiki is 85.8%. accuracy of cnn se is 66.4%.
table 3 shows cross-validation performance on vuamc. b is always significantly different from n (p < .001), and b ∪ n is always significantly different from both b and n (p < .001). c of b c is 1.0. p of b p is 0.475. r of b r is 0.742. f of b f is 0.576. c of n c is 1.0. p of n p is 0.576. r of n r is 0.479. f of n f is 0.522. c of b ∪ n c is 1.0. p of b ∪ n p is 0.615. r of b ∪ n r is 0.539. f of b ∪ n f is 0.574. c of b c is 0.6. p of b p is 0.489. r of b r is 0.733. f of b f is 0.568. c of n c is 0.6. p of n p is 0.572. r of n r is 0.494. f of n f is 0.511. c of b ∪ n c is 1.0. p of b ∪ n p is 0.539. r of b ∪ n r is 0.648. f of b ∪ n f is 0.569. c of b c is 0.6. p of b p is 0.292. r of b r is 0.799. f of b f is 0.416. c of n c is 0.6. p of n p is 0.304. r of n r is 0.626. f of n f is 0.393. c of b ∪ n c is 1.0. p of b ∪ n p is 0.299. r of b ∪ n r is 0.731. f of b ∪ n f is 0.406. c of b c is 0.6. p of b p is 0.349. r of b r is 0.695. f of b f is 0.460. c of n c is 0.6. p of n p is 0.430. r of n r is 0.418. f of n f is 0.421. c of b ∪ n c is 0.6. p of b ∪ n p is 0.409. r of b ∪ n r is 0.551. f of b ∪ n f is 0.465.
table 5 shows comparison of various decoders using the same model from our full system (global a∗). we report f1 with and without the backoff model, the percentage of sentences that the decoder can parse, and the time spent decoding relative to a∗. dev f1 of global a* - backoff is 88.4. dev f1 of global a* backoff is 88.4 (99.8%). relative time of global a* - is 1x. dev f1 of best-first - backoff is 87.5. dev f1 of best-first backoff is 2.8 (6.7%). relative time of best-first - is 293.4x. dev f1 of 10-best reranking - backoff is 87.9. dev f1 of 10-best reranking backoff is 87.9 (99.7%). relative time of 10-best reranking - is 8.5x. dev f1 of 100-best reranking - backoff is 88.2. dev f1 of 100-best reranking backoff is 88.0 (99.4%). relative time of 100-best reranking - is 72.3x. dev f1 of 2-best beam search - backoff is 88.2. dev f1 of 2-best beam search backoff is 85.7 (94.0%). relative time of 2-best beam search - is 2.0x. dev f1 of 4-best beam search - backoff is 88.3. dev f1 of 4-best beam search backoff is 88.1 (99.2%). relative time of 4-best beam search - is 6.7x. dev f1 of 8-best beam search - backoff is 88.2. dev f1 of 8-best beam search backoff is 86.8 (98.1%). relative time of 8-best beam search - is 26.3x.
table 3 shows spe: prediction results auc of unigram tobacco is 0.663. auc of unigram alcohol is 0.672. auc of unigram drug is 0.644. auc of liwc tobacco is 0.731. auc of liwc alcohol is 0.689. auc of liwc drug is 0.758. auc of svd tobacco is 0.779. auc of svd alcohol is 0.724. auc of svd drug is 0.764. auc of userlda tobacco is 0.641. auc of userlda alcohol is 0.603. auc of userlda drug is 0.599. auc of postlda_word tobacco is 0.733. auc of postlda_word alcohol is 0.617. auc of postlda_word drug is 0.628. auc of postlda_doc tobacco is 0.768. auc of postlda_doc alcohol is 0.687. auc of postlda_doc drug is 0.721. auc of post-d-dm tobacco is 0.536. auc of post-d-dm alcohol is 0.622. auc of post-d-dm drug is 0.52. auc of user-d-dm tobacco is 0.775. auc of user-d-dm alcohol is 0.73. auc of user-d-dm drug is 0.767. auc of post-d-dbow tobacco is 0.531. auc of post-d-dbow alcohol is 0.606. auc of post-d-dbow drug is 0.526. auc of user-d-dbow tobacco is 0.802. auc of user-d-dbow alcohol is 0.768. auc of user-d-dbow drug is 0.819.
table 7 shows results obtained by the best model on each category of the test set for the factual vs opinion argument classification task p of fact p is 0.49. r of fact r is 0.5. f1 of fact f1 is 0.5. #argument per category of fact #argument per category is 138. p of opinion p is 0.88. r of opinion r is 0.87. f1 of opinion f1 is 0.88. #argument per category of opinion #argument per category is 575. p of avg/total p is 0.81. r of avg/total r is 0.79. f1 of avg/total f1 is 0.8. #argument per category of avg/total #argument per category is 713.
table 4 shows results for image-sentence ranking experiments on the coco dataset. r@k denotes recall@k (higher is better) and med r is the median rank (lower is better). (†) taken from kiros et al. (2015). (∗) taken from karpathy and fei-fei (2015). (‡) taken from mao et al. (2015). r@1 of uni-skip r@1 is 30.6. med r of uni-skip med r is 3. r@2 of uni-skip r@2 is 22.7. med r of uni-skip med r is 4. r@1 of bi-skip r@1 is 32.7. med r of bi-skip med r is 3. r@2 of bi-skip r@2 is 24.2. med r of bi-skip med r is 4. r@1 of combine-skip r@1 is 33.8. med r of combine-skip med r is 3. r@2 of combine-skip r@2 is 25.9. med r of combine-skip med r is 4. r@1 of hierarchical model+emb. r@1 is 32.7. med r of hierarchical model+emb. med r is 3. r@2 of hierarchical model+emb. r@2 is 25.3. med r of hierarchical model+emb. med r is 4. r@1 of composite model+emb. r@1 is 33.8. med r of composite model+emb. med r is 3. r@2 of composite model+emb. r@2 is 25.7. med r of composite model+emb. med r is 4. r@1 of combine+emb. r@1 is 34.4. med r of combine+emb. med r is 3. r@2 of combine+emb. r@2 is 26.6. med r of combine+emb. med r is 4. r@1 of dvsa* r@1 is 38.4. med r of dvsa* med r is 1. r@2 of dvsa* r@2 is 27.4. med r of dvsa* med r is 3. r@1 of m-rnn r@1 is 41. med r of m-rnn med r is 2. r@2 of m-rnn r@2 is 29. med r of m-rnn med r is 3.
table 4 shows cognate clustering results on the algonquian dataset (in %) with subsets of features. found sets of phonetic only found sets is 52.0 (36.3). purity of phonetic only purity is 70.2. found sets of + definitions found sets is 57.4 (41.7). purity of + definitions purity is 68.4. found sets of + wordnet found sets is 61.9 (46.9). purity of + wordnet purity is 68.1. found sets of + word vectors found sets is 66.2 (51.3). purity of + word vectors purity is 66.5.
table 1 shows results on nist chinese-to-english translation task. “*” indicates statistically significant better than “sennrich-deponly” at p-value < 0.05 and “**” at p-value < 0.01. avg = average bleu scores for test sets. bleu of pbsmt dev (mt02) is 33.15. bleu of pbsmt mt03 is 31.02. bleu of pbsmt mt04 is 33.78. bleu of pbsmt mt05 is 30.33. bleu of pbsmt mt06 is 29.62. bleu of pbsmt mt08 is 23.53. bleu of pbsmt avg is 29.66. bleu of attnmt dev (mt02) is 36.31. bleu of attnmt mt03 is 34.02. bleu of attnmt mt04 is 37.11. bleu of attnmt mt05 is 32.86. bleu of attnmt mt06 is 32.54. bleu of attnmt mt08 is 25.44. bleu of attnmt avg is 32.4. bleu of sennrich-deponly dev (mt02) is 36.68. bleu of sennrich-deponly mt03 is 34.51. bleu of sennrich-deponly mt04 is 38.09. bleu of sennrich-deponly mt05 is 33.37. bleu of sennrich-deponly mt06 is 32.96. bleu of sennrich-deponly mt08 is 26.96. bleu of sennrich-deponly avg is 32.98. bleu of sdrnmt-1 dev (mt02) is 36.88. bleu of sdrnmt-1 mt03 is 34.98*. bleu of sdrnmt-1 mt04 is 38.14. bleu of sdrnmt-1 mt05 is 34.61. bleu of sdrnmt-1 mt06 is 33.58. bleu of sdrnmt-1 mt08 is 27.06. bleu of sdrnmt-1 avg is 33.32. bleu of sdrnmt-2 dev (mt02) is 37.34. bleu of sdrnmt-2 mt03 is 35.91**. bleu of sdrnmt-2 mt04 is 38.73*. bleu of sdrnmt-2 mt05 is 34.18**. bleu of sdrnmt-2 mt06 is 33.76**. bleu of sdrnmt-2 mt08 is 27.64*. bleu of sdrnmt-2 avg is 34.04.
table 4 shows segmentation results. explicit bigrams are useful. accuracy of zhang et al. (2016) accuracy is 95.01. size of zhang et al. (2016) size is −. accuracy of zhang et al. (2016)-combo accuracy is 95.95. size of zhang et al. (2016)-combo size is −. accuracy of small ff 64 dim accuracy is 94.24. size of small ff 64 dim size is 846kb. accuracy of small ff 256 dim accuracy is 94.16. size of small ff 256 dim size is 3.2mb. accuracy of small ff 64 dim bigrams accuracy is 95.18. size of small ff 64 dim bigrams size is 2.0mb.
table 5 shows performance on monolingual word similarity computation with seed lexicon size 6000. accuracy of bilex ws-240 is 60.36. accuracy of bilex ws-297 is 62.17. accuracy of bilex ws-353 is 60.46. accuracy of bilex sl-999 is 27.22. accuracy of clsp-wr ws-240 is 61.27. accuracy of clsp-wr ws-297 is 65.25. accuracy of clsp-wr ws-353 is 60.46. accuracy of clsp-wr sl-999 is 27.22. accuracy of clsp-se ws-240 is 60.84. accuracy of clsp-se ws-297 is 65.62. accuracy of clsp-se ws-353 is 62.47. accuracy of clsp-se sl-999 is 28.79.
table 4 shows comparison with transformer on frenchenglish translation task. the evaluation metric is caseinsensitive bleu score. bleu of transformer dev is 29.42. bleu of transformer test is 35.15. bleu of this work dev is 30.40. bleu of this work test is 36.04.
table 2 shows performance of various models on narrativeqa dataset (ss=sample size ≡ number of relevant sentences) bleu-1 of seq2seq bleu-1 is 15.89. bleu-4 of seq2seq bleu-4 is 1.26. rouge-l of seq2seq rouge-l is 13.15. meteor of seq2seq meteor is 4.08. bleu-1 of asr bleu-1 is 23.20. bleu-4 of asr bleu-4 is 6.39. rouge-l of asr rouge-l is 22.26. meteor of asr meteor is 7.77. bleu-1 of bidaf bleu-1 is 33.72. bleu-4 of bidaf bleu-4 is 15.53. rouge-l of bidaf rouge-l is 36.30. meteor of bidaf meteor is 15.38. bleu-1 of mru (tay et al. 2018) bleu-1 is 36.55. bleu-4 of mru (tay et al. 2018) bleu-4 is 19.79. rouge-l of mru (tay et al. 2018) rouge-l is 41.44. meteor of mru (tay et al. 2018) meteor is 17.87. bleu-1 of baseline 1 (ss=5) bleu-1 is 30.22. bleu-4 of baseline 1 (ss=5) bleu-4 is 14.43. rouge-l of baseline 1 (ss=5) rouge-l is 34.40. meteor of baseline 1 (ss=5) meteor is 13.36. bleu-1 of baseline 2 (ss=5) bleu-1 is 39.35. bleu-4 of baseline 2 (ss=5) bleu-4 is 20.17. rouge-l of baseline 2 (ss=5) rouge-l is 43.36. meteor of baseline 2 (ss=5) meteor is 18.01. bleu-1 of conznet (ss=1) bleu-1 is 28.97. bleu-4 of conznet (ss=1) bleu-4 is 16.70. rouge-l of conznet (ss=1) rouge-l is 36.02. meteor of conznet (ss=1) meteor is 12.39. bleu-1 of conznet (ss=3) bleu-1 is 36.21. bleu-4 of conznet (ss=3) bleu-4 is 19.33. rouge-l of conznet (ss=3) rouge-l is 41.23. meteor of conznet (ss=3) meteor is 17.77. bleu-1 of conznet (ss=5) bleu-1 is 42.76. bleu-4 of conznet (ss=5) bleu-4 is 22.49. rouge-l of conznet (ss=5) rouge-l is 46.67. meteor of conznet (ss=5) meteor is 19.24. bleu-1 of conznet (ss=7) bleu-1 is 40.80. bleu-4 of conznet (ss=7) bleu-4 is 21.14. rouge-l of conznet (ss=7) rouge-l is 44.01. meteor of conznet (ss=7) meteor is 18.67.
table 3 shows published results of other models on the semeval2012 task 2 dataset. accuracy of rink and harabagiu (2012)  accuracy is 0.394. correlation of rink and harabagiu (2012) correlation is 0.229. accuracy of mikolov et al. (2013b)  accuracy is 0.418. correlation of mikolov et al. (2013b) correlation is 0.275. accuracy of levy and goldberg (2014)  accuracy is 0.452. accuracy of zhila et al. (2013)  accuracy is 0.452. correlation of zhila et al. (2013) correlation is 0.353. accuracy of iacobacci et al. (2015)  accuracy is 0.459. correlation of iacobacci et al. (2015) correlation is 0.358. accuracy of turney (2013)  accuracy is 0.472. correlation of turney (2013) correlation is 0.408. accuracy of vecoff  accuracy is 0.443. correlation of vecoff correlation is 0.321. accuracy of lra  accuracy is 0.415. correlation of lra correlation is 0.264. accuracy of nlra  accuracy is 0.453. correlation of nlra correlation is 0.36. accuracy of nlra+vecoff  accuracy is 0.475. correlation of nlra+vecoff correlation is 0.391.
table 1 shows automatic evaluation ppl-v of s2sa ppl-v is 8.41. ppl-t of s2sa ppl-t is 9.05. distinct-1 of s2sa distinct-1 is 0.0809. distinct-2 of s2sa distinct-2 is 0.2110. ppl-v of tas2s ppl-v is 7.38. ppl-t of tas2s ppl-t is 7.84. distinct-1 of tas2s distinct-1 is 0.04759. distinct-2 of tas2s distinct-2 is 0.1087. ppl-v of spmn500 ppl-v is 7.04. ppl-t of spmn500 ppl-t is 7.93. distinct-1 of spmn500 distinct-1 is 0.06430. distinct-2 of spmn500 distinct-2 is 0.1734. ppl-v of spmn1000 ppl-v is 6.28. ppl-t of spmn1000 ppl-t is 7.72. distinct-1 of spmn1000 distinct-1 is 0.07347. distinct-2 of spmn1000 distinct-2 is 0.1909. ppl-v of dpmn100 ppl-v is 6.45. ppl-t of dpmn100 ppl-t is 7.69. distinct-1 of dpmn100 distinct-1 is 0.04350. distinct-2 of dpmn100 distinct-2 is 0.1048.
table 4 shows unsupervised pos tagging results on wsj, with fasttext vectors as the observed embeddings. m-1 of gaussian hmm m-1 is 72.0. vm of gaussian hmm vm is 65.0. m-1 of ours (4 layers) m-1 is 76.4. vm of ours (4 layers) vm is 69.3. m-1 of ours (8 layers) m-1 is 76.8. vm of ours (8 layers) vm is 69.4. m-1 of ours (16 layers) m-1 is 67.3. vm of ours (16 layers) vm is 62.0.
table 2 shows results for simple and neural models on the test set of recipeqa dataset. accuracy of hasty student visual cloze is 27.35. accuracy of hasty student textual cloze is 26.89. accuracy of hasty student visual coherence is 65.80. accuracy of hasty student visual ordering is 40.88. accuracy of impatient reader (text only) textual cloze is 28.03. accuracy of impatient reader (multimodal) visual cloze is 27.36. accuracy of impatient reader (multimodal) textual cloze is 29.07. accuracy of impatient reader (multimodal) visual coherence is 28.08. accuracy of impatient reader (multimodal) visual ordering is 26.74.
table 1 shows accuracy scores on the stanford natural language inference (snli) and multinli mismatched (mnli) tasks. dme=dynamic meta-embeddings; cdme=contextualized dynamic meta-embeddings; *=multiple different embedding sets (see section 4). number of parameters included in parenthesis. results averaged over five runs with different random seeds, using a bilstm-max sentence encoder. accuracy of infersent (conneau et al., 2017) snli is 84.5. accuracy of nse (munkhdalai and yu, 2017) snli is 84.6. accuracy of g-treelstm (choi et al., 2017) snli is 86.0. accuracy of sse (nie and bansal, 2017) snli is 86.1. accuracy of sse (nie and bansal, 2017) mnli is 73.6. accuracy of resan (shen et al., 2018) snli is 86.3. accuracy of glove bilstm-max (8.6m) snli is 85.2}.3. accuracy of glove bilstm-max (8.6m) mnli is 70.0}.5. accuracy of fasttext bilstm-max (8.6m) snli is 85.2}.2. accuracy of fasttext bilstm-max (8.6m) mnli is 70.3}.3. accuracy of naive baseline (9.8m) snli is 85.6}.3. accuracy of naive baseline (9.8m) mnli is 71.1}.2. accuracy of naive baseline (61.3m) snli is 86.0}.5. accuracy of naive baseline (61.3m) mnli is 73.0}.2. accuracy of unweighted dme (8.6m) snli is 86.3}.4. accuracy of unweighted dme (8.6m) mnli is 74.4}.2. accuracy of dme (8.6m) snli is 86.2}.2. accuracy of dme (8.6m) mnli is 74.4}.2. accuracy of cdme (8.6m) snli is 86.4}.3. accuracy of cdme (8.6m) mnli is 74.1}.2. accuracy of dme* (9.0m) snli is 86.7}.2. accuracy of dme* (9.0m) mnli is 74.3}.4. accuracy of cdme* (9.0m) snli is 86.5}.2. accuracy of cdme* (9.0m) mnli is 74.9}.5.
table 2 shows performance of different approaches on the wikisql dataset. the two evaluation metrics are logical form accuracy (acclf ) and execution accuracy (accex). the settings of the training data represent the proportion of supervised data we use. acclf of 100% acclf is 23.3%. accex of 100% accex is 37.0%. acclf of 100% acclf is 23.4%. accex of 100% accex is 35.9%. acclf of 100% acclf is 44.1%. accex of 100% accex is 53.8%. acclf of 100% acclf is 43.3%. accex of 100% accex is 53.3%. acclf of 100% acclf is 51.5%. accex of 100% accex is 58.9%. acclf of 100% acclf is 52.1%. accex of 100% accex is 59.2%. acclf of 100% acclf is 49.5%. accex of 100% accex is 60.8%. acclf of 100% acclf is 48.3%. accex of 100% accex is 59.4%. accex of 100% accex is 69.8%. accex of 100% accex is 68.0%. acclf of 30% acclf is 54.6%. accex of 30% accex is 69.7%. acclf of 30% acclf is 53.7%. accex of 30% accex is 68.9%. acclf of 30% acclf is 61.6%. accex of 30% accex is 74.4%. acclf of 30% acclf is 61.2%. accex of 30% accex is 73.9%. acclf of 100% acclf is 61.5%. accex of 100% accex is 74.8%. acclf of 100% acclf is 60.7%. accex of 100% accex is 74.4%. acclf of 100% acclf is 64.3%. accex of 100% accex is 76.5%. acclf of 100% acclf is 63.7%. accex of 100% accex is 75.5%.
table 5 shows results of using np head alone for bridging anaphora resolution based on different word representation resources. bold indicates statistically significant differences over the baselines (two-sided paired approximate randomization test, p < 0.01). acc of glove gigawiki14 acc is 21.42. acc of glove giga acc is 21.87. acc of embeddings pp acc is 33.03. acc of embeddings bridging acc is 34.84.
table 5 shows comparison of our syn-gcn model with (marcheggiani and titov, 2017), (he et al., 2018) and (cai et al., 2018) on the english test set. ∆ f1 shows the absolute performance gap between syntax-agnostic and syntax-aware settings. f1 of m&t (2017) syntax-agnostic is 87.7. f1 of m&t (2017) syntax-aware is 88.0. f1 of m&t (2017) δf1 is 0.3. f1 of he et al. (2018) syntax-agnostic is 88.7. f1 of he et al. (2018) syntax-aware is 89.5. f1 of he et al. (2018) δf1 is 0.8. f1 of cai et al. (2018) syntax-agnostic is 89.6. f1 of cai et al. (2018) syntax-aware is 89.6. f1 of cai et al. (2018) δf1 is ≈0.0. f1 of our model syntax-agnostic is 88.7. f1 of our model syntax-aware is 89.8. f1 of our model δf1 is 1.1.
table 6 shows human evaluation results. s-ocr refer to 150 sentences from the d-ocr and e-ocr represents the errors in s-ocr, similar for s-asr and easr. r denotes the average recall of three students. numbers in bold denotes the correctly-annotated results by students. correctly-annotated number of stu1 of s-ocr correctly-annotated number of stu1 is 84/150. correctly-annotated number of stu2 of s-ocr correctly-annotated number of stu2 is 100/150. correctly-annotated number of stu3 of s-ocr correctly-annotated number of stu3 is 75/150. r of s-ocr r is 57.3. correctly-annotated number of stu1 of e-ocr correctly-annotated number of stu1 is 104/170. correctly-annotated number of stu2 of e-ocr correctly-annotated number of stu2 is 121/170. correctly-annotated number of stu3 of e-ocr correctly-annotated number of stu3 is 100/170. r of e-ocr r is 72.0. correctly-annotated number of stu1 of s-asr correctly-annotated number of stu1 is 95/150. correctly-annotated number of stu2 of s-asr correctly-annotated number of stu2 is 79/150. correctly-annotated number of stu3 of s-asr correctly-annotated number of stu3 is 106/150. r of s-asr r is 62.0. correctly-annotated number of stu1 of e-asr correctly-annotated number of stu1 is 341/393. correctly-annotated number of stu2 of e-asr correctly-annotated number of stu2 is 179/393. correctly-annotated number of stu3 of e-asr correctly-annotated number of stu3 is 356/393. r of e-asr r is 74.3.
table 2 shows performance of clique-ebm with pruned edges in g. p of 5 p is 90.46. r of 5 r is 92.27. f of 5 f is 91.36. p of 5 p is 83.52. r of 5 r is 80.48. f of 5 f is 81.97. p of 10 p is 92.92. r of 10 r is 95.07. f of 10 f is 93.98. p of 10 p is 85.32. r of 10 r is 84.4. f of 10 f is 84.86. p of 15 p is 94.85. r of 15 r is 96.14. f of 15 f is 95.49. p of 15 p is 87.67. r of 15 r is 86.38. f of 15 f is 87.02. p of 20 p is 95.23. r of 20 r is 96.49. f of 20 f is 95.86. p of 20 p is 89.25. r of 20 r is 88.62. f of 20 f is 88.93.
table 2 shows comparison of various bounding box regressors on flickr30k entities for different iou thresholds. the number of parameters in gr is also shown. accuracy of w/o regression 0.5 is 65.21. accuracy of w/o regression 0.6 is 53.19. accuracy of w/o regression 0.7 is 35.70. accuracy of w/o regression 0.8 is 14.32. accuracy of w/o regression 0.9 is 1.88. # params of 300-16(-4096) - is 0.3m. accuracy of 300-16(-4096) 0.5 is 64.14. accuracy of 300-16(-4096) 0.6 is 57.66. accuracy of 300-16(-4096) 0.7 is 48.22. accuracy of 300-16(-4096) 0.8 is 33.04. accuracy of 300-16(-4096) 0.9 is 9.29. # params of 300-64(-4096) - is 1.1m. accuracy of 300-64(-4096) 0.5 is 63.87. accuracy of 300-64(-4096) 0.6 is 57.43. accuracy of 300-64(-4096) 0.7 is 49.05. accuracy of 300-64(-4096) 0.8 is 33.84. accuracy of 300-64(-4096) 0.9 is 10.55. # params of 300-256(-4096) - is 4.3m. accuracy of 300-256(-4096) 0.5 is 63.84. accuracy of 300-256(-4096) 0.6 is 57.70. accuracy of 300-256(-4096) 0.7 is 48.71. accuracy of 300-256(-4096) 0.8 is 33.87. accuracy of 300-256(-4096) 0.9 is 10.05. # params of 300-1024(-4096) - is 17m. accuracy of 300-1024(-4096) 0.5 is 64.29. accuracy of 300-1024(-4096) 0.6 is 58.05. accuracy of 300-1024(-4096) 0.7 is 48.49. accuracy of 300-1024(-4096) 0.8 is 33.94. accuracy of 300-1024(-4096) 0.9 is 10.09. # params of 300(-256-4096) - is 4.5m. accuracy of 300(-256-4096) 0.5 is 62.82. accuracy of 300(-256-4096) 0.6 is 56.28. accuracy of 300(-256-4096) 0.7 is 48.02. accuracy of 300(-256-4096) 0.8 is 32.71. accuracy of 300(-256-4096) 0.9 is 9.89. # params of 300-4096 - is 1.2m. accuracy of 300-4096 0.5 is 63.23. accuracy of 300-4096 0.6 is 56.92. accuracy of 300-4096 0.7 is 48.17. accuracy of 300-4096 0.8 is 32.66. accuracy of 300-4096 0.9 is 9.20.
table 1 shows single-label classification (accuracy) results. accuracy of linear svm commenting is 42.2. accuracy of linear svm ogling is 35.0. accuracy of linear svm groping is 55.8. accuracy of gaussian nb commenting is 46.8. accuracy of gaussian nb ogling is 74.7. accuracy of gaussian nb groping is 66.0. accuracy of logistic reg. commenting is 61.4. accuracy of logistic reg. ogling is 78.0. accuracy of logistic reg. groping is 69.1. accuracy of svm commenting is 65.5. accuracy of svm ogling is 79.0. accuracy of svm groping is 70.3. accuracy of cnn commenting is 80.9. accuracy of cnn ogling is 82.2. accuracy of cnn groping is 86.0. accuracy of rnn commenting is 81.0. accuracy of rnn ogling is 82.2. accuracy of rnn groping is 86.2. accuracy of cnn-rnn commenting is 81.6. accuracy of cnn-rnn ogling is 84.1. accuracy of cnn-rnn groping is 86.5.
table 6 shows results (presented in percentage) in terms of precision (p), recall (r) and f-measure (f1) on the test set for each label obtained by our hsln-rnn model on the pubmed 20k dataset. p of background p is 78.5. r of background r is 80.0. f1 of background f1 is 79.2. support of background support is 3077. p of objectives p is 74.2. r of objectives r is 69.9. f1 of objectives f1 is 72.0. support of objectives support is 2333. p of methods p is 95.0. r of methods r is 97.7. f1 of methods f1 is 96.3. support of methods support is 9884. p of results p is 96.8. r of results r is 95.3. f1 of results f1 is 96.0. support of results support is 9713. p of conclusions p is 97.6. r of conclusions r is 96.5. f1 of conclusions f1 is 97.1. support of conclusions support is 4571. p of total p is 92.6. r of total r is 92.7. f1 of total f1 is 92.6. support of total support is 29578.
table 4 shows cv coherence scores for topics generated by various models. higher is better. the best result in each column is in bold. cv of lda snippets is 0.436. cv of lda tagmynews is 0.449. cv of lda twitter is 0.436. cv of btm snippets is 0.435. cv of btm tagmynews is 0.463. cv of btm twitter is 0.435. cv of ntm snippets is 0.463. cv of ntm tagmynews is 0.468. cv of ntm twitter is 0.463. cv of tmn snippets is 0.487. cv of tmn tagmynews is 0.499. cv of tmn twitter is 0.468.
table 3 shows comparison of dev set mrr of ours(conve) and models without reward shaping and action dropout. mrr of ours(conve) umls is 73.0. mrr of ours(conve) kinship is 75.0. mrr of ours(conve) fb15k237 is 38.2. mrr of ours(conve) wn18rr is 43.8. mrr of ours(conve) nell995 is 78.8. mrr of -rs umls is 67.7. mrr of -rs kinship is 66.5. mrr of -rs fb15k237 is 35.1. mrr of -rs wn18rr is 45.7. mrr of -rs nell995 is 78.4. mrr of -ad umls is 61.3. mrr of -ad kinship is 65.4. mrr of -ad fb15k237 is 31.0. mrr of -ad wn18rr is 39.1. mrr of -ad nell995 is 76.1.
table 3 shows med accuracy on five randomly selected languages with 1, 2, and 4 sources and combined with paradigm transduction (“+pt”). best results in bold. accuracy of dutch 1 is .00. accuracy of dutch 2 is .00. accuracy of dutch 4 is .00. accuracy of dutch +pt is .49. accuracy of dutch 1 is .04. accuracy of dutch 2 is .01. accuracy of dutch 4 is .00. accuracy of dutch +pt is .78. accuracy of dutch 1 is .43. accuracy of dutch 2 is .65. accuracy of dutch 4 is .72. accuracy of dutch +pt is .87. accuracy of german 1 is .00. accuracy of german 2 is .00. accuracy of german 4 is .00. accuracy of german +pt is .65. accuracy of german 1 is .00. accuracy of german 2 is .00. accuracy of german 4 is .01. accuracy of german +pt is .75. accuracy of german 1 is .44. accuracy of german 2 is .42. accuracy of german 4 is .59. accuracy of german +pt is .88. accuracy of icelandic 1 is .00. accuracy of icelandic 2 is .00. accuracy of icelandic 4 is .00. accuracy of icelandic +pt is .41. accuracy of icelandic 1 is .03. accuracy of icelandic 2 is .02. accuracy of icelandic 4 is .02. accuracy of icelandic +pt is .50. accuracy of icelandic 1 is .24. accuracy of icelandic 2 is .33. accuracy of icelandic 4 is .35. accuracy of icelandic +pt is .77. accuracy of spanish 1 is .00. accuracy of spanish 2 is .00. accuracy of spanish 4 is .00. accuracy of spanish +pt is .92. accuracy of spanish 1 is .03. accuracy of spanish 2 is .09. accuracy of spanish 4 is .09. accuracy of spanish +pt is .98. accuracy of spanish 1 is .59. accuracy of spanish 2 is .63. accuracy of spanish 4 is .83. accuracy of spanish +pt is .99. accuracy of welsh 1 is .00. accuracy of welsh 2 is .00. accuracy of welsh 4 is .00. accuracy of welsh +pt is .91. accuracy of welsh 1 is .05. accuracy of welsh 2 is .14. accuracy of welsh 4 is .15. accuracy of welsh +pt is .97. accuracy of welsh 1 is .35. accuracy of welsh 2 is .53. accuracy of welsh 4 is .70. accuracy of welsh +pt is .99.
table 2 shows situation entity type classification results on the training set of masc+wiki with 10-fold cross-validation. we report accuracy (acc), macro-average f1-score (macro) and class-wise f1 scores for state (sta), event (eve), report (rep), generic (geni), generalizing (gena), question (que) and imperative (imp). macro of humans macro is 78.6. acc of humans acc is 79.6. f1 sta of humans f1 sta is 82.8. f1 eve of humans f1 eve is 80.5. f1 rep of humans f1 rep is 81.5. f1 geni of humans f1 geni is 75.1. f1 gena of humans f1 gena is 45.8. f1 que of humans f1 que is 90.7. f1 imp of humans f1 imp is 93.6. macro of crf (friedrich et al. 2016) macro is 71.2. acc of crf (friedrich et al. 2016) acc is 76.4. f1 sta of crf (friedrich et al. 2016) f1 sta is 80.6. f1 eve of crf (friedrich et al. 2016) f1 eve is 78.6. f1 rep of crf (friedrich et al. 2016) f1 rep is 78.9. f1 geni of crf (friedrich et al. 2016) f1 geni is 68.3. f1 gena of crf (friedrich et al. 2016) f1 gena is 29.4. f1 que of crf (friedrich et al. 2016) f1 que is 84.4. f1 imp of crf (friedrich et al. 2016) f1 imp is 75.3. macro of clause-level bi-lstm macro is 74.4. acc of clause-level bi-lstm acc is 78.3. f1 sta of clause-level bi-lstm f1 sta is 82.6. f1 eve of clause-level bi-lstm f1 eve is 81.3. f1 rep of clause-level bi-lstm f1 rep is 84.9. f1 geni of clause-level bi-lstm f1 geni is 66.2. f1 gena of clause-level bi-lstm f1 gena is 36.1. f1 que of clause-level bi-lstm f1 que is 88.5. f1 imp of clause-level bi-lstm f1 imp is 80.9. macro of paragraph-level model macro is 77.6. acc of paragraph-level model acc is 81.2. f1 sta of paragraph-level model f1 sta is 84.3. f1 eve of paragraph-level model f1 eve is 82.1. f1 rep of paragraph-level model f1 rep is 85.3. f1 geni of paragraph-level model f1 geni is 76.4. f1 gena of paragraph-level model f1 gena is 43.2. f1 que of paragraph-level model f1 que is 90.8. f1 imp of paragraph-level model f1 imp is 81.2. macro of paragraph-level model+crf macro is 77.8. acc of paragraph-level model+crf acc is 81.3. f1 sta of paragraph-level model+crf f1 sta is 84.3. f1 eve of paragraph-level model+crf f1 eve is 82.0. f1 rep of paragraph-level model+crf f1 rep is 85.7. f1 geni of paragraph-level model+crf f1 geni is 77.0. f1 gena of paragraph-level model+crf f1 gena is 43.5. f1 que of paragraph-level model+crf f1 que is 90.4. f1 imp of paragraph-level model+crf f1 imp is 81.5.
table 2 shows narrative reconstruction results in nyt dataset and post grouping results in wikipedia conversation dataset. accuracy of lda + dbscan ami is 0.0627. accuracy of lda + dbscan ari is 0.0117. accuracy of hdp + dbscan ami is 0.0260. accuracy of hdp + dbscan ari is 0.0203. accuracy of hdhp ami is 0.1768. accuracy of hdhp ari is 0.0746. accuracy of hd-gmhp (100d) ami is 0.2479. accuracy of hd-gmhp (100d) ari is 0.1416. accuracy of w2v + dbscan ami is 0.0055. accuracy of w2v + dbscan ari is 0.0001. accuracy of hdhp ami is 0.4240. accuracy of hdhp ari is 0.3512. accuracy of hd-gmhp (100d) ami is 0.5848. accuracy of hd-gmhp (100d) ari is 0.3834.
table 2 shows experimental results on sem2016 with two training settings train and train-all. acc of nbow-mlp acc is 54.31. f1 of nbow-mlp f1 is 52.90. acc of nbow-mlp acc is 79.69. f1 of nbow-mlp f1 is 77.33. acc of nbow-mlp acc is 61.09. f1 of nbow-mlp f1 is 55.91. acc of nbow-mlp acc is 84.90. f1 of nbow-mlp f1 is 82.01. acc of nbow-mlp acc is 70.00. f1 of nbow-mlp f1 is 67.04. acc of cnn acc is 54.67. f1 of cnn f1 is 52.17. acc of cnn acc is 79.79. f1 of cnn f1 is 75.28. acc of cnn acc is 62.68. f1 of cnn f1 is 57.71. acc of cnn acc is 84.10. f1 of cnn f1 is 81.31. acc of cnn acc is 70.31. f1 of cnn f1 is 66.62. acc of bilstm acc is 55.57. f1 of bilstm f1 is 52.33. acc of bilstm acc is 81.90. f1 of bilstm f1 is 77.12. acc of bilstm acc is 63.26. f1 of bilstm f1 is 60.31. acc of bilstm acc is 85.89. f1 of bilstm f1 is 84.14. acc of bilstm acc is 71.66. f1 of bilstm f1 is 68.50. acc of at-bilstm acc is 56.95. f1 of at-bilstm f1 is 54.53. acc of at-bilstm acc is 80.09. f1 of at-bilstm f1 is 73.93. acc of at-bilstm acc is 64.20. f1 of at-bilstm f1 is 61.64. acc of at-bilstm acc is 86.77. f1 of at-bilstm f1 is 83.67. acc of at-bilstm acc is 72.00. f1 of at-bilstm f1 is 68.44. acc of lexicon rnn acc is 51.02. f1 of lexicon rnn f1 is 50.45. acc of lexicon rnn acc is 81.72. f1 of lexicon rnn f1 is 79.00. acc of lexicon rnn acc is 61.41. f1 of lexicon rnn f1 is 60.50. acc of lexicon rnn acc is 86.68. f1 of lexicon rnn f1 is 83.82. acc of lexicon rnn acc is 70.21. f1 of lexicon rnn f1 is 68.44. acc of aglr acc is 59.01. f1 of aglr f1 is 56.67. acc of aglr acc is 82.79. f1 of aglr f1 is 80.10. acc of aglr acc is 66.62. f1 of aglr f1 is 64.36. acc of aglr acc is 87.16. f1 of aglr f1 is 84.98. acc of aglr acc is 73.90. f1 of aglr f1 is 71.53.
table 4 shows bleu scores on en-jp test set. ”0” represents the normal translation results, and ”1” represents the teacher-forcing translation results. bleu of left 0 is 7.90. bleu of left 1 is 9.91. bleu of left 0 is 7.45. bleu of left 1 is 8.95. bleu of right 0 is 8.70. bleu of right 1 is 11.52. bleu of right 0 is 9.24. bleu of right 1 is 10.59.
table 3 shows translation results on the ikea dataset bleu of liumcvc-multi bleu is 59.9 ± 1.9. meteor of liumcvc-multi meteor is 63.8 ± 0.4. bleu of liumcvc-multi bleu is 58.4 ± 1.6. meteor of liumcvc-multi meteor is 64.6 ± 1.8. bleu of text-only nmt bleu is 61.9 ± 0.9. meteor of text-only nmt meteor is 65.6 ± 0.9. bleu of text-only nmt bleu is 65.2 ± 0.7. meteor of text-only nmt meteor is 69.0 ± 0.2. bleu of vag-nmt bleu is 63.5 ± 1.2. meteor of vag-nmt meteor is 65.7 ± 0.1. bleu of vag-nmt bleu is 65.8 ± 1.2. meteor of vag-nmt meteor is 68.9 ± 1.4.
table 2 shows performance of our approaches to qa-style sentiment classification in all domains. macro-f1 of svm macro-f1 is 0.362. accuracy of svm accuracy is 0.684. macro-f1 of svm macro-f1 is 0.381. accuracy of svm accuracy is 0.718. macro-f1 of svm macro-f1 is 0.435. accuracy of svm accuracy is 0.691. macro-f1 of lstm macro-f1 is 0.499. accuracy of lstm accuracy is 0.712. macro-f1 of lstm macro-f1 is 0.520. accuracy of lstm accuracy is 0.754. macro-f1 of lstm macro-f1 is 0.562. accuracy of lstm accuracy is 0.715. macro-f1 of bi-lstm macro-f1 is 0.527. accuracy of bi-lstm accuracy is 0.719. macro-f1 of bi-lstm macro-f1 is 0.531. accuracy of bi-lstm accuracy is 0.759. macro-f1 of bi-lstm macro-f1 is 0.574. accuracy of bi-lstm accuracy is 0.723. macro-f1 of bidirectional-match macro-f1 is 0.526. accuracy of bidirectional-match accuracy is 0.747. macro-f1 of bidirectional-match macro-f1 is 0.557. accuracy of bidirectional-match accuracy is 0.796. macro-f1 of bidirectional-match macro-f1 is 0.582. accuracy of bidirectional-match accuracy is 0.741. macro-f1 of atoq-match macro-f1 is 0.543. accuracy of atoq-match accuracy is 0.745. macro-f1 of atoq-match macro-f1 is 0.602. accuracy of atoq-match accuracy is 0.792. macro-f1 of atoq-match macro-f1 is 0.567. accuracy of atoq-match accuracy is 0.754. macro-f1 of qtoa-match macro-f1 is 0.573. accuracy of qtoa-match accuracy is 0.751. macro-f1 of qtoa-match macro-f1 is 0.647. accuracy of qtoa-match accuracy is 0.807. macro-f1 of qtoa-match macro-f1 is 0.608. accuracy of qtoa-match accuracy is 0.752. macro-f1 of bidirectional-match qa macro-f1 is 0.583. accuracy of bidirectional-match qa accuracy is 0.760. macro-f1 of bidirectional-match qa macro-f1 is 0.666. accuracy of bidirectional-match qa accuracy is 0.815. macro-f1 of bidirectional-match qa macro-f1 is 0.617. accuracy of bidirectional-match qa accuracy is 0.764. macro-f1 of hmn macro-f1 is 0.598. accuracy of hmn accuracy is 0.776. macro-f1 of hmn macro-f1 is 0.683. accuracy of hmn accuracy is 0.827. macro-f1 of hmn macro-f1 is 0.640. accuracy of hmn accuracy is 0.779.
table 2 shows performance on snli and transfer tasks of various sentence encoders. test accuracies on snli, micro and macro averages of accuracies of dev set on transfer tasks are chosen as evaluation metrics. accuracy of bilstm-max snli is 84.5. accuracy of bilstm-max micro is 85.2. accuracy of bilstm-max macro is 83.7. accuracy of adasent snli is 83.4. accuracy of adasent micro is 82.0. accuracy of adasent macro is 80.9. accuracy of tbcnn snli is 82.1. accuracy of tbcnn micro is 81.1. accuracy of tbcnn macro is 79.3. accuracy of disan snli is 85.6. accuracy of disan micro is 84.7. accuracy of disan macro is 83.4. accuracy of psan snli is 86.1. accuracy of psan micro is 85.7. accuracy of psan macro is 84.5.
table 3 shows transfer test results for our model and various baselines. classification accuracy is chosen as evaluation metric for datasets including mr, cr, subj, mpqa, sst, trec and sick-e. accuracy of bilstm-max mr is 79.9. accuracy of bilstm-max cr is 84.6. accuracy of bilstm-max subj is 92.1. accuracy of bilstm-max mpqa is 89.8. accuracy of bilstm-max sst is 83.3. accuracy of bilstm-max trec is 88.7. accuracy of adasent mr is 77.0. accuracy of adasent cr is 82.0. accuracy of adasent subj is 89.9. accuracy of adasent mpqa is 87.2. accuracy of adasent sst is 82.3. accuracy of adasent trec is 85.6. accuracy of tbcnn mr is 75.4. accuracy of tbcnn cr is 81.6. accuracy of tbcnn subj is 89.1. accuracy of tbcnn mpqa is 85.9. accuracy of tbcnn sst is 79.4. accuracy of tbcnn trec is 83.7. accuracy of disan mr is 79.7. accuracy of disan cr is 84.1. accuracy of disan subj is 92.2. accuracy of disan mpqa is 89.5. accuracy of disan sst is 82.9. accuracy of disan trec is 88.3. accuracy of psan mr is 80.0. accuracy of psan cr is 84.2. accuracy of psan subj is 91.9. accuracy of psan mpqa is 89.9. accuracy of psan sst is 83.8. accuracy of psan trec is 89.1.
table 2 shows results of independent training for intent detection in terms of error rate. error(%) of recursive nn (guo et al. 2014) error(%) is 4.60. error(%) of boosting (tur et al. 2010) error(%) is 4.38. error(%) of boosting + simplified sentences(tur et al. 2011) error(%) is 3.02. error(%) of attention enc-dec (liu and lane 2016a) error(%) is 2.02. error(%) of our model error(%) is 2.69.
table 3 shows results of joint training for slot filling and intent detection. f1 of recursive nn (guo et al.2014) f1 is 93.22. error(%) of recursive nn (guo et al.2014) error(%) is 4.60. f1 of recursive nn+viterbi(guo et al. 2014) f1 is 93.96. error(%) of recursive nn+viterbi(guo et al. 2014) error(%) is 4.60. f1 of attention enc-dec (liu and lane 2016a) f1 is 95.87. error(%) of attention enc-dec (liu and lane 2016a) error(%) is 1.57. f1 of attention birnn (liu and lane 2016a) f1 is 95.98. error(%) of attention birnn (liu and lane 2016a) error(%) is 1.79. f1 of our model f1 is 96.52. error(%) of our model error(%) is 1.23.
table 3 shows performances on twitter corpus. rouge-1 of seq2seq rouge-1 is 30.43. rouge-2 of seq2seq rouge-2 is 14.61. bleu of seq2seq bleu is 30.54. meteor of seq2seq meteor is 12.80. rouge-1 of residual lstm rouge-1 is 32.50. rouge-2 of residual lstm rouge-2 is 16.86. bleu of residual lstm bleu is 33.90. meteor of residual lstm meteor is 13.65. rouge-1 of pointer-generator rouge-1 is 38.31. rouge-2 of pointer-generator rouge-2 is 21.22. bleu of pointer-generator bleu is 40.37. meteor of pointer-generator meteor is 17.62. rouge-1 of rl-rouge rouge-1 is 40.16. rouge-2 of rl-rouge rouge-2 is 22.99. bleu of rl-rouge bleu is 42.73. meteor of rl-rouge meteor is 18.89. rouge-1 of rbm-sl (ours) rouge-1 is 41.87. rouge-2 of rbm-sl (ours) rouge-2 is 24.23. bleu of rbm-sl (ours) bleu is 44.67. meteor of rbm-sl (ours) meteor is 19.97. rouge-1 of rbm-irl (ours) rouge-1 is 42.15. rouge-2 of rbm-irl (ours) rouge-2 is 24.73. bleu of rbm-irl (ours) bleu is 45.74. meteor of rbm-irl (ours) meteor is 20.18.
table 2 shows user satisfaction test results. in total, we use the strategy mentioned in section 4.2 to evaluate 400 randomly selected source news pages. overall denotes the average score on these 400 samples. satisfaction of text annotatora is 3.67. satisfaction of text annotatorb is 3.75. satisfaction of text overall is 3.71. satisfaction of pictorial annotatora is 4.14. satisfaction of pictorial annotatorb is 4.20. satisfaction of pictorial overall is 4.17.
table 3 shows multi-task results of mcapsnet. in column avg.△, we use bilstm as baseline and calculate the average improvements over it. accuracy of bilstm mr is 79.3. accuracy of bilstm sst-1 is 46.2. accuracy of bilstm sst-2 is 83.2. accuracy of bilstm subj is 90.5. accuracy of bilstm trec is 89.6. accuracy of bilstm ag’s is 88.2. accuracy of bilstm avg.△ is +0. accuracy of mt-grnn sst-1 is 49.2. accuracy of mt-grnn sst-2 is 87.7. accuracy of mt-grnn subj is 89.3. accuracy of mt-grnn trec is 93.8. accuracy of mt-grnn avg.△ is +2.6. accuracy of mt-rnn sst-1 is 49.6. accuracy of mt-rnn sst-2 is 87.9. accuracy of mt-rnn subj is 94.1. accuracy of mt-rnn trec is 91.8. accuracy of mt-rnn avg.△ is +3.5. accuracy of mt-dnn mr is 82.1. accuracy of mt-dnn sst-1 is 48.1. accuracy of mt-dnn sst-2 is 87.3. accuracy of mt-dnn subj is 93.9. accuracy of mt-dnn trec is 92.2. accuracy of mt-dnn ag’s is 91.8. accuracy of mt-dnn avg.△ is +2.9. accuracy of mt-cnn mr is 81.6. accuracy of mt-cnn sst-1 is 49.0. accuracy of mt-cnn sst-2 is 86.9. accuracy of mt-cnn subj is 93.6. accuracy of mt-cnn trec is 91.8. accuracy of mt-cnn ag’s is 91.9. accuracy of mt-cnn avg.△ is +3.0. accuracy of capsnet-1 mr is 81.5. accuracy of capsnet-1 sst-1 is 48.1. accuracy of capsnet-1 sst-2 is 86.4. accuracy of capsnet-1 subj is 93.3. accuracy of capsnet-1 trec is 91.8. accuracy of capsnet-1 ag’s is 91.1. accuracy of capsnet-1 avg.△ is +2.5. accuracy of capsnet-2 mr is 82.4. accuracy of capsnet-2 sst-1 is 48.7. accuracy of capsnet-2 sst-2 is 87.8. accuracy of capsnet-2 subj is 93.6. accuracy of capsnet-2 trec is 92.9. accuracy of capsnet-2 ag’s is 92.3. accuracy of capsnet-2 avg.△ is +3.3. accuracy of mcapsnet mr is 83.5. accuracy of mcapsnet sst-1 is 49.7. accuracy of mcapsnet sst-2 is 88.6. accuracy of mcapsnet subj is 94.5. accuracy of mcapsnet trec is 94.2. accuracy of mcapsnet ag’s is 93.8. accuracy of mcapsnet avg.△ is +4.6.
table 1 shows performance (%cerr). slash-separated pairs denote fsts incapable/capable of inserting word boundaries, respectively; see section 4. the -kn suffix denotes kneser-ney smoothing. the data from section 3.2 is used for evaluating the modern-orthography language model perplexity, and “corpus” evaluates test-set transliteration performance from the synthetic missionary text back to the original modern text. lm perplexity of fst-(c/cwb)-7gram-kn valid. is 3.07. lm perplexity of fst-(c/cwb)-7gram-kn test is 3.13. transliteration performance (%cerr) of fst-(c/cwb)-7gram-kn corpus is 27.3%. transliteration performance (%cerr) of fst-(c/cwb)-7gram-kn newspaper 1 is 50.1% / 38.7%. transliteration performance (%cerr) of fst-(c/cwb)-7gram-kn newspaper 2 is 52.0% / 47.5%. lm perplexity of fst-(c/cwb)-9gram-kn valid. is 2.95. lm perplexity of fst-(c/cwb)-9gram-kn test is 3.02. transliteration performance (%cerr) of fst-(c/cwb)-9gram-kn corpus is 26.6%. transliteration performance (%cerr) of fst-(c/cwb)-9gram-kn newspaper 1 is 50.7% / 39.3%. transliteration performance (%cerr) of fst-(c/cwb)-9gram-kn newspaper 2 is 52.5% / 47.2%. lm perplexity of fst-(c/cwb)-11gram-kn valid. is 2.94. lm perplexity of fst-(c/cwb)-11gram-kn test is 3.02. transliteration performance (%cerr) of fst-(c/cwb)-11gram-kn corpus is 27.8%. transliteration performance (%cerr) of fst-(c/cwb)-11gram-kn newspaper 1 is 53.9% / 41.3%. transliteration performance (%cerr) of fst-(c/cwb)-11gram-kn newspaper 2 is 54.1% / 48.7%. lm perplexity of fst-rnnlm-(c/cwb) valid. is 2.65. lm perplexity of fst-rnnlm-(c/cwb) test is 2.69. transliteration performance (%cerr) of fst-rnnlm-(c/cwb) corpus is 16.3%. transliteration performance (%cerr) of fst-rnnlm-(c/cwb) newspaper 1 is 47.2% / 34.3%. transliteration performance (%cerr) of fst-rnnlm-(c/cwb) newspaper 2 is 49.8% / 41.2%.
table 4 shows comparison of auc results accuracy of pcnn+att - is 0.341. accuracy of pcnn+att +selfatt is 0.368. accuracy of pcnn+att ccl-ct is 0.381. accuracy of pcnn+one - is 0.325. accuracy of pcnn+one +selfatt is 0.352. accuracy of pcnn+one ccl-ct is 0.38.
table 3 shows experimental results in exploring the shared syntactic order event detector. pre. of cl_trans_mlp pre. is 20.3. rec. of cl_trans_mlp rec. is 16.3. f1 of cl_trans_mlp f1 is 18.1. pre. of cl_trans_cnn pre. is 32.5. rec. of cl_trans_cnn rec. is 16.3. f1 of cl_trans_cnn f1 is 21.7. pre. of cl_trans_hbrid pre. is 30.4. rec. of cl_trans_hbrid rec. is 17.6. f1 of cl_trans_hbrid f1 is 22.3. pre. of cl_trans_self. pre. is 34.9. rec. of cl_trans_self. rec. is 18.3. f1 of cl_trans_self. f1 is 24. pre. of cl_trans_gcn (ours) pre. is 32. rec. of cl_trans_gcn (ours) rec. is 23.4. f1 of cl_trans_gcn (ours) f1 is 27. pre. of cl_trans_gcn_self pre. is 32.1. rec. of cl_trans_gcn_self rec. is 23. f1 of cl_trans_gcn_self f1 is 26.8.
table 8 shows results for each state change type. performance on predicting creation and destruction are highest, partially due to the model’s ability to use verb semantics for these tasks. accuracy of etbert c is 78.51. accuracy of etbert m is 61.6. accuracy of etbert d is 71.5. accuracy of etbert c is 76.68. accuracy of etbert m is 54.12. accuracy of etbert d is 58.62. accuracy of etgpt c is 79.82. accuracy of etgpt m is 56.27. accuracy of etgpt d is 73.83. accuracy of etgpt c is 77.24. accuracy of etgpt m is 50.82. accuracy of etgpt d is 56.27.
table 1 shows comparison of multilingual sentence-image retrieval/matching (german-image) and (english-image) results on multi30k. (visual encoders:vgg† otherwise resnet or faster-rcnn(resnet).) (monolingual models*.) r@1 of vse†* (kiros et al., 2014) r@1 is 20.3. r@5 of vse†* (kiros et al., 2014) r@5 is 47.2. r10 of vse†* (kiros et al., 2014) r10 is 60.1. r@1 of vse†* (kiros et al., 2014) r@1 is 29.3. r@5 of vse†* (kiros et al., 2014) r@5 is 58.1. r10 of vse†* (kiros et al., 2014) r10 is 71.8. r@1 of vse†* (kiros et al., 2014) r@1 is 23.3. r@5 of vse†* (kiros et al., 2014) r@5 is 53.6. r10 of vse†* (kiros et al., 2014) r10 is 65.8. r@1 of vse†* (kiros et al., 2014) r@1 is 31.6. r@5 of vse†* (kiros et al., 2014) r@5 is 60.4. r10 of vse†* (kiros et al., 2014) r10 is 72.7. r@1 of oe†* (vendrov et al., 2015) r@1 is 21.0. r@5 of oe†* (vendrov et al., 2015) r@5 is 48.5. r10 of oe†* (vendrov et al., 2015) r10 is 60.4. r@1 of oe†* (vendrov et al., 2015) r@1 is 26.8. r@5 of oe†* (vendrov et al., 2015) r@5 is 57.5. r10 of oe†* (vendrov et al., 2015) r10 is 70.9. r@1 of oe†* (vendrov et al., 2015) r@1 is 25.8. r@5 of oe†* (vendrov et al., 2015) r@5 is 56.5. r10 of oe†* (vendrov et al., 2015) r10 is 67.8. r@1 of oe†* (vendrov et al., 2015) r@1 is 34.8. r@5 of oe†* (vendrov et al., 2015) r@5 is 63.7. r10 of oe†* (vendrov et al., 2015) r10 is 74.8. r@1 of dan* (nam et al., 2017) r@1 is 31.0. r@5 of dan* (nam et al., 2017) r@5 is 60.9. r10 of dan* (nam et al., 2017) r10 is 71.0. r@1 of dan* (nam et al., 2017) r@1 is 46.5. r@5 of dan* (nam et al., 2017) r@5 is 77.5. r10 of dan* (nam et al., 2017) r10 is 83.0. r@1 of dan* (nam et al., 2017) r@1 is 39.4. r@5 of dan* (nam et al., 2017) r@5 is 69.2. r10 of dan* (nam et al., 2017) r10 is 69.1. r@1 of dan* (nam et al., 2017) r@1 is 55.0. r@5 of dan* (nam et al., 2017) r@5 is 81.8. r10 of dan* (nam et al., 2017) r10 is 89.0. r@1 of vse++* (faghri et al., 2018) r@1 is 31.3. r@5 of vse++* (faghri et al., 2018) r@5 is 62.2. r10 of vse++* (faghri et al., 2018) r10 is 70.9. r@1 of vse++* (faghri et al., 2018) r@1 is 47.5. r@5 of vse++* (faghri et al., 2018) r@5 is 78.5. r10 of vse++* (faghri et al., 2018) r10 is 84.5. r@1 of vse++* (faghri et al., 2018) r@1 is 39.6. r@5 of vse++* (faghri et al., 2018) r@5 is 69.1. r10 of vse++* (faghri et al., 2018) r10 is 79.8. r@1 of vse++* (faghri et al., 2018) r@1 is 53.1. r@5 of vse++* (faghri et al., 2018) r@5 is 82.1. r10 of vse++* (faghri et al., 2018) r10 is 87.5. r@1 of scan* (lee et al., 2018) r@1 is 35.7. r@5 of scan* (lee et al., 2018) r@5 is 64.9. r10 of scan* (lee et al., 2018) r10 is 74.6. r@1 of scan* (lee et al., 2018) r@1 is 52.3. r@5 of scan* (lee et al., 2018) r@5 is 81.8. r10 of scan* (lee et al., 2018) r10 is 88.5. r@1 of scan* (lee et al., 2018) r@1 is 45.8. r@5 of scan* (lee et al., 2018) r@5 is 74.4. r10 of scan* (lee et al., 2018) r10 is 83.0. r@1 of scan* (lee et al., 2018) r@1 is 61.8. r@5 of scan* (lee et al., 2018) r@5 is 87.5. r10 of scan* (lee et al., 2018) r10 is 93.7. r@1 of pivot† (gella et al., 2017) r@1 is 22.5. r@5 of pivot† (gella et al., 2017) r@5 is 49.3. r10 of pivot† (gella et al., 2017) r10 is 61.7. r@1 of pivot† (gella et al., 2017) r@1 is 28.2. r@5 of pivot† (gella et al., 2017) r@5 is 61.9. r10 of pivot† (gella et al., 2017) r10 is 73.4. r@1 of pivot† (gella et al., 2017) r@1 is 26.2. r@5 of pivot† (gella et al., 2017) r@5 is 56.4. r10 of pivot† (gella et al., 2017) r10 is 68.4. r@1 of pivot† (gella et al., 2017) r@1 is 33.8. r@5 of pivot† (gella et al., 2017) r@5 is 62.8. r10 of pivot† (gella et al., 2017) r10 is 75.2. r@1 of ours† (random, vgg19) r@1 is 25.8. r@5 of ours† (random, vgg19) r@5 is 54.9. r10 of ours† (random, vgg19) r10 is 65.1. r@1 of ours† (random, vgg19) r@1 is 34.1. r@5 of ours† (random, vgg19) r@5 is 65.5. r10 of ours† (random, vgg19) r10 is 76.5. r@1 of ours† (random, vgg19) r@1 is 30.1. r@5 of ours† (random, vgg19) r@5 is 62.5. r10 of ours† (random, vgg19) r10 is 71.6. r@1 of ours† (random, vgg19) r@1 is 36.4. r@5 of ours† (random, vgg19) r@5 is 68.0. r10 of ours† (random, vgg19) r10 is 80.9. r@1 of ours (random, no diversity) r@1 is 36.3. r@5 of ours (random, no diversity) r@5 is 65.3. r10 of ours (random, no diversity) r10 is 74.7. r@1 of ours (random, no diversity) r@1 is 53.1. r@5 of ours (random, no diversity) r@5 is 82.3. r10 of ours (random, no diversity) r10 is 88.8. r@1 of ours (random, no diversity) r@1 is 46.2. r@5 of ours (random, no diversity) r@5 is 74.7. r10 of ours (random, no diversity) r10 is 82.9. r@1 of ours (random, no diversity) r@1 is 63.3. r@5 of ours (random, no diversity) r@5 is 87.0. r10 of ours (random, no diversity) r10 is 93.3. r@1 of ours (random) r@1 is 39.2. r@5 of ours (random) r@5 is 67.5. r10 of ours (random) r10 is 76.7. r@1 of ours (random) r@1 is 55.0. r@5 of ours (random) r@5 is 84.7. r10 of ours (random) r10 is 91.2. r@1 of ours (random) r@1 is 48.7. r@5 of ours (random) r@5 is 77.2. r10 of ours (random) r10 is 85.0. r@1 of ours (random) r@1 is 66.4. r@5 of ours (random) r@5 is 88.3. r10 of ours (random) r10 is 93.4. r@1 of ours (w/ fasttext) r@1 is 40.3. r@5 of ours (w/ fasttext) r@5 is 70.1. r10 of ours (w/ fasttext) r10 is 79.0. r@1 of ours (w/ fasttext) r@1 is 60.4. r@5 of ours (w/ fasttext) r@5 is 85.4. r10 of ours (w/ fasttext) r10 is 92.0. r@1 of ours (w/ fasttext) r@1 is 50.1. r@5 of ours (w/ fasttext) r@5 is 78.1. r10 of ours (w/ fasttext) r10 is 85.7. r@1 of ours (w/ fasttext) r@1 is 68.0. r@5 of ours (w/ fasttext) r@5 is 88.8. r10 of ours (w/ fasttext) r10 is 94.0. r@1 of ours (w/ bert) r@1 is 40.7. r@5 of ours (w/ bert) r@5 is 70.5. r10 of ours (w/ bert) r10 is 78.8. r@1 of ours (w/ bert) r@1 is 56.5. r@5 of ours (w/ bert) r@5 is 84.6. r10 of ours (w/ bert) r10 is 91.3. r@1 of ours (w/ bert) r@1 is 48.9. r@5 of ours (w/ bert) r@5 is 78.3. r10 of ours (w/ bert) r10 is 85.8. r@1 of ours (w/ bert) r@1 is 66.5. r@5 of ours (w/ bert) r@5 is 89.1. r10 of ours (w/ bert) r10 is 94.1.
table 8 shows evaluation on auxiliary tasks: multitask models trained on both the translation and simplification dataset improves the performance for the task of english simplification. bleu of simplify bleu is 55.76. sari of simplify sari is 41.7. pcc of simplify pcc is 0.736. bleu of translate and simplify bleu is 56.47. sari of translate and simplify sari is 41.3. pcc of translate and simplify pcc is 0.730. bleu of all tasks bleu is 56.05. sari of all tasks sari is 42.1. pcc of all tasks pcc is 0.736. bleu of translate bleu is 29.09. pcc of translate pcc is 0.769. bleu of translate and simplify bleu is 27.33. pcc of translate and simplify pcc is 0.647. bleu of all tasks bleu is 27.63. pcc of all tasks pcc is 0.658.
table 2 shows results with traditional machine learning (label powerset) fi of svm fi is 0.448. fmacro of svm fmacro is 0.373. acci of svm acci is 0.324. fmicro of svm fmicro is 0.410. fi of svm fi is 0.449. fmacro of svm fmacro is 0.374. acci of svm acci is 0.331. fmicro of svm fmicro is 0.416. fi of svm fi is 0.546. fmacro of svm fmacro is 0.430. acci of svm acci is 0.431. fmicro of svm fmicro is 0.500. fi of svm fi is 0.178. fmacro of svm fmacro is 0.094. acci of svm acci is 0.116. fmicro of svm fmicro is 0.174. fi of lr fi is 0.357. fmacro of lr fmacro is 0.315. acci of lr acci is 0.236. fmicro of lr fmicro is 0.349. fi of lr fi is 0.357. fmacro of lr fmacro is 0.311. acci of lr acci is 0.230. fmicro of lr fmicro is 0.352. fi of lr fi is 0.595. fmacro of lr fmacro is 0.479. acci of lr acci is 0.478. fmicro of lr fmicro is 0.549. fi of lr fi is 0.438. fmacro of lr fmacro is 0.370. acci of lr acci is 0.311. fmicro of lr fmicro is 0.421. fi of rf fi is 0.531. fmacro of rf fmacro is 0.398. acci of rf acci is 0.438. fmicro of rf fmicro is 0.476. fi of rf fi is 0.395. fmacro of rf fmacro is 0.205. acci of rf acci is 0.325. fmicro of rf fmicro is 0.349. fi of rf fi is 0.375. fmacro of rf fmacro is 0.164. acci of rf acci is 0.305. fmicro of rf fmicro is 0.331. fi of rf fi is 0.460. fmacro of rf fmacro is 0.311. acci of rf acci is 0.380. fmicro of rf fmicro is 0.415.
table 3 shows results of performance (auroc) of up-to-date methods on identifying helpful reviews evaluated by the test sets of amazon. (italic f onts∗: the best performance among the baseline approaches; bold fonts: the state-of-the-art performance of all the approaches) area under receiver operating characteristic (auroc) of clothing, shoes & jewelry str is 0.548. area under receiver operating characteristic (auroc) of clothing, shoes & jewelry lex is 0.536. area under receiver operating characteristic (auroc) of clothing, shoes & jewelry galc is 0.562. area under receiver operating characteristic (auroc) of clothing, shoes & jewelry inquirer is 0.601*. area under receiver operating characteristic (auroc) of clothing, shoes & jewelry fusion (svm) is 0.579. area under receiver operating characteristic (auroc) of clothing, shoes & jewelry fusion (r.f.) is 0.55. area under receiver operating characteristic (auroc) of clothing, shoes & jewelry eg-cnn is 0.583. area under receiver operating characteristic (auroc) of clothing, shoes & jewelry mtnl is 0.589. area under receiver operating characteristic (auroc) of clothing, shoes & jewelry r2hp is 0.623 (+0.022). area under receiver operating characteristic (auroc) of electronics str is 0.583. area under receiver operating characteristic (auroc) of electronics lex is 0.549. area under receiver operating characteristic (auroc) of electronics galc is 0.588. area under receiver operating characteristic (auroc) of electronics inquirer is 0.616*. area under receiver operating characteristic (auroc) of electronics fusion (svm) is 0.577. area under receiver operating characteristic (auroc) of electronics fusion (r.f.) is 0.58. area under receiver operating characteristic (auroc) of electronics eg-cnn is 0.611. area under receiver operating characteristic (auroc) of electronics mtnl is 0.613. area under receiver operating characteristic (auroc) of electronics r2hp is 0.661 (+0.045). area under receiver operating characteristic (auroc) of grocery & gourmet food str is 0.536. area under receiver operating characteristic (auroc) of grocery & gourmet food lex is 0.526. area under receiver operating characteristic (auroc) of grocery & gourmet food galc is 0.553. area under receiver operating characteristic (auroc) of grocery & gourmet food inquirer is 0.602. area under receiver operating characteristic (auroc) of grocery & gourmet food fusion (svm) is 0.532. area under receiver operating characteristic (auroc) of grocery & gourmet food fusion (r.f.) is 0.546. area under receiver operating characteristic (auroc) of grocery & gourmet food eg-cnn is 0.611. area under receiver operating characteristic (auroc) of grocery & gourmet food mtnl is 0.626*. area under receiver operating characteristic (auroc) of grocery & gourmet food r2hp is 0.657 (+0.031). area under receiver operating characteristic (auroc) of health & personal care str is 0.558. area under receiver operating characteristic (auroc) of health & personal care lex is 0.523. area under receiver operating characteristic (auroc) of health & personal care galc is 0.559. area under receiver operating characteristic (auroc) of health & personal care inquirer is 0.61. area under receiver operating characteristic (auroc) of health & personal care fusion (svm) is 0.591. area under receiver operating characteristic (auroc) of health & personal care fusion (r.f.) is 0.562. area under receiver operating characteristic (auroc) of health & personal care eg-cnn is 0.613. area under receiver operating characteristic (auroc) of health & personal care mtnl is 0.620*. area under receiver operating characteristic (auroc) of health & personal care r2hp is 0.683 (+0.063). area under receiver operating characteristic (auroc) of home & kitchen str is 0.568. area under receiver operating characteristic (auroc) of home & kitchen lex is 0.537. area under receiver operating characteristic (auroc) of home & kitchen galc is 0.565. area under receiver operating characteristic (auroc) of home & kitchen inquirer is 0.597. area under receiver operating characteristic (auroc) of home & kitchen fusion (svm) is 0.569. area under receiver operating characteristic (auroc) of home & kitchen fusion (r.f.) is 0.573. area under receiver operating characteristic (auroc) of home & kitchen eg-cnn is 0.603. area under receiver operating characteristic (auroc) of home & kitchen mtnl is 0.610*. area under receiver operating characteristic (auroc) of home & kitchen r2hp is 0.646 (+0.036). area under receiver operating characteristic (auroc) of movies & tv str is 0.603. area under receiver operating characteristic (auroc) of movies & tv lex is 0.558. area under receiver operating characteristic (auroc) of movies & tv galc is 0.621. area under receiver operating characteristic (auroc) of movies & tv inquirer is 0.634. area under receiver operating characteristic (auroc) of movies & tv fusion (svm) is 0.603. area under receiver operating characteristic (auroc) of movies & tv fusion (r.f.) is 0.607. area under receiver operating characteristic (auroc) of movies & tv eg-cnn is 0.648. area under receiver operating characteristic (auroc) of movies & tv mtnl is 0.652*. area under receiver operating characteristic (auroc) of movies & tv r2hp is 0.713 (+0.061). area under receiver operating characteristic (auroc) of pet supplies str is 0.56. area under receiver operating characteristic (auroc) of pet supplies lex is 0.542. area under receiver operating characteristic (auroc) of pet supplies galc is 0.585. area under receiver operating characteristic (auroc) of pet supplies inquirer is 0.603. area under receiver operating characteristic (auroc) of pet supplies fusion (svm) is 0.548. area under receiver operating characteristic (auroc) of pet supplies fusion (r.f.) is 0.558. area under receiver operating characteristic (auroc) of pet supplies eg-cnn is 0.58. area under receiver operating characteristic (auroc) of pet supplies mtnl is 0.629*. area under receiver operating characteristic (auroc) of pet supplies r2hp is 0.692 (+0.063). area under receiver operating characteristic (auroc) of tools & home improvement str is 0.584. area under receiver operating characteristic (auroc) of tools & home improvement lex is 0.558. area under receiver operating characteristic (auroc) of tools & home improvement galc is 0.58. area under receiver operating characteristic (auroc) of tools & home improvement inquirer is 0.592. area under receiver operating characteristic (auroc) of tools & home improvement fusion (svm) is 0.575. area under receiver operating characteristic (auroc) of tools & home improvement fusion (r.f.) is 0.586. area under receiver operating characteristic (auroc) of tools & home improvement eg-cnn is 0.617. area under receiver operating characteristic (auroc) of tools & home improvement mtnl is 0.624*. area under receiver operating characteristic (auroc) of tools & home improvement r2hp is 0.672 (+0.048). area under receiver operating characteristic (auroc) of macro average str is 0.568. area under receiver operating characteristic (auroc) of macro average lex is 0.541. area under receiver operating characteristic (auroc) of macro average galc is 0.577. area under receiver operating characteristic (auroc) of macro average inquirer is 0.607. area under receiver operating characteristic (auroc) of macro average fusion (svm) is 0.572. area under receiver operating characteristic (auroc) of macro average fusion (r.f.) is 0.57. area under receiver operating characteristic (auroc) of macro average eg-cnn is 0.608. area under receiver operating characteristic (auroc) of macro average mtnl is 0.620*. area under receiver operating characteristic (auroc) of macro average r2hp is 0.668 (+0.048). area under receiver operating characteristic (auroc) of micro average (primary) str is 0.571. area under receiver operating characteristic (auroc) of micro average (primary) lex is 0.543. area under receiver operating characteristic (auroc) of micro average (primary) galc is 0.58. area under receiver operating characteristic (auroc) of micro average (primary) inquirer is 0.609. area under receiver operating characteristic (auroc) of micro average (primary) fusion (svm) is 0.573. area under receiver operating characteristic (auroc) of micro average (primary) fusion (r.f.) is 0.574. area under receiver operating characteristic (auroc) of micro average (primary) eg-cnn is 0.614. area under receiver operating characteristic (auroc) of micro average (primary) mtnl is 0.625*. area under receiver operating characteristic (auroc) of micro average (primary) r2hp is 0.675 (+0.049).
table 3 shows results for next utterance retrieval on the ubuntu dialog corpus. this table shows previous work, and experimental results with two underlying architectures: a dual encoder model and deep attention matching networks. the results shown in the dam experiments section are performed with the open-sourced implementation of zhou et al. (2018), which obtains slightly worse performance than they report. all bold-face results are statistically significant to p < 0.01. r10@1 of dual encoder (lowe et al., 2015) r10@1 is 63.8. r2@1 of dual encoder (lowe et al., 2015) r2@1 is 90.1. r10@1 of mv-lstm (pang et al., 2016) r10@1 is 65.3. r2@1 of mv-lstm (pang et al., 2016) r2@1 is 90.6. mrr of match-lstm (wang and jiang, 2016) mrr is . r10@1 of match-lstm (wang and jiang, 2016) r10@1 is 65.3. r2@1 of match-lstm (wang and jiang, 2016) r2@1 is 90.4. r10@1 of multiview (zhou et al., 2016) r10@1 is 66.2. r2@1 of multiview (zhou et al., 2016) r2@1 is 90.8. r10@1 of dl2r (yan et al., 2016) r10@1 is 62.6. r2@1 of dl2r (yan et al., 2016) r2@1 is 89.9. r10@1 of smn (wu et al., 2016) r10@1 is 72.6. r2@1 of smn (wu et al., 2016) r2@1 is 92.6. r10@1 of dam (zhou et al., 2018) r10@1 is 76.7. r2@1 of dam (zhou et al., 2018) r2@1 is 93.8. mrr of dual encoder (lowe et al., 2015) mrr is 76.84. r10@1 of dual encoder (lowe et al., 2015) r10@1 is 63.6. r2@1 of dual encoder (lowe et al., 2015) r2@1 is 90.9. mrr of ensemble (5) mrr is 78.91. r10@1 of ensemble (5) r10@1 is 66.9. r2@1 of ensemble (5) r2@1 is 91.7. mrr of multi-granularity (5) mrr is 80.10. r10@1 of multi-granularity (5) r10@1 is 68.7. r2@1 of multi-granularity (5) r2@1 is 91.9. mrr of dam (zhou et al., 2018) (re-trained) mrr is 83.74. r10@1 of dam (zhou et al., 2018) (re-trained) r10@1 is 74.54. r2@1 of dam (zhou et al., 2018) (re-trained) r2@1 is 93.08. mrr of ensemble (5) mrr is 84.03. r10@1 of ensemble (5) r10@1 is 74.95. r2@1 of ensemble (5) r2@1 is 93.27. mrr of multi-granularity (5) mrr is 84.26. r10@1 of multi-granularity (5) r10@1 is 75.3. r2@1 of multi-granularity (5) r2@1 is 93.45.
table 6 shows comparison of the decoder information sharing methods and encoder sharing methods for the job advertisement dataset. the metrics are the same as in table 3. the proposed method (adopting hcl) achieved the best scores (bold) compared to the other sharing methods. r-1 of baseline (pointer-generator network) r-1 is 25.1. r-2 of baseline (pointer-generator network) r-2 is 5.3. r-l of baseline (pointer-generator network) r-l is 21.1. r-1 of baseline (pointer-generator network) r-1 is 30.9. r-2 of baseline (pointer-generator network) r-2 is 10.6. r-l of baseline (pointer-generator network) r-l is 28.7. accuracy of baseline (pointer-generator network) accuracy is 62.8. r-1 of proposed (mtl + sd + hcl) r-1 is 26.9. r-2 of proposed (mtl + sd + hcl) r-2 is 6.1. r-l of proposed (mtl + sd + hcl) r-l is 22.4. r-1 of proposed (mtl + sd + hcl) r-1 is 32.8. r-2 of proposed (mtl + sd + hcl) r-2 is 11.2. r-l of proposed (mtl + sd + hcl) r-l is 30.5. accuracy of proposed (mtl + sd + hcl) accuracy is 64.4. r-1 of comparison of decoder information sharing method mtl + sd r-1 is 26.3. r-2 of comparison of decoder information sharing method mtl + sd r-2 is 6. r-l of comparison of decoder information sharing method mtl + sd r-l is 21.8. r-1 of comparison of decoder information sharing method mtl + sd r-1 is 32.3. r-2 of comparison of decoder information sharing method mtl + sd r-2 is 10.4. r-l of comparison of decoder information sharing method mtl + sd r-l is 29.9. accuracy of comparison of decoder information sharing method mtl + sd accuracy is 63.9. r-1 of comparison of decoder information sharing method mtl + sd + cascade model r-1 is 26.3. r-2 of comparison of decoder information sharing method mtl + sd + cascade model r-2 is 5.6. r-l of comparison of decoder information sharing method mtl + sd + cascade model r-l is 21.6. r-1 of comparison of decoder information sharing method mtl + sd + cascade model r-1 is 31.8. r-2 of comparison of decoder information sharing method mtl + sd + cascade model r-2 is 10.6. r-l of comparison of decoder information sharing method mtl + sd + cascade model r-l is 29.5. accuracy of comparison of decoder information sharing method mtl + sd + cascade model accuracy is 64.4. r-1 of comparison of decoder information sharing method mtl + sd + cascade model (gold) r-1 is 26.5. r-2 of comparison of decoder information sharing method mtl + sd + cascade model (gold) r-2 is 5.8. r-l of comparison of decoder information sharing method mtl + sd + cascade model (gold) r-l is 21.9. r-1 of comparison of decoder information sharing method mtl + sd + cascade model (gold) r-1 is 32.8. r-2 of comparison of decoder information sharing method mtl + sd + cascade model (gold) r-2 is 10.4. r-l of comparison of decoder information sharing method mtl + sd + cascade model (gold) r-l is 30.3. accuracy of comparison of decoder information sharing method mtl + sd + cascade model (gold) accuracy is 64.5. r-1 of comparison of decoder information sharing method mtl + sd + soft-parameter sharing r-1 is 25.8. r-2 of comparison of decoder information sharing method mtl + sd + soft-parameter sharing r-2 is 5.9. r-l of comparison of decoder information sharing method mtl + sd + soft-parameter sharing r-l is 21.4. r-1 of comparison of decoder information sharing method mtl + sd + soft-parameter sharing r-1 is 32.1. r-2 of comparison of decoder information sharing method mtl + sd + soft-parameter sharing r-2 is 10. r-l of comparison of decoder information sharing method mtl + sd + soft-parameter sharing r-l is 29.6. accuracy of comparison of decoder information sharing method mtl + sd + soft-parameter sharing accuracy is 64. r-1 of comparison of decoder information sharing method mtl + sd + non-hierarchical consistency loss r-1 is 25.9. r-2 of comparison of decoder information sharing method mtl + sd + non-hierarchical consistency loss r-2 is 6. r-l of comparison of decoder information sharing method mtl + sd + non-hierarchical consistency loss r-l is 21.4. r-1 of comparison of decoder information sharing method mtl + sd + non-hierarchical consistency loss r-1 is 32.6. r-2 of comparison of decoder information sharing method mtl + sd + non-hierarchical consistency loss r-2 is 10.9. r-l of comparison of decoder information sharing method mtl + sd + non-hierarchical consistency loss r-l is 30.2. accuracy of comparison of decoder information sharing method mtl + sd + non-hierarchical consistency loss accuracy is 64. r-1 of comparison of decoder information sharing method mtl + sd + hcl with normalized attention weights r-1 is 26.2. r-2 of comparison of decoder information sharing method mtl + sd + hcl with normalized attention weights r-2 is 6. r-l of comparison of decoder information sharing method mtl + sd + hcl with normalized attention weights r-l is 21.7. r-1 of comparison of decoder information sharing method mtl + sd + hcl with normalized attention weights r-1 is 31.9. r-2 of comparison of decoder information sharing method mtl + sd + hcl with normalized attention weights r-2 is 10.5. r-l of comparison of decoder information sharing method mtl + sd + hcl with normalized attention weights r-l is 29.5. accuracy of comparison of decoder information sharing method mtl + sd + hcl with normalized attention weights accuracy is 63.9. r-1 of comparison of encoder information sharing method hcl (sd and mtl are not applied) r-1 is 25.8. r-2 of comparison of encoder information sharing method hcl (sd and mtl are not applied) r-2 is 5.6. r-l of comparison of encoder information sharing method hcl (sd and mtl are not applied) r-l is 21.2. r-1 of comparison of encoder information sharing method hcl (sd and mtl are not applied) r-1 is 31. r-2 of comparison of encoder information sharing method hcl (sd and mtl are not applied) r-2 is 10.1. r-l of comparison of encoder information sharing method hcl (sd and mtl are not applied) r-l is 28.7. accuracy of comparison of encoder information sharing method hcl (sd and mtl are not applied) accuracy is 63.1. r-1 of comparison of encoder information sharing method sd + hcl (mtl is not applied) r-1 is 25.6. r-2 of comparison of encoder information sharing method sd + hcl (mtl is not applied) r-2 is 5.6. r-l of comparison of encoder information sharing method sd + hcl (mtl is not applied) r-l is 21.5. r-1 of comparison of encoder information sharing method sd + hcl (mtl is not applied) r-1 is 31.2. r-2 of comparison of encoder information sharing method sd + hcl (mtl is not applied) r-2 is 10.2. r-l of comparison of encoder information sharing method sd + hcl (mtl is not applied) r-l is 28.9. accuracy of comparison of encoder information sharing method sd + hcl (mtl is not applied) accuracy is 62.6.
table 4 shows nested and overlapping event detection f1 (%) score performance on the cg task 2013 development set. f1 of tees nested is 42.7. f1 of tees overlapping is 34.49. f1 of tees flat is 56.81. f1 of tees overall f1 (%) is 52.16. f1 of sbnn k = 8 nested is 45.24. f1 of sbnn k = 8 overlapping is 36.92. f1 of sbnn k = 8 flat is 60.5. f1 of sbnn k = 8 overall f1 (%) is 54.36.
table 4 shows performance comparison on the ontonotes 5.0 chinese dataset. prec. of pradhan et al. (2013) prec. is 78.2. rec. of pradhan et al. (2013) rec. is 66.45. f1 of pradhan et al. (2013) f1 is 71.85. prec. of lattice lstm (z&y, 2018) prec. is 76.34. rec. of lattice lstm (z&y, 2018) rec. is 77.01. f1 of lattice lstm (z&y, 2018) f1 is 76.67. prec. of bilstm-crf (l = 0) prec. is 76.67. rec. of bilstm-crf (l = 0) rec. is 67.79. f1 of bilstm-crf (l = 0) f1 is 71.95. prec. of bilstm-crf (l = 1) prec. is 78.45. rec. of bilstm-crf (l = 1) rec. is 74.59. f1 of bilstm-crf (l = 1) f1 is 76.47. prec. of bilstm-crf (l = 2) prec. is 77.94. rec. of bilstm-crf (l = 2) rec. is 75.33. f1 of bilstm-crf (l = 2) f1 is 76.61. prec. of bilstm-crf (l = 3) prec. is 76.17. rec. of bilstm-crf (l = 3) rec. is 75.23. f1 of bilstm-crf (l = 3) f1 is 75.7. prec. of bilstm-gcn-crf prec. is 76.35. rec. of bilstm-gcn-crf rec. is 75.89. f1 of bilstm-gcn-crf f1 is 76.12. prec. of dglstm-crf (l = 0) prec. is 76.91. rec. of dglstm-crf (l = 0) rec. is 70.65. f1 of dglstm-crf (l = 0) f1 is 73.65. prec. of dglstm-crf (l = 1) prec. is 77.79. rec. of dglstm-crf (l = 1) rec. is 75.29. f1 of dglstm-crf (l = 1) f1 is 76.52. prec. of dglstm-crf (l = 2) prec. is 77.4. rec. of dglstm-crf (l = 2) rec. is 77.41. f1 of dglstm-crf (l = 2) f1 is 77.4. prec. of dglstm-crf (l = 3) prec. is 77.01. rec. of dglstm-crf (l = 3) rec. is 74.9. f1 of dglstm-crf (l = 3) f1 is 75.94. prec. of bilstm-crf (l = 0) + elmo prec. is 75.2. rec. of bilstm-crf (l = 0) + elmo rec. is 73.39. f1 of bilstm-crf (l = 0) + elmo f1 is 74.28. prec. of bilstm-crf (l = 1) + elmo prec. is 79.2. rec. of bilstm-crf (l = 1) + elmo rec. is 79.21. f1 of bilstm-crf (l = 1) + elmo f1 is 79.2. prec. of bilstm-crf(l = 2) + elmo prec. is 78.49. rec. of bilstm-crf(l = 2) + elmo rec. is 79.44. f1 of bilstm-crf(l = 2) + elmo f1 is 78.96. prec. of bilstm-crf (l = 3) + elmo prec. is 78.54. rec. of bilstm-crf (l = 3) + elmo rec. is 79.76. f1 of bilstm-crf (l = 3) + elmo f1 is 79.14. prec. of bilstm-gcn-crf + elmo prec. is 78.71. rec. of bilstm-gcn-crf + elmo rec. is 79.29. f1 of bilstm-gcn-crf + elmo f1 is 79. prec. of dglstm-crf (l = 0) + elmo prec. is 76.27. rec. of dglstm-crf (l = 0) + elmo rec. is 74.61. f1 of dglstm-crf (l = 0) + elmo f1 is 75.43. prec. of dglstm-crf (l = 1) + elmo prec. is 78.91. rec. of dglstm-crf (l = 1) + elmo rec. is 80.22. f1 of dglstm-crf (l = 1) + elmo f1 is 79.56. prec. of dglstm-crf (l = 2) + elmo prec. is 78.86. rec. of dglstm-crf (l = 2) + elmo rec. is 81. f1 of dglstm-crf (l = 2) + elmo f1 is 79.92. prec. of dglstm-crf (l = 3) + elmo prec. is 79.3. rec. of dglstm-crf (l = 3) + elmo rec. is 79.86. f1 of dglstm-crf (l = 3) + elmo f1 is 79.58.
table 2 shows comparison of mean accuracy (%) on arsc mean acc of matching networks (vinyals et al., 2016) mean acc is 65.73. mean acc of prototypical networks (snell et al., 2017) mean acc is 68.17. mean acc of graph network (garcia and bruna, 2017) mean acc is 82.61. mean acc of relation network (sung et al., 2018) mean acc is 83.07. mean acc of snail (mishra et al., 2018) mean acc is 82.57. mean acc of robusttc-fsl (yu et al., 2018) mean acc is 83.12. mean acc of induction networks (ours) mean acc is 85.63.
table 2 shows results of bilingual lexicon induction (accuracy % p@1) for similar and distant language pairs on the dataset bli-1. procrustes and muse represent the supervised and unsupervised model of lample et al. (2018), vecmap is the supervised model of artetxe et al. (2017). word translations are retrieved by using csls. bold face indicates the best result overall and italics indicate the best result between the two columns without refinement. p@1 of de-en vecmap is 72.5. p@1 of de-en procrustes is 72. p@1 of de-en muse is 55.3. p@1 of de-en musemethod is 66.3. p@1 of de-en our is 72.2. p@1 of de-en method is 72.4. p@1 of en-de vecmap is 72.5. p@1 of en-de procrustes is 72.1. p@1 of en-de muse is 59.2. p@1 of en-de musemethod is 68.2. p@1 of en-de our is 72.5. p@1 of en-de method is 73.4. p@1 of es-en vecmap is 83.4. p@1 of es-en procrustes is 82.9. p@1 of es-en muse is 78.1. p@1 of es-en musemethod is 78. p@1 of es-en our is 83.1. p@1 of es-en method is 83.6. p@1 of en-es vecmap is 81.9. p@1 of en-es procrustes is 81.5. p@1 of en-es muse is 75.5. p@1 of en-es musemethod is 74.9. p@1 of en-es our is 81.1. p@1 of en-es method is 80.9. p@1 of fr-en vecmap is 81.5. p@1 of fr-en procrustes is 82.2. p@1 of fr-en muse is 72.2. p@1 of fr-en musemethod is 75.6. p@1 of fr-en our is 81.2. p@1 of fr-en method is 83.3. p@1 of en-fr vecmap is 81.3. p@1 of en-fr procrustes is 81.3. p@1 of en-fr muse is 77.8. p@1 of en-fr musemethod is 78.3. p@1 of en-fr our is 82.2. p@1 of en-fr method is 82.2. p@1 of it-en vecmap is 77.7. p@1 of it-en procrustes is 79.1. p@1 of it-en muse is 61. p@1 of it-en musemethod is 63.3. p@1 of it-en our is 76.4. p@1 of it-en method is 76.2. p@1 of en-it vecmap is 77.2. p@1 of en-it procrustes is 77.5. p@1 of en-it muse is 63.3. p@1 of en-it musemethod is 64.3. p@1 of en-it our is 77.4. p@1 of en-it method is 78.2. p@1 of avg. vecmap is 78.5. p@1 of avg. procrustes is 78.6. p@1 of avg. muse is 67.8. p@1 of avg. musemethod is 71.1. p@1 of avg. our is 78.3. p@1 of avg. method is 78.8. p@1 of fi-en vecmap is 55.9. p@1 of fi-en procrustes is 56.2. p@1 of fi-en muse is 0. p@1 of fi-en musemethod is 42.8. p@1 of fi-en our is 28.6. p@1 of fi-en method is 53.5. p@1 of en-fi vecmap is 39.5. p@1 of en-fi procrustes is 39.3. p@1 of en-fi muse is 0. p@1 of en-fi musemethod is 35.2. p@1 of en-fi our is 21. p@1 of en-fi method is 43. p@1 of ru-en vecmap is 60.5. p@1 of ru-en procrustes is 61.3. p@1 of ru-en muse is 43.8. p@1 of ru-en musemethod is 54.3. p@1 of ru-en our is 49.9. p@1 of ru-en method is 58.9. p@1 of en-ru vecmap is 48. p@1 of en-ru procrustes is 50.9. p@1 of en-ru muse is 28.7. p@1 of en-ru musemethod is 43.2. p@1 of en-ru our is 37.6. p@1 of en-ru method is 51.1. p@1 of tr-en vecmap is 55.7. p@1 of tr-en procrustes is 58. p@1 of tr-en muse is 12.2. p@1 of tr-en musemethod is 42.1. p@1 of tr-en our is 25.5. p@1 of tr-en method is 54.7. p@1 of en-tr vecmap is 37.3. p@1 of en-tr procrustes is 37.1. p@1 of en-tr muse is 26.2. p@1 of en-tr musemethod is 33.1. p@1 of en-tr our is 39.4. p@1 of en-tr method is 41.4. p@1 of zh-en vecmap is 45. p@1 of zh-en procrustes is 41.2. p@1 of zh-en muse is 26.9. p@1 of zh-en musemethod is 32.1. p@1 of zh-en our is 30.8. p@1 of zh-en method is 42. p@1 of en-zh vecmap is 45.4. p@1 of en-zh procrustes is 52.1. p@1 of en-zh muse is 29.4. p@1 of en-zh musemethod is 37.7. p@1 of en-zh our is 33. p@1 of en-zh method is 48. p@1 of avg. vecmap is 48.4. p@1 of avg. procrustes is 49.5. p@1 of avg. muse is 20.9. p@1 of avg. musemethod is 40.1. p@1 of avg. our is 33.2. p@1 of avg. method is 49.1.
table 1 shows seq vs. camb system results on cwi macro of news camb is 0.8633. f-score of news seq is 0.8763. macro of wikinews camb is 0.8317. f-score of wikinews seq is 0.854. macro of wikipedia camb is 0.778. f-score of wikipedia seq is 0.814.
table 3 shows performance comparisons of different methods for citation count prediction using two datasets. “↑" ( “↓") indicates that a larger (smaller) value corresponds to a better performance. mae of lr mae is 0.1776. rmse of lr rmse is 0.1903. or(@30) of lr or(@30) is 0.27. or(@50) of lr or(@50) is 0.33. spearman rank of lr spearman rank is 0.4776. mae of knn mae is 0.1701. rmse of knn rmse is 0.19. or(@30) of knn or(@30) is 0.33. or(@50) of knn or(@50) is 0.36. spearman rank of knn spearman rank is 0.4848. mae of svr mae is 0.1677. rmse of svr rmse is 0.1856. or(@30) of svr or(@30) is 0.33. or(@50) of svr or(@50) is 0.4. spearman rank of svr spearman rank is 0.5279. mae of gbrt mae is 0.1863. rmse of gbrt rmse is 0.1974. or(@30) of gbrt or(@30) is 0.23. or(@50) of gbrt or(@50) is 0.34. spearman rank of gbrt spearman rank is 0.531. mae of wide&deep mae is 0.147. rmse of wide&deep rmse is 0.1848. or(@30) of wide&deep or(@30) is 0.3. or(@50) of wide&deep or(@50) is 0.38. spearman rank of wide&deep spearman rank is 0.5351. mae of milam mae is 0.1426. rmse of milam rmse is 0.1792. or(@30) of milam or(@30) is 0.37. or(@50) of milam or(@50) is 0.38. spearman rank of milam spearman rank is 0.5458. mae of our model mae is 0.1349. rmse of our model rmse is 0.1726. or(@30) of our model or(@30) is 0.4. or(@50) of our model or(@50) is 0.42. spearman rank of our model spearman rank is 0.5561. mae of lr mae is 0.2395. rmse of lr rmse is 0.2723. or(@30) of lr or(@30) is 0.4. or(@50) of lr or(@50) is 0.7. spearman rank of lr spearman rank is 0.1475. mae of knn mae is 0.2293. rmse of knn rmse is 0.2674. or(@30) of knn or(@30) is 0.4. or(@50) of knn or(@50) is 0.72. spearman rank of knn spearman rank is 0.1874. mae of svr mae is 0.2226. rmse of svr rmse is 0.2578. or(@30) of svr or(@30) is 0.4. or(@50) of svr or(@50) is 0.7. spearman rank of svr spearman rank is 0.1328. mae of gbrt mae is 0.2223. rmse of gbrt rmse is 0.2607. or(@30) of gbrt or(@30) is 0.43. or(@50) of gbrt or(@50) is 0.7. spearman rank of gbrt spearman rank is 0.1469. mae of wide&deep mae is 0.2182. rmse of wide&deep rmse is 0.2607. or(@30) of wide&deep or(@30) is 0.47. or(@50) of wide&deep or(@50) is 0.72. spearman rank of wide&deep spearman rank is 0.244. mae of milam mae is 0.2093. rmse of milam rmse is 0.251. or(@30) of milam or(@30) is 0.47. or(@50) of milam or(@50) is 0.72. spearman rank of milam spearman rank is 0.251. mae of our model mae is 0.1866. rmse of our model rmse is 0.2279. or(@30) of our model or(@30) is 0.5. or(@50) of our model or(@50) is 0.76. spearman rank of our model spearman rank is 0.3026.
table 1 shows results of completion generation. we group mpc, character language model baseline, and two subword language models separately. +r implies the retrace algorithm. +m implies reranking with approximate marginalization. qps stands for query per seconds. the higher the qps, the better. the best results for each column related to accuracy are shown in bold for each segmentation algorithm (bpe and sr). sr model shows higher unseen pmrr scores (underlined). our models are faster than the character baseline. mrr seen of mpc mrr seen is .570. mrr unseen of mpc mrr unseen is .000. mrr all of mpc mrr all is .290. pmrr seen of mpc pmrr seen is .616. pmrr unseen of mpc pmrr unseen is .095. pmrr all of mpc pmrr all is .360. mrl seen of mpc mrl seen is 8.06. mrl unseen of mpc mrl unseen is 0.00. mrl all of mpc mrl all is 4.10. execution speed (qps) cpu of mpc execution speed (qps) cpu is >100. execution speed (qps) gpu of mpc execution speed (qps) gpu is >100. decode length of mpc decode length is n/a. mrr seen of char mrr seen is .458. mrr unseen of char mrr unseen is .160. mrr all of char mrr all is .311. pmrr seen of char pmrr seen is .552. pmrr unseen of char pmrr unseen is .372. pmrr all of char pmrr all is .464. mrl seen of char mrl seen is 5.77. mrl unseen of char mrl unseen is 4.24. mrl all of char mrl all is 5.02. execution speed (qps) cpu of char execution speed (qps) cpu is 11.0 (1.0x). execution speed (qps) gpu of char execution speed (qps) gpu is 16.5 (1.0x). decode length of char decode length is 14.5. mrr seen of bpe mrr seen is .242. mrr unseen of bpe mrr unseen is .085. mrr all of bpe mrr all is .164. pmrr seen of bpe pmrr seen is .305. pmrr unseen of bpe pmrr unseen is .232. pmrr all of bpe pmrr all is .269. mrl seen of bpe mrl seen is 0.49. mrl unseen of bpe mrl unseen is 0.54. mrl all of bpe mrl all is 0.51. execution speed (qps) cpu of bpe execution speed (qps) cpu is 24.2 (2.2x). execution speed (qps) gpu of bpe execution speed (qps) gpu is 37.4 (2.3x). decode length of bpe decode length is 7.1. mrr seen of bpe+r1 mrr seen is .427. mrr unseen of bpe+r1 mrr unseen is .156. mrr all of bpe+r1 mrr all is .294. pmrr seen of bpe+r1 pmrr seen is .517. pmrr unseen of bpe+r1 pmrr unseen is .368. pmrr all of bpe+r1 pmrr all is .444. mrl seen of bpe+r1 mrl seen is 5.28. mrl unseen of bpe+r1 mrl unseen is 3.98. mrl all of bpe+r1 mrl all is 4.64. execution speed (qps) cpu of bpe+r1 execution speed (qps) cpu is 15.8 (1.4x). execution speed (qps) gpu of bpe+r1 execution speed (qps) gpu is 27.3 (1.7x). decode length of bpe+r1 decode length is 11.8. mrr seen of bpe+r2 mrr seen is .430. mrr unseen of bpe+r2 mrr unseen is .157. mrr all of bpe+r2 mrr all is .296. pmrr seen of bpe+r2 pmrr seen is .520. pmrr unseen of bpe+r2 pmrr unseen is .369. pmrr all of bpe+r2 pmrr all is .446. mrl seen of bpe+r2 mrl seen is 5.44. mrl unseen of bpe+r2 mrl unseen is 4.01. mrl all of bpe+r2 mrl all is 4.74. execution speed (qps) cpu of bpe+r2 execution speed (qps) cpu is 15.5 (1.4x). execution speed (qps) gpu of bpe+r2 execution speed (qps) gpu is 27.2 (1.6x). decode length of bpe+r2 decode length is 12.2. mrr seen of bpe+r∞ mrr seen is .431. mrr unseen of bpe+r∞ mrr unseen is .157. mrr all of bpe+r∞ mrr all is .296. pmrr seen of bpe+r∞ pmrr seen is .520. pmrr unseen of bpe+r∞ pmrr unseen is .369. pmrr all of bpe+r∞ pmrr all is .446. mrl seen of bpe+r∞ mrl seen is 5.50. mrl unseen of bpe+r∞ mrl unseen is 4.01. mrl all of bpe+r∞ mrl all is 4.76. execution speed (qps) cpu of bpe+r∞ execution speed (qps) cpu is 15.3 (1.4x). execution speed (qps) gpu of bpe+r∞ execution speed (qps) gpu is 26.9 (1.6x). decode length of bpe+r∞ decode length is 12.2. mrr seen of sr mrr seen is .422. mrr unseen of sr mrr unseen is .148. mrr all of sr mrr all is .288. pmrr seen of sr pmrr seen is .541. pmrr unseen of sr pmrr unseen is .379. pmrr all of sr pmrr all is .461. mrl seen of sr mrl seen is 5.11. mrl unseen of sr mrl unseen is 3.82. mrl all of sr mrl all is 4.48. execution speed (qps) cpu of sr execution speed (qps) cpu is 20.8 (1.9x). execution speed (qps) gpu of sr execution speed (qps) gpu is 40.1 (2.4x). decode length of sr decode length is 6.8. mrr seen of sr+m mrr seen is .424. mrr unseen of sr+m mrr unseen is .149. mrr all of sr+m mrr all is .289. pmrr seen of sr+m pmrr seen is .535. pmrr unseen of sr+m pmrr unseen is .373. pmrr all of sr+m pmrr all is .455. mrl seen of sr+m mrl seen is 5.14. mrl unseen of sr+m mrl unseen is 3.85. mrl all of sr+m mrl all is 4.50. execution speed (qps) cpu of sr+m execution speed (qps) cpu is 19.6 (1.8x). execution speed (qps) gpu of sr+m execution speed (qps) gpu is 40.0 (2.4x). decode length of sr+m decode length is 6.8. mrr seen of sr+r∞ mrr seen is .423. mrr unseen of sr+r∞ mrr unseen is .148. mrr all of sr+r∞ mrr all is .289. pmrr seen of sr+r∞ pmrr seen is .541. pmrr unseen of sr+r∞ pmrr unseen is .378. pmrr all of sr+r∞ pmrr all is .461. mrl seen of sr+r∞ mrl seen is 5.14. mrl unseen of sr+r∞ mrl unseen is 3.83. mrl all of sr+r∞ mrl all is 4.50. execution speed (qps) cpu of sr+r∞ execution speed (qps) cpu is 16.3 (1.5x). execution speed (qps) gpu of sr+r∞ execution speed (qps) gpu is 29.6 (1.8x). decode length of sr+r∞ decode length is 7.4. mrr seen of sr+r∞+m mrr seen is .427. mrr unseen of sr+r∞+m mrr unseen is .150. mrr all of sr+r∞+m mrr all is .291. pmrr seen of sr+r∞+m pmrr seen is .538. pmrr unseen of sr+r∞+m pmrr unseen is .375. pmrr all of sr+r∞+m pmrr all is .458. mrl seen of sr+r∞+m mrl seen is 5.19. mrl unseen of sr+r∞+m mrl unseen is 3.88. mrl all of sr+r∞+m mrl all is 4.54. execution speed (qps) cpu of sr+r∞+m execution speed (qps) cpu is 16.2 (1.5x). execution speed (qps) gpu of sr+r∞+m execution speed (qps) gpu is 28.7 (1.7x). decode length of sr+r∞+m decode length is 7.4.
table 4 shows evaluation results on automatic metrics and human judgment. human evaluation results are calculated by combining labels from the three judges. “kappa” means fleiss’ kappa. numbers in bold mean that improvement over the best baseline is statistically significant. meteor of ir-t meteor is 0.107. w-meteor of ir-t w-meteor is 0.086. rouge_l of ir-t rouge_l is 0.254. w-rouge_l of ir-t w-rouge_l is 0.217. cider of ir-t cider is 0.018. w-cider of ir-t w-cider is 0.014. bleu-1 of ir-t bleu-1 is 0.495. w-bleu-1 of ir-t w-bleu-1 is 0.470. human of ir-t human is 2.43. kappa of ir-t kappa is 0.64. meteor of ir-tc meteor is 0.127. w-meteor of ir-tc w-meteor is 0.101. rouge_l of ir-tc rouge_l is 0.266. w-rouge_l of ir-tc w-rouge_l is 0.225. cider of ir-tc cider is 0.056. w-cider of ir-tc w-cider is 0.044. bleu-1 of ir-tc bleu-1 is 0.474. w-bleu-1 of ir-tc w-bleu-1 is 0.436. human of ir-tc human is 2.57. kappa of ir-tc kappa is 0.71. meteor of seq2seq meteor is 0.064. w-meteor of seq2seq w-meteor is 0.047. rouge_l of seq2seq rouge_l is 0.196. w-rouge_l of seq2seq w-rouge_l is 0.150. cider of seq2seq cider is 0.011. w-cider of seq2seq w-cider is 0.008. bleu-1 of seq2seq bleu-1 is 0.374. w-bleu-1 of seq2seq w-bleu-1 is 0.320. human of seq2seq human is 1.68. kappa of seq2seq kappa is 0.83. meteor of att meteor is 0.080. w-meteor of att w-meteor is 0.058. rouge_l of att rouge_l is 0.246. w-rouge_l of att w-rouge_l is 0.186. cider of att cider is 0.010. w-cider of att w-cider is 0.007. bleu-1 of att bleu-1 is 0.481. w-bleu-1 of att w-bleu-1 is 0.453. human of att human is 1.81. kappa of att kappa is 0.79. meteor of att-tc meteor is 0.114. w-meteor of att-tc w-meteor is 0.082. rouge_l of att-tc rouge_l is 0.299. w-rouge_l of att-tc w-rouge_l is 0.223. cider of att-tc cider is 0.023. w-cider of att-tc w-cider is 0.017. bleu-1 of att-tc bleu-1 is 0.602. w-bleu-1 of att-tc w-bleu-1 is 0.551. human of att-tc human is 2.26. kappa of att-tc kappa is 0.69. meteor of gann meteor is 0.097. w-meteor of gann w-meteor is 0.075. rouge_l of gann rouge_l is 0.282. w-rouge_l of gann w-rouge_l is 0.222. cider of gann cider is 0.010. w-cider of gann w-cider is 0.008. bleu-1 of gann bleu-1 is 0.312. w-bleu-1 of gann w-bleu-1 is 0.278. human of gann human is 2.06. kappa of gann kappa is 0.73. meteor of deepcom meteor is 0.181. w-meteor of deepcom w-meteor is 0.138. rouge_l of deepcom rouge_l is 0.317. w-rouge_l of deepcom w-rouge_l is 0.250. cider of deepcom cider is 0.029. w-cider of deepcom w-cider is 0.023. bleu-1 of deepcom bleu-1 is 0.721. w-bleu-1 of deepcom w-bleu-1 is 0.656. human of deepcom human is 3.58. kappa of deepcom kappa is 0.65. meteor of ir-t meteor is 0.114. rouge_l of ir-t rouge_l is 0.214. cider of ir-t cider is 0.014. bleu-1 of ir-t bleu-1 is 0.472. human of ir-t human is 2.71. kappa of ir-t kappa is 0.67. meteor of ir-tc meteor is 0.117. rouge_l of ir-tc rouge_l is 0.219. cider of ir-tc cider is 0.017. bleu-1 of ir-tc bleu-1 is 0.483. human of ir-tc human is 2.86. kappa of ir-tc kappa is 0.61. meteor of seq2seq meteor is 0.061. rouge_l of seq2seq rouge_l is 0.203. cider of seq2seq cider is 0.011. bleu-1 of seq2seq bleu-1 is 0.365. human of seq2seq human is 2.26. kappa of seq2seq kappa is 0.68. meteor of att meteor is 0.075. rouge_l of att rouge_l is 0.217. cider of att cider is 0.017. bleu-1 of att bleu-1 is 0.462. human of att human is 2.29. kappa of att kappa is 0.78. meteor of att-tc meteor is 0.089. rouge_l of att-tc rouge_l is 0.246. cider of att-tc cider is 0.022. bleu-1 of att-tc bleu-1 is 0.515. human of att-tc human is 2.74. kappa of att-tc kappa is 0.63. meteor of gann meteor is 0.079. rouge_l of gann rouge_l is 0.228. cider of gann cider is 0.019. bleu-1 of gann bleu-1 is 0.496. human of gann human is 2.52. kappa of gann kappa is 0.64. meteor of deepcom meteor is 0.107. rouge_l of deepcom rouge_l is 0.263. cider of deepcom cider is 0.024. bleu-1 of deepcom bleu-1 is 0.665. human of deepcom human is 3.35. kappa of deepcom kappa is 0.68.
table 4 shows results for small labeled training data. given the performance with the full dataset, we show bert trained only with the al data. acc of bert acc is 0.876. acc of bert acc is 0.886. acc of bigru acc is 0.83. acc of bigru acc is 0.879.
table 4 shows the performance (bleu) of base cmlm with different amounts of mask-predict iterations (t ) on wmt’14 en-de, bucketed by target sequence length (n). decoding with (cid:96) = 1 length candidates. bleu of 1 ? n < 10 t = 4 is 21.8. bleu of 1 ? n < 10 t = 10 is 22.4. bleu of 1 ? n < 10 t = n is 22.4. bleu of 10 ? n < 20 t = 4 is 24.6. bleu of 10 ? n < 20 t = 10 is 25.9. bleu of 10 ? n < 20 t = n is 26. bleu of 20 ? n < 30 t = 4 is 24.9. bleu of 20 ? n < 30 t = 10 is 26.7. bleu of 20 ? n < 30 t = n is 27.1. bleu of 30 ? n < 40 t = 4 is 24.9. bleu of 30 ? n < 40 t = 10 is 26.7. bleu of 30 ? n < 40 t = n is 27.6. bleu of 40 ? n t = 4 is 25. bleu of 40 ? n t = 10 is 27.5. bleu of 40 ? n t = n is 28.1.
table 2 shows human evaluation results. the values in parentheses are standard deviations. average of proposed average is 4.11. score of proposed score is (0.99). average of baseline average is 2.18. score of baseline score is (1.17).
table 9 shows results of crfs on con dataset. accuracy of meb qc3 is 56.67. accuracy of meb ta is 63.29. accuracy of b-lstmp qc3 is 65.15. accuracy of b-lstmp ta is 66.93. accuracy of mee qc3 is 59.94. accuracy of mee ta is 59.55. accuracy of crf (lc-no) qc3 is 62.20. accuracy of crf (lc-no) ta is 60.30. accuracy of crf (lc-lc) qc3 is 62.35. accuracy of crf (lc-lc) ta is 60.30. accuracy of crf (lc-lc1) qc3 is 65.94. accuracy of crf (lc-lc1) ta is 61.58. accuracy of crf (lc-fc1) qc3 is 61.18. accuracy of crf (lc-fc1) ta is 60.00. accuracy of crf (fc-fc) qc3 is 64.54. accuracy of crf (fc-fc) ta is 61.64.
table 4 shows parsing results (las) on test sets for different sentence boundaries. las of gold wsj is 90.22. las of gold switchboard is 84.99. las of gold wsj* is 88.71. las of marmot wsj is 89.81. las of marmot switchboard is 78.93. las of marmot wsj* is 83.37. las of nosyntax wsj is 89.95. las of nosyntax switchboard is 80.30†. las of nosyntax wsj* is 83.61. las of joint wsj is 89.71. las of joint switchboard is 79.97†. las of joint wsj* is 85.66†‡. las of joint-reparsed wsj is 89.93. las of joint-reparsed switchboard is 80.61†‡∗. las of joint-reparsed wsj* is 85.38†‡.
table 4 shows best f1 measures on the accurat evaluation sets f1 of 1:1 en-de is 96.0. f1 of 1:1 en-el is 89.5. f1 of 1:1 en-et is 88.9. f1 of 1:1 en-lt is 93.1. f1 of 1:1 en-lv is 95.0. f1 of 1:1 en-ro is 99.4. f1 of 1:1 en-sl is 88.5. f1 of 1:1 en-de is 96.7. f1 of 1:1 en-el is 88.0. f1 of 1:1 en-et is 92.0. f1 of 1:1 en-lt is 96.1. f1 of 1:1 en-lv is 96.6. f1 of 1:1 en-ro is 98.8. f1 of 1:1 en-sl is 89.5. f1 of 2:1 en-de is 83.4. f1 of 2:1 en-el is 83.2. f1 of 2:1 en-et is 73.9. f1 of 2:1 en-lt is 81.2. f1 of 2:1 en-lv is 83.8. f1 of 2:1 en-ro is 95.3. f1 of 2:1 en-sl is 81.6. f1 of 2:1 en-de is 89.2. f1 of 2:1 en-el is 83.2. f1 of 2:1 en-et is 79.9. f1 of 2:1 en-lt is 86.9. f1 of 2:1 en-lv is 88.2. f1 of 2:1 en-ro is 95.3. f1 of 2:1 en-sl is 82.3. f1 of 100:1 en-de is 16.6. f1 of 100:1 en-el is 22.7. f1 of 100:1 en-et is 34.2. f1 of 100:1 en-lt is 45.1. f1 of 100:1 en-lv is 45.1. f1 of 100:1 en-ro is 70.4. f1 of 100:1 en-sl is 24.9. f1 of 100:1 en-de is 33.7. f1 of 100:1 en-el is 37.3. f1 of 100:1 en-et is 42.5. f1 of 100:1 en-lt is 56.0. f1 of 100:1 en-lv is 56.2. f1 of 100:1 en-ro is 75.7. f1 of 100:1 en-sl is 35.3.
table 4 shows performance of the basic ed model. ann uses pre-trained word embeddings while annrandom uses randomly initialized embeddings. pre of nguyen’s cnn (2015) pre is 71.8. rec of nguyen’s cnn (2015) rec is 66.4. f1 of nguyen’s cnn (2015) f1 is 69.0. pre of chen’s dmcnn (2015) pre is 75.6. rec of chen’s dmcnn (2015) rec is 63.6. f1 of chen’s dmcnn (2015) f1 is 69.1. pre of liu’s approach (2016) pre is 75.3. rec of liu’s approach (2016) rec is 64.4. f1 of liu’s approach (2016) f1 is 69.4. pre of ann (ours) pre is 79.5. rec of ann (ours) rec is 60.7. f1 of ann (ours) f1 is 68.8. pre of ann-random (ours) pre is 81.0. rec of ann-random (ours) rec is 49.5. f1 of ann-random (ours) f1 is 61.5.
table 5 shows results of manual evaluations. precision (%) of ann precision (%) is 77.5. precision (%) of sf precision (%) is 72.0. precision (%) of rf precision (%) is 71.0. precision (%) of sl precision (%) is 79.5. precision (%) of psl-based approach precision (%) is 81.0.
table 2 shows comparison of different methods on english event detection. p of maxent p is 76.2. r of maxent r is 60.5. f of maxent f is 67.4. p of maxent p is 74.5. r of maxent r is 59.1. f of maxent f is 65.9. p of cross-event p is n/a. r of cross-event r is n/a. f of cross-event f is n/a. p of cross-event p is 68.7. r of cross-event r is 68.9. f of cross-event f is 68.8. p of cross-entity p is n/a. r of cross-entity r is n/a. f of cross-entity f is n/a. p of cross-entity p is 72.9. r of cross-entity r is 64.3. f of cross-entity f is 68.3. p of joint model p is 76.9. r of joint model r is 65.0. f of joint model f is 70.4. p of joint model p is 73.7. r of joint model r is 62.3. f of joint model f is 67.5. p of pr p is n/a. r of pr r is n/a. f of pr f is n/a. p of pr p is 68.9. r of pr r is 72.0. f of pr f is 70.4. p of cnn p is 80.4. r of cnn r is 67.7. f of cnn f is 73.5. p of cnn p is 75.6. r of cnn r is 63.6. f of cnn f is 69.1. p of rnn p is 73.2. r of rnn r is 63.5. f of rnn f is 67.4. p of rnn p is 67.3. r of rnn r is 59.9. f of rnn f is 64.2. p of lstm p is 78.6. r of lstm r is 67.4. f of lstm f is 72.6. p of lstm p is 74.5. r of lstm r is 60.7. f of lstm f is 66.9. p of bi-lstm p is 80.1. r of bi-lstm r is 69.4. f of bi-lstm f is 74.3. p of bi-lstm p is 81.6. r of bi-lstm r is 62.3. f of bi-lstm f is 70.6. p of hnn p is 80.8. r of hnn r is 71.5. f of hnn f is 75.9. p of hnn p is 84.6. r of hnn r is 64.9. f of hnn f is 73.4.
table 4 shows results on spanish event detection. p of lstm p is 62.2. r of lstm r is 52.9. f of lstm f is 57.2. p of lstm p is 56.9. r of lstm r is 32.6. f of lstm f is 41.6. p of bi-lstm p is 76.2. r of bi-lstm r is 63.1. f of bi-lstm f is 68.7. p of bi-lstm p is 61.5. r of bi-lstm r is 42.2. f of bi-lstm f is 50.1. p of hnn p is 81.4. r of hnn r is 65.2. f of hnn f is 71.6. p of hnn p is 66.3. r of hnn r is 47.8. f of hnn f is 55.5.
table 5 shows svm classification results across linguistic features (lf, bigrams here (mukherjee et al., 2013b)), behavioral features (bf: rl, rd, mcs (mukherjee et al., 2013b)); the svm classification results by the intuitive method that finding the most similar existing review by edit distance ratio and take the found reviewers’ behavioral features as approximation (bf editsim+lf), and results by the intuitive method that finding the most similar existing review by averaged pre-trained word embeddings (using word2vec) (bf w2vsim+w2v); and the svm classification results across the learnt review embeddings (re), the learnt review’s rating embeddings (rre), the learnt product’s average rating embeddings (pre) by our model. improvements of our model are statistically significant with p<0.005 based on paired t-test. p of lf hotel is 54.5. p of lf restaurant is 53.8. r of lf hotel is 71.1. r of lf restaurant is 80.8. f1 of lf hotel is 61.7. f1 of lf restaurant is 64.6. a of lf hotel is 55.9. a of lf restaurant is 55.8. p of lf+bf hotel is 63.4. p of lf+bf restaurant is 58.1. r of lf+bf hotel is 52.6. r of lf+bf restaurant is 61.2. f1 of lf+bf hotel is 57.5. f1 of lf+bf restaurant is 59.6. a of lf+bf hotel is 61.1. a of lf+bf restaurant is 58.5. p of bf editsim+lf hotel is 55.3. p of bf editsim+lf restaurant is 53.9. r of bf editsim+lf hotel is 69.7. r of bf editsim+lf restaurant is 82.82. f1 of bf editsim+lf hotel is 61.6. f1 of bf editsim+lf restaurant is 65.1. a of bf editsim+lf hotel is 56.6. a of bf editsim+lf restaurant is 56.0. p of bf w2vsim+w2v hotel is 58.4. p of bf w2vsim+w2v restaurant is 56.3. r of bf w2vsim+w2v hotel is 65.9. r of bf w2vsim+w2v restaurant is 73.4. f1 of bf w2vsim+w2v hotel is 61.9. f1 of bf w2vsim+w2v restaurant is 63.7. a of bf w2vsim+w2v hotel is 59.5. a of bf w2vsim+w2v restaurant is 58.2. p of ours re hotel is 62.1. p of ours re restaurant is 58.4. r of ours re hotel is 68.3. r of ours re restaurant is 75.1. f1 of ours re hotel is 65.1. f1 of ours re restaurant is 65.7. a of ours re hotel is 63.3. a of ours re restaurant is 60.8. p of ours re+rre+pre hotel is 63.6. p of ours re+rre+pre restaurant is 59.0. r of ours re+rre+pre hotel is 71.2. r of ours re+rre+pre restaurant is 78.8. f1 of ours re+rre+pre hotel is 67.2. f1 of ours re+rre+pre restaurant is 67.5. a of ours re+rre+pre hotel is 65.3. a of ours re+rre+pre restaurant is 62.0.
table 2 shows the performance of predicting present keyphrases of various models on five benchmark datasets f1@5 of tf-idf f1@5 is 0.221. f1@10 of tf-idf f1@10 is 0.313. f1@5 of tf-idf f1@5 is 0.129. f1@10 of tf-idf f1@10 is 0.160. f1@5 of tf-idf f1@5 is 0.136. f1@10 of tf-idf f1@10 is 0.184. f1@5 of tf-idf f1@5 is 0.128. f1@10 of tf-idf f1@10 is 0.194. f1@5 of tf-idf f1@5 is 0.102. f1@10 of tf-idf f1@10 is 0.126. f1@5 of textrank f1@5 is 0.223. f1@10 of textrank f1@10 is 0.281. f1@5 of textrank f1@5 is 0.189. f1@10 of textrank f1@10 is 0.162. f1@5 of textrank f1@5 is 0.195. f1@10 of textrank f1@10 is 0.196. f1@5 of textrank f1@5 is 0.176. f1@10 of textrank f1@10 is 0.187. f1@5 of textrank f1@5 is 0.175. f1@10 of textrank f1@10 is 0.147. f1@5 of singlerank f1@5 is 0.214. f1@10 of singlerank f1@10 is 0.306. f1@5 of singlerank f1@5 is 0.189. f1@10 of singlerank f1@10 is 0.162. f1@5 of singlerank f1@5 is 0.140. f1@10 of singlerank f1@10 is 0.173. f1@5 of singlerank f1@5 is 0.135. f1@10 of singlerank f1@10 is 0.176. f1@5 of singlerank f1@5 is 0.096. f1@10 of singlerank f1@10 is 0.119. f1@5 of expandrank f1@5 is 0.210. f1@10 of expandrank f1@10 is 0.304. f1@5 of expandrank f1@5 is 0.081. f1@10 of expandrank f1@10 is 0.126. f1@5 of expandrank f1@5 is 0.132. f1@10 of expandrank f1@10 is 0.164. f1@5 of expandrank f1@5 is 0.139. f1@10 of expandrank f1@10 is 0.170. f1@5 of expandrank f1@5 is n/a. f1@10 of expandrank f1@10 is n/a. f1@5 of maui f1@5 is 0.040. f1@10 of maui f1@10 is 0.042. f1@5 of maui f1@5 is 0.249. f1@10 of maui f1@10 is 0.216. f1@5 of maui f1@5 is 0.249. f1@10 of maui f1@10 is 0.268. f1@5 of maui f1@5 is 0.044. f1@10 of maui f1@10 is 0.039. f1@5 of maui f1@5 is 0.270. f1@10 of maui f1@10 is 0.230. f1@5 of kea f1@5 is 0.098. f1@10 of kea f1@10 is 0.126. f1@5 of kea f1@5 is 0.110. f1@10 of kea f1@10 is 0.152. f1@5 of kea f1@5 is 0.069. f1@10 of kea f1@10 is 0.084. f1@5 of kea f1@5 is 0.025. f1@10 of kea f1@10 is 0.026. f1@5 of kea f1@5 is 0.171. f1@10 of kea f1@10 is 0.154. f1@5 of rnn f1@5 is 0.085. f1@10 of rnn f1@10 is 0.064. f1@5 of rnn f1@5 is 0.135. f1@10 of rnn f1@10 is 0.088. f1@5 of rnn f1@5 is 0.169. f1@10 of rnn f1@10 is 0.127. f1@5 of rnn f1@5 is 0.157. f1@10 of rnn f1@10 is 0.124. f1@5 of rnn f1@5 is 0.179. f1@10 of rnn f1@10 is 0.189. f1@5 of copyrnn f1@5 is 0.278. f1@10 of copyrnn f1@10 is 0.342. f1@5 of copyrnn f1@5 is 0.311. f1@10 of copyrnn f1@10 is 0.266. f1@5 of copyrnn f1@5 is 0.334. f1@10 of copyrnn f1@10 is 0.326. f1@5 of copyrnn f1@5 is 0.293. f1@10 of copyrnn f1@10 is 0.304. f1@5 of copyrnn f1@5 is 0.333. f1@10 of copyrnn f1@10 is 0.262.
table 4 shows comparison of our proposed methods on spanish-french and german-french translation tasks from the europarl corpus. english is treated as the pivot language. bleu of sent-greedy dev is 31.00. bleu of sent-greedy test is 31.05. bleu of sent-greedy dev is 22.34. bleu of sent-greedy test is 21.88. bleu of sent-beam dev is 31.57. bleu of sent-beam test is 31.64. bleu of sent-beam dev is 24.95. bleu of sent-beam test is 24.39. bleu of word-greedy dev is 31.37. bleu of word-greedy test is 31.92. bleu of word-greedy dev is 24.72. bleu of word-greedy test is 25.15. bleu of word-beam dev is 30.81. bleu of word-beam test is 31.21. bleu of word-beam dev is 24.64. bleu of word-beam test is 24.19. bleu of word-sampling dev is 33.65. bleu of word-sampling test is 33.86. bleu of word-sampling dev is 26.99. bleu of word-sampling test is 27.03.
table 1 shows results on the celex dataset accuracy of med (kann and schutze 2016a) 13sia is 83.9. accuracy of med (kann and schutze 2016a) 2pie is 95. accuracy of med (kann and schutze 2016a) 2pke is 87.6. accuracy of med (kann and schutze 2016a) rp is 84. avg accuracy of med (kann and schutze 2016a) avg. is 87.62. accuracy of nwfst (rastogi et al. 2016) 13sia is 86.8. accuracy of nwfst (rastogi et al. 2016) 2pie is 94.8. accuracy of nwfst (rastogi et al. 2016) 2pke is 87.9. accuracy of nwfst (rastogi et al. 2016) rp is 81.1. avg accuracy of nwfst (rastogi et al. 2016) avg. is 87.65. accuracy of lat (dreyer et al. 2008) 13sia is 87.5. accuracy of lat (dreyer et al. 2008) 2pie is 93.4. accuracy of lat (dreyer et al. 2008) 2pke is 87.4. accuracy of lat (dreyer et al. 2008) rp is 84.9. avg accuracy of lat (dreyer et al. 2008) avg. is 88.3. accuracy of soft 13sia is 83.1. accuracy of soft 2pie is 93.8. accuracy of soft 2pke is 88. accuracy of soft rp is 83.2. avg accuracy of soft avg. is 87. accuracy of hard 13sia is 85.8. accuracy of hard 2pie is 95.1. accuracy of hard 2pke is 89.5. accuracy of hard rp is 87.2. avg accuracy of hard avg. is 89.44.
table 1 shows performance of the reasoning module on manually formalized pre-university problems success % of dev success % is 75.3% (131/174). avg. time of dev avg. time is 10.5s. timeout of dev timeout is 16.70%. other of dev other is 8.10%. success % of test success % is 78.2% (172/220). avg. time of test avg. time is 16.2s. timeout of test timeout is 15.00%. other of test other is 6.80%.
table 1 shows unlabeled dependency accuracy results with the 5x5 models and test sets. ∇ shows the slope of deterioration in parser performance. uas of baseline 0% is 91.43. uas of baseline 5% is 89.99. uas of baseline 10% is 87.84. uas of baseline 15% is 85.64. uas of baseline 20% is 84.12. uas of e05 0% is 91.12. uas of e05 5% is 90. uas of e05 10% is 87.99. uas of e05 15% is 86.18. uas of e05 20% is 84.78. uas of e10 0% is 90.87. uas of e10 5% is 89.87. uas of e10 10% is 88.07. uas of e10 15% is 86.54. uas of e10 20% is 85.28. uas of e15 0% is 90.61. uas of e15 5% is 89.72. uas of e15 10% is 88.14. uas of e15 15% is 86.75. uas of e15 20% is 85.5. uas of e20 0% is 90.29. uas of e20 5% is 89.48. uas of e20 10% is 88.04. uas of e20 15% is 86.82. uas of e20 20% is 85.76.
table 3 shows performance comparison. examples/s are normalized by the number of gpus used in the training job. flops are computed assuming that source and target sequence length are both 50. example/s of convs2s examples/s is 80. flops of convs2s flops is 15.7b. params of convs2s params is 263.4m. example/s of trans. base examples/s is 160. flops of trans. base flops is 6.2b. params of trans. base params is 93.3m. example/s of trans. big examples/s is 50. flops of trans. big flops is 31.2b. params of trans. big params is 375.4m. example/s of rnmt+ examples/s is 30. flops of rnmt+ flops is 28.1b. params of rnmt+ params is 378.9m.
table 5 shows human evaluation results on grammaticality (gram), informativeness (info), and relevance (rel) of arguments. our model with separate decoder and attention over keyphrases receives significantly better ratings in informativeness and relevance than seq2seq (one-way anova, p < 0.005). gram of retrieval gram is 4.5 } 0.6. info of retrieval  info is 3.7 } 0.9. rel of retrieval rel is 3.3 } 1.1. gram of seq2seq gram is 3.3 } 1.1. info of seq2seq  info is 1.2 } 0.5. rel of seq2seq rel is 1.4 } 0.7. gram of our model gram is 2.5 } 0.8. info of our model  info is 1.6 } 0.8. rel of our model rel is 1.8 } 0.8.
table 1 shows parsing accuracy of the best existing grammar-free and -based models as well as our shrg-based model. results are copied from (oepen et al., 2015; peng et al., 2017a; buys and blunsom, 2017). accuracy of no sdg is 89.4. accuracy of no eds is 85.48. accuracy of no dmrs is 84.16. accuracy of unification sdg is 92.8. accuracy of unification eds is 89.58. accuracy of unification dmrs is 89.64. accuracy of rewriting eds is 90.39. accuracy of rewriting dmrs is 89.51.
table 2 shows detection performance (trigger identification plus multi-class classification) p (%) of msep-emd p (%) is 70.4. r (%) of msep-emd r (%) is 65. f (%) of msep-emd f (%) is 67.6. p (%) of cross-event p (%) is 68.8. r (%) of cross-event r (%) is 68.9. f (%) of cross-event f (%) is 68.8. p (%) of cross-entity p (%) is 72.9. r (%) of cross-entity r (%) is 64.3. f (%) of cross-entity f (%) is 68.3. p (%) of joint (local+global) p (%) is 73.7. r (%) of joint (local+global) r (%) is 62.3. f (%) of joint (local+global) f (%) is 67.5. p (%) of cnn p (%) is 71.8. r (%) of cnn r (%) is 66.4. f (%) of cnn f (%) is 69. p (%) of dm-cnn p (%) is 75.6. r (%) of dm-cnn r (%) is 63.6. f (%) of dm-cnn f (%) is 69.1. f (%) of nc-cnn f (%) is 71.3. p (%) of fb-rnn (gru) p (%) is 66.8. r (%) of fb-rnn (gru) r (%) is 68. f (%) of fb-rnn (gru) f (%) is 67.4. p (%) of bi-rnn (gru) p (%) is 66. r (%) of bi-rnn (gru) r (%) is 73. f (%) of bi-rnn (gru) f (%) is 69.3. p (%) of anns (ace+fn) p (%) is 77.6. r (%) of anns (ace+fn) r (%) is 65.2. f (%) of anns (ace+fn) f (%) is 70.7. p (%) of dm-cnn?(ace+wiki) p (%) is 75.7. r (%) of dm-cnn?(ace+wiki) r (%) is 66. f (%) of dm-cnn?(ace+wiki) f (%) is 70.5. p (%) of ann-s2 (ace+fn) p (%) is 76.8. r (%) of ann-s2 (ace+fn) r (%) is 67.5. f (%) of ann-s2 (ace+fn) f (%) is 71.9. p (%) of hybrid: bi-lstm+cnn p (%) is 84.6. r (%) of hybrid: bi-lstm+cnn r (%) is 64.9. f (%) of hybrid: bi-lstm+cnn f (%) is 73.4. p (%) of self: bi-lstm+gan p (%) is 71.3. r (%) of self: bi-lstm+gan r (%) is 74.7. f (%) of self: bi-lstm+gan f (%) is 73.
table 6 shows results on mcnc task acc.(%) of (chambers and jurafsky 2008) acc.(%) is 30.92. acc.(%) of (granroth-wilding and clark 2016) acc.(%) is 43.28. acc.(%) of (pichotta and mooney 2016) acc.(%) is 43.17. acc.(%) of (wang et al. 2017) acc.(%) is 46.67. acc.(%) of our results acc.(%) is 48.83.
table 5 shows results for key-value knowledge retrieval (cn5sel, 50 facts). subj/obj and integration. means: we attend over the fact subject (key) and take the weighted fact object as value (value). cn accurary of subj/obj dev is 76.65. accurary of subj/obj test is 71.52. accurary of subj/obj dev is 71.85. accurary of subj/obj test is 67.64. accurary of obj/obj dev is 76.7. accurary of obj/obj test is 71.28. accurary of obj/obj dev is 71.25. accurary of obj/obj test is 67.48.
table 4 shows human evaluation results on judging the homogeneity of latent actions in smd. exp agree of di-vae exp agree is 85.60%. worker κ of di-vae worker κ is 0.52. match rate of di-vae match rate is 71.30%. exp agree of di-vst exp agree is 93.30%. worker κ of di-vst worker κ is 0.48. match rate of di-vst match rate is 74.90%.
table 4 shows results on the human evaluation. +2 of seq2seq-att +2 is 29.32%. +1 of seq2seq-att +1 is 25.27%. +0 of seq2seq-att +0 is 45.41%. kappa of seq2seq-att kappa is 0.448. +2 of mmi-bidi +2 is 30.40%. +1 of mmi-bidi +1 is 24.85%. +0 of mmi-bidi +0 is 44.75%. kappa of mmi-bidi kappa is 0.471. +2 of marm +2 is 20.11%. +1 of marm +1 is 27.96%. +0 of marm +0 is 51.93%. kappa of marm kappa is 0.404. +2 of seq2seq+idf +2 is 28.81%. +1 of seq2seq+idf +1 is 23.87%. +0 of seq2seq+idf +0 is 47.33%. kappa of seq2seq+idf kappa is 0.418. +2 of sc-seq2seqniwf s=1 +2 is 42.47%. +1 of sc-seq2seqniwf s=1 +1 is 14.29%. +0 of sc-seq2seqniwf s=1 +0 is 43.24%. kappa of sc-seq2seqniwf s=1 kappa is 0.507. +2 of sc-seq2seqniwf s=0.5 +2 is 20.62%. +1 of sc-seq2seqniwf s=0.5 +1 is 40.16%. +0 of sc-seq2seqniwf s=0.5 +0 is 39.22%. kappa of sc-seq2seqniwf s=0.5 kappa is 0.451. +2 of sc-seq2seqniwf s=0 +2 is 14.34%. +1 of sc-seq2seqniwf s=0 +1 is 46.38%. +0 of sc-seq2seqniwf s=0 +0 is 39.28%. kappa of sc-seq2seqniwf s=0 kappa is 0.526.
table 2 shows experiment results on ace2005 and kbpeval2017. * indicates the result adapted from the original paper. for kbpeval2017, “trigger identification” corresponds to the “span” metric and “trigger classification” corresponds to the “type” metric reported in official evaluation. p of fbrnn(char) p is 61.3. r of fbrnn(char) r is 45.6. f1 of fbrnn(char) f1 is 52.3. p of fbrnn(char) p is 57.5. r of fbrnn(char) r is 42.8. f1 of fbrnn(char) f1 is 49.1. p of fbrnn(char) p is 57.97. r of fbrnn(char) r is 36.92. f1 of fbrnn(char) f1 is 45.11. p of fbrnn(char) p is 51.71. r of fbrnn(char) r is 32.94. f1 of fbrnn(char) f1 is 40.24. p of dmcnn(char) p is 60.1. r of dmcnn(char) r is 61.6. f1 of dmcnn(char) f1 is 60.9. p of dmcnn(char) p is 57.1. r of dmcnn(char) r is 58.5. f1 of dmcnn(char) f1 is 57.8. p of dmcnn(char) p is 53.67. r of dmcnn(char) r is 49.92. f1 of dmcnn(char) f1 is 51.73. p of dmcnn(char) p is 50.03. r of dmcnn(char) r is 46.53. f1 of dmcnn(char) f1 is 48.22. p of c-bilstm* p is 65.6. r of c-bilstm* r is 66.7. f1 of c-bilstm* f1 is 66.1. p of c-bilstm* p is 60.0. r of c-bilstm* r is 60.9. f1 of c-bilstm* f1 is 60.4. p of fbrnn(word) p is 64.1. r of fbrnn(word) r is 63.7. f1 of fbrnn(word) f1 is 63.9. p of fbrnn(word) p is 59.9. r of fbrnn(word) r is 59.6. f1 of fbrnn(word) f1 is 59.7. p of fbrnn(word) p is 65.10. r of fbrnn(word) r is 46.86. f1 of fbrnn(word) f1 is 54.50. p of fbrnn(word) p is 60.05. r of fbrnn(word) r is 43.22. f1 of fbrnn(word) f1 is 50.27. p of dmcnn(word) p is 66.6. r of dmcnn(word) r is 63.6. f1 of dmcnn(word) f1 is 65.1. p of dmcnn(word) p is 61.6. r of dmcnn(word) r is 58.8. f1 of dmcnn(word) f1 is 60.2. p of dmcnn(word) p is 60.43. r of dmcnn(word) r is 51.64. f1 of dmcnn(word) f1 is 55.69. p of dmcnn(word) p is 54.81. r of dmcnn(word) r is 46.84. f1 of dmcnn(word) f1 is 50.51. p of hnn* p is 74.2. r of hnn* r is 63.1. f1 of hnn* f1 is 68.2. p of hnn* p is 77.1. r of hnn* r is 53.1. f1 of hnn* f1 is 63.0. p of rich-c* p is 62.2. r of rich-c* r is 71.9. f1 of rich-c* f1 is 66.7. p of rich-c* p is 58.9. r of rich-c* r is 68.1. f1 of rich-c* f1 is 63.2. p of kbp2017 best* p is 67.76. r of kbp2017 best* r is 45.92. f1 of kbp2017 best* f1 is 54.74. p of kbp2017 best* p is 62.69. r of kbp2017 best* r is 42.48. f1 of kbp2017 best* f1 is 50.64. p of npn(concat) p is 76.5. r of npn(concat) r is 59.8. f1 of npn(concat) f1 is 67.1. p of npn(concat) p is 72.8. r of npn(concat) r is 56.9. f1 of npn(concat) f1 is 63.9. p of npn(concat) p is 64.58. r of npn(concat) r is 50.31. f1 of npn(concat) f1 is 56.56. p of npn(concat) p is 59.14. r of npn(concat) r is 46.07. f1 of npn(concat) f1 is 51.80. p of npn(general) p is 71.5. r of npn(general) r is 63.2. f1 of npn(general) f1 is 67.1. p of npn(general) p is 67.3. r of npn(general) r is 59.6. f1 of npn(general) f1 is 63.2. p of npn(general) p is 63.67. r of npn(general) r is 51.32. f1 of npn(general) f1 is 56.83. p of npn(general) p is 57.78. r of npn(general) r is 46.58. f1 of npn(general) f1 is 51.57. p of npn(task-specific) p is 64.8. r of npn(task-specific) r is 73.8. f1 of npn(task-specific) f1 is 69.0. p of npn(task-specific) p is 60.9. r of npn(task-specific) r is 69.3. f1 of npn(task-specific) f1 is 64.8. p of npn(task-specific) p is 64.32. r of npn(task-specific) r is 53.16. f1 of npn(task-specific) f1 is 58.21. p of npn(task-specific) p is 57.63. r of npn(task-specific) r is 47.63. f1 of npn(task-specific) f1 is 52.15.
table 4 shows results on the development, public test (test-p) and hidden test (test-h) sets. for each model, we report both accuracy and consistency. acc. of majority acc. is 55.3. acc. of majority acc. is 56.2. acc. of majority acc. is 55.4. acc. of maxent acc. is 68.0. acc. of maxent acc. is 67.7. acc. of maxent acc. is 67.8. acc. of rule acc. is 66.0. con. of rule con. is 29.2. acc. of rule acc. is 66.3. con. of rule con. is 32.7. acc. of sup. acc. is 67.7. con. of sup. con. is 36.7. acc. of sup. acc. is 66.9. con. of sup. con. is 38.3. acc. of sup.+disc acc. is 77.7. con. of sup.+disc con. is 52.4. acc. of sup.+disc acc. is 76.6. con. of sup.+disc con. is 51.8. acc. of weaksup. acc. is 84.3. con. of weaksup. con. is 66.3. acc. of weaksup. acc. is 81.7. con. of weaksup. con. is 60.1. acc. of w.+disc acc. is 85.7. con. of w.+disc con. is 67.4. acc. of w.+disc acc. is 84.0. con. of w.+disc con. is 65.0. acc. of w.+disc acc. is 82.5. con. of w.+disc con. is 63.9.
table 5 shows human evaluation results for question generation. “grammaticality”, “making sense” and “answerability” are rated on a 1–5 scale (5 for the best, see the supplementary materials for a detailed rating scheme), “average rank” is rated on a 1–3 scale (1 for the most preferred, ties are allowed.) two-tailed t-test results are shown for our method compared to contextnqg (stat. significance is indicated with ∗(p < 0.05), ∗∗(p < 0.01).) grammaticality of contextnqg grammaticality is 3.793. making sense of contextnqg making sense is 3.836. answerability of contextnqg answerability is 3.892. avg. rank of contextnqg avg. rank is 1.768. grammaticality of corefnqg grammaticality is 3.804*. making sense of corefnqg making sense is 3.847**. answerability of corefnqg answerability is 3.895*. avg. rank of corefnqg avg. rank is 1.762. grammaticality of human grammaticality is 3.807. making sense of human making sense is 3.850. answerability of human answerability is 3.902. avg. rank of human avg. rank is 1.758.
table 3 shows human evaluations: ranking of various systems. rank 1st is best and rank 4th, worst. numbers show the percentage of times a system gets ranked at a certain position. percentage of lead 1st is 0.15. percentage of lead 2nd is 0.17. percentage of lead 3rd is 0.47. percentage of lead 4th is 0.21. percentage of pointernet 1st is 0.16. percentage of pointernet 2nd is 0.05. percentage of pointernet 3rd is 0.31. percentage of pointernet 4th is 0.48. percentage of xnet 1st is 0.28. percentage of xnet 2nd is 0.53. percentage of xnet 3rd is 0.15. percentage of xnet 4th is 0.04. percentage of human 1st is 0.41. percentage of human 2nd is 0.25. percentage of human 3rd is 0.07. percentage of human 4th is 0.27.
table 6 shows joint span detection and question generation results on the dense development set, using exact-match for both spans and questions. p of span + local p is 37.8. r of span + local r is 43.7. f of span + local f is 40.6. p of span + seq. (tau = 0.5) p is 39.6. r of span + seq. (tau = 0.5) r is 45.8. f of span + seq. (tau = 0.5) f is 42.4.
table 2 shows performance of our approach on sentence ordering dataset from barzilay and lapata (2008). accuracy of sequential cg accidents is 0.813. accuracy of sequential cg earthquakes is 0.946. accuracy of vlv-gm (2017) accidents is 0.77. accuracy of vlv-gm (2017) earthquakes is 0.931. accuracy of hmm (2012) accidents is 0.822. accuracy of hmm (2012) earthquakes is 0.938. accuracy of hmm+entity (2012) accidents is 0.842. accuracy of hmm+entity (2012) earthquakes is 0.911. accuracy of hmm+content (2012) accidents is 0.742. accuracy of hmm+content (2012) earthquakes is 0.953. accuracy of dm (2017) accidents is 0.93. accuracy of dm (2017) earthquakes is 0.992. accuracy of recursive (2014) accidents is 0.864. accuracy of recursive (2014) earthquakes is 0.976. accuracy of entity-grid (2008) accidents is 0.904. accuracy of entity-grid (2008) earthquakes is 0.872. accuracy of graph (2013) accidents is 0.846. accuracy of graph (2013) earthquakes is 0.635.
table 3 shows f1-score (%) of different passes from 1 to 5 on the test data sets. it shows that appropriate number of passes can boost the performance as well as avoid over-fitting of the model. f1-score of 1 se2 is 71.6. f1-score of 1 se3 is 70.3. f1-score of 1 se13 is 67.0. f1-score of 1 se15 is 72.5. f1-score of 1 all is 70.3. f1-score of 2 se2 is 71.9. f1-score of 2 se3 is 70.2. f1-score of 2 se13 is 67.1. f1-score of 2 se15 is 72.8. f1-score of 2 all is 70.4. f1-score of 3 se2 is 72.2. f1-score of 3 se3 is 70.5. f1-score of 3 se13 is 67.2. f1-score of 3 se15 is 72.6. f1-score of 3 all is 70.6. f1-score of 4 se2 is 72.1. f1-score of 4 se3 is 70.4. f1-score of 4 se13 is 67.2. f1-score of 4 se15 is 72.4. f1-score of 4 all is 70.5. f1-score of 5 se2 is 72.0. f1-score of 5 se3 is 70.4. f1-score of 5 se13 is 67.1. f1-score of 5 se15 is 71.5. f1-score of 5 all is 70.3.
table 2 shows human evaluation results on a sample from conceptual captions. good (out of 3) of conceptual captions 1+ is 96.9%. good (out of 3) of conceptual captions 2+ is 90.3%. good (out of 3) of conceptual captions 3 is 78.5%.
table 1 shows the proportion of images determined to be good representations of their corresponding word. in columns 2-5, we bucket the results by the word’s ground-truth concreteness, while column 6 shows the results over all words. the last row shows the number of words in each bucket of concreteness, and the number of words overall for each language. concreteness ratings of english 1-2 is 0.804. concreteness ratings of english 2-3 is 0.814. concreteness ratings of english 3-4 is 0.855. concreteness ratings of english 4-5 is 0.913. concreteness ratings of english overall is 0.857. concreteness ratings of french 1-2 is 0.622. concreteness ratings of french 2-3 is 0.653. concreteness ratings of french 3-4 is 0.706. concreteness ratings of french 4-5 is 0.828. concreteness ratings of french overall is 0.721. concreteness ratings of indonesian 1-2 is 0.505. concreteness ratings of indonesian 2-3 is 0.569. concreteness ratings of indonesian 3-4 is 0.665. concreteness ratings of indonesian 4-5 is 0.785. concreteness ratings of indonesian overall is 0.661. concreteness ratings of uzbek 1-2 is 0.568. concreteness ratings of uzbek 2-3 is 0.53. concreteness ratings of uzbek 3-4 is 0.594. concreteness ratings of uzbek 4-5 is 0.683. concreteness ratings of uzbek overall is 0.601. concreteness ratings of all 1-2 is 0.628. concreteness ratings of all 2-3 is 0.649. concreteness ratings of all 3-4 is 0.713. concreteness ratings of all 4-5 is 0.81. concreteness ratings of all overall is 0.717. concreteness ratings of # words 1-2 is 77. concreteness ratings of # words 2-3 is 292. concreteness ratings of # words 3-4 is 292. concreteness ratings of # words 4-5 is 302. concreteness ratings of # words overall is 963.
table 3 shows results on the triplets data accuracy of mean-vectors accuracy is 0.65. accuracy of skip-thoughts-cs accuracy is 0.615. accuracy of skip-thoughts-sick accuracy is 0.547. accuracy of triplets-sen accuracy is 0.74.
table 3 shows performance (aucpr) of each noise reduction method; in bold are the best scores. aucpr of cnn+one original is 0.18. aucpr of cnn+one original+hits is 0.183. aucpr of cnn+one original+lsa is 0.173. aucpr of cnn+one original+nmf is 0.178. aucpr of cnn+one original+ensemble is 0.181. aucpr of cnn+att original is 0.234. aucpr of cnn+att original+hits is 0.235. aucpr of cnn+att original+lsa is 0.235. aucpr of cnn+att original+nmf is 0.233. aucpr of cnn+att original+ensemble is 0.236. aucpr of pcnn+one original is 0.231. aucpr of pcnn+one original+hits is 0.234. aucpr of pcnn+one original+lsa is 0.233. aucpr of pcnn+one original+nmf is 0.234. aucpr of pcnn+one original+ensemble is 0.235. aucpr of pcnn+att original is 0.248. aucpr of pcnn+att original+hits is 0.253. aucpr of pcnn+att original+lsa is 0.25. aucpr of pcnn+att original+nmf is 0.252. aucpr of pcnn+att original+ensemble is 0.255.
table 1 shows average alignment performance across images. msfc provides the best recall and lowest aer, and modified k-means the best precision. in all cases, the alignment framework yields stronger results than either of the timing-based baselines. precision of simultaneous precision is 0.42. recall of simultaneous recall is 0.30. aer of simultaneous aer is 0.65. precision of simultaneous precision is 0.49. recall of simultaneous recall is 0.17. aer of simultaneous aer is 0.74. precision of 1-second delay precision is 0.43. recall of 1-second delay recall is 0.31. aer of 1-second delay aer is 0.64. precision of 1-second delay precision is 0.50. recall of 1-second delay recall is 0.17. aer of 1-second delay aer is 0.74. precision of alignment framework precision is 0.43. recall of alignment framework recall is 0.50. aer of alignment framework aer is 0.54. precision of alignment framework precision is 0.56. recall of alignment framework recall is 0.31. aer of alignment framework aer is 0.60.
table 8 shows results on math problems (§5.5). uf1 of depccg uf1 is 88.49. lf1 of depccg lf1 is 66.15. uf1 of + elmo uf1 is 89.32. lf1 of + elmo lf1 is 70.74. uf1 of + proposed uf1 is 95.83. lf1 of + proposed lf1 is 80.53.
table 4 shows unimodal, bimodal and trimodal results of hffn. here, l, a and v denotes language, acoustic and visual modalities, respectively. acc of l acc is 78.59. f1 of l f1 is 78.52. acc of l acc is 81.46. f1 of l f1 is 81.54. acc of a acc is 48.14. f1 of a f1 is 48.3. acc of a acc is 38.08. f1 of a f1 is 38.17. acc of v acc is 56.97. f1 of v f1 is 57.48. acc of v acc is 34.52. f1 of v f1 is 29.15. acc of l+a acc is 78.06. f1 of l+a f1 is 78.29. acc of l+a acc is 80.38. f1 of l+a f1 is 80.6. acc of l+v acc is 79.39. f1 of l+v f1 is 79.38. acc of l+v acc is 80.05. f1 of l+v f1 is 80.26. acc of a+v acc is 55.17. f1 of a+v f1 is 55.76. acc of a+v acc is 55.17. f1 of a+v f1 is 55.79. acc of l+a+v acc is 80.19. f1 of l+a+v f1 is 80.34. acc of l+a+v acc is 82.37. f1 of l+a+v f1 is 82.42.
table 2 shows the overall mention detection results on the test set of ontonotes. the f1 improvement is statistically significant under t-test with p < 0.05. prec. of our full model prec. is 89.6. rec. of our full model rec. is 82.2. f1 of our full model f1 is 85.7. prec. of lee et al. (2018) prec. is 86.2. rec. of lee et al. (2018) rec. is 83.7. f1 of lee et al. (2018) f1 is 84.9.
table 2 shows translation performance on one-to-two, one-to-three and one-to-four translation tasks. rep denotes our proposed representor. emb, attn, and dis represent our proposed language-sensitive methods to address multilingual translation. note that the source language of all our experiments is english. bleu of de nmt baselines is 27.5. bleu of de multi-nmt baselines johnson et al.(2017) is 27.26. bleu of de three-stgy wang et al.(2018) is 27.35. bleu of de rep+emb is 26.6. bleu of de rep+emb+attn is 26.96. bleu of de rep+emb+attn+dis is 27.74. bleu of lv nmt baselines is 16.28. bleu of lv multi-nmt baselines johnson et al.(2017) is 16.32. bleu of lv three-stgy wang et al.(2018) is 16.38. bleu of lv rep+emb is 15.37. bleu of lv rep+emb+attn is 15.87. bleu of lv rep+emb+attn+dis is 16.79. bleu of de nmt baselines is 27.5. bleu of de multi-nmt baselines johnson et al.(2017) is 27.88. bleu of de three-stgy wang et al.(2018) is 27.89. bleu of de rep+emb is 26.96. bleu of de rep+emb+attn is 27.32. bleu of de rep+emb+attn+dis is 27.96. bleu of fi nmt baselines is 16.83. bleu of fi multi-nmt baselines johnson et al.(2017) is 16.47. bleu of fi three-stgy wang et al.(2018) is 16.7. bleu of fi rep+emb is 15.78. bleu of fi rep+emb+attn is 16.58. bleu of fi rep+emb+attn+dis is 16.89. bleu of de nmt baselines is 27.5. bleu of de multi-nmt baselines johnson et al.(2017) is 26.8. bleu of de three-stgy wang et al.(2018) is 26.99. bleu of de rep+emb is 26.08. bleu of de rep+emb+attn is 26.68. bleu of de rep+emb+attn+dis is 27.45. bleu of zh nmt baselines is 26.04. bleu of zh multi-nmt baselines johnson et al.(2017) is 25.54. bleu of zh three-stgy wang et al.(2018) is 25.78. bleu of zh rep+emb is 24.48. bleu of zh rep+emb+attn is 25.33. bleu of zh rep+emb+attn+dis is 26.17. bleu of de nmt baselines is 27.5. bleu of de multi-nmt baselines johnson et al.(2017) is 25.44. bleu of de three-stgy wang et al.(2018) is 25.55. bleu of de rep+emb is 24.82. bleu of de rep+emb+attn is 25.45. bleu of de rep+emb+attn+dis is 26.06. bleu of zh nmt baselines is 26.04. bleu of zh multi-nmt baselines johnson et al.(2017) is 24.87. bleu of zh three-stgy wang et al.(2018) is 25.63. bleu of zh rep+emb is 24.12. bleu of zh rep+emb+attn is 24.93. bleu of zh rep+emb+attn+dis is 26.12. bleu of fi nmt baselines is 16.83. bleu of fi multi-nmt baselines johnson et al.(2017) is 16.86. bleu of fi three-stgy wang et al.(2018) is 16.97. bleu of fi rep+emb is 16.06. bleu of fi rep+emb+attn is 16.78. bleu of fi rep+emb+attn+dis is 17.12. bleu of de nmt baselines is 27.5. bleu of de multi-nmt baselines johnson et al.(2017) is 25.98. bleu of de three-stgy wang et al.(2018) is 26.12. bleu of de rep+emb is 24.88. bleu of de rep+emb+attn is 25.8. bleu of de rep+emb+attn+dis is 26.42. bleu of lv nmt baselines is 16.28. bleu of lv multi-nmt baselines johnson et al.(2017) is 14.88. bleu of lv three-stgy wang et al.(2018) is 15.44. bleu of lv rep+emb is 14.51. bleu of lv rep+emb+attn is 15.58. bleu of lv rep+emb+attn+dis is 16.31. bleu of fi nmt baselines is 16.83. bleu of fi multi-nmt baselines johnson et al.(2017) is 16.94. bleu of fi three-stgy wang et al.(2018) is 17.05. bleu of fi rep+emb is 16.15. bleu of fi rep+emb+attn is 16.79. bleu of fi rep+emb+attn+dis is 17.22. bleu of de nmt baselines is 27.5. bleu of de multi-nmt baselines johnson et al.(2017) is 23.59. bleu of de three-stgy wang et al.(2018) is 22.88. bleu of de rep+emb is 22.88. bleu of de rep+emb+attn is 23.58. bleu of de rep+emb+attn+dis is 24.08. bleu of lv nmt baselines is 16.28. bleu of lv multi-nmt baselines johnson et al.(2017) is 15.57. bleu of lv three-stgy wang et al.(2018) is 16.02. bleu of lv rep+emb is 15. bleu of lv rep+emb+attn is 16.21. bleu of lv rep+emb+attn+dis is 16.57. bleu of zh nmt baselines is 26.04. bleu of zh multi-nmt baselines johnson et al.(2017) is 25.24. bleu of zh three-stgy wang et al.(2018) is 25.83. bleu of zh rep+emb is 24.15. bleu of zh rep+emb+attn is 25.27. bleu of zh rep+emb+attn+dis is 26.29. bleu of fi nmt baselines is 16.83. bleu of fi multi-nmt baselines johnson et al.(2017) is 13.45. bleu of fi three-stgy wang et al.(2018) is 14.12. bleu of fi rep+emb is 12.99. bleu of fi rep+emb+attn is 14.11. bleu of fi rep+emb+attn+dis is 15.03.
table 1 shows precision evaluated automatically for the top rated relation instances. † marks results reported in the original paper. ‡ marks our results using the opennre implementation. auc of mintz auc is 0.107. p@100 of mintz p@100 is 52.3. p@200 of mintz p@200 is 50.2. p@300 of mintz p@300 is 45. p@500 of mintz p@500 is 39.7. p@1000 of mintz p@1000 is 33.6. p@2000 of mintz p@2000 is 23.4. auc of pcnn+att auc is 0.341. p@100 of pcnn+att p@100 is 73. p@200 of pcnn+att p@200 is 68. p@300 of pcnn+att p@300 is 67.3. p@500 of pcnn+att p@500 is 63.6. p@1000 of pcnn+att p@1000 is 53.3. p@2000 of pcnn+att p@2000 is 40. auc of reside auc is 0.415. p@100 of reside p@100 is 81.8. p@200 of reside p@200 is 75.4. p@300 of reside p@300 is 74.3. p@500 of reside p@500 is 69.7. p@1000 of reside p@1000 is 59.3. p@2000 of reside p@2000 is 45. auc of distre auc is 0.422. p@100 of distre p@100 is 68. p@200 of distre p@200 is 67. p@300 of distre p@300 is 65.3. p@500 of distre p@500 is 65. p@1000 of distre p@1000 is 60.2. p@2000 of distre p@2000 is 47.9.
table 2 shows comparisons without post-processing of methods. results reproduced from (smith et al., 2017) for fair comparison. left: comparisons using the same expert dictionary as (smith et al., 2017). right: comparisons using the pseudo-dictionary from (smith et al., 2017). @1 of mikolov et. al. @1 is 33.8. @5 of mikolov et. al. @5 is 48.3. @10 of mikolov et. al. @10 is 53.9. @1 of mikolov et. al. @1 is 24.9. @5 of mikolov et. al. @5 is 41. @10 of mikolov et. al. @10 is 47.4. @1 of mikolov et. al. @1 is 1. @5 of mikolov et. al. @5 is 2.8. @10 of mikolov et. al. @10 is 3.9. @1 of mikolov et. al. @1 is 2.5. @5 of mikolov et. al. @5 is 6.4. @10 of mikolov et. al. @10 is 9.1. @1 of cca (sklearn) @1 is 36.1. @5 of cca (sklearn) @5 is 52.7. @10 of cca (sklearn) @10 is 58.1. @1 of cca (sklearn) @1 is 31. @5 of cca (sklearn) @5 is 49.9. @10 of cca (sklearn) @10 is 57. @1 of cca (sklearn) @1 is 29.1. @5 of cca (sklearn) @5 is 46.4. @10 of cca (sklearn) @10 is 53. @1 of cca (sklearn) @1 is 27. @5 of cca (sklearn) @5 is 47. @10 of cca (sklearn) @10 is 52.3. @1 of cca @1 is 30.9. @5 of cca @5 is 48.1. @10 of cca @10 is 52.7. @1 of cca @1 is 27.7. @5 of cca @5 is 45.5. @10 of cca @10 is 51. @1 of cca @1 is 26.5. @5 of cca @5 is 42.5. @10 of cca @10 is 48.1. @1 of cca @1 is 22.8. @5 of cca @5 is 40.1. @10 of cca @10 is 45.5. @1 of svd @1 is 36.9. @5 of svd @5 is 52.7. @10 of svd @10 is 57.9. @1 of svd @1 is 32.2. @5 of svd @5 is 49.6. @10 of svd @10 is 55.7. @1 of svd @1 is 27.1. @5 of svd @5 is 43.4. @10 of svd @10 is 49.3. @1 of svd @1 is 26.2. @5 of svd @5 is 42.1. @10 of svd @10 is 49. @1 of ibfa (ours) @1 is 39.3. @5 of ibfa (ours) @5 is 55.3. @10 of ibfa (ours) @10 is 60.1. @1 of ibfa (ours) @1 is 34.7. @5 of ibfa (ours) @5 is 53.5. @10 of ibfa (ours) @10 is 59.4. @1 of ibfa (ours) @1 is 34.7. @5 of ibfa (ours) @5 is 52.6. @10 of ibfa (ours) @10 is 58.3. @1 of ibfa (ours) @1 is 33.7. @5 of ibfa (ours) @5 is 53.3. @10 of ibfa (ours) @10 is 59.2.
table 4 shows human evaluations of ablation study. consistency of full model consistency is 3.84. novelty of full model novelty is 3.24. diversity of full model diversity is 3.16. coherence of full model coherence is 3.61. consistency of w/o adversarial consistency is 3.31. novelty of w/o adversarial novelty is 3.07. diversity of w/o adversarial diversity is 3.14. coherence of w/o adversarial coherence is 3.43. consistency of w/o memory consistency is 3.53. novelty of w/o memory novelty is 2.73. diversity of w/o memory diversity is 2.77. coherence of w/o memory coherence is 3.19. consistency of w/o dynamic consistency is 3.62. novelty of w/o dynamic novelty is 2.91. diversity of w/o dynamic diversity is 2.95. coherence of w/o dynamic coherence is 3.37.
table 2 shows english dev set performance of joint span hpsg parsing. the converted means the corresponding dependency parsing results are from the corresponding constituent parse tree using head rules. f1 of separate constituent f1 is 93.47. f1 of converted dependency f1 is 93.47. uas of converted dependency uas is 95.06. las of converted dependency las is 93.81. f1 of hpsg parser joint span lambda = 1.0 f1 is 93.67. uas of hpsg parser joint span lambda = 0.0 uas is 95.82. las of hpsg parser joint span lambda = 0.0 las is 94.43. f1 of hpsg parser joint span lambda = 0.5 f1 is 93.78. uas of hpsg parser joint span lambda = 0.5 uas is 95.92. las of hpsg parser joint span lambda = 0.5 las is 94.49. f1 of hpsg parser converted dependency f1 is 93.78. uas of hpsg parser converted dependency uas is 95.69. las of hpsg parser converted dependency las is 94.45.
table 5 shows averaged slot coherence results. ave slot coherence of nguyen et al. (2015) ave slot coherence is 0.1. ave slot coherence of odee-f ave slot coherence is 0.1. ave slot coherence of odee-fe ave slot coherence is 0.16. ave slot coherence of odee-fer ave slot coherence is 0.18.
table 1 shows performance of various models on en-de and en-fr translation tasks. bleu of vaswani et al. (2017) transformer big en-de is 28.4. bleu of vaswani et al. (2017) transformer big en-fr is 41. bleu of wu et al. (2018) transformer big + sequence-loss en-de is 28.75. bleu of wu et al. (2018) transformer big + sequence-loss en-fr is 41.47. bleu of yang et al. (2018) transformer big + localness en-de is 28.89. bleu of yang et al. (2018) transformer big + localness en-fr is n/a. bleu of this work transformer big + hard-attention en-de is 29.29. bleu of this work transformer big + hard-attention en-fr is 42.26.
table 5 shows validation set performances of using different layers for attention relay. rouge-1 of 1 rouge-1 is 44.7. rouge-2 of 1 rouge-2 is 21.8. rouge-l of 1 rouge-l is 41.6. rouge-1 of 2 rouge-1 is 44.7. rouge-2 of 2 rouge-2 is 22.3. rouge-l of 2 rouge-l is 41.7. rouge-1 of 3 rouge-1 is 45. rouge-2 of 3 rouge-2 is 22. rouge-l of 3 rouge-l is 41.8. rouge-1 of 4 rouge-1 is 44.9. rouge-2 of 4 rouge-2 is 22.1. rouge-l of 4 rouge-l is 41.3. rouge-1 of 5 rouge-1 is 44.9. rouge-2 of 5 rouge-2 is 22.1. rouge-l of 5 rouge-l is 41.9. rouge-1 of 6 rouge-1 is 45.1. rouge-2 of 6 rouge-2 is 22.3. rouge-l of 6 rouge-l is 42.
table 3 shows bucc results (f1) on the test set. we use the ratio function with maximum score retrieval and the filtering threshold optimized on the training set. f1 of azpeitia et al. (2017) en-de is 83.7. f1 of azpeitia et al. (2017) en-fr is 79.5. f1 of azpeitia et al. (2018) en-de is 85.5. f1 of azpeitia et al. (2018) en-fr is 81.5. f1 of azpeitia et al. (2018) en-ru is 81.3. f1 of azpeitia et al. (2018) en-zh is 77.5. f1 of bouamor and sajjad (2018) en-fr is 76. f1 of schwenk (2018) en-de is 76.9. f1 of schwenk (2018) en-fr is 75.8. f1 of schwenk (2018) en-ru is 73.8. f1 of schwenk (2018) en-zh is 71.6. f1 of proposed method (europarl) en-de is 95.6. f1 of proposed method (europarl) en-fr is 92.9. f1 of proposed method (un) en-ru is 92. f1 of proposed method (un) en-zh is 92.6.
table 1 shows baseline (cordeiro et al., 2016) results on the reduced version of three gold-standard datasets ordered in decreasing overall performance along with the results of using only poincar´e embedding. correlation of w2v-cbow rd-r is 0.8045. correlation of w2v-cbow rd++-r is 0.6964. correlation of w2v-cbow fd-r is 0.3405. correlation of w2v-sg rd-r is 0.8034. correlation of w2v-sg rd++-r is 0.6963. correlation of w2v-sg fd-r is 0.3396. correlation of glove rd-r is 0.7604. correlation of glove rd++-r is 0.6487. correlation of glove fd-r is 0.262. correlation of ppmi-svd rd-r is 0.7484. correlation of ppmi-svd rd++-r is 0.6468. correlation of ppmi-svd fd-r is 0.2428. correlation of poincare rd-r is 0.6023. correlation of poincare rd++-r is 0.4765. correlation of poincare fd-r is 0.2007.
table 8 shows human evaluation in wikianswers→quora mean rank of mtl+copy mean rank is 3.22. agreement of mtl+copy agreement is 0.446. mean rank of naïve dnpg mean rank is 3.13. agreement of naïve dnpg agreement is 0.323. mean rank of adapted dnpg mean rank is 1.79. agreement of adapted dnpg agreement is 0.383. mean rank of reference mean rank is 1.48. agreement of reference agreement is 0.338.
table 2 shows evaluation results of co-teaching initialized with different networks. map of smn-pre-training map is 0.527. mrr of smn-pre-training mrr is 0.57. p@1 of smn-pre-training p@1 is 0.396. r10@1 of smn-pre-training r10@1 is 0.236. r10@2 of smn-pre-training r10@2 is 0.392. r10@5 of smn-pre-training r10@5 is 0.734. map of smn-pre-training map is 0.662. mrr of smn-pre-training mrr is 0.742. p@1 of smn-pre-training p@1 is 0.598. r10@1 of smn-pre-training r10@1 is 0.302. r10@2 of smn-pre-training r10@2 is 0.464. r10@5 of smn-pre-training r10@5 is 0.757. map of smn-co-teaching map is 0.558. mrr of smn-co-teaching mrr is 0.602. p@1 of smn-co-teaching p@1 is 0.42. r10@1 of smn-co-teaching r10@1 is 0.255. r10@2 of smn-co-teaching r10@2 is 0.431. r10@5 of smn-co-teaching r10@5 is 0.787. map of smn-co-teaching map is 0.674. mrr of smn-co-teaching mrr is 0.765. p@1 of smn-co-teaching p@1 is 0.626. r10@1 of smn-co-teaching r10@1 is 0.322. r10@2 of smn-co-teaching r10@2 is 0.485. r10@5 of smn-co-teaching r10@5 is 0.779. map of dam-pre-training map is 0.552. mrr of dam-pre-training mrr is 0.605. p@1 of dam-pre-training p@1 is 0.426. r10@1 of dam-pre-training r10@1 is 0.258. r10@2 of dam-pre-training r10@2 is 0.408. r10@5 of dam-pre-training r10@5 is 0.766. map of dam-pre-training map is 0.685. mrr of dam-pre-training mrr is 0.756. p@1 of dam-pre-training p@1 is 0.621. r10@1 of dam-pre-training r10@1 is 0.325. r10@2 of dam-pre-training r10@2 is 0.491. r10@5 of dam-pre-training r10@5 is 0.772. map of dam-co-teaching map is 0.57. mrr of dam-co-teaching mrr is 0.617. p@1 of dam-co-teaching p@1 is 0.438. r10@1 of dam-co-teaching r10@1 is 0.27. r10@2 of dam-co-teaching r10@2 is 0.455. r10@5 of dam-co-teaching r10@5 is 0.781. map of dam-co-teaching map is 0.696. mrr of dam-co-teaching mrr is 0.775. p@1 of dam-co-teaching p@1 is 0.652. r10@1 of dam-co-teaching r10@1 is 0.341. r10@2 of dam-co-teaching r10@2 is 0.499. r10@5 of dam-co-teaching r10@5 is 0.784.
table 1 shows performance on the chimera benchmark dataset with different numbers of context sentences, which is measured by spearman correlation. baseline results are from the corresponding papers. correlation of word2vec    2-shot is 0.1459. correlation of word2vec    4-shot is 0.2457. correlation of word2vec    6-shot is 0.2498. correlation of fasttext    2-shot is 0.1775. correlation of fasttext    4-shot is 0.1738. correlation of fasttext    6-shot is 0.1294. correlation of additive    2-shot is 0.3627. correlation of additive    4-shot is 0.3701. correlation of additive    6-shot is 0.3595. correlation of additive, no stop words 2-shot is 0.3376. correlation of additive, no stop words 4-shot is 0.3624. correlation of additive, no stop words 6-shot is 0.408. correlation of nonce2vec    2-shot is 0.332. correlation of nonce2vec    4-shot is 0.3668. correlation of nonce2vec    6-shot is 0.389. correlation of ã  la carte  2-shot is 0.3634. correlation of ã  la carte  4-shot is 0.3844. correlation of ã  la carte  6-shot is 0.3941. correlation of hice w/o morph  2-shot is 0.371. correlation of hice w/o morph  4-shot is 0.3872. correlation of hice w/o morph  6-shot is 0.4277. correlation of hice + morph  2-shot is 0.3796. correlation of hice + morph  4-shot is 0.3916. correlation of hice + morph  6-shot is 0.4253. correlation of hice + morph + fine-tune 2-shot is 0.1403. correlation of hice + morph + fine-tune 4-shot is 0.1837. correlation of hice + morph + fine-tune 6-shot is 0.3145. correlation of hice + morph + maml 2-shot is 0.3781. correlation of hice + morph + maml 4-shot is 0.4053. correlation of hice + morph + maml 6-shot is 0.4307. correlation of oracle embedding   2-shot is 0.416. correlation of oracle embedding   4-shot is 0.4381. correlation of oracle embedding   6-shot is 0.4427.
table 1 shows discourse segmentation results. superscript (cid:63) indicates the model is significantly superior to the wlyelmo model with a p-value < 0.01. precision of - precision is 98.5. recall of - recall is 98.2. f1 of - f1 is 98.3. precision of spade (soricut and marcu, 2003) precision is 83.8. recall of spade (soricut and marcu, 2003) recall is 86.8. f1 of spade (soricut and marcu, 2003) f1 is 85.2. precision of f&r (fisher and roark, 2007) precision is 91.3. recall of f&r (fisher and roark, 2007) recall is 89.7. f1 of f&r (fisher and roark, 2007) f1 is 90.5. precision of jcn (joty et al., 2012) precision is 88. recall of jcn (joty et al., 2012) recall is 92.3. f1 of jcn (joty et al., 2012) f1 is 90.1. precision of segbotglove (li et al., 2018) precision is 91.08±0.46. recall of segbotglove (li et al., 2018) recall is 91.03 ±0.42. f1 of segbotglove (li et al., 2018) f1 is 91.05 ±0.11. precision of wlyelmo (wang et al., 2018) precision is 92.04 ±0.43. recall of wlyelmo (wang et al., 2018) recall is 94.41 ±0.53. f1 of wlyelmo (wang et al., 2018) f1 is 93.21 ±0.33. precision of pointer net (glove) precision is 90.55 ±0.33. recall of pointer net (glove) recall is 92.29 ±0.09. f1 of pointer net (glove) f1 is 91.41 ±0.21. precision of pointer net (bert) precision is 92.05 ±0.44. recall of pointer net (bert) recall is 95.03 ±0.28. f1 of pointer net (bert) f1 is 93.51 ±0.16. precision of pointer net (elmo) precision is 94.12±0.20. recall of pointer net (elmo) recall is  96.63±0.12. f1 of pointer net (elmo) f1 is  95.35±0.10. precision of pointer net (elmo) + joint training precision is 93.34±0.23. recall of pointer net (elmo) + joint training recall is 97.88±0.16. f1 of pointer net (elmo) + joint training f1 is 95.55±0.13.
table 2 shows parsing results with gold segmentation. superscript (cid:63) indicates the model is significantly superior to the 2-stage parser with a p-value < 0.01. f1 of - span is 95.7. f1 of - nuclearity is 90.4. f1 of - relation is 83. f1 of spade (soricut and marcu, 2003) span is 93.5. f1 of spade (soricut and marcu, 2003) nuclearity is 85.8. f1 of spade (soricut and marcu, 2003) relation is 67.6. f1 of dcrf (joty et al., 2012) span is 94.6. f1 of dcrf (joty et al., 2012) nuclearity is 86.9. f1 of dcrf (joty et al., 2012) relation is 77.1. f1 of dplp (ji and eisenstein, 2014) span is 93.5. f1 of dplp (ji and eisenstein, 2014) nuclearity is 81.3. f1 of dplp (ji and eisenstein, 2014) relation is 70.5. f1 of 2-stage parser (wang et al., 2017) span is 95.6. f1 of 2-stage parser (wang et al., 2017) nuclearity is 87.8. f1 of 2-stage parser (wang et al., 2017) relation is 77.6. f1 of stack pointer (elmo-medium) span is 96.37. f1 of stack pointer (elmo-medium) nuclearity is 89.04*. f1 of stack pointer (elmo-medium) relation is 79.03*. f1 of stack pointer (elmo-large) span is 96.86*. f1 of stack pointer (elmo-large) nuclearity is 90.77*. f1 of stack pointer (elmo-large) relation is 81.12*. f1 of + partial tree information span is 96.94*. f1 of + partial tree information nuclearity is 90.89*. f1 of + partial tree information relation is 81.28*. f1 of + joint training span is 97.44*. f1 of + joint training nuclearity is 91.34*. f1 of + joint training relation is 81.70*.
table 4 shows results on translating four languages to english for mle, bleu, simile and half. † denotes statistical significance (p < 0.05) over bleu and ‡ denotes statistical significance over mle. statistical significance was computed using paired bootstrap resampling. bleu of mle bleu is 27.52. sim of mle sim is 74.96. bleu of mle bleu is 17.02. sim of mle sim is 67.18. bleu of mle bleu is 17.92. sim of mle sim is 70.24. bleu of mle bleu is 14.47. sim of mle sim is 63.52. bleu of bleu bleu is 27.95. sim of bleu sim is 86.93. bleu of bleu bleu is 17.29. sim of bleu sim is 81.92. bleu of bleu bleu is 17.92. sim of bleu sim is 84.63. bleu of bleu bleu is 15.00. sim of bleu sim is 80.30. bleu of simile bleu is 28.28. sim of simile sim is 87.32. bleu of simile bleu is 17.51. sim of simile sim is 82.12. bleu of simile bleu is 18.23. sim of simile sim is 85.12. bleu of simile bleu is 15.28. sim of simile sim is 81.04. bleu of half bleu is 28.24. sim of half sim is 87.11. bleu of half bleu is 17.50. sim of half sim is 82.11. bleu of half bleu is 18.24. sim of half sim is 85.10. bleu of half bleu is 15.34. sim of half sim is 80.64.
table 4 shows ablation study results. base model means that we does not use schema linking (sl), memory augmented pointer network (mem) and the coarse-to-fine framework (cf) on irnet. f1 of base model irnet is  40.5%. f1 of base model irnet(bert) is 53.90%. f1 of +sl irnet is  48.5%. f1 of +sl irnet(bert) is 60.30%. f1 of +sl + mem irnet is  51.3%. f1 of +sl + mem irnet(bert) is 60.60%. f1 of +sl + mem + cf irnet is  53.2%. f1 of +sl + mem + cf irnet(bert) is 61.90%.
table 5 shows low-shot learning results for the yahoo dataset, in error percentages. transfer learning-based 3 of the total dataset, while the methods are trained on 1 other models are trained on the whole dataset. error of rnn baseline yahoo binary is 35.29. error of lstm baseline yahoo binary is 32.41. error of kimcnn kim (2014) yahoo binary is 14.25. error of self attention lin et al. (2017) yahoo binary is 13.16. error of rcnn lai et al. (2015) yahoo binary is 12.67. error of bcn+elmo yahoo binary is 13.51. error of ulmfit yahoo binary is 10.62. error of bertbase yahoo binary is 10.14.
table 2 shows results on recast mnli and joci. log loss of mnli 1 log loss is 93.6. margin loss of mnli 1 margin loss is 93.4. log loss of mnli 2 log loss is 87.9. margin loss of mnli 2 margin loss is 87.9. log loss of joci 1 log loss is 86.6. margin loss of joci 1 margin loss is 86.9. log loss of joci 2 log loss is 76.6. margin loss of joci 2 margin loss is 78.
table 3 shows experimental results on copa test set. acc (%) of pmi (jabeen et al., 2014) acc (%) is 58.8. acc (%) of pmi ex (gordon et al., 2011) acc (%) is 65.4. acc (%) of cs (luo et al., 2016) acc (%) is 70.2. acc (%) of cs mwp (sasaki et al., 2017) acc (%) is 71.2. acc (%) of bertlog (ours) acc (%) is 73.4. acc (%) of bertmargin (ours) acc (%) is 75.4.
table 2 shows results on cqa dev-random-split with cos-e used during training. accuracy (%) of bert (baseline) accuracy (%) is 63.8. accuracy (%) of cos-e-open-ended accuracy (%) is 65.5. accuracy (%) of cage-reasoning accuracy (%) is 72.6.
table 2 shows comparison for nonlocal dependency identification on the test data. pre. of (johnson, 2002) pre. is 85. rec. of (johnson, 2002) rec. is 74. f1 of (johnson, 2002) f1 is 79. pre. of (johnson, 2002) pre. is 73. rec. of (johnson, 2002) rec. is 63. f1 of (johnson, 2002) f1 is 68. pre. of (johnson, 2002) pre. is –. rec. of (johnson, 2002) rec. is –. f1 of (johnson, 2002) f1 is –. pre. of (dienes and dubey, 2003) pre. is –. rec. of (dienes and dubey, 2003) rec. is –. f1 of (dienes and dubey, 2003) f1 is –. pre. of (dienes and dubey, 2003) pre. is 81.5. rec. of (dienes and dubey, 2003) rec. is 68.7. f1 of (dienes and dubey, 2003) f1 is 74.6. pre. of (dienes and dubey, 2003) pre. is –. rec. of (dienes and dubey, 2003) rec. is –. f1 of (dienes and dubey, 2003) f1 is –. pre. of (campbell, 2004) pre. is 85.2. rec. of (campbell, 2004) rec. is 81.7. f1 of (campbell, 2004) f1 is 83.4. pre. of (campbell, 2004) pre. is 78.3. rec. of (campbell, 2004) rec. is 75.1. f1 of (campbell, 2004) f1 is 76.7. pre. of (campbell, 2004) pre. is –. rec. of (campbell, 2004) rec. is –. f1 of (campbell, 2004) f1 is –. pre. of (schmid, 2006) pre. is 86. rec. of (schmid, 2006) rec. is 82.3. f1 of (schmid, 2006) f1 is 84.1. pre. of (schmid, 2006) pre. is –. rec. of (schmid, 2006) rec. is –. f1 of (schmid, 2006) f1 is –. pre. of (schmid, 2006) pre. is 81.7. rec. of (schmid, 2006) rec. is 73.5. f1 of (schmid, 2006) f1 is 77.4. pre. of (cai et al., 2011) pre. is 90.1. rec. of (cai et al., 2011) rec. is 79.5. f1 of (cai et al., 2011) f1 is 84.5. pre. of (cai et al., 2011) pre. is –. rec. of (cai et al., 2011) rec. is –. f1 of (cai et al., 2011) f1 is –. pre. of (cai et al., 2011) pre. is –. rec. of (cai et al., 2011) rec. is –. f1 of (cai et al., 2011) f1 is –. pre. of (hayashi and nagata, 2016) pre. is 90.3. rec. of (hayashi and nagata, 2016) rec. is 81.7. f1 of (hayashi and nagata, 2016) f1 is 85.8. pre. of (hayashi and nagata, 2016) pre. is –. rec. of (hayashi and nagata, 2016) rec. is –. f1 of (hayashi and nagata, 2016) f1 is –. pre. of (hayashi and nagata, 2016) pre. is –. rec. of (hayashi and nagata, 2016) rec. is –. f1 of (hayashi and nagata, 2016) f1 is –. pre. of (kato and matsubara, 2016) pre. is 88.5. rec. of (kato and matsubara, 2016) rec. is 82.1. f1 of (kato and matsubara, 2016) f1 is 85.2. pre. of (kato and matsubara, 2016) pre. is 81.4. rec. of (kato and matsubara, 2016) rec. is 75.5. f1 of (kato and matsubara, 2016) f1 is 78.4. pre. of (kato and matsubara, 2016) pre. is 79.8. rec. of (kato and matsubara, 2016) rec. is 73.8. f1 of (kato and matsubara, 2016) f1 is 76.7. pre. of (kummerfeld and klein, 2017) pre. is 89.5. rec. of (kummerfeld and klein, 2017) rec. is 81.6. f1 of (kummerfeld and klein, 2017) f1 is 85.4. pre. of (kummerfeld and klein, 2017) pre. is 74.3. rec. of (kummerfeld and klein, 2017) rec. is 67.3. f1 of (kummerfeld and klein, 2017) f1 is 70.6. pre. of (kummerfeld and klein, 2017) pre. is –. rec. of (kummerfeld and klein, 2017) rec. is –. f1 of (kummerfeld and klein, 2017) f1 is –. pre. of (johnson, 2002) pre. is 93. rec. of (johnson, 2002) rec. is 83. f1 of (johnson, 2002) f1 is 88. pre. of (johnson, 2002) pre. is 80. rec. of (johnson, 2002) rec. is 70. f1 of (johnson, 2002) f1 is 75. pre. of (johnson, 2002) pre. is –. rec. of (johnson, 2002) rec. is –. f1 of (johnson, 2002) f1 is –. pre. of (campbell, 2004) pre. is 94.9. rec. of (campbell, 2004) rec. is 91.1. f1 of (campbell, 2004) f1 is 93. pre. of (campbell, 2004) pre. is 90.1. rec. of (campbell, 2004) rec. is 86.6. f1 of (campbell, 2004) f1 is 88.4. pre. of (campbell, 2004) pre. is –. rec. of (campbell, 2004) rec. is –. f1 of (campbell, 2004) f1 is –. pre. of ours pre. is 92.6. rec. of ours rec. is 87.7. f1 of ours f1 is 90.1. pre. of ours pre. is 88.1. rec. of ours rec. is 83.4. f1 of ours f1 is 85.7. pre. of ours pre. is 88.4. rec. of ours rec. is 81.1. f1 of ours f1 is 84.6. pre. of ours (with elmo) pre. is 94.2. rec. of ours (with elmo) rec. is 90.3. f1 of ours (with elmo) f1 is 92.3. pre. of ours (with elmo) pre. is 89.9. rec. of ours (with elmo) rec. is 86.2. f1 of ours (with elmo) f1 is 88. pre. of ours (with elmo) pre. is 90.4. rec. of ours (with elmo) rec. is 84.1. f1 of ours (with elmo) f1 is 87.2. pre. of ours (with bert) pre. is 94.9. rec. of ours (with bert) rec. is 91.4. f1 of ours (with bert) f1 is 93.1. pre. of ours (with bert) pre. is 90.8. rec. of ours (with bert) rec. is 87.4. f1 of ours (with bert) f1 is 89. pre. of ours (with bert) pre. is 91.6. rec. of ours (with bert) rec. is 84.9. f1 of ours (with bert) f1 is 88.1.
table 4 shows results of self-play evaluation. succ. (%) of retrieval succ. (%) is 9.8. #turns of retrieval #turns is 3.26. succ. (%) of retrieval-stgy succ. (%) is 67.2. #turns of retrieval-stgy #turns is 6.56. succ. (%) of ours-pmi succ. (%) is 47.4. #turns of ours-pmi #turns is 5.12. succ. (%) of ours-neural succ. (%) is 51.6. #turns of ours-neural #turns is 4.29. succ. (%) of ours-kernel succ. (%) is 75. #turns of ours-kernel #turns is 4.2.
table 7 shows comparison on the scws task. setting (a) for wordctx2sense uses λ = 0.1 for all pairs, and setting (b) uses λ = 10−3 for pairs containing same target words and λ = 0.1 for all other pairs. word2sense, word2vec and word2gm neglect context and compare target words. a numbers reported from (mu et al., 2017) whose experimental setup we could replicate. pearson-coefficient of wordctx2sense (a) pearson-coefficient is 0.666. pearson-coefficient of wordctx2sense (b) pearson-coefficient is 0.67. pearson-coefficient of word2sense pearson-coefficient is 0.644. pearson-coefficient of word2vec pearson-coefficient is 0.651. pearson-coefficient of (mu et al., 2017) pearson-coefficient is 0.637. pearson-coefficient of (huang et al., 2012) pearson-coefficient is 0.657. pearson-coefficient of (arora et al., 2018) pearson-coefficient is 0.652. pearson-coefficient of word2gm pearson-coefficient is 0.655. pearson-coefficient of mssg.300d.30k pearson-coefficient is 0.679a. pearson-coefficient of mssg.300d.6k pearson-coefficient is 0.678a.
table 2 shows medical bilingual lexicon induction results showing the quality of the bwe based dictionaries using 1-best and 5-best translations. acc1 of (braune et al., 2018) acc1 is 38.6. acc5 of (braune et al., 2018) acc5 is 47.4. acc1 of eu+ufal+orth acc1 is 25.9. acc5 of eu+ufal+orth acc5 is 40.6. acc1 of (braune et al., 2018) acc1 is 26.3. acc5 of (braune et al., 2018) acc5 is 28.2. acc1 of eu+ufal+orth acc1 is 17.5. acc5 of eu+ufal+orth acc5 is 28.8.
table 1 shows automatic evaluation of sentiment analyzers. h-m senticons of rule-based (rb) h-m senticons is 0.936. h-m senticons of regression model (rm) h-m senticons is 0.846. h-m senticons of domain adversarial (da) h-m senticons is 0.747.
table 4 shows f1 score on the test set of hotpotqa distractor and full wiki setting. all numbers from the official leaderboard. all models except bidaf are concurrent work (not published). decomprc achieves the best result out of models reported to both distractor and full wiki setting. dist f1 of decomprc dist f1 is 69.63. open f1 of decomprc open f1 is 40.65. open f1 of cognitive graph open f1 is 48.87. dist f1 of bert plus dist f1 is 69.76. open f1 of multiqa open f1 is 40.23. dist f1 of dfgn+bert dist f1 is 68.49. dist f1 of qfe dist f1 is 68.06. open f1 of qfe open f1 is 38.06. dist f1 of grn dist f1 is 66.71. open f1 of grn open f1 is 36.48. dist f1 of bidaf dist f1 is 59.02. open f1 of bidaf open f1 is .
table 1 shows performance comparison on the private test set of hotpotqa in the distractor setting. our dfgn is the second best result on the leaderboard before submission (on march 1st). the baseline model is from yang et al. (2018) and the results with ∗ is unpublished. dfgn(ours)† refers to the same model with a revised entity graph, whose entities are recognized by a bert ner model. note that the result of dfgn(ours)† is submitted to the leaderboard during the review process of our paper. em of baseline model em is 45.60. f1 of baseline model f1 is 59.02. em of baseline model em is 20.32. f1 of baseline model f1 is 64.49. em of baseline model em is 10.83. f1 of baseline model f1 is 40.16. em of grn∗ em is 52.92. f1 of grn∗ f1 is 66.71. em of grn∗ em is 52.37. f1 of grn∗ f1 is 84.11. em of grn∗ em is 31.77. f1 of grn∗ f1 is 58.47. em of dfgn(ours) em is 55.17. f1 of dfgn(ours) f1 is 68.49. em of dfgn(ours) em is 49.85. f1 of dfgn(ours) f1 is 81.06. em of dfgn(ours) em is 31.87. f1 of dfgn(ours) f1 is 58.23. em of qfe∗ em is 53.86. f1 of qfe∗ f1 is 68.06. em of qfe∗ em is 57.75. f1 of qfe∗ f1 is 84.49. em of qfe∗ em is 34.63. f1 of qfe∗ f1 is 59.61. em of dfgn(ours)† em is 56.31. f1 of dfgn(ours)† f1 is 69.69. em of dfgn(ours)† em is 51.50. f1 of dfgn(ours)† f1 is 81.62. em of dfgn(ours)† em is 33.62. f1 of dfgn(ours)† f1 is 59.82.
