table 5 show the results in the filtered setting.on each dataset we report the metrics on three sets: test-i, test-ii, and the whole test set (denoted by test-all).from the results, we can see that in both settings: (i) kale-pre and kale-joint outperform (or at least perform as well as) the other methods which use triples alone on almost all the test sets, demonstrating the superiority of incorporating logical rules.(ii) on the test-i sets which contain triples beyond the scope of pure logical inference, kale-joint performs significantly better than kale-pre.on these sets kale-joint can still beat all the baselines by a significant margin in most cases, while kale-pre can hardly outperform kale-trip.(iii) on the test-ii sets which contain directly inferable triples, kale-pre can easily beat all the baselines (even kale-joint).

table 3 displays the validation perplexities of the benchmark models, where the joint asc+fsc1 model trained on the full labelled and unlabelled datasets performs the best on modelling compression languages.

table 4 shows the results on the maximally covered datasets.for each of the network architectures, we see a marked improvement of multi-modal representations over uni-modal linguistic representations.in many cases, we also see visual representations outperforming linguistic ones, especially on simlex.for the esp game dataset (on which performance is quite low) and imagenet, we observe an increase in performance as we move to the right in the table.interestingly, vggnet on imagenet scores very highly, which seems to indicate that vggnet is somehow more “specialized” on imagenet than the others.the difference between mean and max aggregation is relatively small, although the former seems to work better for simlex while the latter does slightly better for men.

table 4 compares our approach with the state-of-the-art on vqa test set.this ensemble is 1.8 points above the next best approach on the vqa open-ended task and 0.8 points above the next best approach on the multiple-choice task (on test-dev).even without ensembles, our “mcb + genome + att. + glove” model still outperforms the next best result by 0.5 points, with an accuracy of 65.4% versus 64.9% on the open-ended task (on test-dev).

table 1 shows that most extractions are made from wikipedia articles, whereas the highest precision can be observed for newswire text.according to our expectations, web pages are most challenging, presumably due to noisier language.

the results in table 3 show that our mcnn-based method achieved better average precision than the single-column cnn baseline except the method that uses only the base column set for the cataphoric case.the results also demonstrate that each column set consistently contributes to improving the average precision for both the anaphoric and cataphoric cases.however, table 3 shows that the average precision for the cataphoric set remains low.

table 3 shows our results compared with prior work.the result with panlex is substantially worse than with wiktionary.our model when randomly picking the translation is similar to gouws and søgaard (2015), using the panlex dictionary.for a high coverage yet noisy dictionary such as panlex, our approach gives better average score.comparing our two most basic models (em selection and random selection), it is clear that the model using em to select the translation outperforms random selection by a significant margin.our joint model, as described in equation (3) which predicts both target word and the translation, further improves the performance, especially for nl-en.this modification during training substantially improves the performance.more importantly, all our improvements are consistent for all three language pairs and both evaluation metrics, showing the robustness of our models.our combined model out-performed previous approaches by a large margin.their performance on nl-en is poor because their comparable data between en and nl is small.table 3 (+lemmatization) shows some improvements but minor.it demonstrates that our model is already good at disambiguating morphology.

table 1 reports meteor, we notice that removing the semantic coherence scores in -sem hurts the performance compared to full, this confirms our claim that semantic compatibility is crucial for building coherent stories.on the other hand, -syn performs similarly to full.closer inspection of the -syn system’s output reveals a greater diversity in thematic elements as a result of the relaxed syntactic compatibility constraints.hence it is more likely to have greater overlap with any of the reference rewrites, resulting in higher meteor scores.

in table 2 we report the results of our experiments on the aforementioned datasets, and we distinguish our proposed regularizers lsi, gow, word2vec with underlining.our results are inline and confirm that of (yogatama and smith, 2014a) showing the advantages of using structured regularizers in the text categorization task.the group based regularizers perform systematically better than the baseline ones.

table 1 presents our reproduced classification accuracy test results (two labels: positive or negative politeness) for the bag-of-words and linguistic features based models of danescu-niculescu-mizil et al. (2013) (for our dataset splits) as well as the performance of our cnn model.as seen, without using any manually defined, theory-inspired linguistic features, the simple cnn model performs better than the feature-based methods.

finally, table 3 shows the effect of the different feature sets on vuamc used by klebanov et al. (2014).on this data, n features alone are significantly outperformed by b (p < 0.01).on the other hand, for the genres “academic” and “fiction”, combining n and b features improves classification performance over b, and the difference is always statistically significant.besides, the addition of n always leads to more balanced models, by compensating for the relatively lower precision of b.

table 5 shows the f1 scores with and without the backoff model, the portion of the sentences that each decoder is able to parse, and the time spent decoding relative to the a* parser.results indicate that both approximate methods are less accurate and slower than a*.

we evaluate the performance of our models using 10-fold cross validation.the evaluation results shown in table 3 are based on weighted roc auc of the best models.among all the feature learning methods for facebook status updates, user-d-dbow performed the best.it significantly outperformed all the baseline systems that only rely on supervised training (p < 0.01 based on t-tests).it also significantly outperformed all the traditional feature learning methods such as lda and svd (p < 0.01 based on t-tests).moreover, in terms of whether to treat all the posts by the same user as one big document or separate documents, lda prefers one post one document (models with a "post" prefix) while all the document vector-based methods prefer one user one document (models with a user prefix).moreover, to use post-level lda to derive the spe of a user, the document-based aggregation method (postlda_doc) performed better than the wordbased method (postlda_word).

table 7 reports on the results obtained by the best configuration, i.e. lr + all features, per each category.

table 4 shows our results.as can be seen, we obtain the same median rank as in kiros et al.(2015), indicating that our encoder is as competitive as the skip-thought vectors (kiros et al., 2015).the performance gain between our encoder and the combine-skip model of kiros et al.(2015) on the r@1 score is significant, which shows that the cnn encoder has more discriminative power on retrieving the most correct item than the skip-thought vector.

table 4 shows the results.the phonetic features alone are sufficient to detect just over half of the cognate sets.each successive variant substantially improves the recall at a cost of slightly lower precision.the full feature set yields a 27% relative increase in the number of found sets over the phonetic-only variant, with only a 5% drop in cluster purity.

table 1 shows the translation performances on test sets measured in bleu score.the attnmt significantly outperforms pbsmt by 2.74 bleu points on average, indicating that it is a strong baseline nmt system.the baseline sennrichdeponly improves the performance over the attnmt by 0.58 bleu points on average.moreover, sdrnmt-1 gains improvements of 0.92 and 0.34 bleu points on average than the attnmt and sennrich-deponly.especially, the proposed sdrnmt2 outperforms the attnmt and sennrich-deponly on average by 1.64 and 1.03 bleu points.

the results in table 4 show that spending our memory budget on small bigram embeddings is more effective than on larger character embeddings, in terms of both accuracy and model size.our model featuring bigrams runs at 110kb of text per second, or 39k tokens/second.

table 5 shows the results of monolingual word similarity computation on four datasets.from the table, we find that:.(1) our models perform better than bilex on both chinese word similarity datasets.(2) clsp-wr model does not enhance english word similarity results but clspse model does.

table 4 shows that our model also outperforms transformer by 0.89 bleu points on french-english translation task.

table 2 shows the performance of various models on narrativeqa.it can be noted that our model with sample size 5 (choosing 5 relevant sentences) outperforms the best rouge-l score available so far by 12.62% compared to tay et al. (2018).the low performance of baseline 1 shows that the hybrid approach (conznet) for generating words from a fixed vocabulary as well as copying words from the document is better suited than span prediction models (seq2seq, asr, bidaf, mru).the performance of our model gradually dropped from sample size 7 onwards.the performance of the model improved dramatically with sample sizes 3 and 5 compared to the sample size of 1.in addition, the low performance of baseline 2 indicates that just selecting multiple sentences is not enough, they should also be related to each other.

table 3 displays those results.nlra+vecoff achieved a competitive performance to the state-of-the-art method of turney (2013).in their setting, nlra+vecoff achieved 0.47 accuracy, outperforming their model.

table 1 shows the automatic evaluation results.we see that both spmn and dpmn obtain huge improvements over the two baselines in terms of ppl-v and ppl-t.also, we observe that spmn1000 outperforms spmn500 in all the four automatic metrics.as for the dpmn, we can see that dpmn achieves the best performance with only 100 prototypes in terms of ppl-t, compared with the other 4 methods.note that s2sa outperforms the others in terms of distinct-1 and distinct-2.

these results are summarized in table 4 and table 5.while fasttext embeddings lead to reduced performance with our model, our approach still yields an improvement over the gaussian baseline with the new observed embeddings space.

we report the performance of the baseline models in table 2 which indicates the ratio of correct answers against the total questions in the test.for the textual cloze, the comparison between text-only and multimodal impatient reader models shows that the additional visual modality helps the model to understand the question better and to provide more accurate answers.while for the cloze style questions, the impatient reader outperforms the hasty student, for the visual coherence and visual ordering style questions hasty student gives way better results.

table 1 shows the results.we report accuracy scores averaged over five runs with different random seeds, together with their standard deviation, for the snli and multinli datasets.both naive baseline models outperform the single encoders that have only glove or fasttext embeddings.next, we observe that the dme embeddings outperform the naive concatenation baselines, while having fewer parameters.differences between the three dme variants are small and not significant, although we do note that we found the highest maximum performance for the contextualized version, which adds very few additional parameters.finally, we obtain results for using the six different embedding types (marked *), and show that adding in more embeddings increases performance further.

from table 2, we can see that stamp performs better than existing systems when trained on the full wikisql training dataset, achieving state-ofthe-art execution accuracy and logical form accuracy on wikisql.the second column “training data” in table 2 and the x-axis in figure 3 represent the proportion of wikisql training data we use for training the qg model and semantic parser.results show that qg empowers the stamp model to achieve the same accuracy on wikisql dataset with 30% of the training data.applying qg to the stamp model under the full setting brings further improvements, resulting in new state-of-the-art accuracies.

table 5 lists the results for bridging anaphora resolution based on different word representation resources.we notice that there is not much difference between glove gigawiki14 and glove giga.we find that using embeddings pp achieves an accuracy of 33.03% on the isnotes corpus, which outperforms the results based on glove gigawiki14 and glove giga by a large margin.using embeddings bridging further improves the result by 1.8%.

table 5 reports f1 scores of our syn-gcn model, marcheggiani and titov (2017), he et al. (2018) and cai et al. (2018) on english test set in both syntax-agnostic and syntax-aware settings.the comparison shows that our framework is more effective for incorporating syntactic information by giving more performance improvement through introducing syntax over syntax-agnostic srl than previous state-ofthe-art systems did.

table 6 shows the information of 300 sentences and the annotation results on them.the average recall in the table illustrates that three students have a higher recognition rate for errors from d-asr than that from d-ocr, which, to some extent, indicates that p-style errors are easier to be detected than v-style ones.besides, we observe that three volunteers fail to identify around 36.9% errors on average, which may indicate that our generated sentences include some challenging errors which are likely to be made by human.

table 2 shows the results for different values of k.interestingly, the results show a monotonic increase with the increase in context window, and the results with the entire context are still better than those with k = 20, even though only marginally.

table 2 shows the results with and without regressor.the regressor significantly improved the accuracy with high iou thresholds, which demonstrates that the regressor improved the localization accuracy.in addition, the accuracy did not decrease as a result of sharing the hidden layer or reducing the number of units in the hidden layer.the number of parameters was significantly reduced by these tricks, to even fewer than in the linear transformation.the accuracy slightly decreased with a threshold of 0.5, because the regressor was not learned properly for the categories that did not frequently appear in the training data.

see table 1 for single-label results on the selected harassment categories, where cnn-rnn was the best performing model compared to several nonneural and neural baselines.

table 6 and 7 detail the results of classification for each label in terms of performance scores (precision, recall and f1) and confusion matrix, respectively (for our hsln-rnn model trained on the pubmed 20k dataset).these show that the classifier is very good at predicting the labels methods, results and conclusions, whereas the greatest difficulty the classifier has is in distinguishing background sections from objectives sections.

table 4 shows the comparison results of lda, btm, ntm, and tmn on the three english datasets.as can be seen, tmn yields higher cv scores by large margins than all others in comparison.we also observe that ntm produces better results than lda and btm, which implies the effectiveness of inducing topic models by neural networks.

as shown in table 3, on most datasets, removing each component results in a significant performance drop.the exception is wn18rr, where removing the conve reward shaping module improves the performance.removing reward shaping on nell995 does not change the results significantly.

table 3 shows that, for set3, four sources (column header"4") are generally better than two sources ("2"), which in turn are better than one source ("1"); thus, as expected, making additional sources available in training improves results.

table 2 reports the cross-validation classification results.consistently, our clause-level baseline model already outperforms the previous best model.by exploiting paragraph-wide contexts, the basic paragraph-level model obtains consistent performance improvements across all the classes compared with the baseline clause-level prediction model, especially for the classes generic and generalizing, where the improvements are significant.after using the crf layer to fine-tune the predicted se label sequence, slight performance improvements were observed on the four small classes.overall, the full paragraphlevel neural network model achieves the best macro-average f1-score of 77.8% in predicting se types, which not only outperforms all previous approaches but also reaches human-like performance on some classes.

table 2 shows the clustering accuracy of our method and the baseline methods in real world datasets.from the results, we establish that our model outperforms the baseline methods in both the nyt narrative reconstruction task and the wikipedia thread reconstruction task.from the results, we establish that our model performs better than the baseline model, hdhp.

table 1 and table 2 report the results of our experiments.the results on train-all are higher than train for semeval16 in lieu of the larger dataset.firstly, we observe that our proposed aglr outperforms all neural baselines on 3-way classification.the overall performance of aglr achieves state-of-the-art performance.on average, aglr outperforms lexicon rnn and at-bilstm by 1% âˆ’ 3% in terms of f1 score.we also observe that aglr always improves at-bilstm which ascertains the effectiveness of learning auxiliary lexicon embeddings.we also observe that lexicon rnn does not handle 3-way classification well.even though it has achieved good performance on binary classification, the performance on 3-way classification is lackluster (the performance of aglr outperforms lexicon rnn by up to 8% on semeval16 train).finally, we observe that bilstm and at-bilstm outperform lexicon rnn on average with lexicon rnn being slightly better on binary classification.

table 4 shows the bleu score on the en-jp test set.it can be observed that regardless of decoding direction (i.e., from left-to-right or from right-toleft) and with or without teacher forcing, the accuracy of the right half is always higher than that in the left half.

table 3 shows the results.our vag-nmt has a higher value in bleu and a comparable value in meteor compared to the text-only nmt baseline.our vag-nmt outperforms liumcvc's multimodal system by a large margin, which shows that the liumcvc's multimodal's good performance on multi30k does not generalize to this real-world product dataset.

table 2 summarizes the experimental results of all the approaches above, and we can find that:.(1) all lstm-based approaches are superior to svm, indicating the effectiveness of neural network for this task.(2) the proposed approaches, with novel qa contextual representation, outperform the other baseline approaches.(3) when only employing qa bidirectional matching mechanism, bidirectional-match qa, which takes the sentence segmentation strategy, consistently outperforms bidirectional-match (without sentence segmentation) in all domains.(4) when comparing to qa unidirectional matching mechanism, bidirectional-match qa, which employs qa bidirectional matching mechanism, performs better than atoqmatch and qtoa-match.(5) impressively, the proposed approach hmn significantly outperforms all the other approaches in all domains (p-value<0.05 via t-test).

experiment results of our model and four baselines are shown in table 2.psan achieves the state-of-the-art performance with considerably fewer parameters, outperforming a rnn-based model, a cnn-based model, a fully attention-based model and a model that utilize syntactic information.especially when compared with previous best model bilstm-max, psan can outperform their model with only 5% of their parameter numbers, demonstrating the effectiveness of our model at extracting semantically important information from a sentence.

in table 3, we compare our model with baseline sentence encoders in each transfer task.psan can consistently outperform the baselines in almost every task considered.

the results of separate training for slot filling and intent detection are reported in table 1 and table 2 respectively.table 2 compares the performance of our proposed model to previously reported results on intent detection task.our model gives good performance in terms of classification error rate, but not as good as attention encoder-decoder (with aligned inputs) method (liu and lane, 2016a).

table 3 compares our joint model with reported results from previous works.we can see that our model achieves state-of-the-art results and outperforms previous best result by 0.54% in terms of f1-score on slot filling, and by 0.34% in terms of error rate on intent detection.besides, the joint learning achieves better results than separate learning.

table 3 shows the performances on the twitter corpus.our models again outperform the baselines in terms of all the evaluation measures.note that rbm-irl performs better than rbm-sl in this case.

table 2 shows our results for user satisfaction test.user ratings of pictorial summaries are 12.4% higher than text summaries.it shows that users prefer this way of presenting information.

we simultaneously train our model mcapsnet on six tasks in table 1 and compare it with singletask scenario (table 3).as table 3 shows, mcapsnet also outperforms the state-of-the-art multi-task learning models by at least 1.1%.

table 1 reports the results of the approaches we described in section 4.out of the kneser-ney n-gram models, we found that the fst-c-9gramkn and the version modelling word boundaries (fst-cwb-9gram-kn) to perform best on the synthetic parallel corpus and newspapers, respectively.the hybrid models (fst-rnnlm) outperformed all fst-only approaches.thus, we found the wordsplitting hybrid model (fst-rnnlm-cwb) to be the best performing model overall (table 1).

table 4 gives another comparison using auc with all p-values less than 5e-02 from t-test evaluation.the results indicate that the larger auc, the better performance.a simple ensemble model of two networks (auc: 0.371) has a similar result as a single model (netatt, auc: 0.368).with the ccl strategies, our models improve performances by removing 'hard' (noisy) entity pair bags during.

table 3 shows the results.from the results, cl trans mlp, cl trans cnn, and cl trans hbrid behave poorly, as expected.cl trans self. yields relatively good performance.our syntactic order event detector yields the best performance.

we do a finer-grained study in table 8 by breaking down the performance for the three state changes: creation (c), movement (m), and destruction (d), separately.across the three state changes, the model suffers a loss of performance in the movement cases.

table 1 presents the results on the multi30k testing set.in contrast, pivot (gella et al., 2017) and the proposed model are capable of handling multilingual input queries with single model.for a fair comparison with pivot, we also report the result of swapping faster-rcnn with vgg as the visual feature encoder in our model.as can be seen, the proposed models successfully obtain state-of-the-art results, outperforming other baselines by a significant margin.while the model with bert performs better in english, fasttext is preferred for german-image matching.

table 8 summarizes the results: the multitask model slightly outperforms a dedicated simplification model on english simplification, showing the benefits of the additional training data from other tasks.by contrast, on the resource-rich mt task, the standalone translation system performs better.

table 2 shows results produced using traditional ml methods (svm, rf, and lr) across four different feature sets (word n-grams, character ngrams, averaged elmo vectors, and composite features).among these combinations, logistic regression with averaged elmo embeddings as features performs the best.

in line with table 3, mtnl (fan et al., 2018) achieves the best performance up-to-date on this classification task among the baseline approaches as it achieves the best performance on 12 of 14 categories in amazon and yelp datasets.r2hp surpasses mtnl on both datasets and obtains state-of-the-art (microaveraged) results of 67.5% auroc (amazon) and 75.1% auroc (yelp) with absolute improvements of 4.9% auroc and 4.7% auroc, respectively.

the results shown in table 3 show the performance of mgt using two different underlying architectures, as well as previous work.across both base architectures, mgt outperforms ensembling.the primary difference between these two methods is that mgt explicitly ensures that several granularities of representation are learned.

it can be observed from table 6 that the cascade model achieves a lower score than mtl + sd.however, the cascade model (gold) improves the rouge-l score.our proposed hcl has an advantage in that it does not suffer from an error of the classifier, and thus this method achieves the best score.the non-hierarchical consistency loss is almost the same as the hierarchical consistency loss (eqn. 10).we replace a ramp function in eqn. 10 with an absolute value function.neither the soft-parameter sharing method nor the non-hierarchical consistency loss has the ability to consider the hierarchy among tasks.it can be observed from table 6 that both methods achieved a lower score than the hierarchical consistency loss.hcl with normalized attention weights is not as effective as hcl with non-normalized attention weights.as a result, our proposed hcl with non-normalized attention weights can accurately compute this inconsistency, contrary to the hcl with normalized attention weights.table 6 presents the results of two methods for the job advertisement dataset.

table 4 shows the performance comparison of the models on nested, overlapping and flat event detection.our model yields higher f1-scores than tees which can be attributed to its ability to maintain multiple beams and to detect events from all these beams during search.

ontonotes chinese table 4 shows the performance comparison on the chinese datasets.similar to the english dataset, our model with l = 0 significantly improves the performance compared to the bilstm-crf (l = 0) model.our dglstm-crf model achieves the best performance with l = 2 and is consistently better (p < 0.02) than the strong bilstm-crf baselines. as we can see from the table, the improvements of the dglstm-crf model mainly come from recall (p < 0.001) compared to the bilstm model, especially in the scenario with word embeddings only.empirically, we also found that those correctly retrieved entities of the dglstm-crf (compared against the baseline) mostly correlate with the following dependency relations: “nn”, “nsubj”, “nummod”.however, dglstm-crf achieves lower precisions against bilstm-crf, which indicates that the dglstm-crf model makes more false-positive predictions.

the proposed induction networks achieves a 85.63% accuracy, outperforming the existing state-of-the-art model, robusttc-fsl, by a notable 3% improvement.

the results from table 2 clearly confirm this.for example, without refinement, the mapping trained by the unsupervised gan method can only correctly predict 12% of the words from turkish to english.given that the quality of preliminary mappings can seriously affect the effect of refinement, the low-quality preliminary mappings for distant language pairs severely limits the improvements brought by post-refinement.notice that the method of lample et al. (2018) scores a null result for english to finnish on both bli-1 and bli2, indicating that totally unsupervised adversarial training can yields rather unpredictable results.

table 1 shows that the seq model outperforms the current sota system on all three text genres for binary cwi (statistically significant using mcnemar's test, p=0.0016, chi-square=9.95).

table 3 presents the performance of different methods on citation count prediction.we can make the following observations.first, the four traditional baselines (lr, knn, svr and gbrt) perform worse than the two deep learning baselines (w&d, milam).second, milam performs consistently better than w&d, since it has designed a more elaborate architecture to model the review text.finally, our model outperforms all the baselines with a substantial margin, especially for the iclr dataset.

we performed comprehensive experiments to analyze the performance of query auto-completion.table 1 shows the generation result of mpc, the character baseline, and our model variants.for bpe models, we varied the maximum retrace step to 0 (without retrace algorithm), 1, 2, and âˆž (no limitation on retracing step size).for sr models, we compare decoding results without any techniques, with marginalization only, with retrace algorithm only, and with both.mpc is a very fast and remarkably strong baseline.it is worse than language models in the overall score (mrr, pmrr, and mrl), but better for previously seen queries.mrrs and pmrrs of our best methods are close to that of the character model with less than 0.02 point drop.notably, the sr model has better generalization ability in that their pmrr for unseen queries is higher than that of the character model.

table 4 reports evaluation results in terms of both automatic metrics and human annotations.the improvement on bleu-1 and w-bleu-1 is much bigger than that on other metrics.in human evaluation, although the absolute numbers are different from those reported in (qin et al., 2018) due to the difference between human judgements, the overall trend is consistent.in human evaluation, the values of fleiss' kappa over all models are more than 0:6, indicating substantial agreement among the annotators.although built in a complicated structure, gann does not bring much improvement over other baseline methods, which demonstrates that only using news titles is not enough in comment generation.ir-tc and att-tc represent the best retrieval model and the best generation model among the baselines on both datasets, implying that news bodies, even used in a simple way, can provide useful information to comment generation.

contrary to our expectations, supervised models (acp) outperformed semi-supervised models (acp+al+ca+co).as the results shown in table 4 demonstrate, our method is effective when labeled data are small.

table 4 shows that increasing the number of decoding iterations (t) appears to mainly improve the performance on longer sequences.having said that, the performance differences across length buckets are not very large, and it seems that even 4 mask-predict iterations are enough to produce decent translations for long sequences (40 ? n).

table 2 shows the human evaluation results, where we compute the average score by first computing the average score of the three evaluators for each of the top-k outputs, then selecting the maximum of the k scores as the score of the method. the average shown in the table is the average of such maximum scores.the proposed method failed to generate any anagram for 3 instances.we therefore set the score of the proposed method to 1.0 for them.we can see that the proposed method yielded significantly higher human evaluation scores.

table 9 shows our results.when we compare between crf variants, we notice that the model that does not consider any link across comments perform the worst; see crf (lc-no).a simple linear chain connection between sentences in their temporal order does not improve much (crf (lc-lc)), which indicates that the widely used linear chain crf (lafferty et al., 2001) is not the most appropriate model for capturing conversational dependencies in these conversations.the crf (lc-lc1) is one of the best performing models and perform significantly better than b-lstmp.

table 4 shows the results on the test sets.for wsj, where sentence segmentation is almost trivial, we see only minor drops in las between gold and the systems that use predicted sentence boundaries.for wsj∗ and switchboard the picture is much different.compared to gold, all systems show considerable drops in accuracy which asserts that errors from the sentence boundary detection task propagate to the parser and worsen the parser accuracy.on switchboard the parsers yield significantly better results than marmot.the best result is obtained after reparsing and this is also significantly better than any other system.although there is a slight drop in accuracy between nosyntax and joint, this difference is not significant.here, both joint and joint-reparsed obtain significantly better parsing accuracies than the systems that do not have access to syntax during sentence boundary prediction.although joint-reparsed performs a bit worse, the difference compared to joint is not significant.

table 4 presents the results on the accurat test sets for lexacc and stacc using their respective optimal similarity thresholds.on the 21 test sets, the two systems were tied on two occasions, with stacc obtaining better results in 89.5% of the remaining cases.on the noisiest datasets, stacc was consistently and markedly better across language pairs.

table 4 shows the experimental results, from which we can see that the three-layer ann model is surprisingly effective for event detection, which even yields competitive results compared with nguyen’s cnn and chen’s dmcnn.moreover, our basic model achieves much higher precision than state-of-the-art approaches (79.5% vs. 75.6%).the result shows that randomly initialized word embeddings decrease the f1 score by 7.3 (61.5 vs. 68.8).

table 5 shows the results of manual evaluations.through the comparison of ann and sf, we can see that the application of h1 caused a loss of 5.5 point.moreover, sl obtains a gain of 2.0% improvement compared with ann, which demonstrates that the ”same lu” hypothesis is very useful.finally, with all the hypotheses, the psl-based approach achieves the best performance, which demonstrates that our hypotheses are useful and it is an effective way to jointly utilize them as soft constraints through psl for event detection in fn.

table 2 shows the overall performance of all methods on the ace2005 english corpus.we can see that our approach significantly outperforms all previous methods.(4) rnn and lstm perform slightly worse than bi-lstm.

table 4 presents the performance of our method on the spanish ere corpus.the results show that hnn approach performed better than lstm and bi-lstm.it indicates that our proposed model could achieve the best performance in multiple languages than other neural network methods.

the results are shown in table 5.as table 5 showed, our model observably performs better in detecting review spam for the cold-start task in both hotel and restaurant domains.

table 2 provides the performances of the six baseline models, as well as our proposed models (i.e., rnn and copyrnn).for each method, the table lists its f-measure at top 5 and top 10 predictions on the five datasets.the results show that the four unsupervised models (tf-idf, textrank, singlerank and expandrank) have a robust performance across different datasets.the performance of the two supervised models (i.e., maui and kea) were unstable on some datasets, but maui achieved the best performances on three datasets among all the baseline models.as for our proposed keyphrase prediction approaches, the rnn model with the attention mechanism did not perform as well as we expected.the copyrnn model, by considering more contextual information, significantly outperforms not only the rnn model but also all baselines, exceeding the best baselines by more than 20% on average.

table 4 shows bleu scores on the europarl corpus of our proposed methods.for sentence-level approaches, the sent-beam method outperforms the sent-greedy method by +0.59 bleu points over spanish-french translation and +2.51 bleu points over german-french translation on the test set.for word-level experiments, we observe that the word-sampling method performs much better than the other two methods: +1.94 bleu points on spanish-french translation and +1.88 bleu points on german-french translation over the word-greedy method; +2.65 bleu points on spanish-french translation and +2.84 bleu points on german-french translation over the word-beam method.

on the low resource setting (celex), our hard attention model significantly outperforms both the recent neural models of kann and schutze (2016a) (med) and rastogi et al. (2016) (nwfst) and the morphologically aware latent variable model of dreyer et al. (2008) (lat), as detailed in table 1.in addition, it significantly outperforms our implementation of the soft attention model (soft).

it solves 70% of the them in the time limit of 10 minutes per problem.table 1 shows the rate of successfully solved problems in the manually formalized version of the benchmark problems used in the current paper.

table 1 shows the result of unlabeled dependency accuracy (uas).on the error-free test set (0%), the baseline (ef pipeline) outperforms other eref models; the accuracy is lower when the parser is trained on noisier data.the difference among the models becomes small when the test set has 10% error injection rate.as the rate increases further, the trend of parser accuracy reverses.when the test set has 15% or higher noise, the e20 is the most accurate parser.

table 3 summarizes training performance and model statistics.the transformer base model is the fastest model in terms of training speed.rnmt+ is slower to train than the transformer

table 5 shows that our model with separate decoder and attention over keyphrases produce significantly more informative and relevant arguments than seq2seq trained without evidence.8 .however, we also observe that human judges prefer the retrieved arguments over generation-based models, illustrating the gap between system arguments and human edited text.

our parser achieves an accuracy of 90.35 for eds and 89.51 for dmrs in terms of elementary dependency match (edm) which outperforms the best existing grammar-free model (buys and blunsom, 2017) by a significant margin (see table 1).this marked result affirms the value of modeling the syntacto-semantic composition process for semantic parsing.on sentences that can be parsed by erg-guided parsers, e.g. pet2 or ace3 , significant accuracy gaps between erg-guided parsers and data-driven parsers are repeatedly reported (see table 1).

table 2 shows the performance of multi-class classification.self achieves nearly the same f-score as feng et al (2016)'s hybrid, and outperforms the others.more importantly, self is the only one which obtains a performance higher than 70% for both precision and recall.? similar to the pattern classifiers that are based on hand-designed features, the cnn models enable higher precision to be obtained.however the recall is lower.? the rnn models contribute to achieving a higher recall.however the precision is lower.let us turn to the structurally more complicated models, self and hybrid.self inherits the merits of the rnn models, classifying the events with higher recall.so that it outperforms other rnns, with improvements of no less than 4.5% precision and 1.7% recall.

table 6 shows the comparisons of our results with the performance of several previous models, which were all trained with 1,500k event chains extracted from the nyt portion of the gigaword corpus (graff and cieri 2003).it is encouraging that by using event knowledge extracted from automatically identified narratives, we achieved the best event prediction performance, which is 2.2% higher than the best neural network model.

table 5 shows that for the ne dataset, the two strategies performequally well on the dev set, whereas the subj/obj strategy works slightly better on the test set.for common nouns, subj/obj is better.

table 4 shows that both methods work well and di-vst achieved better homogeneity than di-vae.

table 4 shows the human evaluation results.we can observe that: (1) sc-seq2seqniwf s=1 generates the most informative responses and interesting (labeled as g+2h) and the least general responses than all the baseline models.meanwhile, sc-seq2seqniwf s=0 generates the most general responses (labeled as g+1h); (2) marm generates the most bad responses (labeled as g+0h), which indicates the drawbacks of the unknown latent responding mechanisms; (3) the kappa values of our models are all larger than 0.4, considered as gmoderate agreementh regarding quality of responses.the largest kappa value is achieved by sc-seq2seqniwf s=0, which seems reasonable since it is easy to reach an agreement on general responses.

table 2 shows the results on ace2005 and kbpeval2017.from this table, we can see that:.1) npns steadily outperform all baselines significantly.compared with baselines, npn(taskspecific) gains at least 1.6 (2.5%) and 1.5 (3.0%) f1-score improvements on trigger classification task on ace2005 and kbpeval2017 respectively.as shown in table 2, npn(taskspecific) achieved significant f1-score improvements on trigger identification task on both datasets.comparing with corresponding characterbased methods3 , word-based methods achieved 2 to 3 f1-score improvements, which indicates that words can provide additional information for event detection.

table 4 describes our main results.our weakly-supervised semantic parser with re-ranking (w.+disc) obtains 84.0 accuracy and 65.0 consistency on the public test set and 82.5 accuracy and 63.9 on the hidden one, improving accuracy by 14.7 points compared to state-of-theart.the accuracy of the rule-based parser (rule) is less than 2 points below maxent, showing that a semantic parsing approach is very suitable for this task.the supervised parser obtains better performance (especially in consistency), and with re-ranking reaches 76.6 accuracy, showing that generalizing from generated examples is better than memorizing manually-defined patterns.our weakly-supervised parser significantly improves over sup., reaching an accuracy of 81.7 before reranking, and 84.0 after re-ranking (on the public test set).consistency results show an even crisper trend of improvement across the models.

table 5 shows the results of the human evaluation.bold indicates top scores.we see that the original human questions are preferred over the two nqg ssystemsfoutputs, which is understandable given the examples in figure 3.the humangenerated questions make more sense and correspond better with the provided answers, particularly when they require information in the preceding context.in terms of grammaticality, however, the neural models do quite well, achieving very close to human performance.in addition, we see that our method (corefnqg) performs statistically significantly better across all metrics in comparison to the baseline model (contextnqg), which has access to the entire preceding context in the passage.

the results of our human evaluation study are shown in table 3.as one might imagine, human gets ranked 1st most of the time (41%).however, it is closely followed by xnet which ranked 1st 28% of the time.in comparison, pointernet and lead were mostly ranked at 3rd and 4th places.it showed that xnet is significantly better than lead and pointernet, and it does not differ significantly from human.on the other hand, pointernet does not differ significantly from lead and it differs significantly from both xnet and human.

table 6 shows precision and recall for joint span detection and question generation, using exact match for both.this metric is exceedingly hard, but it shows that almost 40% of predictions are exactly correct in both span and question.

table 2 shows performance of the method compared with other approaches for coherence prediction.in comparison, the hmm based approaches use significant annotation and syntactic features.sequential-cg also outperforms several discriminative approaches for the task.

furthermore, table 3 shows the effectiveness of multi-pass operation in the memory module.it shows that multiple passes operation performs better than one pass, though the improvement is not significant.in table 3, with the increasing number of passes, the f1-score increases.however, when the number of passes is larger than 3, the f1-score stops increasing or even decreases due to over-fitting.

the results are presented in table 2 and show that, out of 3 annotations, over 90% of the captions receive a majority (2+) of good judgments.

table 1 shows the fraction of images that were judged to be good representations of the search word.it also demonstrates that as the concreteness of a word increases, the proportion of good images associated with that word increases as well.overall, 85% of the english images, 72% of french, 66% of indonesian, and 60% of uzbek were judged to be good.

table 3 indicates that our method, denoted by triplet-sen, clearly outperforms the other tested methods.surprizingly, skip-thoughts-sick is inferior to skip-thoughts-cs.

table 3 presents the performance of noise reduction methods.the performances of the hits-based, lsa-based, and nmf-based noise reduction methods are presented in table 3.we also perform experiments with an ensemble method that combines the hits-based and lsa-based strategies to merge rankings from their outputs, with half of the triples coming from the lsa-based method and the other half from the hits-based method.table 3 indicates that our proposed methods improved the performance of all cnn and pcnn models.our ensemble method achieved the best improvements for three out of four systems, except that the hits-based method obtained the best score for cnn+one.

table 1 shows that the alignment framework performs better than either baseline.msfc yields the highest recall and lowest aer with an absolute improvement of 0%, 19%, and 10% for precision, recall and aer, over the 1-second delay baseline.modified k-means achieves higher precision with an absolute improvement of 6%, 14%, and 14%

table 8 shows the f1 scores of depccg in the respective settings.remarkably, we observe huge additive performance improvement.while, in terms of labeled f1, elmo contributes about 4 points on top of the plain depccg, adding the new training set (converted from dependency trees) improves more than 10 points.

to explore the underlying information of each modality, we carry out an experiment to compare the performance among unimodal, bimodal and trimodal models.for unimodal models, we can infer from table 4 that language modality is the most predictive for emotion prediction, outperforming acoustic and visual modalities with significant margin.when coupled with acoustic and visual modalities, the trimodal hffn performs best, whose result is 1% ~ 2% better than the language-hffn, indicting that acoustic and visual modalities actually play auxiliary roles while language is dominant.however, in our model, when conducting outer product, all three modalities are treated equally, which is probably not the optimal choice.

table 2 shows the mention detection results on the test set.similar to coreference linking results, our model achieves higher precision and f1 score, which indicates that our model can significantly reduce false positive mentions while it can still ï¬nd a reasonable number of mentions.

the main results on the one-to-many translation scenario, including one-to-two, one-to-three and one-to-four translation tasks are reported in table 2.we present a typical multi-nmt adopting johnson et al. (2017) method on transformer as our multi-nmt baselines model.obviously, multi-nmt baselines cannot outperform nmt baselines in all cases, among which four directions are comparable and twelve are worse.with respect to our proposed method, it is clear that our compact method consistently outperforms the baseline systems.compared with another strong one-to-many translation model three-stgy proposed by wang et al. (2018), our compact method can achieve better results as well.moreover, our method can perform even better than individually trained systems in most cases (eleven out of sixteen cases).the results demonstrate the effectiveness of our method.

distre with selective attention achieves a new state-of-the-art auc value of 0.422.the precision-recall curve in figure 4 shows that it outperforms reside and pcnn+att at higher recall levels, while precision is lower for top predicted relation instances.the results of the pcnn+att model indicate that its performance is only better in the very beginning of the curve, but its precision drops early and only achieves an auc value of 0.341.similar, reside performs better in the beginning but drops in precision after a recall-level of approximately 0.25.this suggests that our method yields a more balanced overall performance, which we believe is important in many real-world applications.table 1 also shows detailed precision values measured at different points along the p-r curve.we again can observe that while distre has lower precision for the top 500 predicted relation instances, it shows a state-of-the-art precision of 60.2% for the top 1000 and continues to perform higher for the remaining, much larger part of the predictions.

in table 2 we reproduce the results from (smith et al., 2017) using the dictionaries and embeddings provided by (dinu et al., 2014)3 and we compare our method (ibfa) using both the expert dictionaries from (dinu et al., 2014) and the pseudo-dictionaries as constructed in (smith et al., 2017).we significantly outperform both svd and cca, especially when using the pseudo-dictionaries.

in detail, table 4 shows that the consistency score given by humans for ablated versions without adversarial training and memory mechanism decline 0.53 and 0.31, respectively.

table 2 shows that even in such a work mode, our hpsg parser still outperforms the separate constituent parser in terms of either constituent and dependency parsing performance.as lambda is set to 0.5, our hpsg parser will give constituent and dependency structures at the same time, which are shown better than the work alone mode of either constituent or dependency parsing.besides, the comparison also shows that the directly predicted dependencies from our model are slightly better than those converted from the predicted constituent parse trees.

table 5 shows the comparison of averaged slot coherence results over all the slots in  the  schemas.   .the averaged slot coherence of odee-fer is the highest, which is consistent with the conclusion from table 4.   .the averaged slot coherence of odee-f is comparable to that of nguyen et al.(2015)(p= 0.3415),  which again demonstrates that  the  contextual  features  are  a  strong  alterna-tive to discrete features.the scores of odee-fe(p= 0.06) and odee-fer(p= 10^5) are both higher than that of odee-f, which proves that the latent event type is critical in odee.

table 1 shows the performance of various models on en-de and en-fr translation tasks.the bleu score difference between our hard-attention based transformer model and the original soft-attention based transformer model indicates the effectiveness of selecting a few relevant source tokens for each target token.the performance gap between our method and sequence loss based transformer (wu et al., 2018) shows that the improvements are indeed coming from the hard-attention mechanism.our approach of incorporating hard-attention into decoder’s top selfattention layer to select relevant tokens yielded better results compared to the localness selfattention (yang et al., 2018) approach of incorporating localness bias only to lower self-attention layers.it can be noted that our model achieved 29.29 and 42.26 bleu points on en-de and enfr tasks respectively – surpassing the previously published models.

we study the attention relay effects on all six layers.the results in table 5 show that relaying attention on the last layer achieves the best performance.

table 3 reports the results on the test set for both the europarl and the un model in comparison to previous work.9 .our proposed system outperforms all previous methods by a large margin, obtaining improvements of 10-15 f1 points and showing very consistent performance across different languages, including distant ones.

table 1 shows the performance for all the baselines in terms of spearman's rank correlation.we observe that w2v-cbow model produces the best performance across all the three datasets and w2v-sg achieves the second-best performance.as noted in the table, the poincare embeddings on their own perform worse than all the other baselines.

mtl+copy, dnpg and adapted dnpg as well as the reference.table 8 shows the mean rank and inter-annotator agreement (cohen's kappa) of each model.adapted dnpg again significantly outperforms mtl+copy by a large margin (p-value < 0.01).the performance of the original dnpg and mtl+copy has no significant difference (p-value = 0.18).all of the interannotator agreement is regarded as fair or above.

table 2 shows comparison between models before/after co-teaching.we find that co-teaching is still effective when starting from two networks, as both smn and dam get improved on the two data sets.

in particular, our method (hice+morph+maml)6 achieves the best performance among all the other baseline methods under most settings.compared with the current state-of-the-art method, `a la carte, the relative improvements (i.e., the performance difference divided by the baseline performance) of hice are 4.0%, 5.4% and 9.3% in terms of 2,4,6shot learning, respectively.as is shown, when the number of context sentences (k) is relatively large (i.e., k = 6), the performance of hice is on a par with the upper bound (oracle embedding) and the relative performance difference is merely 2.7%.

table 1 shows our segmentation results.using elmo,  our  segmenter  outperforms  all  the  baselines  in  all  three  measures.we  achieve  2.3%-11.9%, 2.4%-11.3% and 2.3%-12.3% relative improvements  inf1,  recall  and  precision,  respectively.   jointly  training  with  the  parser  improves this further (95.55 f1).it is worthwhile to mention that our segmenter’s performance of 95.55 f1 is very close to the human agreement of 98.3 f1.surprisingly, the results with bert were not as good.

the last row of table 2 shows the results when we train the model jointly, and feed the parser with gold edu segmentation during inference.the performance is improved further with joint training, achieving 97.44, 91.34, 81.70 f1 score, in span, nuclearity and relation, respectively.our parser surpasses human agreement in span and nuclearity.

the results are shown in table 4.from the table, we see that using simile performs the best when using bleu and sim as evaluation metrics for  all  four  languages.it  is  interesting  that  using simile in the cost leads to larger bleu improvements than using bleu alone,  the reasons for which we examine further in the following sections. similarly, using bleu as the cost function leads to large gains in sim, though these gains are not as large as when using simile in training.

table 4 presents the ablation study results.performing schema linking (‘+sl’) brings about 8.5% and 6.4% absolute improvement on irnet and irnet(bert).using the memory augmented pointer network (‘+mem’) further improves the performance of irnet and irnet(bert).at last, adopting the coarse-to-fine framework (‘+cf’) can further boost performance.

experimental results in table 5 show that, with only 1 3 of the total dataset, ulmfit and bert perform better than task-specific models, while bcn+elmo shows a comparable result.clearly, this shows that the models have learned significantly during the transfer learning process.

table 2 shows results on the recast mnli and joci datasets.we find that for the two synthetic mnli datasets, margin-loss performs similarly to cross entropy log-loss.shifting to the joci datasets, with less extreme (contradiction / entailed) hypotheses, especially in the adversarial joci2 variant, marginloss outperforms log-loss.

table 3 shows our results on copa.compared with previous state-of-theart knowledge-driven baseline methods, a bert model trained with a log-loss achieves better performance.when training the bert model with a margin-loss instead of a log-loss, our method gets the new state-of-the-art result on the established copa splits, with an accuracy of 75.4%.

table 2 shows results that compare a bert baseline that uses only the cqa inputs and the same architecture but trained using inputs that contain explanations from cos-e during training.the bert baseline model reaches 64% accuracy and adding open-ended human explanations (cos-e-open-ended) alongside the questions during training results in a 2% boost in accuracy.these explanations (cage-reasoning) can be provided during both training and validation and increases the accuracy to 72%.

table 2 summarizes the performances of our system and previous ones.these results demonstrate that our system significantly outperforms the previous methods in nonlocal dependency identification.on the other hand, although post-processing approach can use any parser in pre-processing, our approach outperforms the post-processing approach, even if the pre-processing parser is assumed to always generate gold ptb trees.

table 4 shows the results of 500 simulations for each of the comparison systems.our system with kernel transition obtains the highest success rate, significantly improving over other approaches.the success rate of the base retrieval agent is lower than 10%, which proves that a chitchat agent without a target-guided strategy can hardly accomplish our task.the retrieval-stgy agent has a relatively high success rate, while taking more turns (6.56) to accomplish this.

table 7 shows that sense embeddings using context information perform better than all the existing models, except mssg models (neelakantan et al., 2015).also, computing the embeddings of a word using the contextual information improves results by aprox. 0.025, compared to the case when words embeddings are used directly.

accuracies of 1-best and 5-best translations in table 2 show comparable word translation quality to previous work, although we do not employ any task specific steps in contrast to braune et al. (2018).

table 1 shows the automatic evaluation results of three sentiment analyzers.we find that: (1) the rule-based method rb performs the best.(2) da can not improve the performance of sentiment analysis in our task compared to rm.

finally, table 4 shows the f1 score on the test set for distractor setting and full wiki setting on the leaderboard.decomprc achieves the best result out of models that report both distractor and full wiki setting.

table 1 shows the performance of different models in the private test set of hotpotqa.from the table we can see that our model achieves the second best result on the leaderboard now3 (on march 1st).besides, the answer performance and the joint performance of our model are competitive against state-of-the-art unpublished models.we also include the result of our model with a revised entity graph whose entities are recognized by a bert ner model (devlin et al., 2018).the results show that our model achieves a 1.5% gain in the joint f1-score with the entity graph built from a better entity recognizer.

